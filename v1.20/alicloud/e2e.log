I0324 02:26:43.423739      21 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-543228437
I0324 02:26:43.423758      21 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0324 02:26:43.423837      21 e2e.go:129] Starting e2e run "0e7ab19b-68f5-4fce-a9f4-03e42eb6f230" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1616552802 - Will randomize all specs
Will run 311 of 5667 specs

Mar 24 02:26:43.433: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:26:43.435: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 24 02:26:43.456: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 24 02:26:43.491: INFO: 38 / 38 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 24 02:26:43.491: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Mar 24 02:26:43.491: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 24 02:26:43.498: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cloud-controller-manager' (0 seconds elapsed)
Mar 24 02:26:43.498: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'csi-plugin' (0 seconds elapsed)
Mar 24 02:26:43.498: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Mar 24 02:26:43.498: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-master' (0 seconds elapsed)
Mar 24 02:26:43.498: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-worker' (0 seconds elapsed)
Mar 24 02:26:43.498: INFO: e2e test version: v1.20.4
Mar 24 02:26:43.498: INFO: kube-apiserver version: v1.20.4-aliyun.1
Mar 24 02:26:43.498: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:26:43.502: INFO: Cluster IP family: ipv4
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:26:43.502: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
Mar 24 02:26:43.528: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Mar 24 02:26:43.534: INFO: PSP annotation exists on dry run pod: "ack.privileged"; assuming PodSecurityPolicy is enabled
Mar 24 02:26:43.539: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3633
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Mar 24 02:26:48.189: INFO: Successfully updated pod "labelsupdate5f22a7b5-f620-46fb-b01d-9b2ffc8bd4ed"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:26:50.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3633" for this suite.

â€¢ [SLOW TEST:6.708 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":1,"skipped":2,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:26:50.210: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-899
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Mar 24 02:26:50.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-899 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 24 02:26:50.532: INFO: stderr: ""
Mar 24 02:26:50.532: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Mar 24 02:26:50.532: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 24 02:26:50.532: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-899" to be "running and ready, or succeeded"
Mar 24 02:26:50.538: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.281059ms
Mar 24 02:26:52.543: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.010470559s
Mar 24 02:26:52.543: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 24 02:26:52.543: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar 24 02:26:52.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-899 logs logs-generator logs-generator'
Mar 24 02:26:52.609: INFO: stderr: ""
Mar 24 02:26:52.609: INFO: stdout: "I0324 02:26:51.424099       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/qt86 549\nI0324 02:26:51.624134       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/ntl 385\nI0324 02:26:51.824129       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/9gjc 302\nI0324 02:26:52.024139       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/b4l 340\nI0324 02:26:52.224135       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/rnn4 467\nI0324 02:26:52.424129       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/mpk7 403\n"
STEP: limiting log lines
Mar 24 02:26:52.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-899 logs logs-generator logs-generator --tail=1'
Mar 24 02:26:52.671: INFO: stderr: ""
Mar 24 02:26:52.671: INFO: stdout: "I0324 02:26:52.624124       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/f6s 312\n"
Mar 24 02:26:52.671: INFO: got output "I0324 02:26:52.624124       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/f6s 312\n"
STEP: limiting log bytes
Mar 24 02:26:52.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-899 logs logs-generator logs-generator --limit-bytes=1'
Mar 24 02:26:52.736: INFO: stderr: ""
Mar 24 02:26:52.737: INFO: stdout: "I"
Mar 24 02:26:52.737: INFO: got output "I"
STEP: exposing timestamps
Mar 24 02:26:52.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-899 logs logs-generator logs-generator --tail=1 --timestamps'
Mar 24 02:26:52.797: INFO: stderr: ""
Mar 24 02:26:52.797: INFO: stdout: "2021-03-24T02:26:52.624205327Z I0324 02:26:52.624124       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/f6s 312\n"
Mar 24 02:26:52.797: INFO: got output "2021-03-24T02:26:52.624205327Z I0324 02:26:52.624124       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/f6s 312\n"
STEP: restricting to a time range
Mar 24 02:26:55.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-899 logs logs-generator logs-generator --since=1s'
Mar 24 02:26:55.363: INFO: stderr: ""
Mar 24 02:26:55.363: INFO: stdout: "I0324 02:26:54.424139       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/972 585\nI0324 02:26:54.624130       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/2lmh 265\nI0324 02:26:54.824130       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/kpg 204\nI0324 02:26:55.024137       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/6pkr 219\nI0324 02:26:55.224140       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/8vh 477\n"
Mar 24 02:26:55.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-899 logs logs-generator logs-generator --since=24h'
Mar 24 02:26:55.424: INFO: stderr: ""
Mar 24 02:26:55.424: INFO: stdout: "I0324 02:26:51.424099       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/qt86 549\nI0324 02:26:51.624134       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/ntl 385\nI0324 02:26:51.824129       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/9gjc 302\nI0324 02:26:52.024139       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/b4l 340\nI0324 02:26:52.224135       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/rnn4 467\nI0324 02:26:52.424129       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/mpk7 403\nI0324 02:26:52.624124       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/f6s 312\nI0324 02:26:52.824121       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/q6p 458\nI0324 02:26:53.024121       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/qtb2 207\nI0324 02:26:53.224127       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/gc9n 413\nI0324 02:26:53.424143       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/bkp 362\nI0324 02:26:53.624171       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/nhf 270\nI0324 02:26:53.824122       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/ctrx 260\nI0324 02:26:54.024123       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/xdfm 343\nI0324 02:26:54.224128       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/xd54 487\nI0324 02:26:54.424139       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/972 585\nI0324 02:26:54.624130       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/2lmh 265\nI0324 02:26:54.824130       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/kpg 204\nI0324 02:26:55.024137       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/6pkr 219\nI0324 02:26:55.224140       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/8vh 477\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Mar 24 02:26:55.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-899 delete pod logs-generator'
Mar 24 02:27:02.257: INFO: stderr: ""
Mar 24 02:27:02.257: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:27:02.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-899" for this suite.

â€¢ [SLOW TEST:12.061 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":2,"skipped":5,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:27:02.271: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5072
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:27:02.399: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar 24 02:27:04.423: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:27:05.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5072" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":3,"skipped":6,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:27:05.438: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 24 02:27:09.594: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-846 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 02:27:09.594: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:27:09.707: INFO: Exec stderr: ""
Mar 24 02:27:09.707: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-846 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 02:27:09.707: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:27:09.816: INFO: Exec stderr: ""
Mar 24 02:27:09.816: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-846 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 02:27:09.816: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:27:09.905: INFO: Exec stderr: ""
Mar 24 02:27:09.905: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-846 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 02:27:09.905: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:27:10.005: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 24 02:27:10.005: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-846 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 02:27:10.005: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:27:10.102: INFO: Exec stderr: ""
Mar 24 02:27:10.102: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-846 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 02:27:10.102: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:27:10.209: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 24 02:27:10.209: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-846 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 02:27:10.209: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:27:10.304: INFO: Exec stderr: ""
Mar 24 02:27:10.304: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-846 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 02:27:10.304: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:27:10.410: INFO: Exec stderr: ""
Mar 24 02:27:10.410: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-846 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 02:27:10.410: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:27:10.511: INFO: Exec stderr: ""
Mar 24 02:27:10.511: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-846 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 02:27:10.511: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:27:10.633: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:27:10.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-846" for this suite.

â€¢ [SLOW TEST:5.203 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":4,"skipped":33,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:27:10.641: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7714
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Mar 24 02:27:10.776: INFO: created test-pod-1
Mar 24 02:27:10.779: INFO: created test-pod-2
Mar 24 02:27:10.784: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:27:10.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7714" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":5,"skipped":53,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:27:10.813: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5403
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 02:27:11.304: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 02:27:14.319: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:27:14.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5403" for this suite.
STEP: Destroying namespace "webhook-5403-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":6,"skipped":61,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:27:14.475: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1408
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:27:14.608: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-a6d80efb-6169-4023-9b0f-4d9581f02b50" in namespace "security-context-test-1408" to be "Succeeded or Failed"
Mar 24 02:27:14.610: INFO: Pod "alpine-nnp-false-a6d80efb-6169-4023-9b0f-4d9581f02b50": Phase="Pending", Reason="", readiness=false. Elapsed: 1.61027ms
Mar 24 02:27:16.613: INFO: Pod "alpine-nnp-false-a6d80efb-6169-4023-9b0f-4d9581f02b50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004550598s
Mar 24 02:27:16.613: INFO: Pod "alpine-nnp-false-a6d80efb-6169-4023-9b0f-4d9581f02b50" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:27:16.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1408" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":7,"skipped":92,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:27:16.625: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1187
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 24 02:27:16.757: INFO: Waiting up to 5m0s for pod "pod-c0e3f426-723b-4cce-bfba-0fc4f402be5c" in namespace "emptydir-1187" to be "Succeeded or Failed"
Mar 24 02:27:16.759: INFO: Pod "pod-c0e3f426-723b-4cce-bfba-0fc4f402be5c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.839384ms
Mar 24 02:27:18.764: INFO: Pod "pod-c0e3f426-723b-4cce-bfba-0fc4f402be5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007036066s
STEP: Saw pod success
Mar 24 02:27:18.764: INFO: Pod "pod-c0e3f426-723b-4cce-bfba-0fc4f402be5c" satisfied condition "Succeeded or Failed"
Mar 24 02:27:18.766: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod pod-c0e3f426-723b-4cce-bfba-0fc4f402be5c container test-container: <nil>
STEP: delete the pod
Mar 24 02:27:18.781: INFO: Waiting for pod pod-c0e3f426-723b-4cce-bfba-0fc4f402be5c to disappear
Mar 24 02:27:18.782: INFO: Pod pod-c0e3f426-723b-4cce-bfba-0fc4f402be5c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:27:18.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1187" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":8,"skipped":104,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:27:18.788: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-9374
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:27:18.914: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Creating first CR 
Mar 24 02:27:24.461: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-24T02:27:24Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-24T02:27:24Z]] name:name1 resourceVersion:777853 uid:e3d63fac-9a80-4e76-9082-ad149cfc9ed2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar 24 02:27:34.476: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-24T02:27:34Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-24T02:27:34Z]] name:name2 resourceVersion:777899 uid:1ee03f72-fbe2-4606-970e-7a83267a020c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar 24 02:27:44.492: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-24T02:27:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-24T02:27:44Z]] name:name1 resourceVersion:777938 uid:e3d63fac-9a80-4e76-9082-ad149cfc9ed2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar 24 02:27:54.507: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-24T02:27:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-24T02:27:54Z]] name:name2 resourceVersion:777976 uid:1ee03f72-fbe2-4606-970e-7a83267a020c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar 24 02:28:04.524: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-24T02:27:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-24T02:27:44Z]] name:name1 resourceVersion:778014 uid:e3d63fac-9a80-4e76-9082-ad149cfc9ed2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar 24 02:28:14.541: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-24T02:27:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-24T02:27:54Z]] name:name2 resourceVersion:778053 uid:1ee03f72-fbe2-4606-970e-7a83267a020c] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:28:25.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9374" for this suite.

â€¢ [SLOW TEST:66.283 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":9,"skipped":118,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:28:25.072: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3809
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-3809
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 24 02:28:25.198: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 24 02:28:25.221: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 24 02:28:27.225: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 02:28:29.226: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 02:28:31.227: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 02:28:33.227: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 02:28:35.227: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 02:28:37.225: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 02:28:39.227: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 02:28:41.227: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 24 02:28:41.231: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 24 02:28:41.234: INFO: The status of Pod netserver-2 is Running (Ready = false)
Mar 24 02:28:43.240: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 24 02:28:45.256: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 24 02:28:45.256: INFO: Breadth first check of 10.43.0.205 on host 192.168.0.13...
Mar 24 02:28:45.257: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.43.1.102:9080/dial?request=hostname&protocol=http&host=10.43.0.205&port=8080&tries=1'] Namespace:pod-network-test-3809 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 02:28:45.257: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:28:45.349: INFO: Waiting for responses: map[]
Mar 24 02:28:45.349: INFO: reached 10.43.0.205 after 0/1 tries
Mar 24 02:28:45.349: INFO: Breadth first check of 10.43.1.101 on host 192.168.0.14...
Mar 24 02:28:45.352: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.43.1.102:9080/dial?request=hostname&protocol=http&host=10.43.1.101&port=8080&tries=1'] Namespace:pod-network-test-3809 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 02:28:45.352: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:28:45.451: INFO: Waiting for responses: map[]
Mar 24 02:28:45.451: INFO: reached 10.43.1.101 after 0/1 tries
Mar 24 02:28:45.451: INFO: Breadth first check of 10.43.1.8 on host 192.168.0.15...
Mar 24 02:28:45.454: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.43.1.102:9080/dial?request=hostname&protocol=http&host=10.43.1.8&port=8080&tries=1'] Namespace:pod-network-test-3809 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 02:28:45.454: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:28:45.553: INFO: Waiting for responses: map[]
Mar 24 02:28:45.553: INFO: reached 10.43.1.8 after 0/1 tries
Mar 24 02:28:45.553: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:28:45.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3809" for this suite.

â€¢ [SLOW TEST:20.491 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":10,"skipped":131,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:28:45.563: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5351
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar 24 02:28:45.711: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5351  b4ee2c00-27ec-4a12-9cc4-70a7efa4dbdc 778232 0 2021-03-24 02:28:45 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-03-24 02:28:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 02:28:45.711: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5351  b4ee2c00-27ec-4a12-9cc4-70a7efa4dbdc 778233 0 2021-03-24 02:28:45 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-03-24 02:28:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:28:45.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5351" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":11,"skipped":156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:28:45.718: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-353
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 24 02:28:45.859: INFO: starting watch
STEP: patching
STEP: updating
Mar 24 02:28:45.864: INFO: waiting for watch events with expected annotations
Mar 24 02:28:45.864: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:28:45.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-353" for this suite.
â€¢{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":12,"skipped":183,"failed":0}
SS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:28:45.888: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4163
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:28:46.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4163" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
â€¢{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":13,"skipped":185,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:28:46.024: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar 24 02:28:46.151: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 24 02:28:46.156: INFO: Waiting for terminating namespaces to be deleted...
Mar 24 02:28:46.158: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.13 before test
Mar 24 02:28:46.163: INFO: coredns-58d46886cf-fl2z7 from kube-system started at 2021-03-23 11:44:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:46.163: INFO: 	Container coredns ready: true, restart count 0
Mar 24 02:28:46.163: INFO: csi-plugin-78sns from kube-system started at 2021-03-22 03:28:49 +0000 UTC (4 container statuses recorded)
Mar 24 02:28:46.163: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 24 02:28:46.163: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 24 02:28:46.163: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 24 02:28:46.163: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 24 02:28:46.163: INFO: csi-provisioner-57c8d966fb-spnmk from kube-system started at 2021-03-22 03:29:32 +0000 UTC (7 container statuses recorded)
Mar 24 02:28:46.163: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 24 02:28:46.163: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
Mar 24 02:28:46.163: INFO: 	Container external-disk-attacher ready: true, restart count 0
Mar 24 02:28:46.163: INFO: 	Container external-disk-provisioner ready: true, restart count 0
Mar 24 02:28:46.163: INFO: 	Container external-disk-resizer ready: true, restart count 0
Mar 24 02:28:46.163: INFO: 	Container external-nas-provisioner ready: true, restart count 0
Mar 24 02:28:46.163: INFO: 	Container external-snapshot-controller ready: true, restart count 0
Mar 24 02:28:46.163: INFO: kube-flannel-ds-77s6x from kube-system started at 2021-03-22 08:51:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:46.163: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 02:28:46.163: INFO: kube-proxy-worker-kfc4c from kube-system started at 2021-03-22 03:28:49 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:46.163: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 24 02:28:46.163: INFO: metrics-server-7d6f974b9f-tglpj from kube-system started at 2021-03-22 03:29:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:46.163: INFO: 	Container metrics-server ready: true, restart count 2
Mar 24 02:28:46.163: INFO: nginx-ingress-controller-67bc64c7-5tcjs from kube-system started at 2021-03-23 11:44:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:46.163: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 24 02:28:46.163: INFO: netserver-0 from pod-network-test-3809 started at 2021-03-24 02:28:25 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:46.163: INFO: 	Container webserver ready: true, restart count 0
Mar 24 02:28:46.163: INFO: sonobuoy-systemd-logs-daemon-set-46debedd236a484a-5dl8c from sonobuoy started at 2021-03-24 02:26:37 +0000 UTC (2 container statuses recorded)
Mar 24 02:28:46.163: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 24 02:28:46.163: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 24 02:28:46.163: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.14 before test
Mar 24 02:28:46.173: INFO: csi-plugin-sdgx9 from kube-system started at 2021-03-22 03:28:49 +0000 UTC (4 container statuses recorded)
Mar 24 02:28:46.173: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 24 02:28:46.173: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 24 02:28:46.173: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 24 02:28:46.173: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 24 02:28:46.173: INFO: kube-flannel-ds-bqvbv from kube-system started at 2021-03-22 08:51:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:46.173: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 02:28:46.173: INFO: kube-proxy-worker-46686 from kube-system started at 2021-03-22 03:28:49 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:46.173: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 24 02:28:46.173: INFO: netserver-1 from pod-network-test-3809 started at 2021-03-24 02:28:25 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:46.173: INFO: 	Container webserver ready: true, restart count 0
Mar 24 02:28:46.173: INFO: test-container-pod from pod-network-test-3809 started at 2021-03-24 02:28:43 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:46.173: INFO: 	Container webserver ready: true, restart count 0
Mar 24 02:28:46.173: INFO: sonobuoy from sonobuoy started at 2021-03-24 02:26:36 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:46.173: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 24 02:28:46.173: INFO: sonobuoy-systemd-logs-daemon-set-46debedd236a484a-l98ph from sonobuoy started at 2021-03-24 02:26:37 +0000 UTC (2 container statuses recorded)
Mar 24 02:28:46.173: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 24 02:28:46.173: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 24 02:28:46.173: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.15 before test
Mar 24 02:28:46.178: INFO: coredns-58d46886cf-h7vgt from kube-system started at 2021-03-22 03:29:29 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:46.178: INFO: 	Container coredns ready: true, restart count 23
Mar 24 02:28:46.178: INFO: csi-plugin-v7dd9 from kube-system started at 2021-03-22 03:28:49 +0000 UTC (4 container statuses recorded)
Mar 24 02:28:46.178: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 24 02:28:46.178: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 24 02:28:46.178: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 24 02:28:46.178: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 24 02:28:46.178: INFO: kube-flannel-ds-bswlq from kube-system started at 2021-03-22 08:51:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:46.178: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 02:28:46.178: INFO: kube-proxy-worker-xvh5g from kube-system started at 2021-03-22 03:28:49 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:46.178: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 24 02:28:46.178: INFO: nginx-ingress-controller-67bc64c7-zm4r9 from kube-system started at 2021-03-22 11:02:12 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:46.178: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 24 02:28:46.178: INFO: netserver-2 from pod-network-test-3809 started at 2021-03-24 02:28:25 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:46.178: INFO: 	Container webserver ready: true, restart count 0
Mar 24 02:28:46.178: INFO: sonobuoy-systemd-logs-daemon-set-46debedd236a484a-sb56k from sonobuoy started at 2021-03-24 02:26:37 +0000 UTC (2 container statuses recorded)
Mar 24 02:28:46.178: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 24 02:28:46.178: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a3fe25ca-9fa7-481c-a143-d5dc5b210eec 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-a3fe25ca-9fa7-481c-a143-d5dc5b210eec off the node cn-hongkong.192.168.0.15
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a3fe25ca-9fa7-481c-a143-d5dc5b210eec
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:28:50.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7749" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":14,"skipped":191,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:28:50.238: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1525
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar 24 02:28:50.365: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 24 02:28:50.370: INFO: Waiting for terminating namespaces to be deleted...
Mar 24 02:28:50.372: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.13 before test
Mar 24 02:28:50.377: INFO: coredns-58d46886cf-fl2z7 from kube-system started at 2021-03-23 11:44:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:50.377: INFO: 	Container coredns ready: true, restart count 0
Mar 24 02:28:50.377: INFO: csi-plugin-78sns from kube-system started at 2021-03-22 03:28:49 +0000 UTC (4 container statuses recorded)
Mar 24 02:28:50.377: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 24 02:28:50.377: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 24 02:28:50.377: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 24 02:28:50.377: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 24 02:28:50.377: INFO: csi-provisioner-57c8d966fb-spnmk from kube-system started at 2021-03-22 03:29:32 +0000 UTC (7 container statuses recorded)
Mar 24 02:28:50.377: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 24 02:28:50.377: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
Mar 24 02:28:50.377: INFO: 	Container external-disk-attacher ready: true, restart count 0
Mar 24 02:28:50.377: INFO: 	Container external-disk-provisioner ready: true, restart count 0
Mar 24 02:28:50.377: INFO: 	Container external-disk-resizer ready: true, restart count 0
Mar 24 02:28:50.377: INFO: 	Container external-nas-provisioner ready: true, restart count 0
Mar 24 02:28:50.377: INFO: 	Container external-snapshot-controller ready: true, restart count 0
Mar 24 02:28:50.377: INFO: kube-flannel-ds-77s6x from kube-system started at 2021-03-22 08:51:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:50.377: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 02:28:50.377: INFO: kube-proxy-worker-kfc4c from kube-system started at 2021-03-22 03:28:49 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:50.377: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 24 02:28:50.377: INFO: metrics-server-7d6f974b9f-tglpj from kube-system started at 2021-03-22 03:29:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:50.377: INFO: 	Container metrics-server ready: true, restart count 2
Mar 24 02:28:50.377: INFO: nginx-ingress-controller-67bc64c7-5tcjs from kube-system started at 2021-03-23 11:44:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:50.377: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 24 02:28:50.377: INFO: netserver-0 from pod-network-test-3809 started at 2021-03-24 02:28:25 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:50.377: INFO: 	Container webserver ready: true, restart count 0
Mar 24 02:28:50.377: INFO: sonobuoy-systemd-logs-daemon-set-46debedd236a484a-5dl8c from sonobuoy started at 2021-03-24 02:26:37 +0000 UTC (2 container statuses recorded)
Mar 24 02:28:50.377: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 24 02:28:50.377: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 24 02:28:50.377: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.14 before test
Mar 24 02:28:50.382: INFO: csi-plugin-sdgx9 from kube-system started at 2021-03-22 03:28:49 +0000 UTC (4 container statuses recorded)
Mar 24 02:28:50.382: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 24 02:28:50.382: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 24 02:28:50.382: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 24 02:28:50.382: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 24 02:28:50.382: INFO: kube-flannel-ds-bqvbv from kube-system started at 2021-03-22 08:51:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:50.382: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 02:28:50.382: INFO: kube-proxy-worker-46686 from kube-system started at 2021-03-22 03:28:49 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:50.382: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 24 02:28:50.382: INFO: netserver-1 from pod-network-test-3809 started at 2021-03-24 02:28:25 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:50.382: INFO: 	Container webserver ready: true, restart count 0
Mar 24 02:28:50.382: INFO: test-container-pod from pod-network-test-3809 started at 2021-03-24 02:28:43 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:50.382: INFO: 	Container webserver ready: true, restart count 0
Mar 24 02:28:50.382: INFO: sonobuoy from sonobuoy started at 2021-03-24 02:26:36 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:50.382: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 24 02:28:50.382: INFO: sonobuoy-systemd-logs-daemon-set-46debedd236a484a-l98ph from sonobuoy started at 2021-03-24 02:26:37 +0000 UTC (2 container statuses recorded)
Mar 24 02:28:50.382: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 24 02:28:50.382: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 24 02:28:50.382: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.15 before test
Mar 24 02:28:50.386: INFO: coredns-58d46886cf-h7vgt from kube-system started at 2021-03-22 03:29:29 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:50.386: INFO: 	Container coredns ready: true, restart count 23
Mar 24 02:28:50.386: INFO: csi-plugin-v7dd9 from kube-system started at 2021-03-22 03:28:49 +0000 UTC (4 container statuses recorded)
Mar 24 02:28:50.386: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 24 02:28:50.386: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 24 02:28:50.386: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 24 02:28:50.386: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 24 02:28:50.386: INFO: kube-flannel-ds-bswlq from kube-system started at 2021-03-22 08:51:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:50.386: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 02:28:50.386: INFO: kube-proxy-worker-xvh5g from kube-system started at 2021-03-22 03:28:49 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:50.386: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 24 02:28:50.386: INFO: nginx-ingress-controller-67bc64c7-zm4r9 from kube-system started at 2021-03-22 11:02:12 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:50.386: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 24 02:28:50.386: INFO: netserver-2 from pod-network-test-3809 started at 2021-03-24 02:28:25 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:50.386: INFO: 	Container webserver ready: true, restart count 0
Mar 24 02:28:50.386: INFO: with-labels from sched-pred-7749 started at 2021-03-24 02:28:48 +0000 UTC (1 container statuses recorded)
Mar 24 02:28:50.386: INFO: 	Container with-labels ready: true, restart count 0
Mar 24 02:28:50.386: INFO: sonobuoy-systemd-logs-daemon-set-46debedd236a484a-sb56k from sonobuoy started at 2021-03-24 02:26:37 +0000 UTC (2 container statuses recorded)
Mar 24 02:28:50.386: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 24 02:28:50.386: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node cn-hongkong.192.168.0.13
STEP: verifying the node has the label node cn-hongkong.192.168.0.14
STEP: verifying the node has the label node cn-hongkong.192.168.0.15
Mar 24 02:28:50.433: INFO: Pod coredns-58d46886cf-fl2z7 requesting resource cpu=100m on Node cn-hongkong.192.168.0.13
Mar 24 02:28:50.433: INFO: Pod coredns-58d46886cf-h7vgt requesting resource cpu=100m on Node cn-hongkong.192.168.0.15
Mar 24 02:28:50.433: INFO: Pod csi-plugin-78sns requesting resource cpu=100m on Node cn-hongkong.192.168.0.13
Mar 24 02:28:50.433: INFO: Pod csi-plugin-sdgx9 requesting resource cpu=100m on Node cn-hongkong.192.168.0.14
Mar 24 02:28:50.433: INFO: Pod csi-plugin-v7dd9 requesting resource cpu=100m on Node cn-hongkong.192.168.0.15
Mar 24 02:28:50.433: INFO: Pod csi-provisioner-57c8d966fb-spnmk requesting resource cpu=100m on Node cn-hongkong.192.168.0.13
Mar 24 02:28:50.433: INFO: Pod kube-flannel-ds-77s6x requesting resource cpu=100m on Node cn-hongkong.192.168.0.13
Mar 24 02:28:50.433: INFO: Pod kube-flannel-ds-bqvbv requesting resource cpu=100m on Node cn-hongkong.192.168.0.14
Mar 24 02:28:50.433: INFO: Pod kube-flannel-ds-bswlq requesting resource cpu=100m on Node cn-hongkong.192.168.0.15
Mar 24 02:28:50.433: INFO: Pod kube-proxy-worker-46686 requesting resource cpu=0m on Node cn-hongkong.192.168.0.14
Mar 24 02:28:50.433: INFO: Pod kube-proxy-worker-kfc4c requesting resource cpu=0m on Node cn-hongkong.192.168.0.13
Mar 24 02:28:50.433: INFO: Pod kube-proxy-worker-xvh5g requesting resource cpu=0m on Node cn-hongkong.192.168.0.15
Mar 24 02:28:50.433: INFO: Pod metrics-server-7d6f974b9f-tglpj requesting resource cpu=100m on Node cn-hongkong.192.168.0.13
Mar 24 02:28:50.433: INFO: Pod nginx-ingress-controller-67bc64c7-5tcjs requesting resource cpu=100m on Node cn-hongkong.192.168.0.13
Mar 24 02:28:50.433: INFO: Pod nginx-ingress-controller-67bc64c7-zm4r9 requesting resource cpu=100m on Node cn-hongkong.192.168.0.15
Mar 24 02:28:50.433: INFO: Pod netserver-0 requesting resource cpu=0m on Node cn-hongkong.192.168.0.13
Mar 24 02:28:50.433: INFO: Pod netserver-1 requesting resource cpu=0m on Node cn-hongkong.192.168.0.14
Mar 24 02:28:50.433: INFO: Pod netserver-2 requesting resource cpu=0m on Node cn-hongkong.192.168.0.15
Mar 24 02:28:50.433: INFO: Pod test-container-pod requesting resource cpu=0m on Node cn-hongkong.192.168.0.14
Mar 24 02:28:50.433: INFO: Pod with-labels requesting resource cpu=0m on Node cn-hongkong.192.168.0.15
Mar 24 02:28:50.433: INFO: Pod sonobuoy requesting resource cpu=0m on Node cn-hongkong.192.168.0.14
Mar 24 02:28:50.433: INFO: Pod sonobuoy-systemd-logs-daemon-set-46debedd236a484a-5dl8c requesting resource cpu=0m on Node cn-hongkong.192.168.0.13
Mar 24 02:28:50.433: INFO: Pod sonobuoy-systemd-logs-daemon-set-46debedd236a484a-l98ph requesting resource cpu=0m on Node cn-hongkong.192.168.0.14
Mar 24 02:28:50.433: INFO: Pod sonobuoy-systemd-logs-daemon-set-46debedd236a484a-sb56k requesting resource cpu=0m on Node cn-hongkong.192.168.0.15
STEP: Starting Pods to consume most of the cluster CPU.
Mar 24 02:28:50.433: INFO: Creating a pod which consumes cpu=2380m on Node cn-hongkong.192.168.0.13
Mar 24 02:28:50.438: INFO: Creating a pod which consumes cpu=2660m on Node cn-hongkong.192.168.0.14
Mar 24 02:28:50.444: INFO: Creating a pod which consumes cpu=2520m on Node cn-hongkong.192.168.0.15
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e1d547b-713e-43ee-a652-4cb282e4e4dd.166f26534bff5a0a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1525/filler-pod-4e1d547b-713e-43ee-a652-4cb282e4e4dd to cn-hongkong.192.168.0.14]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e1d547b-713e-43ee-a652-4cb282e4e4dd.166f26536f1cb387], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e1d547b-713e-43ee-a652-4cb282e4e4dd.166f2653711aba49], Reason = [Created], Message = [Created container filler-pod-4e1d547b-713e-43ee-a652-4cb282e4e4dd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4e1d547b-713e-43ee-a652-4cb282e4e4dd.166f26537651ca4e], Reason = [Started], Message = [Started container filler-pod-4e1d547b-713e-43ee-a652-4cb282e4e4dd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-857ca720-fa2e-4dd5-8a7a-288b41bb529c.166f26534bc7ed23], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1525/filler-pod-857ca720-fa2e-4dd5-8a7a-288b41bb529c to cn-hongkong.192.168.0.13]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-857ca720-fa2e-4dd5-8a7a-288b41bb529c.166f26536cee9a3a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-857ca720-fa2e-4dd5-8a7a-288b41bb529c.166f26536ed99cec], Reason = [Created], Message = [Created container filler-pod-857ca720-fa2e-4dd5-8a7a-288b41bb529c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-857ca720-fa2e-4dd5-8a7a-288b41bb529c.166f265376171360], Reason = [Started], Message = [Started container filler-pod-857ca720-fa2e-4dd5-8a7a-288b41bb529c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-96f7234c-9c82-46f3-9687-08151f582cb0.166f26534c50f2d1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1525/filler-pod-96f7234c-9c82-46f3-9687-08151f582cb0 to cn-hongkong.192.168.0.15]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-96f7234c-9c82-46f3-9687-08151f582cb0.166f26536de8c3c5], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-96f7234c-9c82-46f3-9687-08151f582cb0.166f26536fb77017], Reason = [Created], Message = [Created container filler-pod-96f7234c-9c82-46f3-9687-08151f582cb0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-96f7234c-9c82-46f3-9687-08151f582cb0.166f26537509f9d0], Reason = [Started], Message = [Started container filler-pod-96f7234c-9c82-46f3-9687-08151f582cb0]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.166f2653c47a07a4], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.166f2653c4db0a99], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: removing the label node off the node cn-hongkong.192.168.0.13
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node cn-hongkong.192.168.0.14
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node cn-hongkong.192.168.0.15
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:28:53.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1525" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":15,"skipped":207,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:28:53.508: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4975
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:28:59.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4975" for this suite.

â€¢ [SLOW TEST:6.142 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":16,"skipped":222,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:28:59.650: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2768
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-a363dfd2-9189-43c2-b7f6-cd81504b5438
STEP: Creating a pod to test consume secrets
Mar 24 02:28:59.789: INFO: Waiting up to 5m0s for pod "pod-secrets-b26b753a-58d6-4dad-aced-8f344dc65a8e" in namespace "secrets-2768" to be "Succeeded or Failed"
Mar 24 02:28:59.791: INFO: Pod "pod-secrets-b26b753a-58d6-4dad-aced-8f344dc65a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.852822ms
Mar 24 02:29:01.796: INFO: Pod "pod-secrets-b26b753a-58d6-4dad-aced-8f344dc65a8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006977934s
STEP: Saw pod success
Mar 24 02:29:01.796: INFO: Pod "pod-secrets-b26b753a-58d6-4dad-aced-8f344dc65a8e" satisfied condition "Succeeded or Failed"
Mar 24 02:29:01.798: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-secrets-b26b753a-58d6-4dad-aced-8f344dc65a8e container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 02:29:01.816: INFO: Waiting for pod pod-secrets-b26b753a-58d6-4dad-aced-8f344dc65a8e to disappear
Mar 24 02:29:01.818: INFO: Pod pod-secrets-b26b753a-58d6-4dad-aced-8f344dc65a8e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:29:01.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2768" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":17,"skipped":229,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:29:01.828: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1685
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:29:13.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1685" for this suite.

â€¢ [SLOW TEST:11.183 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":18,"skipped":270,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:29:13.012: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-9456
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:29:13.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9456" for this suite.
â€¢{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":19,"skipped":304,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:29:13.163: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4585
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:29:41.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4585" for this suite.

â€¢ [SLOW TEST:28.178 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":20,"skipped":320,"failed":0}
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:29:41.341: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4258
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4258.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4258.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4258.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4258.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4258.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4258.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 02:29:43.493: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481: the server could not find the requested resource (get pods dns-test-83c82576-e671-486a-8381-72b1864d3481)
Mar 24 02:29:43.495: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481: the server could not find the requested resource (get pods dns-test-83c82576-e671-486a-8381-72b1864d3481)
Mar 24 02:29:43.501: INFO: Unable to read jessie_udp@PodARecord from pod dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481: the server could not find the requested resource (get pods dns-test-83c82576-e671-486a-8381-72b1864d3481)
Mar 24 02:29:43.503: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481: the server could not find the requested resource (get pods dns-test-83c82576-e671-486a-8381-72b1864d3481)
Mar 24 02:29:43.503: INFO: Lookups using dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 02:29:48.511: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481: the server could not find the requested resource (get pods dns-test-83c82576-e671-486a-8381-72b1864d3481)
Mar 24 02:29:48.513: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481: the server could not find the requested resource (get pods dns-test-83c82576-e671-486a-8381-72b1864d3481)
Mar 24 02:29:48.519: INFO: Unable to read jessie_udp@PodARecord from pod dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481: the server could not find the requested resource (get pods dns-test-83c82576-e671-486a-8381-72b1864d3481)
Mar 24 02:29:48.520: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481: the server could not find the requested resource (get pods dns-test-83c82576-e671-486a-8381-72b1864d3481)
Mar 24 02:29:48.520: INFO: Lookups using dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 02:29:53.512: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481: the server could not find the requested resource (get pods dns-test-83c82576-e671-486a-8381-72b1864d3481)
Mar 24 02:29:53.520: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481: the server could not find the requested resource (get pods dns-test-83c82576-e671-486a-8381-72b1864d3481)
Mar 24 02:29:53.520: INFO: Lookups using dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481 failed for: [wheezy_tcp@PodARecord jessie_tcp@PodARecord]

Mar 24 02:29:58.512: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481: the server could not find the requested resource (get pods dns-test-83c82576-e671-486a-8381-72b1864d3481)
Mar 24 02:29:58.520: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481: the server could not find the requested resource (get pods dns-test-83c82576-e671-486a-8381-72b1864d3481)
Mar 24 02:29:58.520: INFO: Lookups using dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481 failed for: [wheezy_tcp@PodARecord jessie_tcp@PodARecord]

Mar 24 02:30:03.512: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481: the server could not find the requested resource (get pods dns-test-83c82576-e671-486a-8381-72b1864d3481)
Mar 24 02:30:03.520: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481: the server could not find the requested resource (get pods dns-test-83c82576-e671-486a-8381-72b1864d3481)
Mar 24 02:30:03.520: INFO: Lookups using dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481 failed for: [wheezy_tcp@PodARecord jessie_tcp@PodARecord]

Mar 24 02:30:08.513: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481: the server could not find the requested resource (get pods dns-test-83c82576-e671-486a-8381-72b1864d3481)
Mar 24 02:30:08.520: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481: the server could not find the requested resource (get pods dns-test-83c82576-e671-486a-8381-72b1864d3481)
Mar 24 02:30:08.520: INFO: Lookups using dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481 failed for: [wheezy_tcp@PodARecord jessie_tcp@PodARecord]

Mar 24 02:30:13.519: INFO: DNS probes using dns-4258/dns-test-83c82576-e671-486a-8381-72b1864d3481 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:30:13.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4258" for this suite.

â€¢ [SLOW TEST:32.204 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":21,"skipped":320,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:30:13.545: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1234
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-1dbc0ba4-c2d9-4242-98f0-4f4d720bf122 in namespace container-probe-1234
Mar 24 02:30:15.691: INFO: Started pod liveness-1dbc0ba4-c2d9-4242-98f0-4f4d720bf122 in namespace container-probe-1234
STEP: checking the pod's current state and verifying that restartCount is present
Mar 24 02:30:15.693: INFO: Initial restart count of pod liveness-1dbc0ba4-c2d9-4242-98f0-4f4d720bf122 is 0
Mar 24 02:30:35.743: INFO: Restart count of pod container-probe-1234/liveness-1dbc0ba4-c2d9-4242-98f0-4f4d720bf122 is now 1 (20.050399742s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:30:35.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1234" for this suite.

â€¢ [SLOW TEST:22.212 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":22,"skipped":322,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:30:35.758: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1841
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:30:35.886: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:30:41.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1841" for this suite.

â€¢ [SLOW TEST:6.153 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":23,"skipped":333,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:30:41.911: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6278
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar 24 02:30:42.038: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:30:48.817: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:31:02.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6278" for this suite.

â€¢ [SLOW TEST:20.774 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":24,"skipped":333,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:31:02.686: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:31:02.825: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar 24 02:31:02.830: INFO: Number of nodes with available pods: 0
Mar 24 02:31:02.830: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar 24 02:31:02.842: INFO: Number of nodes with available pods: 0
Mar 24 02:31:02.843: INFO: Node cn-hongkong.192.168.0.15 is running more than one daemon pod
Mar 24 02:31:03.846: INFO: Number of nodes with available pods: 0
Mar 24 02:31:03.846: INFO: Node cn-hongkong.192.168.0.15 is running more than one daemon pod
Mar 24 02:31:04.847: INFO: Number of nodes with available pods: 1
Mar 24 02:31:04.847: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar 24 02:31:04.861: INFO: Number of nodes with available pods: 1
Mar 24 02:31:04.861: INFO: Number of running nodes: 0, number of available pods: 1
Mar 24 02:31:05.865: INFO: Number of nodes with available pods: 0
Mar 24 02:31:05.865: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 24 02:31:05.872: INFO: Number of nodes with available pods: 0
Mar 24 02:31:05.872: INFO: Node cn-hongkong.192.168.0.15 is running more than one daemon pod
Mar 24 02:31:06.877: INFO: Number of nodes with available pods: 0
Mar 24 02:31:06.877: INFO: Node cn-hongkong.192.168.0.15 is running more than one daemon pod
Mar 24 02:31:07.876: INFO: Number of nodes with available pods: 0
Mar 24 02:31:07.876: INFO: Node cn-hongkong.192.168.0.15 is running more than one daemon pod
Mar 24 02:31:08.877: INFO: Number of nodes with available pods: 0
Mar 24 02:31:08.877: INFO: Node cn-hongkong.192.168.0.15 is running more than one daemon pod
Mar 24 02:31:09.877: INFO: Number of nodes with available pods: 0
Mar 24 02:31:09.877: INFO: Node cn-hongkong.192.168.0.15 is running more than one daemon pod
Mar 24 02:31:10.876: INFO: Number of nodes with available pods: 0
Mar 24 02:31:10.876: INFO: Node cn-hongkong.192.168.0.15 is running more than one daemon pod
Mar 24 02:31:11.877: INFO: Number of nodes with available pods: 0
Mar 24 02:31:11.877: INFO: Node cn-hongkong.192.168.0.15 is running more than one daemon pod
Mar 24 02:31:12.876: INFO: Number of nodes with available pods: 0
Mar 24 02:31:12.876: INFO: Node cn-hongkong.192.168.0.15 is running more than one daemon pod
Mar 24 02:31:13.877: INFO: Number of nodes with available pods: 1
Mar 24 02:31:13.877: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7541, will wait for the garbage collector to delete the pods
Mar 24 02:31:13.938: INFO: Deleting DaemonSet.extensions daemon-set took: 5.607775ms
Mar 24 02:31:14.638: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.06178ms
Mar 24 02:31:22.347: INFO: Number of nodes with available pods: 0
Mar 24 02:31:22.347: INFO: Number of running nodes: 0, number of available pods: 0
Mar 24 02:31:22.350: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"779421"},"items":null}

Mar 24 02:31:22.352: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"779421"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:31:22.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7541" for this suite.

â€¢ [SLOW TEST:19.710 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":25,"skipped":347,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:31:22.395: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-375
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-c6a2bfe0-5db7-4590-8e82-46fff875467e
STEP: Creating a pod to test consume secrets
Mar 24 02:31:22.531: INFO: Waiting up to 5m0s for pod "pod-secrets-b425c2c6-5916-4130-8589-5aa9315656f9" in namespace "secrets-375" to be "Succeeded or Failed"
Mar 24 02:31:22.533: INFO: Pod "pod-secrets-b425c2c6-5916-4130-8589-5aa9315656f9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.720201ms
Mar 24 02:31:24.539: INFO: Pod "pod-secrets-b425c2c6-5916-4130-8589-5aa9315656f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007701101s
STEP: Saw pod success
Mar 24 02:31:24.539: INFO: Pod "pod-secrets-b425c2c6-5916-4130-8589-5aa9315656f9" satisfied condition "Succeeded or Failed"
Mar 24 02:31:24.541: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-secrets-b425c2c6-5916-4130-8589-5aa9315656f9 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 02:31:24.562: INFO: Waiting for pod pod-secrets-b425c2c6-5916-4130-8589-5aa9315656f9 to disappear
Mar 24 02:31:24.563: INFO: Pod pod-secrets-b425c2c6-5916-4130-8589-5aa9315656f9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:31:24.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-375" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":26,"skipped":357,"failed":0}
SSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:31:24.573: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1405
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:31:24.707: INFO: Waiting up to 5m0s for pod "busybox-user-65534-77c14244-1531-479c-ae9c-021bb49db810" in namespace "security-context-test-1405" to be "Succeeded or Failed"
Mar 24 02:31:24.709: INFO: Pod "busybox-user-65534-77c14244-1531-479c-ae9c-021bb49db810": Phase="Pending", Reason="", readiness=false. Elapsed: 1.880379ms
Mar 24 02:31:26.714: INFO: Pod "busybox-user-65534-77c14244-1531-479c-ae9c-021bb49db810": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007269002s
Mar 24 02:31:26.714: INFO: Pod "busybox-user-65534-77c14244-1531-479c-ae9c-021bb49db810" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:31:26.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1405" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":27,"skipped":361,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:31:26.724: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8446
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-352d4aa3-7ad3-48c7-bad0-4bfcd8c0b78f
STEP: Creating a pod to test consume configMaps
Mar 24 02:31:26.858: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0d92ecf6-6ced-4aa5-aaca-00b4a9767161" in namespace "projected-8446" to be "Succeeded or Failed"
Mar 24 02:31:26.860: INFO: Pod "pod-projected-configmaps-0d92ecf6-6ced-4aa5-aaca-00b4a9767161": Phase="Pending", Reason="", readiness=false. Elapsed: 1.892358ms
Mar 24 02:31:28.865: INFO: Pod "pod-projected-configmaps-0d92ecf6-6ced-4aa5-aaca-00b4a9767161": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006668068s
STEP: Saw pod success
Mar 24 02:31:28.865: INFO: Pod "pod-projected-configmaps-0d92ecf6-6ced-4aa5-aaca-00b4a9767161" satisfied condition "Succeeded or Failed"
Mar 24 02:31:28.867: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-projected-configmaps-0d92ecf6-6ced-4aa5-aaca-00b4a9767161 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 02:31:28.881: INFO: Waiting for pod pod-projected-configmaps-0d92ecf6-6ced-4aa5-aaca-00b4a9767161 to disappear
Mar 24 02:31:28.882: INFO: Pod pod-projected-configmaps-0d92ecf6-6ced-4aa5-aaca-00b4a9767161 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:31:28.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8446" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":28,"skipped":368,"failed":0}
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:31:28.888: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-824
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-824/configmap-test-db37991c-2994-4cb6-ae4c-ea155a65ac8f
STEP: Creating a pod to test consume configMaps
Mar 24 02:31:29.024: INFO: Waiting up to 5m0s for pod "pod-configmaps-a5e0d1f4-fc59-4428-9cf8-0d54f6242770" in namespace "configmap-824" to be "Succeeded or Failed"
Mar 24 02:31:29.026: INFO: Pod "pod-configmaps-a5e0d1f4-fc59-4428-9cf8-0d54f6242770": Phase="Pending", Reason="", readiness=false. Elapsed: 1.742579ms
Mar 24 02:31:31.032: INFO: Pod "pod-configmaps-a5e0d1f4-fc59-4428-9cf8-0d54f6242770": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007407517s
STEP: Saw pod success
Mar 24 02:31:31.032: INFO: Pod "pod-configmaps-a5e0d1f4-fc59-4428-9cf8-0d54f6242770" satisfied condition "Succeeded or Failed"
Mar 24 02:31:31.034: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-configmaps-a5e0d1f4-fc59-4428-9cf8-0d54f6242770 container env-test: <nil>
STEP: delete the pod
Mar 24 02:31:31.048: INFO: Waiting for pod pod-configmaps-a5e0d1f4-fc59-4428-9cf8-0d54f6242770 to disappear
Mar 24 02:31:31.050: INFO: Pod pod-configmaps-a5e0d1f4-fc59-4428-9cf8-0d54f6242770 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:31:31.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-824" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":29,"skipped":374,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:31:31.056: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-750
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Mar 24 02:31:31.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-750 api-versions'
Mar 24 02:31:31.237: INFO: stderr: ""
Mar 24 02:31:31.237: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\nalert.alibabacloud.com/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:31:31.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-750" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":30,"skipped":378,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:31:31.244: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1514
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1514
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Mar 24 02:31:31.378: INFO: Found 0 stateful pods, waiting for 3
Mar 24 02:31:41.388: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 02:31:41.388: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 02:31:41.388: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 24 02:31:41.414: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar 24 02:31:51.445: INFO: Updating stateful set ss2
Mar 24 02:31:51.449: INFO: Waiting for Pod statefulset-1514/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 24 02:32:01.460: INFO: Waiting for Pod statefulset-1514/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Mar 24 02:32:11.483: INFO: Found 1 stateful pods, waiting for 3
Mar 24 02:32:21.492: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 02:32:21.492: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 02:32:21.492: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar 24 02:32:21.514: INFO: Updating stateful set ss2
Mar 24 02:32:21.517: INFO: Waiting for Pod statefulset-1514/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 24 02:32:31.528: INFO: Waiting for Pod statefulset-1514/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 24 02:32:41.549: INFO: Updating stateful set ss2
Mar 24 02:32:41.553: INFO: Waiting for StatefulSet statefulset-1514/ss2 to complete update
Mar 24 02:32:41.553: INFO: Waiting for Pod statefulset-1514/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 24 02:32:51.563: INFO: Deleting all statefulset in ns statefulset-1514
Mar 24 02:32:51.566: INFO: Scaling statefulset ss2 to 0
Mar 24 02:33:31.583: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 02:33:31.586: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:33:31.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1514" for this suite.

â€¢ [SLOW TEST:120.360 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":31,"skipped":394,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:33:31.604: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-407b1390-9f0c-4f23-b328-2c84868ce884
STEP: Creating a pod to test consume configMaps
Mar 24 02:33:31.739: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5bb522da-c461-4765-8b00-eee871ef95cb" in namespace "projected-8484" to be "Succeeded or Failed"
Mar 24 02:33:31.741: INFO: Pod "pod-projected-configmaps-5bb522da-c461-4765-8b00-eee871ef95cb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.787932ms
Mar 24 02:33:33.746: INFO: Pod "pod-projected-configmaps-5bb522da-c461-4765-8b00-eee871ef95cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006893653s
STEP: Saw pod success
Mar 24 02:33:33.746: INFO: Pod "pod-projected-configmaps-5bb522da-c461-4765-8b00-eee871ef95cb" satisfied condition "Succeeded or Failed"
Mar 24 02:33:33.748: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-projected-configmaps-5bb522da-c461-4765-8b00-eee871ef95cb container agnhost-container: <nil>
STEP: delete the pod
Mar 24 02:33:33.771: INFO: Waiting for pod pod-projected-configmaps-5bb522da-c461-4765-8b00-eee871ef95cb to disappear
Mar 24 02:33:33.773: INFO: Pod pod-projected-configmaps-5bb522da-c461-4765-8b00-eee871ef95cb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:33:33.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8484" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":32,"skipped":399,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:33:33.780: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5558
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:33:33.914: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-c463196e-449c-4872-9efc-d633e3fdd3e2" in namespace "security-context-test-5558" to be "Succeeded or Failed"
Mar 24 02:33:33.916: INFO: Pod "busybox-privileged-false-c463196e-449c-4872-9efc-d633e3fdd3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.880224ms
Mar 24 02:33:35.921: INFO: Pod "busybox-privileged-false-c463196e-449c-4872-9efc-d633e3fdd3e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007214358s
Mar 24 02:33:35.921: INFO: Pod "busybox-privileged-false-c463196e-449c-4872-9efc-d633e3fdd3e2" satisfied condition "Succeeded or Failed"
Mar 24 02:33:35.926: INFO: Got logs for pod "busybox-privileged-false-c463196e-449c-4872-9efc-d633e3fdd3e2": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:33:35.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5558" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":33,"skipped":412,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:33:35.933: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7592
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-73f211d9-1a3a-4c1c-b159-16d89d36c58b
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-73f211d9-1a3a-4c1c-b159-16d89d36c58b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:33:40.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7592" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":34,"skipped":437,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:33:40.109: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3169
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-d684bb70-20b8-4fc1-b916-317f38b44568
STEP: Creating a pod to test consume configMaps
Mar 24 02:33:40.247: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-394e9f56-ca6d-468d-9754-0d53cd052692" in namespace "projected-3169" to be "Succeeded or Failed"
Mar 24 02:33:40.249: INFO: Pod "pod-projected-configmaps-394e9f56-ca6d-468d-9754-0d53cd052692": Phase="Pending", Reason="", readiness=false. Elapsed: 1.848625ms
Mar 24 02:33:42.255: INFO: Pod "pod-projected-configmaps-394e9f56-ca6d-468d-9754-0d53cd052692": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007737375s
STEP: Saw pod success
Mar 24 02:33:42.255: INFO: Pod "pod-projected-configmaps-394e9f56-ca6d-468d-9754-0d53cd052692" satisfied condition "Succeeded or Failed"
Mar 24 02:33:42.257: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-projected-configmaps-394e9f56-ca6d-468d-9754-0d53cd052692 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 02:33:42.272: INFO: Waiting for pod pod-projected-configmaps-394e9f56-ca6d-468d-9754-0d53cd052692 to disappear
Mar 24 02:33:42.276: INFO: Pod pod-projected-configmaps-394e9f56-ca6d-468d-9754-0d53cd052692 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:33:42.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3169" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":35,"skipped":442,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:33:42.282: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-8024
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 24 02:33:43.121: INFO: starting watch
STEP: patching
STEP: updating
Mar 24 02:33:43.128: INFO: waiting for watch events with expected annotations
Mar 24 02:33:43.128: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:33:43.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-8024" for this suite.
â€¢{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":36,"skipped":491,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:33:43.173: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6082
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 02:33:43.851: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 02:33:46.866: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:33:46.870: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8537-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:33:52.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6082" for this suite.
STEP: Destroying namespace "webhook-6082-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

â€¢ [SLOW TEST:9.815 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":37,"skipped":494,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:33:52.989: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2884
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 24 02:33:53.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2884 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Mar 24 02:33:53.189: INFO: stderr: ""
Mar 24 02:33:53.189: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Mar 24 02:33:53.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2884 delete pods e2e-test-httpd-pod'
Mar 24 02:34:02.539: INFO: stderr: ""
Mar 24 02:34:02.539: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:34:02.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2884" for this suite.

â€¢ [SLOW TEST:9.569 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":38,"skipped":510,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:34:02.558: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7539
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 02:34:03.121: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 02:34:06.140: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:34:06.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7539" for this suite.
STEP: Destroying namespace "webhook-7539-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":39,"skipped":510,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:34:06.220: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7374
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-7374
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7374 to expose endpoints map[]
Mar 24 02:34:06.359: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Mar 24 02:34:07.366: INFO: successfully validated that service multi-endpoint-test in namespace services-7374 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7374
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7374 to expose endpoints map[pod1:[100]]
Mar 24 02:34:09.385: INFO: successfully validated that service multi-endpoint-test in namespace services-7374 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7374
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7374 to expose endpoints map[pod1:[100] pod2:[101]]
Mar 24 02:34:10.403: INFO: successfully validated that service multi-endpoint-test in namespace services-7374 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-7374
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7374 to expose endpoints map[pod2:[101]]
Mar 24 02:34:10.418: INFO: successfully validated that service multi-endpoint-test in namespace services-7374 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7374
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7374 to expose endpoints map[]
Mar 24 02:34:10.431: INFO: successfully validated that service multi-endpoint-test in namespace services-7374 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:34:10.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7374" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
â€¢{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":40,"skipped":534,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:34:10.451: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8201
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:34:10.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8201" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":41,"skipped":546,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:34:10.602: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8569
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 24 02:34:10.734: INFO: Waiting up to 5m0s for pod "pod-4883f4c4-b57d-44fa-b77c-81ce2d17f8e0" in namespace "emptydir-8569" to be "Succeeded or Failed"
Mar 24 02:34:10.736: INFO: Pod "pod-4883f4c4-b57d-44fa-b77c-81ce2d17f8e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006305ms
Mar 24 02:34:12.742: INFO: Pod "pod-4883f4c4-b57d-44fa-b77c-81ce2d17f8e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007628336s
STEP: Saw pod success
Mar 24 02:34:12.742: INFO: Pod "pod-4883f4c4-b57d-44fa-b77c-81ce2d17f8e0" satisfied condition "Succeeded or Failed"
Mar 24 02:34:12.744: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-4883f4c4-b57d-44fa-b77c-81ce2d17f8e0 container test-container: <nil>
STEP: delete the pod
Mar 24 02:34:12.757: INFO: Waiting for pod pod-4883f4c4-b57d-44fa-b77c-81ce2d17f8e0 to disappear
Mar 24 02:34:12.759: INFO: Pod pod-4883f4c4-b57d-44fa-b77c-81ce2d17f8e0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:34:12.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8569" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":42,"skipped":547,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:34:12.765: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5946
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:34:12.891: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 24 02:34:20.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-5946 --namespace=crd-publish-openapi-5946 create -f -'
Mar 24 02:34:21.009: INFO: stderr: ""
Mar 24 02:34:21.009: INFO: stdout: "e2e-test-crd-publish-openapi-698-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 24 02:34:21.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-5946 --namespace=crd-publish-openapi-5946 delete e2e-test-crd-publish-openapi-698-crds test-cr'
Mar 24 02:34:21.098: INFO: stderr: ""
Mar 24 02:34:21.098: INFO: stdout: "e2e-test-crd-publish-openapi-698-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 24 02:34:21.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-5946 --namespace=crd-publish-openapi-5946 apply -f -'
Mar 24 02:34:21.324: INFO: stderr: ""
Mar 24 02:34:21.324: INFO: stdout: "e2e-test-crd-publish-openapi-698-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 24 02:34:21.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-5946 --namespace=crd-publish-openapi-5946 delete e2e-test-crd-publish-openapi-698-crds test-cr'
Mar 24 02:34:21.380: INFO: stderr: ""
Mar 24 02:34:21.380: INFO: stdout: "e2e-test-crd-publish-openapi-698-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 24 02:34:21.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-5946 explain e2e-test-crd-publish-openapi-698-crds'
Mar 24 02:34:21.572: INFO: stderr: ""
Mar 24 02:34:21.572: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-698-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:34:24.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5946" for this suite.

â€¢ [SLOW TEST:11.578 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":43,"skipped":596,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:34:24.343: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-758
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 24 02:34:25.485: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:34:25.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-758" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":44,"skipped":604,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:34:25.502: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2617
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-b4219239-cb58-4122-a891-26b8741554bd
STEP: Creating a pod to test consume secrets
Mar 24 02:34:25.636: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4ab86c0d-9099-4b94-9462-928f19a44adb" in namespace "projected-2617" to be "Succeeded or Failed"
Mar 24 02:34:25.638: INFO: Pod "pod-projected-secrets-4ab86c0d-9099-4b94-9462-928f19a44adb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.866666ms
Mar 24 02:34:27.643: INFO: Pod "pod-projected-secrets-4ab86c0d-9099-4b94-9462-928f19a44adb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006792441s
STEP: Saw pod success
Mar 24 02:34:27.643: INFO: Pod "pod-projected-secrets-4ab86c0d-9099-4b94-9462-928f19a44adb" satisfied condition "Succeeded or Failed"
Mar 24 02:34:27.645: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-projected-secrets-4ab86c0d-9099-4b94-9462-928f19a44adb container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 24 02:34:27.660: INFO: Waiting for pod pod-projected-secrets-4ab86c0d-9099-4b94-9462-928f19a44adb to disappear
Mar 24 02:34:27.661: INFO: Pod pod-projected-secrets-4ab86c0d-9099-4b94-9462-928f19a44adb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:34:27.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2617" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":45,"skipped":613,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:34:27.667: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4455
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 02:34:27.798: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f531e75-0e94-41be-a148-053c48a7a10e" in namespace "projected-4455" to be "Succeeded or Failed"
Mar 24 02:34:27.800: INFO: Pod "downwardapi-volume-6f531e75-0e94-41be-a148-053c48a7a10e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.597905ms
Mar 24 02:34:29.805: INFO: Pod "downwardapi-volume-6f531e75-0e94-41be-a148-053c48a7a10e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006892772s
STEP: Saw pod success
Mar 24 02:34:29.805: INFO: Pod "downwardapi-volume-6f531e75-0e94-41be-a148-053c48a7a10e" satisfied condition "Succeeded or Failed"
Mar 24 02:34:29.807: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod downwardapi-volume-6f531e75-0e94-41be-a148-053c48a7a10e container client-container: <nil>
STEP: delete the pod
Mar 24 02:34:29.822: INFO: Waiting for pod downwardapi-volume-6f531e75-0e94-41be-a148-053c48a7a10e to disappear
Mar 24 02:34:29.824: INFO: Pod downwardapi-volume-6f531e75-0e94-41be-a148-053c48a7a10e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:34:29.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4455" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":46,"skipped":616,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:34:29.830: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar 24 02:34:32.479: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1999 pod-service-account-bf40a654-a58c-49c4-9cca-3365ecd413c8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar 24 02:34:32.631: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1999 pod-service-account-bf40a654-a58c-49c4-9cca-3365ecd413c8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar 24 02:34:32.779: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1999 pod-service-account-bf40a654-a58c-49c4-9cca-3365ecd413c8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:34:32.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1999" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":47,"skipped":628,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:34:32.924: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4497
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4497
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-4497
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4497
Mar 24 02:34:33.063: INFO: Found 0 stateful pods, waiting for 1
Mar 24 02:34:43.072: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 24 02:34:43.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-4497 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 02:34:43.216: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 02:34:43.216: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 02:34:43.216: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 02:34:43.218: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 24 02:34:53.228: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 02:34:53.228: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 02:34:53.238: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Mar 24 02:34:53.238: INFO: ss-0  cn-hongkong.192.168.0.14  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  }]
Mar 24 02:34:53.238: INFO: 
Mar 24 02:34:53.238: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 24 02:34:54.243: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997892148s
Mar 24 02:34:55.248: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992845175s
Mar 24 02:34:56.252: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988798865s
Mar 24 02:34:57.257: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983836787s
Mar 24 02:34:58.260: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.979090093s
Mar 24 02:34:59.265: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975948348s
Mar 24 02:35:00.269: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971107048s
Mar 24 02:35:01.274: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96698949s
Mar 24 02:35:02.279: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.010533ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4497
Mar 24 02:35:03.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-4497 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 02:35:03.432: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 24 02:35:03.432: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 02:35:03.432: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 24 02:35:03.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-4497 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 02:35:03.580: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 24 02:35:03.580: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 02:35:03.580: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 24 02:35:03.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-4497 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 02:35:03.729: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 24 02:35:03.729: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 02:35:03.729: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 24 02:35:03.731: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 02:35:03.731: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 02:35:03.731: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar 24 02:35:03.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-4497 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 02:35:03.875: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 02:35:03.875: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 02:35:03.875: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 02:35:03.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-4497 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 02:35:04.018: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 02:35:04.018: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 02:35:04.018: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 02:35:04.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-4497 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 02:35:04.164: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 02:35:04.164: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 02:35:04.164: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 02:35:04.164: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 02:35:04.166: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 24 02:35:14.179: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 02:35:14.179: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 02:35:14.179: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 02:35:14.187: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Mar 24 02:35:14.187: INFO: ss-0  cn-hongkong.192.168.0.14  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  }]
Mar 24 02:35:14.187: INFO: ss-1  cn-hongkong.192.168.0.15  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:14.187: INFO: ss-2  cn-hongkong.192.168.0.14  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:14.187: INFO: 
Mar 24 02:35:14.187: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 24 02:35:15.191: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Mar 24 02:35:15.191: INFO: ss-0  cn-hongkong.192.168.0.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  }]
Mar 24 02:35:15.191: INFO: ss-1  cn-hongkong.192.168.0.15  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:15.191: INFO: ss-2  cn-hongkong.192.168.0.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:15.191: INFO: 
Mar 24 02:35:15.191: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 24 02:35:16.196: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Mar 24 02:35:16.196: INFO: ss-0  cn-hongkong.192.168.0.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  }]
Mar 24 02:35:16.196: INFO: ss-1  cn-hongkong.192.168.0.15  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:16.196: INFO: ss-2  cn-hongkong.192.168.0.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:16.196: INFO: 
Mar 24 02:35:16.196: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 24 02:35:17.202: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Mar 24 02:35:17.202: INFO: ss-0  cn-hongkong.192.168.0.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  }]
Mar 24 02:35:17.202: INFO: ss-1  cn-hongkong.192.168.0.15  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:17.202: INFO: ss-2  cn-hongkong.192.168.0.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:17.202: INFO: 
Mar 24 02:35:17.202: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 24 02:35:18.205: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Mar 24 02:35:18.205: INFO: ss-0  cn-hongkong.192.168.0.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  }]
Mar 24 02:35:18.205: INFO: ss-1  cn-hongkong.192.168.0.15  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:18.205: INFO: ss-2  cn-hongkong.192.168.0.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:18.205: INFO: 
Mar 24 02:35:18.205: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 24 02:35:19.210: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Mar 24 02:35:19.210: INFO: ss-0  cn-hongkong.192.168.0.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  }]
Mar 24 02:35:19.210: INFO: ss-1  cn-hongkong.192.168.0.15  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:19.210: INFO: ss-2  cn-hongkong.192.168.0.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:19.210: INFO: 
Mar 24 02:35:19.210: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 24 02:35:20.214: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Mar 24 02:35:20.214: INFO: ss-0  cn-hongkong.192.168.0.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  }]
Mar 24 02:35:20.214: INFO: ss-1  cn-hongkong.192.168.0.15  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:20.214: INFO: ss-2  cn-hongkong.192.168.0.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:20.214: INFO: 
Mar 24 02:35:20.214: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 24 02:35:21.219: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Mar 24 02:35:21.219: INFO: ss-0  cn-hongkong.192.168.0.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  }]
Mar 24 02:35:21.219: INFO: ss-1  cn-hongkong.192.168.0.15  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:21.219: INFO: ss-2  cn-hongkong.192.168.0.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:21.219: INFO: 
Mar 24 02:35:21.219: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 24 02:35:22.223: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Mar 24 02:35:22.223: INFO: ss-0  cn-hongkong.192.168.0.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:33 +0000 UTC  }]
Mar 24 02:35:22.223: INFO: ss-1  cn-hongkong.192.168.0.15  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:22.223: INFO: ss-2  cn-hongkong.192.168.0.14  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:35:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 02:34:53 +0000 UTC  }]
Mar 24 02:35:22.223: INFO: 
Mar 24 02:35:22.223: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 24 02:35:23.227: INFO: Verifying statefulset ss doesn't scale past 0 for another 961.231058ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4497
Mar 24 02:35:24.232: INFO: Scaling statefulset ss to 0
Mar 24 02:35:24.239: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 24 02:35:24.241: INFO: Deleting all statefulset in ns statefulset-4497
Mar 24 02:35:24.243: INFO: Scaling statefulset ss to 0
Mar 24 02:35:24.249: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 02:35:24.250: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:35:24.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4497" for this suite.

â€¢ [SLOW TEST:51.342 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":48,"skipped":642,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:35:24.267: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2056
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-06168a1a-4a54-40f0-a6b0-9421d0968813
STEP: Creating secret with name s-test-opt-upd-efe1dd90-daa3-497b-bb90-868126700ce7
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-06168a1a-4a54-40f0-a6b0-9421d0968813
STEP: Updating secret s-test-opt-upd-efe1dd90-daa3-497b-bb90-868126700ce7
STEP: Creating secret with name s-test-opt-create-dd7ac214-c92a-4941-9ead-54aed7109ff6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:35:28.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2056" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":49,"skipped":662,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:35:28.470: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5144
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:35:35.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5144" for this suite.

â€¢ [SLOW TEST:7.147 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":50,"skipped":672,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:35:35.617: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-948
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 24 02:35:35.754: INFO: Waiting up to 5m0s for pod "pod-87a8fd04-0527-4581-b2c9-4211c571908c" in namespace "emptydir-948" to be "Succeeded or Failed"
Mar 24 02:35:35.755: INFO: Pod "pod-87a8fd04-0527-4581-b2c9-4211c571908c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.698865ms
Mar 24 02:35:37.760: INFO: Pod "pod-87a8fd04-0527-4581-b2c9-4211c571908c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00685228s
STEP: Saw pod success
Mar 24 02:35:37.760: INFO: Pod "pod-87a8fd04-0527-4581-b2c9-4211c571908c" satisfied condition "Succeeded or Failed"
Mar 24 02:35:37.763: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-87a8fd04-0527-4581-b2c9-4211c571908c container test-container: <nil>
STEP: delete the pod
Mar 24 02:35:37.778: INFO: Waiting for pod pod-87a8fd04-0527-4581-b2c9-4211c571908c to disappear
Mar 24 02:35:37.780: INFO: Pod pod-87a8fd04-0527-4581-b2c9-4211c571908c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:35:37.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-948" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":51,"skipped":678,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:35:37.786: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7690
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 02:35:38.304: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 02:35:41.323: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar 24 02:35:41.338: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:35:41.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7690" for this suite.
STEP: Destroying namespace "webhook-7690-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":52,"skipped":714,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:35:41.388: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9486
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 24 02:35:41.521: INFO: Waiting up to 5m0s for pod "pod-82337fa0-c322-4737-9f8b-b82db85a6c71" in namespace "emptydir-9486" to be "Succeeded or Failed"
Mar 24 02:35:41.523: INFO: Pod "pod-82337fa0-c322-4737-9f8b-b82db85a6c71": Phase="Pending", Reason="", readiness=false. Elapsed: 1.826906ms
Mar 24 02:35:43.528: INFO: Pod "pod-82337fa0-c322-4737-9f8b-b82db85a6c71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007119596s
STEP: Saw pod success
Mar 24 02:35:43.528: INFO: Pod "pod-82337fa0-c322-4737-9f8b-b82db85a6c71" satisfied condition "Succeeded or Failed"
Mar 24 02:35:43.530: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod pod-82337fa0-c322-4737-9f8b-b82db85a6c71 container test-container: <nil>
STEP: delete the pod
Mar 24 02:35:43.550: INFO: Waiting for pod pod-82337fa0-c322-4737-9f8b-b82db85a6c71 to disappear
Mar 24 02:35:43.552: INFO: Pod pod-82337fa0-c322-4737-9f8b-b82db85a6c71 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:35:43.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9486" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":53,"skipped":722,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:35:43.558: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-6895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:35:43.684: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6895
I0324 02:35:43.692621      21 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6895, replica count: 1
I0324 02:35:44.742848      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0324 02:35:45.742971      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 02:35:45.853: INFO: Created: latency-svc-4zqsf
Mar 24 02:35:45.857: INFO: Got endpoints: latency-svc-4zqsf [14.40478ms]
Mar 24 02:35:45.864: INFO: Created: latency-svc-w2nsp
Mar 24 02:35:45.868: INFO: Got endpoints: latency-svc-w2nsp [11.096949ms]
Mar 24 02:35:45.870: INFO: Created: latency-svc-xscn8
Mar 24 02:35:45.874: INFO: Got endpoints: latency-svc-xscn8 [16.359414ms]
Mar 24 02:35:45.877: INFO: Created: latency-svc-g7k9x
Mar 24 02:35:45.880: INFO: Got endpoints: latency-svc-g7k9x [22.628451ms]
Mar 24 02:35:45.882: INFO: Created: latency-svc-bjvbc
Mar 24 02:35:45.886: INFO: Got endpoints: latency-svc-bjvbc [12.554517ms]
Mar 24 02:35:45.888: INFO: Created: latency-svc-csvwg
Mar 24 02:35:45.892: INFO: Got endpoints: latency-svc-csvwg [35.0484ms]
Mar 24 02:35:45.895: INFO: Created: latency-svc-wtzdr
Mar 24 02:35:45.899: INFO: Got endpoints: latency-svc-wtzdr [42.200941ms]
Mar 24 02:35:45.901: INFO: Created: latency-svc-k4kz4
Mar 24 02:35:45.905: INFO: Got endpoints: latency-svc-k4kz4 [47.525196ms]
Mar 24 02:35:45.922: INFO: Created: latency-svc-48bpr
Mar 24 02:35:45.925: INFO: Got endpoints: latency-svc-48bpr [67.910734ms]
Mar 24 02:35:45.929: INFO: Created: latency-svc-48bpc
Mar 24 02:35:45.931: INFO: Got endpoints: latency-svc-48bpc [73.784291ms]
Mar 24 02:35:45.932: INFO: Created: latency-svc-7gqhg
Mar 24 02:35:45.937: INFO: Got endpoints: latency-svc-7gqhg [79.933663ms]
Mar 24 02:35:45.938: INFO: Created: latency-svc-xxqjs
Mar 24 02:35:45.941: INFO: Got endpoints: latency-svc-xxqjs [83.420027ms]
Mar 24 02:35:45.943: INFO: Created: latency-svc-znhr2
Mar 24 02:35:45.948: INFO: Got endpoints: latency-svc-znhr2 [90.467829ms]
Mar 24 02:35:45.949: INFO: Created: latency-svc-8g5x5
Mar 24 02:35:45.955: INFO: Got endpoints: latency-svc-8g5x5 [97.606997ms]
Mar 24 02:35:45.956: INFO: Created: latency-svc-kqhnj
Mar 24 02:35:45.959: INFO: Got endpoints: latency-svc-kqhnj [101.438718ms]
Mar 24 02:35:45.962: INFO: Created: latency-svc-dglpr
Mar 24 02:35:45.966: INFO: Got endpoints: latency-svc-dglpr [108.436443ms]
Mar 24 02:35:45.967: INFO: Created: latency-svc-knm5d
Mar 24 02:35:45.972: INFO: Got endpoints: latency-svc-knm5d [115.096138ms]
Mar 24 02:35:45.973: INFO: Created: latency-svc-p8xbl
Mar 24 02:35:45.976: INFO: Got endpoints: latency-svc-p8xbl [108.283793ms]
Mar 24 02:35:45.980: INFO: Created: latency-svc-wgf5b
Mar 24 02:35:45.982: INFO: Got endpoints: latency-svc-wgf5b [101.78841ms]
Mar 24 02:35:45.984: INFO: Created: latency-svc-4q7ws
Mar 24 02:35:45.989: INFO: Got endpoints: latency-svc-4q7ws [102.931343ms]
Mar 24 02:35:45.989: INFO: Created: latency-svc-n6mdn
Mar 24 02:35:45.991: INFO: Got endpoints: latency-svc-n6mdn [98.627096ms]
Mar 24 02:35:45.996: INFO: Created: latency-svc-w8hfq
Mar 24 02:35:45.999: INFO: Got endpoints: latency-svc-w8hfq [99.434497ms]
Mar 24 02:35:46.001: INFO: Created: latency-svc-jlb7p
Mar 24 02:35:46.011: INFO: Created: latency-svc-dnhlw
Mar 24 02:35:46.015: INFO: Got endpoints: latency-svc-dnhlw [90.223292ms]
Mar 24 02:35:46.016: INFO: Got endpoints: latency-svc-jlb7p [110.927628ms]
Mar 24 02:35:46.021: INFO: Created: latency-svc-2g27v
Mar 24 02:35:46.023: INFO: Got endpoints: latency-svc-2g27v [91.910491ms]
Mar 24 02:35:46.025: INFO: Created: latency-svc-xcdwd
Mar 24 02:35:46.029: INFO: Got endpoints: latency-svc-xcdwd [91.99757ms]
Mar 24 02:35:46.031: INFO: Created: latency-svc-wmbz6
Mar 24 02:35:46.035: INFO: Got endpoints: latency-svc-wmbz6 [94.107125ms]
Mar 24 02:35:46.037: INFO: Created: latency-svc-zkp9w
Mar 24 02:35:46.040: INFO: Got endpoints: latency-svc-zkp9w [92.130103ms]
Mar 24 02:35:46.042: INFO: Created: latency-svc-r5q9p
Mar 24 02:35:46.045: INFO: Got endpoints: latency-svc-r5q9p [90.101758ms]
Mar 24 02:35:46.048: INFO: Created: latency-svc-8pv2p
Mar 24 02:35:46.051: INFO: Got endpoints: latency-svc-8pv2p [91.813216ms]
Mar 24 02:35:46.054: INFO: Created: latency-svc-h9bss
Mar 24 02:35:46.056: INFO: Got endpoints: latency-svc-h9bss [90.691725ms]
Mar 24 02:35:46.057: INFO: Created: latency-svc-ngj6w
Mar 24 02:35:46.063: INFO: Got endpoints: latency-svc-ngj6w [90.493095ms]
Mar 24 02:35:46.065: INFO: Created: latency-svc-29q26
Mar 24 02:35:46.068: INFO: Got endpoints: latency-svc-29q26 [91.930965ms]
Mar 24 02:35:46.072: INFO: Created: latency-svc-lj699
Mar 24 02:35:46.074: INFO: Got endpoints: latency-svc-lj699 [92.232693ms]
Mar 24 02:35:46.075: INFO: Created: latency-svc-gjm47
Mar 24 02:35:46.080: INFO: Created: latency-svc-rkzfd
Mar 24 02:35:46.085: INFO: Created: latency-svc-trnjd
Mar 24 02:35:46.089: INFO: Created: latency-svc-6g45d
Mar 24 02:35:46.094: INFO: Created: latency-svc-rtdhw
Mar 24 02:35:46.099: INFO: Created: latency-svc-s289c
Mar 24 02:35:46.103: INFO: Created: latency-svc-xcgvv
Mar 24 02:35:46.108: INFO: Got endpoints: latency-svc-gjm47 [118.994304ms]
Mar 24 02:35:46.110: INFO: Created: latency-svc-6psc8
Mar 24 02:35:46.115: INFO: Created: latency-svc-bwtgr
Mar 24 02:35:46.118: INFO: Created: latency-svc-thjgd
Mar 24 02:35:46.122: INFO: Created: latency-svc-nrvj9
Mar 24 02:35:46.127: INFO: Created: latency-svc-pg84q
Mar 24 02:35:46.130: INFO: Created: latency-svc-pbm4v
Mar 24 02:35:46.134: INFO: Created: latency-svc-75wtx
Mar 24 02:35:46.139: INFO: Created: latency-svc-hfnk7
Mar 24 02:35:46.142: INFO: Created: latency-svc-sknlm
Mar 24 02:35:46.157: INFO: Got endpoints: latency-svc-rkzfd [165.858762ms]
Mar 24 02:35:46.163: INFO: Created: latency-svc-57wrb
Mar 24 02:35:46.207: INFO: Got endpoints: latency-svc-trnjd [208.379976ms]
Mar 24 02:35:46.215: INFO: Created: latency-svc-9qwgn
Mar 24 02:35:46.257: INFO: Got endpoints: latency-svc-6g45d [241.751292ms]
Mar 24 02:35:46.264: INFO: Created: latency-svc-xjjwn
Mar 24 02:35:46.306: INFO: Got endpoints: latency-svc-rtdhw [290.733834ms]
Mar 24 02:35:46.314: INFO: Created: latency-svc-ch9qk
Mar 24 02:35:46.357: INFO: Got endpoints: latency-svc-s289c [333.799556ms]
Mar 24 02:35:46.365: INFO: Created: latency-svc-b96dr
Mar 24 02:35:46.406: INFO: Got endpoints: latency-svc-xcgvv [377.09847ms]
Mar 24 02:35:46.415: INFO: Created: latency-svc-bk947
Mar 24 02:35:46.457: INFO: Got endpoints: latency-svc-6psc8 [422.525171ms]
Mar 24 02:35:46.465: INFO: Created: latency-svc-r52sc
Mar 24 02:35:46.507: INFO: Got endpoints: latency-svc-bwtgr [467.085593ms]
Mar 24 02:35:46.515: INFO: Created: latency-svc-c2lqg
Mar 24 02:35:46.556: INFO: Got endpoints: latency-svc-thjgd [510.693396ms]
Mar 24 02:35:46.563: INFO: Created: latency-svc-jqg2g
Mar 24 02:35:46.607: INFO: Got endpoints: latency-svc-nrvj9 [555.910372ms]
Mar 24 02:35:46.615: INFO: Created: latency-svc-hrqdh
Mar 24 02:35:46.657: INFO: Got endpoints: latency-svc-pg84q [600.213149ms]
Mar 24 02:35:46.664: INFO: Created: latency-svc-f7fx7
Mar 24 02:35:46.706: INFO: Got endpoints: latency-svc-pbm4v [643.156888ms]
Mar 24 02:35:46.714: INFO: Created: latency-svc-qr22h
Mar 24 02:35:46.756: INFO: Got endpoints: latency-svc-75wtx [687.794978ms]
Mar 24 02:35:46.764: INFO: Created: latency-svc-ngw7j
Mar 24 02:35:46.808: INFO: Got endpoints: latency-svc-hfnk7 [733.70606ms]
Mar 24 02:35:46.816: INFO: Created: latency-svc-5g4kw
Mar 24 02:35:46.857: INFO: Got endpoints: latency-svc-sknlm [748.926285ms]
Mar 24 02:35:46.864: INFO: Created: latency-svc-xr8j4
Mar 24 02:35:46.909: INFO: Got endpoints: latency-svc-57wrb [752.022475ms]
Mar 24 02:35:46.917: INFO: Created: latency-svc-kh8ln
Mar 24 02:35:46.957: INFO: Got endpoints: latency-svc-9qwgn [749.863758ms]
Mar 24 02:35:46.965: INFO: Created: latency-svc-9dvsw
Mar 24 02:35:47.006: INFO: Got endpoints: latency-svc-xjjwn [749.103473ms]
Mar 24 02:35:47.013: INFO: Created: latency-svc-hx9x2
Mar 24 02:35:47.056: INFO: Got endpoints: latency-svc-ch9qk [750.156777ms]
Mar 24 02:35:47.065: INFO: Created: latency-svc-tk7wg
Mar 24 02:35:47.107: INFO: Got endpoints: latency-svc-b96dr [749.897958ms]
Mar 24 02:35:47.114: INFO: Created: latency-svc-gx766
Mar 24 02:35:47.157: INFO: Got endpoints: latency-svc-bk947 [750.421276ms]
Mar 24 02:35:47.164: INFO: Created: latency-svc-x6x5c
Mar 24 02:35:47.208: INFO: Got endpoints: latency-svc-r52sc [750.313892ms]
Mar 24 02:35:47.215: INFO: Created: latency-svc-pr5h8
Mar 24 02:35:47.257: INFO: Got endpoints: latency-svc-c2lqg [750.205926ms]
Mar 24 02:35:47.265: INFO: Created: latency-svc-rrlld
Mar 24 02:35:47.307: INFO: Got endpoints: latency-svc-jqg2g [751.449654ms]
Mar 24 02:35:47.319: INFO: Created: latency-svc-5bq77
Mar 24 02:35:47.357: INFO: Got endpoints: latency-svc-hrqdh [750.807138ms]
Mar 24 02:35:47.365: INFO: Created: latency-svc-mk97p
Mar 24 02:35:47.407: INFO: Got endpoints: latency-svc-f7fx7 [750.412235ms]
Mar 24 02:35:47.415: INFO: Created: latency-svc-v9zrn
Mar 24 02:35:47.457: INFO: Got endpoints: latency-svc-qr22h [750.546511ms]
Mar 24 02:35:47.464: INFO: Created: latency-svc-6p9hc
Mar 24 02:35:47.507: INFO: Got endpoints: latency-svc-ngw7j [750.378456ms]
Mar 24 02:35:47.515: INFO: Created: latency-svc-8g7d5
Mar 24 02:35:47.556: INFO: Got endpoints: latency-svc-5g4kw [748.5884ms]
Mar 24 02:35:47.564: INFO: Created: latency-svc-fkbrv
Mar 24 02:35:47.607: INFO: Got endpoints: latency-svc-xr8j4 [749.456958ms]
Mar 24 02:35:47.613: INFO: Created: latency-svc-qzswn
Mar 24 02:35:47.657: INFO: Got endpoints: latency-svc-kh8ln [748.738972ms]
Mar 24 02:35:47.665: INFO: Created: latency-svc-v54nh
Mar 24 02:35:47.706: INFO: Got endpoints: latency-svc-9dvsw [748.85859ms]
Mar 24 02:35:47.714: INFO: Created: latency-svc-64bst
Mar 24 02:35:47.757: INFO: Got endpoints: latency-svc-hx9x2 [750.503312ms]
Mar 24 02:35:47.764: INFO: Created: latency-svc-f44kg
Mar 24 02:35:47.807: INFO: Got endpoints: latency-svc-tk7wg [750.782521ms]
Mar 24 02:35:47.815: INFO: Created: latency-svc-897kd
Mar 24 02:35:47.857: INFO: Got endpoints: latency-svc-gx766 [750.661814ms]
Mar 24 02:35:47.865: INFO: Created: latency-svc-nhwrt
Mar 24 02:35:47.908: INFO: Got endpoints: latency-svc-x6x5c [751.225925ms]
Mar 24 02:35:47.915: INFO: Created: latency-svc-ln265
Mar 24 02:35:47.957: INFO: Got endpoints: latency-svc-pr5h8 [748.804405ms]
Mar 24 02:35:47.964: INFO: Created: latency-svc-6s7f6
Mar 24 02:35:48.007: INFO: Got endpoints: latency-svc-rrlld [749.173686ms]
Mar 24 02:35:48.015: INFO: Created: latency-svc-znp6f
Mar 24 02:35:48.057: INFO: Got endpoints: latency-svc-5bq77 [749.318717ms]
Mar 24 02:35:48.064: INFO: Created: latency-svc-7qcwd
Mar 24 02:35:48.108: INFO: Got endpoints: latency-svc-mk97p [750.337564ms]
Mar 24 02:35:48.115: INFO: Created: latency-svc-g9nl5
Mar 24 02:35:48.157: INFO: Got endpoints: latency-svc-v9zrn [749.656305ms]
Mar 24 02:35:48.202: INFO: Created: latency-svc-j7nqg
Mar 24 02:35:48.207: INFO: Got endpoints: latency-svc-6p9hc [750.279438ms]
Mar 24 02:35:48.214: INFO: Created: latency-svc-qkfjp
Mar 24 02:35:48.258: INFO: Got endpoints: latency-svc-8g7d5 [750.96244ms]
Mar 24 02:35:48.266: INFO: Created: latency-svc-d6pgx
Mar 24 02:35:48.307: INFO: Got endpoints: latency-svc-fkbrv [750.869911ms]
Mar 24 02:35:48.315: INFO: Created: latency-svc-956zp
Mar 24 02:35:48.357: INFO: Got endpoints: latency-svc-qzswn [750.306155ms]
Mar 24 02:35:48.364: INFO: Created: latency-svc-w9qht
Mar 24 02:35:48.407: INFO: Got endpoints: latency-svc-v54nh [749.265633ms]
Mar 24 02:35:48.415: INFO: Created: latency-svc-n27dn
Mar 24 02:35:48.456: INFO: Got endpoints: latency-svc-64bst [750.319387ms]
Mar 24 02:35:48.464: INFO: Created: latency-svc-8nxjq
Mar 24 02:35:48.508: INFO: Got endpoints: latency-svc-f44kg [751.060801ms]
Mar 24 02:35:48.515: INFO: Created: latency-svc-q98np
Mar 24 02:35:48.557: INFO: Got endpoints: latency-svc-897kd [749.830146ms]
Mar 24 02:35:48.567: INFO: Created: latency-svc-pvb8f
Mar 24 02:35:48.606: INFO: Got endpoints: latency-svc-nhwrt [748.89854ms]
Mar 24 02:35:48.614: INFO: Created: latency-svc-6zlzl
Mar 24 02:35:48.657: INFO: Got endpoints: latency-svc-ln265 [748.882417ms]
Mar 24 02:35:48.664: INFO: Created: latency-svc-6478x
Mar 24 02:35:48.706: INFO: Got endpoints: latency-svc-6s7f6 [749.627388ms]
Mar 24 02:35:48.714: INFO: Created: latency-svc-lm2gq
Mar 24 02:35:48.757: INFO: Got endpoints: latency-svc-znp6f [750.804615ms]
Mar 24 02:35:48.765: INFO: Created: latency-svc-gk4sw
Mar 24 02:35:48.807: INFO: Got endpoints: latency-svc-7qcwd [750.446725ms]
Mar 24 02:35:48.814: INFO: Created: latency-svc-qhdsh
Mar 24 02:35:48.857: INFO: Got endpoints: latency-svc-g9nl5 [749.595508ms]
Mar 24 02:35:48.865: INFO: Created: latency-svc-rqjvq
Mar 24 02:35:48.907: INFO: Got endpoints: latency-svc-j7nqg [749.82701ms]
Mar 24 02:35:48.914: INFO: Created: latency-svc-wv9pv
Mar 24 02:35:48.956: INFO: Got endpoints: latency-svc-qkfjp [748.9304ms]
Mar 24 02:35:48.964: INFO: Created: latency-svc-dzfg4
Mar 24 02:35:49.007: INFO: Got endpoints: latency-svc-d6pgx [749.181367ms]
Mar 24 02:35:49.015: INFO: Created: latency-svc-556zx
Mar 24 02:35:49.058: INFO: Got endpoints: latency-svc-956zp [751.176656ms]
Mar 24 02:35:49.066: INFO: Created: latency-svc-f5vp4
Mar 24 02:35:49.107: INFO: Got endpoints: latency-svc-w9qht [749.72168ms]
Mar 24 02:35:49.113: INFO: Created: latency-svc-rkbpw
Mar 24 02:35:49.157: INFO: Got endpoints: latency-svc-n27dn [750.618336ms]
Mar 24 02:35:49.166: INFO: Created: latency-svc-xnlvm
Mar 24 02:35:49.207: INFO: Got endpoints: latency-svc-8nxjq [750.884201ms]
Mar 24 02:35:49.215: INFO: Created: latency-svc-n98vp
Mar 24 02:35:49.257: INFO: Got endpoints: latency-svc-q98np [749.239184ms]
Mar 24 02:35:49.265: INFO: Created: latency-svc-6tx7h
Mar 24 02:35:49.307: INFO: Got endpoints: latency-svc-pvb8f [749.316265ms]
Mar 24 02:35:49.314: INFO: Created: latency-svc-kbw8g
Mar 24 02:35:49.357: INFO: Got endpoints: latency-svc-6zlzl [750.47712ms]
Mar 24 02:35:49.365: INFO: Created: latency-svc-pmjcx
Mar 24 02:35:49.406: INFO: Got endpoints: latency-svc-6478x [749.192924ms]
Mar 24 02:35:49.413: INFO: Created: latency-svc-4f5xr
Mar 24 02:35:49.458: INFO: Got endpoints: latency-svc-lm2gq [751.724234ms]
Mar 24 02:35:49.465: INFO: Created: latency-svc-4857p
Mar 24 02:35:49.507: INFO: Got endpoints: latency-svc-gk4sw [749.924272ms]
Mar 24 02:35:49.515: INFO: Created: latency-svc-kmvsk
Mar 24 02:35:49.557: INFO: Got endpoints: latency-svc-qhdsh [749.856154ms]
Mar 24 02:35:49.567: INFO: Created: latency-svc-bqmpt
Mar 24 02:35:49.608: INFO: Got endpoints: latency-svc-rqjvq [750.231742ms]
Mar 24 02:35:49.616: INFO: Created: latency-svc-m5w6t
Mar 24 02:35:49.657: INFO: Got endpoints: latency-svc-wv9pv [750.725091ms]
Mar 24 02:35:49.665: INFO: Created: latency-svc-m7nb4
Mar 24 02:35:49.706: INFO: Got endpoints: latency-svc-dzfg4 [750.110526ms]
Mar 24 02:35:49.713: INFO: Created: latency-svc-4h6cc
Mar 24 02:35:49.758: INFO: Got endpoints: latency-svc-556zx [751.188695ms]
Mar 24 02:35:49.766: INFO: Created: latency-svc-zxq9v
Mar 24 02:35:49.807: INFO: Got endpoints: latency-svc-f5vp4 [749.069692ms]
Mar 24 02:35:49.815: INFO: Created: latency-svc-gkmgb
Mar 24 02:35:49.857: INFO: Got endpoints: latency-svc-rkbpw [750.569412ms]
Mar 24 02:35:49.864: INFO: Created: latency-svc-r2jjp
Mar 24 02:35:49.907: INFO: Got endpoints: latency-svc-xnlvm [750.086636ms]
Mar 24 02:35:49.915: INFO: Created: latency-svc-xr28q
Mar 24 02:35:49.957: INFO: Got endpoints: latency-svc-n98vp [749.930584ms]
Mar 24 02:35:49.965: INFO: Created: latency-svc-vdp5g
Mar 24 02:35:50.006: INFO: Got endpoints: latency-svc-6tx7h [749.055352ms]
Mar 24 02:35:50.014: INFO: Created: latency-svc-pm7cs
Mar 24 02:35:50.057: INFO: Got endpoints: latency-svc-kbw8g [749.973351ms]
Mar 24 02:35:50.064: INFO: Created: latency-svc-lcmr8
Mar 24 02:35:50.107: INFO: Got endpoints: latency-svc-pmjcx [750.516865ms]
Mar 24 02:35:50.121: INFO: Created: latency-svc-7kf6t
Mar 24 02:35:50.156: INFO: Got endpoints: latency-svc-4f5xr [750.314279ms]
Mar 24 02:35:50.164: INFO: Created: latency-svc-mdhsh
Mar 24 02:35:50.208: INFO: Got endpoints: latency-svc-4857p [749.674029ms]
Mar 24 02:35:50.215: INFO: Created: latency-svc-hqsqv
Mar 24 02:35:50.258: INFO: Got endpoints: latency-svc-kmvsk [751.030645ms]
Mar 24 02:35:50.266: INFO: Created: latency-svc-hvp26
Mar 24 02:35:50.308: INFO: Got endpoints: latency-svc-bqmpt [750.612996ms]
Mar 24 02:35:50.314: INFO: Created: latency-svc-xbqrp
Mar 24 02:35:50.358: INFO: Got endpoints: latency-svc-m5w6t [749.879677ms]
Mar 24 02:35:50.366: INFO: Created: latency-svc-9m77h
Mar 24 02:35:50.406: INFO: Got endpoints: latency-svc-m7nb4 [748.905212ms]
Mar 24 02:35:50.415: INFO: Created: latency-svc-4wmsl
Mar 24 02:35:50.456: INFO: Got endpoints: latency-svc-4h6cc [750.089068ms]
Mar 24 02:35:50.464: INFO: Created: latency-svc-225ng
Mar 24 02:35:50.509: INFO: Got endpoints: latency-svc-zxq9v [750.875691ms]
Mar 24 02:35:50.517: INFO: Created: latency-svc-sc4cz
Mar 24 02:35:50.557: INFO: Got endpoints: latency-svc-gkmgb [749.439871ms]
Mar 24 02:35:50.567: INFO: Created: latency-svc-tlgmq
Mar 24 02:35:50.607: INFO: Got endpoints: latency-svc-r2jjp [749.273098ms]
Mar 24 02:35:50.614: INFO: Created: latency-svc-8p58m
Mar 24 02:35:50.657: INFO: Got endpoints: latency-svc-xr28q [749.346187ms]
Mar 24 02:35:50.664: INFO: Created: latency-svc-v7kbq
Mar 24 02:35:50.707: INFO: Got endpoints: latency-svc-vdp5g [749.893406ms]
Mar 24 02:35:50.714: INFO: Created: latency-svc-c8nlg
Mar 24 02:35:50.757: INFO: Got endpoints: latency-svc-pm7cs [750.735182ms]
Mar 24 02:35:50.765: INFO: Created: latency-svc-j875s
Mar 24 02:35:50.808: INFO: Got endpoints: latency-svc-lcmr8 [751.393315ms]
Mar 24 02:35:50.816: INFO: Created: latency-svc-nvm4v
Mar 24 02:35:50.857: INFO: Got endpoints: latency-svc-7kf6t [749.763776ms]
Mar 24 02:35:50.865: INFO: Created: latency-svc-bmd9x
Mar 24 02:35:50.906: INFO: Got endpoints: latency-svc-mdhsh [749.872806ms]
Mar 24 02:35:50.914: INFO: Created: latency-svc-qw87h
Mar 24 02:35:50.966: INFO: Got endpoints: latency-svc-hqsqv [758.338393ms]
Mar 24 02:35:50.975: INFO: Created: latency-svc-hzswn
Mar 24 02:35:51.006: INFO: Got endpoints: latency-svc-hvp26 [747.927774ms]
Mar 24 02:35:51.013: INFO: Created: latency-svc-g6k2n
Mar 24 02:35:51.057: INFO: Got endpoints: latency-svc-xbqrp [749.554868ms]
Mar 24 02:35:51.065: INFO: Created: latency-svc-hv5kr
Mar 24 02:35:51.107: INFO: Got endpoints: latency-svc-9m77h [749.302368ms]
Mar 24 02:35:51.115: INFO: Created: latency-svc-sf2fc
Mar 24 02:35:51.157: INFO: Got endpoints: latency-svc-4wmsl [750.986629ms]
Mar 24 02:35:51.165: INFO: Created: latency-svc-wq427
Mar 24 02:35:51.207: INFO: Got endpoints: latency-svc-225ng [750.705113ms]
Mar 24 02:35:51.214: INFO: Created: latency-svc-gb4d7
Mar 24 02:35:51.257: INFO: Got endpoints: latency-svc-sc4cz [748.273687ms]
Mar 24 02:35:51.269: INFO: Created: latency-svc-9lcpj
Mar 24 02:35:51.306: INFO: Got endpoints: latency-svc-tlgmq [749.444586ms]
Mar 24 02:35:51.314: INFO: Created: latency-svc-47psq
Mar 24 02:35:51.359: INFO: Got endpoints: latency-svc-8p58m [752.611617ms]
Mar 24 02:35:51.367: INFO: Created: latency-svc-5lgjf
Mar 24 02:35:51.407: INFO: Got endpoints: latency-svc-v7kbq [750.102025ms]
Mar 24 02:35:51.415: INFO: Created: latency-svc-6rzfx
Mar 24 02:35:51.457: INFO: Got endpoints: latency-svc-c8nlg [749.447598ms]
Mar 24 02:35:51.465: INFO: Created: latency-svc-dlkk9
Mar 24 02:35:51.506: INFO: Got endpoints: latency-svc-j875s [748.99478ms]
Mar 24 02:35:51.514: INFO: Created: latency-svc-gmb6h
Mar 24 02:35:51.557: INFO: Got endpoints: latency-svc-nvm4v [748.78465ms]
Mar 24 02:35:51.564: INFO: Created: latency-svc-c2nt2
Mar 24 02:35:51.607: INFO: Got endpoints: latency-svc-bmd9x [750.300655ms]
Mar 24 02:35:51.615: INFO: Created: latency-svc-62rbr
Mar 24 02:35:51.657: INFO: Got endpoints: latency-svc-qw87h [750.964458ms]
Mar 24 02:35:51.665: INFO: Created: latency-svc-kt5kg
Mar 24 02:35:51.707: INFO: Got endpoints: latency-svc-hzswn [740.880307ms]
Mar 24 02:35:51.716: INFO: Created: latency-svc-89s65
Mar 24 02:35:51.757: INFO: Got endpoints: latency-svc-g6k2n [750.919822ms]
Mar 24 02:35:51.765: INFO: Created: latency-svc-9lp8z
Mar 24 02:35:51.807: INFO: Got endpoints: latency-svc-hv5kr [749.896772ms]
Mar 24 02:35:51.815: INFO: Created: latency-svc-rzhzd
Mar 24 02:35:51.858: INFO: Got endpoints: latency-svc-sf2fc [750.635455ms]
Mar 24 02:35:51.865: INFO: Created: latency-svc-s56dw
Mar 24 02:35:51.907: INFO: Got endpoints: latency-svc-wq427 [749.297068ms]
Mar 24 02:35:51.914: INFO: Created: latency-svc-fpvcr
Mar 24 02:35:51.956: INFO: Got endpoints: latency-svc-gb4d7 [749.056624ms]
Mar 24 02:35:51.964: INFO: Created: latency-svc-wpcpj
Mar 24 02:35:52.007: INFO: Got endpoints: latency-svc-9lcpj [749.2474ms]
Mar 24 02:35:52.014: INFO: Created: latency-svc-5sj9k
Mar 24 02:35:52.059: INFO: Got endpoints: latency-svc-47psq [752.797225ms]
Mar 24 02:35:52.067: INFO: Created: latency-svc-tctjt
Mar 24 02:35:52.106: INFO: Got endpoints: latency-svc-5lgjf [747.257169ms]
Mar 24 02:35:52.114: INFO: Created: latency-svc-4fx29
Mar 24 02:35:52.156: INFO: Got endpoints: latency-svc-6rzfx [749.197946ms]
Mar 24 02:35:52.165: INFO: Created: latency-svc-zpk6h
Mar 24 02:35:52.207: INFO: Got endpoints: latency-svc-dlkk9 [750.690822ms]
Mar 24 02:35:52.215: INFO: Created: latency-svc-kw5gp
Mar 24 02:35:52.257: INFO: Got endpoints: latency-svc-gmb6h [751.069936ms]
Mar 24 02:35:52.264: INFO: Created: latency-svc-2cqx8
Mar 24 02:35:52.306: INFO: Got endpoints: latency-svc-c2nt2 [749.496491ms]
Mar 24 02:35:52.314: INFO: Created: latency-svc-d4f44
Mar 24 02:35:52.356: INFO: Got endpoints: latency-svc-62rbr [749.007695ms]
Mar 24 02:35:52.364: INFO: Created: latency-svc-jxkl8
Mar 24 02:35:52.406: INFO: Got endpoints: latency-svc-kt5kg [749.009807ms]
Mar 24 02:35:52.414: INFO: Created: latency-svc-fzpjp
Mar 24 02:35:52.457: INFO: Got endpoints: latency-svc-89s65 [749.899333ms]
Mar 24 02:35:52.465: INFO: Created: latency-svc-t5x2c
Mar 24 02:35:52.508: INFO: Got endpoints: latency-svc-9lp8z [750.778449ms]
Mar 24 02:35:52.515: INFO: Created: latency-svc-c8r9k
Mar 24 02:35:52.556: INFO: Got endpoints: latency-svc-rzhzd [748.922279ms]
Mar 24 02:35:52.563: INFO: Created: latency-svc-gw9cp
Mar 24 02:35:52.607: INFO: Got endpoints: latency-svc-s56dw [749.776085ms]
Mar 24 02:35:52.615: INFO: Created: latency-svc-j8h42
Mar 24 02:35:52.657: INFO: Got endpoints: latency-svc-fpvcr [750.706242ms]
Mar 24 02:35:52.665: INFO: Created: latency-svc-tgwq7
Mar 24 02:35:52.707: INFO: Got endpoints: latency-svc-wpcpj [751.038865ms]
Mar 24 02:35:52.714: INFO: Created: latency-svc-s6vgn
Mar 24 02:35:52.756: INFO: Got endpoints: latency-svc-5sj9k [749.800523ms]
Mar 24 02:35:52.764: INFO: Created: latency-svc-44c98
Mar 24 02:35:52.806: INFO: Got endpoints: latency-svc-tctjt [747.222312ms]
Mar 24 02:35:52.814: INFO: Created: latency-svc-55xnw
Mar 24 02:35:52.858: INFO: Got endpoints: latency-svc-4fx29 [751.104327ms]
Mar 24 02:35:52.865: INFO: Created: latency-svc-rc2gs
Mar 24 02:35:52.907: INFO: Got endpoints: latency-svc-zpk6h [750.836261ms]
Mar 24 02:35:52.916: INFO: Created: latency-svc-6wnqn
Mar 24 02:35:52.957: INFO: Got endpoints: latency-svc-kw5gp [750.026733ms]
Mar 24 02:35:52.965: INFO: Created: latency-svc-q85gb
Mar 24 02:35:53.007: INFO: Got endpoints: latency-svc-2cqx8 [749.628357ms]
Mar 24 02:35:53.014: INFO: Created: latency-svc-bkgtw
Mar 24 02:35:53.057: INFO: Got endpoints: latency-svc-d4f44 [750.747964ms]
Mar 24 02:35:53.064: INFO: Created: latency-svc-vc2pk
Mar 24 02:35:53.108: INFO: Got endpoints: latency-svc-jxkl8 [751.397557ms]
Mar 24 02:35:53.115: INFO: Created: latency-svc-zhx9h
Mar 24 02:35:53.156: INFO: Got endpoints: latency-svc-fzpjp [749.546976ms]
Mar 24 02:35:53.164: INFO: Created: latency-svc-cddxb
Mar 24 02:35:53.207: INFO: Got endpoints: latency-svc-t5x2c [749.295709ms]
Mar 24 02:35:53.216: INFO: Created: latency-svc-fwtz4
Mar 24 02:35:53.257: INFO: Got endpoints: latency-svc-c8r9k [749.106805ms]
Mar 24 02:35:53.265: INFO: Created: latency-svc-ch48t
Mar 24 02:35:53.306: INFO: Got endpoints: latency-svc-gw9cp [750.223106ms]
Mar 24 02:35:53.320: INFO: Created: latency-svc-jkm58
Mar 24 02:35:53.357: INFO: Got endpoints: latency-svc-j8h42 [749.43673ms]
Mar 24 02:35:53.365: INFO: Created: latency-svc-bq6w8
Mar 24 02:35:53.407: INFO: Got endpoints: latency-svc-tgwq7 [749.226816ms]
Mar 24 02:35:53.415: INFO: Created: latency-svc-g28sn
Mar 24 02:35:53.456: INFO: Got endpoints: latency-svc-s6vgn [749.30192ms]
Mar 24 02:35:53.464: INFO: Created: latency-svc-ncwc9
Mar 24 02:35:53.507: INFO: Got endpoints: latency-svc-44c98 [750.451126ms]
Mar 24 02:35:53.521: INFO: Created: latency-svc-svzcb
Mar 24 02:35:53.557: INFO: Got endpoints: latency-svc-55xnw [750.931334ms]
Mar 24 02:35:53.567: INFO: Created: latency-svc-jp4wc
Mar 24 02:35:53.608: INFO: Got endpoints: latency-svc-rc2gs [750.60058ms]
Mar 24 02:35:53.615: INFO: Created: latency-svc-8l6rl
Mar 24 02:35:53.658: INFO: Got endpoints: latency-svc-6wnqn [750.418617ms]
Mar 24 02:35:53.665: INFO: Created: latency-svc-t84fz
Mar 24 02:35:53.707: INFO: Got endpoints: latency-svc-q85gb [749.806233ms]
Mar 24 02:35:53.756: INFO: Got endpoints: latency-svc-bkgtw [749.15267ms]
Mar 24 02:35:53.807: INFO: Got endpoints: latency-svc-vc2pk [749.882718ms]
Mar 24 02:35:53.858: INFO: Got endpoints: latency-svc-zhx9h [749.614949ms]
Mar 24 02:35:53.906: INFO: Got endpoints: latency-svc-cddxb [750.42062ms]
Mar 24 02:35:53.958: INFO: Got endpoints: latency-svc-fwtz4 [750.956138ms]
Mar 24 02:35:54.007: INFO: Got endpoints: latency-svc-ch48t [749.242306ms]
Mar 24 02:35:54.057: INFO: Got endpoints: latency-svc-jkm58 [750.233376ms]
Mar 24 02:35:54.108: INFO: Got endpoints: latency-svc-bq6w8 [750.723231ms]
Mar 24 02:35:54.157: INFO: Got endpoints: latency-svc-g28sn [749.831085ms]
Mar 24 02:35:54.206: INFO: Got endpoints: latency-svc-ncwc9 [749.637069ms]
Mar 24 02:35:54.257: INFO: Got endpoints: latency-svc-svzcb [749.516055ms]
Mar 24 02:35:54.308: INFO: Got endpoints: latency-svc-jp4wc [750.190363ms]
Mar 24 02:35:54.356: INFO: Got endpoints: latency-svc-8l6rl [747.914886ms]
Mar 24 02:35:54.407: INFO: Got endpoints: latency-svc-t84fz [749.439988ms]
Mar 24 02:35:54.407: INFO: Latencies: [11.096949ms 12.554517ms 16.359414ms 22.628451ms 35.0484ms 42.200941ms 47.525196ms 67.910734ms 73.784291ms 79.933663ms 83.420027ms 90.101758ms 90.223292ms 90.467829ms 90.493095ms 90.691725ms 91.813216ms 91.910491ms 91.930965ms 91.99757ms 92.130103ms 92.232693ms 94.107125ms 97.606997ms 98.627096ms 99.434497ms 101.438718ms 101.78841ms 102.931343ms 108.283793ms 108.436443ms 110.927628ms 115.096138ms 118.994304ms 165.858762ms 208.379976ms 241.751292ms 290.733834ms 333.799556ms 377.09847ms 422.525171ms 467.085593ms 510.693396ms 555.910372ms 600.213149ms 643.156888ms 687.794978ms 733.70606ms 740.880307ms 747.222312ms 747.257169ms 747.914886ms 747.927774ms 748.273687ms 748.5884ms 748.738972ms 748.78465ms 748.804405ms 748.85859ms 748.882417ms 748.89854ms 748.905212ms 748.922279ms 748.926285ms 748.9304ms 748.99478ms 749.007695ms 749.009807ms 749.055352ms 749.056624ms 749.069692ms 749.103473ms 749.106805ms 749.15267ms 749.173686ms 749.181367ms 749.192924ms 749.197946ms 749.226816ms 749.239184ms 749.242306ms 749.2474ms 749.265633ms 749.273098ms 749.295709ms 749.297068ms 749.30192ms 749.302368ms 749.316265ms 749.318717ms 749.346187ms 749.43673ms 749.439871ms 749.439988ms 749.444586ms 749.447598ms 749.456958ms 749.496491ms 749.516055ms 749.546976ms 749.554868ms 749.595508ms 749.614949ms 749.627388ms 749.628357ms 749.637069ms 749.656305ms 749.674029ms 749.72168ms 749.763776ms 749.776085ms 749.800523ms 749.806233ms 749.82701ms 749.830146ms 749.831085ms 749.856154ms 749.863758ms 749.872806ms 749.879677ms 749.882718ms 749.893406ms 749.896772ms 749.897958ms 749.899333ms 749.924272ms 749.930584ms 749.973351ms 750.026733ms 750.086636ms 750.089068ms 750.102025ms 750.110526ms 750.156777ms 750.190363ms 750.205926ms 750.223106ms 750.231742ms 750.233376ms 750.279438ms 750.300655ms 750.306155ms 750.313892ms 750.314279ms 750.319387ms 750.337564ms 750.378456ms 750.412235ms 750.418617ms 750.42062ms 750.421276ms 750.446725ms 750.451126ms 750.47712ms 750.503312ms 750.516865ms 750.546511ms 750.569412ms 750.60058ms 750.612996ms 750.618336ms 750.635455ms 750.661814ms 750.690822ms 750.705113ms 750.706242ms 750.723231ms 750.725091ms 750.735182ms 750.747964ms 750.778449ms 750.782521ms 750.804615ms 750.807138ms 750.836261ms 750.869911ms 750.875691ms 750.884201ms 750.919822ms 750.931334ms 750.956138ms 750.96244ms 750.964458ms 750.986629ms 751.030645ms 751.038865ms 751.060801ms 751.069936ms 751.104327ms 751.176656ms 751.188695ms 751.225925ms 751.393315ms 751.397557ms 751.449654ms 751.724234ms 752.022475ms 752.611617ms 752.797225ms 758.338393ms]
Mar 24 02:35:54.407: INFO: 50 %ile: 749.554868ms
Mar 24 02:35:54.407: INFO: 90 %ile: 750.956138ms
Mar 24 02:35:54.407: INFO: 99 %ile: 752.797225ms
Mar 24 02:35:54.407: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:35:54.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6895" for this suite.

â€¢ [SLOW TEST:10.861 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":54,"skipped":732,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:35:54.419: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-0c60a227-3e23-43e6-82fa-17900c82d5ba
STEP: Creating a pod to test consume secrets
Mar 24 02:35:54.557: INFO: Waiting up to 5m0s for pod "pod-secrets-d7b169b9-9565-4d82-9868-6e7da6aef144" in namespace "secrets-4846" to be "Succeeded or Failed"
Mar 24 02:35:54.559: INFO: Pod "pod-secrets-d7b169b9-9565-4d82-9868-6e7da6aef144": Phase="Pending", Reason="", readiness=false. Elapsed: 1.839056ms
Mar 24 02:35:56.566: INFO: Pod "pod-secrets-d7b169b9-9565-4d82-9868-6e7da6aef144": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008178908s
STEP: Saw pod success
Mar 24 02:35:56.566: INFO: Pod "pod-secrets-d7b169b9-9565-4d82-9868-6e7da6aef144" satisfied condition "Succeeded or Failed"
Mar 24 02:35:56.568: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-secrets-d7b169b9-9565-4d82-9868-6e7da6aef144 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 02:35:56.583: INFO: Waiting for pod pod-secrets-d7b169b9-9565-4d82-9868-6e7da6aef144 to disappear
Mar 24 02:35:56.584: INFO: Pod pod-secrets-d7b169b9-9565-4d82-9868-6e7da6aef144 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:35:56.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4846" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":55,"skipped":747,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:35:56.591: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9285
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9285
STEP: creating service affinity-clusterip in namespace services-9285
STEP: creating replication controller affinity-clusterip in namespace services-9285
I0324 02:35:56.728023      21 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-9285, replica count: 3
I0324 02:35:59.778321      21 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 02:35:59.785: INFO: Creating new exec pod
Mar 24 02:36:02.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-9285 exec execpod-affinitybjt4g -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Mar 24 02:36:02.964: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar 24 02:36:02.964: INFO: stdout: ""
Mar 24 02:36:02.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-9285 exec execpod-affinitybjt4g -- /bin/sh -x -c nc -zv -t -w 2 172.16.25.173 80'
Mar 24 02:36:03.105: INFO: stderr: "+ nc -zv -t -w 2 172.16.25.173 80\nConnection to 172.16.25.173 80 port [tcp/http] succeeded!\n"
Mar 24 02:36:03.105: INFO: stdout: ""
Mar 24 02:36:03.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-9285 exec execpod-affinitybjt4g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.25.173:80/ ; done'
Mar 24 02:36:03.296: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.25.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.25.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.25.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.25.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.25.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.25.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.25.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.25.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.25.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.25.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.25.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.25.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.25.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.25.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.25.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.25.173:80/\n"
Mar 24 02:36:03.296: INFO: stdout: "\naffinity-clusterip-9llc9\naffinity-clusterip-9llc9\naffinity-clusterip-9llc9\naffinity-clusterip-9llc9\naffinity-clusterip-9llc9\naffinity-clusterip-9llc9\naffinity-clusterip-9llc9\naffinity-clusterip-9llc9\naffinity-clusterip-9llc9\naffinity-clusterip-9llc9\naffinity-clusterip-9llc9\naffinity-clusterip-9llc9\naffinity-clusterip-9llc9\naffinity-clusterip-9llc9\naffinity-clusterip-9llc9\naffinity-clusterip-9llc9"
Mar 24 02:36:03.296: INFO: Received response from host: affinity-clusterip-9llc9
Mar 24 02:36:03.296: INFO: Received response from host: affinity-clusterip-9llc9
Mar 24 02:36:03.296: INFO: Received response from host: affinity-clusterip-9llc9
Mar 24 02:36:03.296: INFO: Received response from host: affinity-clusterip-9llc9
Mar 24 02:36:03.296: INFO: Received response from host: affinity-clusterip-9llc9
Mar 24 02:36:03.296: INFO: Received response from host: affinity-clusterip-9llc9
Mar 24 02:36:03.296: INFO: Received response from host: affinity-clusterip-9llc9
Mar 24 02:36:03.296: INFO: Received response from host: affinity-clusterip-9llc9
Mar 24 02:36:03.296: INFO: Received response from host: affinity-clusterip-9llc9
Mar 24 02:36:03.296: INFO: Received response from host: affinity-clusterip-9llc9
Mar 24 02:36:03.296: INFO: Received response from host: affinity-clusterip-9llc9
Mar 24 02:36:03.296: INFO: Received response from host: affinity-clusterip-9llc9
Mar 24 02:36:03.296: INFO: Received response from host: affinity-clusterip-9llc9
Mar 24 02:36:03.296: INFO: Received response from host: affinity-clusterip-9llc9
Mar 24 02:36:03.296: INFO: Received response from host: affinity-clusterip-9llc9
Mar 24 02:36:03.296: INFO: Received response from host: affinity-clusterip-9llc9
Mar 24 02:36:03.296: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9285, will wait for the garbage collector to delete the pods
Mar 24 02:36:03.360: INFO: Deleting ReplicationController affinity-clusterip took: 4.41309ms
Mar 24 02:36:04.060: INFO: Terminating ReplicationController affinity-clusterip pods took: 700.147317ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:36:12.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9285" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

â€¢ [SLOW TEST:15.996 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":56,"skipped":756,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:36:12.587: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5813
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 24 02:36:15.239: INFO: Successfully updated pod "pod-update-activedeadlineseconds-f6093cf0-ce1e-46cb-b9ed-8f1b032a39d4"
Mar 24 02:36:15.239: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-f6093cf0-ce1e-46cb-b9ed-8f1b032a39d4" in namespace "pods-5813" to be "terminated due to deadline exceeded"
Mar 24 02:36:15.242: INFO: Pod "pod-update-activedeadlineseconds-f6093cf0-ce1e-46cb-b9ed-8f1b032a39d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.294964ms
Mar 24 02:36:17.247: INFO: Pod "pod-update-activedeadlineseconds-f6093cf0-ce1e-46cb-b9ed-8f1b032a39d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007983982s
Mar 24 02:36:19.252: INFO: Pod "pod-update-activedeadlineseconds-f6093cf0-ce1e-46cb-b9ed-8f1b032a39d4": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.012475053s
Mar 24 02:36:19.252: INFO: Pod "pod-update-activedeadlineseconds-f6093cf0-ce1e-46cb-b9ed-8f1b032a39d4" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:36:19.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5813" for this suite.

â€¢ [SLOW TEST:6.673 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":57,"skipped":762,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:36:19.261: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-667
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3116
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6461
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:36:25.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-667" for this suite.
STEP: Destroying namespace "nsdeletetest-3116" for this suite.
Mar 24 02:36:25.669: INFO: Namespace nsdeletetest-3116 was already deleted
STEP: Destroying namespace "nsdeletetest-6461" for this suite.

â€¢ [SLOW TEST:6.412 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":58,"skipped":793,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:36:25.672: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8013
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-ec5d60db-401c-4982-a959-10b70926e408
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:36:27.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8013" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":59,"skipped":820,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:36:27.837: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 02:36:27.969: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a83c59ab-5335-4b07-aaf4-d50b48996c68" in namespace "projected-799" to be "Succeeded or Failed"
Mar 24 02:36:27.971: INFO: Pod "downwardapi-volume-a83c59ab-5335-4b07-aaf4-d50b48996c68": Phase="Pending", Reason="", readiness=false. Elapsed: 1.882967ms
Mar 24 02:36:29.975: INFO: Pod "downwardapi-volume-a83c59ab-5335-4b07-aaf4-d50b48996c68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006351199s
STEP: Saw pod success
Mar 24 02:36:29.975: INFO: Pod "downwardapi-volume-a83c59ab-5335-4b07-aaf4-d50b48996c68" satisfied condition "Succeeded or Failed"
Mar 24 02:36:29.977: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod downwardapi-volume-a83c59ab-5335-4b07-aaf4-d50b48996c68 container client-container: <nil>
STEP: delete the pod
Mar 24 02:36:29.990: INFO: Waiting for pod downwardapi-volume-a83c59ab-5335-4b07-aaf4-d50b48996c68 to disappear
Mar 24 02:36:29.992: INFO: Pod downwardapi-volume-a83c59ab-5335-4b07-aaf4-d50b48996c68 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:36:29.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-799" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":60,"skipped":821,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:36:29.997: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7202
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:36:30.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-7202 version'
Mar 24 02:36:30.178: INFO: stderr: ""
Mar 24 02:36:30.178: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.4\", GitCommit:\"e87da0bd6e03ec3fea7933c4b5263d151aafd07c\", GitTreeState:\"clean\", BuildDate:\"2021-02-18T16:12:00Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20+\", GitVersion:\"v1.20.4-aliyun.1\", GitCommit:\"70b68c4\", GitTreeState:\"\", BuildDate:\"2021-03-15T08:30:07Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:36:30.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7202" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":61,"skipped":822,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:36:30.185: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4854
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 02:36:30.318: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8aeab30-7897-4d2c-a0d1-f7665161ab47" in namespace "downward-api-4854" to be "Succeeded or Failed"
Mar 24 02:36:30.320: INFO: Pod "downwardapi-volume-f8aeab30-7897-4d2c-a0d1-f7665161ab47": Phase="Pending", Reason="", readiness=false. Elapsed: 1.848528ms
Mar 24 02:36:32.326: INFO: Pod "downwardapi-volume-f8aeab30-7897-4d2c-a0d1-f7665161ab47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007524863s
STEP: Saw pod success
Mar 24 02:36:32.326: INFO: Pod "downwardapi-volume-f8aeab30-7897-4d2c-a0d1-f7665161ab47" satisfied condition "Succeeded or Failed"
Mar 24 02:36:32.328: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod downwardapi-volume-f8aeab30-7897-4d2c-a0d1-f7665161ab47 container client-container: <nil>
STEP: delete the pod
Mar 24 02:36:32.341: INFO: Waiting for pod downwardapi-volume-f8aeab30-7897-4d2c-a0d1-f7665161ab47 to disappear
Mar 24 02:36:32.343: INFO: Pod downwardapi-volume-f8aeab30-7897-4d2c-a0d1-f7665161ab47 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:36:32.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4854" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":62,"skipped":837,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:36:32.348: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7050
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 02:36:33.060: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 24 02:36:35.070: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752150193, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752150193, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752150193, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752150193, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 02:36:38.085: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:36:38.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7050" for this suite.
STEP: Destroying namespace "webhook-7050-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

â€¢ [SLOW TEST:5.832 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":63,"skipped":842,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:36:38.181: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8615
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-a1e55d73-7586-402e-a81e-6092c46512c0 in namespace container-probe-8615
Mar 24 02:36:40.322: INFO: Started pod liveness-a1e55d73-7586-402e-a81e-6092c46512c0 in namespace container-probe-8615
STEP: checking the pod's current state and verifying that restartCount is present
Mar 24 02:36:40.324: INFO: Initial restart count of pod liveness-a1e55d73-7586-402e-a81e-6092c46512c0 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:40:41.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8615" for this suite.

â€¢ [SLOW TEST:243.038 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":64,"skipped":860,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:40:41.219: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5196
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 02:40:41.594: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 02:40:44.610: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:40:44.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5196" for this suite.
STEP: Destroying namespace "webhook-5196-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":65,"skipped":879,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:40:44.699: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2466
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 24 02:40:44.835: INFO: Waiting up to 5m0s for pod "pod-0c642f49-bb8d-482e-be52-7bc5e6b3e9b2" in namespace "emptydir-2466" to be "Succeeded or Failed"
Mar 24 02:40:44.836: INFO: Pod "pod-0c642f49-bb8d-482e-be52-7bc5e6b3e9b2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.710816ms
Mar 24 02:40:46.842: INFO: Pod "pod-0c642f49-bb8d-482e-be52-7bc5e6b3e9b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007069693s
STEP: Saw pod success
Mar 24 02:40:46.842: INFO: Pod "pod-0c642f49-bb8d-482e-be52-7bc5e6b3e9b2" satisfied condition "Succeeded or Failed"
Mar 24 02:40:46.844: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-0c642f49-bb8d-482e-be52-7bc5e6b3e9b2 container test-container: <nil>
STEP: delete the pod
Mar 24 02:40:46.865: INFO: Waiting for pod pod-0c642f49-bb8d-482e-be52-7bc5e6b3e9b2 to disappear
Mar 24 02:40:46.867: INFO: Pod pod-0c642f49-bb8d-482e-be52-7bc5e6b3e9b2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:40:46.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2466" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":66,"skipped":886,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:40:46.873: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7948
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar 24 02:40:47.005: INFO: Waiting up to 5m0s for pod "downward-api-4921563f-5eaf-4f91-a1ea-fd95dfc2b0e3" in namespace "downward-api-7948" to be "Succeeded or Failed"
Mar 24 02:40:47.006: INFO: Pod "downward-api-4921563f-5eaf-4f91-a1ea-fd95dfc2b0e3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.697455ms
Mar 24 02:40:49.011: INFO: Pod "downward-api-4921563f-5eaf-4f91-a1ea-fd95dfc2b0e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006695311s
STEP: Saw pod success
Mar 24 02:40:49.011: INFO: Pod "downward-api-4921563f-5eaf-4f91-a1ea-fd95dfc2b0e3" satisfied condition "Succeeded or Failed"
Mar 24 02:40:49.013: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod downward-api-4921563f-5eaf-4f91-a1ea-fd95dfc2b0e3 container dapi-container: <nil>
STEP: delete the pod
Mar 24 02:40:49.028: INFO: Waiting for pod downward-api-4921563f-5eaf-4f91-a1ea-fd95dfc2b0e3 to disappear
Mar 24 02:40:49.030: INFO: Pod downward-api-4921563f-5eaf-4f91-a1ea-fd95dfc2b0e3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:40:49.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7948" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":67,"skipped":906,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:40:49.036: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-590c60e6-f6a3-4966-a9f5-6512e71b5590
STEP: Creating a pod to test consume configMaps
Mar 24 02:40:49.171: INFO: Waiting up to 5m0s for pod "pod-configmaps-5151fabc-e33a-41da-a738-adec6442a692" in namespace "configmap-3961" to be "Succeeded or Failed"
Mar 24 02:40:49.173: INFO: Pod "pod-configmaps-5151fabc-e33a-41da-a738-adec6442a692": Phase="Pending", Reason="", readiness=false. Elapsed: 1.921776ms
Mar 24 02:40:51.180: INFO: Pod "pod-configmaps-5151fabc-e33a-41da-a738-adec6442a692": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009299908s
STEP: Saw pod success
Mar 24 02:40:51.180: INFO: Pod "pod-configmaps-5151fabc-e33a-41da-a738-adec6442a692" satisfied condition "Succeeded or Failed"
Mar 24 02:40:51.182: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-configmaps-5151fabc-e33a-41da-a738-adec6442a692 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 02:40:51.195: INFO: Waiting for pod pod-configmaps-5151fabc-e33a-41da-a738-adec6442a692 to disappear
Mar 24 02:40:51.196: INFO: Pod pod-configmaps-5151fabc-e33a-41da-a738-adec6442a692 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:40:51.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3961" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":68,"skipped":925,"failed":0}
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:40:51.202: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-555
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:40:53.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-555" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":69,"skipped":931,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:40:53.361: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7123
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 02:40:53.987: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 24 02:40:55.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752150454, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752150454, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752150454, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752150453, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 02:40:59.014: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:40:59.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7123" for this suite.
STEP: Destroying namespace "webhook-7123-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

â€¢ [SLOW TEST:5.717 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":70,"skipped":994,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:40:59.078: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6160
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Mar 24 02:40:59.205: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-6160 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:40:59.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6160" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":71,"skipped":998,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:40:59.256: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-1394
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar 24 02:40:59.390: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 24 02:41:59.423: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Mar 24 02:41:59.438: INFO: Created pod: pod0-sched-preemption-low-priority
Mar 24 02:41:59.481: INFO: Created pod: pod1-sched-preemption-medium-priority
Mar 24 02:41:59.492: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:42:05.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1394" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

â€¢ [SLOW TEST:66.308 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":72,"skipped":1005,"failed":0}
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:42:05.564: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-4854
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar 24 02:42:05.697: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 24 02:43:05.730: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:43:05.732: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-2448
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Mar 24 02:43:07.886: INFO: found a healthy node: cn-hongkong.192.168.0.14
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:43:13.936: INFO: pods created so far: [1 1 1]
Mar 24 02:43:13.936: INFO: length of pods created so far: 3
Mar 24 02:43:27.946: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:43:34.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2448" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:43:34.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4854" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

â€¢ [SLOW TEST:89.453 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":73,"skipped":1005,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:43:35.017: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8701
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 24 02:43:39.195: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 02:43:39.197: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 02:43:41.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 02:43:41.201: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 02:43:43.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 02:43:43.202: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 02:43:45.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 02:43:45.203: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 02:43:47.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 02:43:47.202: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 02:43:49.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 02:43:49.203: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 02:43:51.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 02:43:51.201: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 24 02:43:53.197: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 24 02:43:53.202: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:43:53.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8701" for this suite.

â€¢ [SLOW TEST:18.193 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":74,"skipped":1078,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:43:53.210: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1848
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 02:43:53.738: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 24 02:43:55.747: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752150633, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752150633, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752150633, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752150633, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 02:43:58.763: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:44:08.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1848" for this suite.
STEP: Destroying namespace "webhook-1848-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

â€¢ [SLOW TEST:15.680 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":75,"skipped":1083,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:44:08.890: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7995
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 24 02:44:11.035: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:44:11.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7995" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":76,"skipped":1114,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:44:11.052: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5716
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5716
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-5716
Mar 24 02:44:11.190: INFO: Found 0 stateful pods, waiting for 1
Mar 24 02:44:21.195: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 24 02:44:21.207: INFO: Deleting all statefulset in ns statefulset-5716
Mar 24 02:44:21.209: INFO: Scaling statefulset ss to 0
Mar 24 02:44:41.221: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 02:44:41.223: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:44:41.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5716" for this suite.

â€¢ [SLOW TEST:30.187 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":77,"skipped":1139,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:44:41.240: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9018
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 02:44:41.695: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 02:44:44.713: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:44:44.717: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:44:50.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9018" for this suite.
STEP: Destroying namespace "webhook-9018-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

â€¢ [SLOW TEST:9.606 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":78,"skipped":1168,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:44:50.846: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8066
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Mar 24 02:44:50.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-8066 create -f -'
Mar 24 02:44:51.314: INFO: stderr: ""
Mar 24 02:44:51.314: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 24 02:44:52.319: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 24 02:44:52.319: INFO: Found 0 / 1
Mar 24 02:44:53.318: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 24 02:44:53.318: INFO: Found 1 / 1
Mar 24 02:44:53.318: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar 24 02:44:53.320: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 24 02:44:53.320: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 24 02:44:53.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-8066 patch pod agnhost-primary-htv6s -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 24 02:44:53.385: INFO: stderr: ""
Mar 24 02:44:53.385: INFO: stdout: "pod/agnhost-primary-htv6s patched\n"
STEP: checking annotations
Mar 24 02:44:53.388: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 24 02:44:53.388: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:44:53.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8066" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":79,"skipped":1170,"failed":0}

------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:44:53.394: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9204
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-9204
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9204 to expose endpoints map[]
Mar 24 02:44:53.532: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Mar 24 02:44:54.539: INFO: successfully validated that service endpoint-test2 in namespace services-9204 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9204
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9204 to expose endpoints map[pod1:[80]]
Mar 24 02:44:56.557: INFO: successfully validated that service endpoint-test2 in namespace services-9204 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-9204
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9204 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 24 02:44:57.584: INFO: successfully validated that service endpoint-test2 in namespace services-9204 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-9204
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9204 to expose endpoints map[pod2:[80]]
Mar 24 02:44:57.598: INFO: successfully validated that service endpoint-test2 in namespace services-9204 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-9204
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9204 to expose endpoints map[]
Mar 24 02:44:57.610: INFO: successfully validated that service endpoint-test2 in namespace services-9204 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:44:57.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9204" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
â€¢{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":80,"skipped":1170,"failed":0}
SSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:44:57.633: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9929
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Mar 24 02:44:57.761: INFO: created test-event-1
Mar 24 02:44:57.763: INFO: created test-event-2
Mar 24 02:44:57.766: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Mar 24 02:44:57.767: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Mar 24 02:44:57.780: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:44:57.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9929" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":81,"skipped":1173,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:44:57.788: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9236
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-ff545d3f-d8bd-475c-bc5b-bc24cf214e13
STEP: Creating a pod to test consume secrets
Mar 24 02:44:57.923: INFO: Waiting up to 5m0s for pod "pod-secrets-431f1ccd-d274-42ba-8959-13c772354656" in namespace "secrets-9236" to be "Succeeded or Failed"
Mar 24 02:44:57.924: INFO: Pod "pod-secrets-431f1ccd-d274-42ba-8959-13c772354656": Phase="Pending", Reason="", readiness=false. Elapsed: 1.723329ms
Mar 24 02:44:59.930: INFO: Pod "pod-secrets-431f1ccd-d274-42ba-8959-13c772354656": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006836774s
STEP: Saw pod success
Mar 24 02:44:59.930: INFO: Pod "pod-secrets-431f1ccd-d274-42ba-8959-13c772354656" satisfied condition "Succeeded or Failed"
Mar 24 02:44:59.931: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod pod-secrets-431f1ccd-d274-42ba-8959-13c772354656 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 02:44:59.945: INFO: Waiting for pod pod-secrets-431f1ccd-d274-42ba-8959-13c772354656 to disappear
Mar 24 02:44:59.946: INFO: Pod pod-secrets-431f1ccd-d274-42ba-8959-13c772354656 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:44:59.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9236" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":82,"skipped":1194,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:44:59.953: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3546
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Mar 24 02:45:00.095: INFO: Waiting up to 5m0s for pod "test-pod-fe83d9af-77fa-4b96-8eb0-ac9b0dfcdeaf" in namespace "svcaccounts-3546" to be "Succeeded or Failed"
Mar 24 02:45:00.097: INFO: Pod "test-pod-fe83d9af-77fa-4b96-8eb0-ac9b0dfcdeaf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.789992ms
Mar 24 02:45:02.103: INFO: Pod "test-pod-fe83d9af-77fa-4b96-8eb0-ac9b0dfcdeaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007528377s
STEP: Saw pod success
Mar 24 02:45:02.103: INFO: Pod "test-pod-fe83d9af-77fa-4b96-8eb0-ac9b0dfcdeaf" satisfied condition "Succeeded or Failed"
Mar 24 02:45:02.105: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod test-pod-fe83d9af-77fa-4b96-8eb0-ac9b0dfcdeaf container agnhost-container: <nil>
STEP: delete the pod
Mar 24 02:45:02.122: INFO: Waiting for pod test-pod-fe83d9af-77fa-4b96-8eb0-ac9b0dfcdeaf to disappear
Mar 24 02:45:02.124: INFO: Pod test-pod-fe83d9af-77fa-4b96-8eb0-ac9b0dfcdeaf no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:45:02.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3546" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":83,"skipped":1222,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:45:02.130: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7638
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:45:05.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7638" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":84,"skipped":1232,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:45:05.288: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2123
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0324 02:45:45.455188      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 24 02:46:47.471: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Mar 24 02:46:47.471: INFO: Deleting pod "simpletest.rc-2z86g" in namespace "gc-2123"
Mar 24 02:46:47.489: INFO: Deleting pod "simpletest.rc-64nqd" in namespace "gc-2123"
Mar 24 02:46:47.496: INFO: Deleting pod "simpletest.rc-6nfss" in namespace "gc-2123"
Mar 24 02:46:47.503: INFO: Deleting pod "simpletest.rc-7w8tj" in namespace "gc-2123"
Mar 24 02:46:47.520: INFO: Deleting pod "simpletest.rc-85mgp" in namespace "gc-2123"
Mar 24 02:46:47.528: INFO: Deleting pod "simpletest.rc-96wxq" in namespace "gc-2123"
Mar 24 02:46:47.535: INFO: Deleting pod "simpletest.rc-cdk4r" in namespace "gc-2123"
Mar 24 02:46:47.544: INFO: Deleting pod "simpletest.rc-j9m5j" in namespace "gc-2123"
Mar 24 02:46:47.552: INFO: Deleting pod "simpletest.rc-jk8mc" in namespace "gc-2123"
Mar 24 02:46:47.559: INFO: Deleting pod "simpletest.rc-mr954" in namespace "gc-2123"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:46:47.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2123" for this suite.

â€¢ [SLOW TEST:102.288 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":85,"skipped":1245,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:46:47.576: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7010
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7010
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7010
STEP: creating replication controller externalsvc in namespace services-7010
I0324 02:46:47.724226      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7010, replica count: 2
I0324 02:46:50.774469      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar 24 02:46:50.788: INFO: Creating new exec pod
Mar 24 02:46:52.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-7010 exec execpodg7v9p -- /bin/sh -x -c nslookup clusterip-service.services-7010.svc.cluster.local'
Mar 24 02:46:52.954: INFO: stderr: "+ nslookup clusterip-service.services-7010.svc.cluster.local\n"
Mar 24 02:46:52.954: INFO: stdout: "Server:\t\t172.16.0.10\nAddress:\t172.16.0.10#53\n\nclusterip-service.services-7010.svc.cluster.local\tcanonical name = externalsvc.services-7010.svc.cluster.local.\nName:\texternalsvc.services-7010.svc.cluster.local\nAddress: 172.16.139.206\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7010, will wait for the garbage collector to delete the pods
Mar 24 02:46:53.013: INFO: Deleting ReplicationController externalsvc took: 6.246212ms
Mar 24 02:46:53.714: INFO: Terminating ReplicationController externalsvc pods took: 701.105238ms
Mar 24 02:47:02.332: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:47:02.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7010" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

â€¢ [SLOW TEST:14.775 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":86,"skipped":1284,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:47:02.351: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6580
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 02:47:02.488: INFO: Waiting up to 5m0s for pod "downwardapi-volume-772b2844-9f14-4dda-96f4-2566dc3c2b2a" in namespace "downward-api-6580" to be "Succeeded or Failed"
Mar 24 02:47:02.490: INFO: Pod "downwardapi-volume-772b2844-9f14-4dda-96f4-2566dc3c2b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.855377ms
Mar 24 02:47:04.496: INFO: Pod "downwardapi-volume-772b2844-9f14-4dda-96f4-2566dc3c2b2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007674502s
STEP: Saw pod success
Mar 24 02:47:04.496: INFO: Pod "downwardapi-volume-772b2844-9f14-4dda-96f4-2566dc3c2b2a" satisfied condition "Succeeded or Failed"
Mar 24 02:47:04.498: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod downwardapi-volume-772b2844-9f14-4dda-96f4-2566dc3c2b2a container client-container: <nil>
STEP: delete the pod
Mar 24 02:47:04.519: INFO: Waiting for pod downwardapi-volume-772b2844-9f14-4dda-96f4-2566dc3c2b2a to disappear
Mar 24 02:47:04.521: INFO: Pod downwardapi-volume-772b2844-9f14-4dda-96f4-2566dc3c2b2a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:47:04.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6580" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":87,"skipped":1298,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:47:04.527: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5769
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-5769
STEP: creating service affinity-nodeport in namespace services-5769
STEP: creating replication controller affinity-nodeport in namespace services-5769
I0324 02:47:04.667383      21 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-5769, replica count: 3
I0324 02:47:07.717620      21 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 02:47:07.728: INFO: Creating new exec pod
Mar 24 02:47:10.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-5769 exec execpod-affinityw72rr -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Mar 24 02:47:10.900: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar 24 02:47:10.900: INFO: stdout: ""
Mar 24 02:47:10.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-5769 exec execpod-affinityw72rr -- /bin/sh -x -c nc -zv -t -w 2 172.16.77.244 80'
Mar 24 02:47:11.050: INFO: stderr: "+ nc -zv -t -w 2 172.16.77.244 80\nConnection to 172.16.77.244 80 port [tcp/http] succeeded!\n"
Mar 24 02:47:11.050: INFO: stdout: ""
Mar 24 02:47:11.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-5769 exec execpod-affinityw72rr -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.15 30134'
Mar 24 02:47:11.189: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.15 30134\nConnection to 192.168.0.15 30134 port [tcp/30134] succeeded!\n"
Mar 24 02:47:11.189: INFO: stdout: ""
Mar 24 02:47:11.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-5769 exec execpod-affinityw72rr -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.14 30134'
Mar 24 02:47:11.328: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.14 30134\nConnection to 192.168.0.14 30134 port [tcp/30134] succeeded!\n"
Mar 24 02:47:11.328: INFO: stdout: ""
Mar 24 02:47:11.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-5769 exec execpod-affinityw72rr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.13:30134/ ; done'
Mar 24 02:47:11.523: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:30134/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:30134/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:30134/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:30134/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:30134/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:30134/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:30134/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:30134/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:30134/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:30134/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:30134/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:30134/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:30134/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:30134/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:30134/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:30134/\n"
Mar 24 02:47:11.523: INFO: stdout: "\naffinity-nodeport-cmbxv\naffinity-nodeport-cmbxv\naffinity-nodeport-cmbxv\naffinity-nodeport-cmbxv\naffinity-nodeport-cmbxv\naffinity-nodeport-cmbxv\naffinity-nodeport-cmbxv\naffinity-nodeport-cmbxv\naffinity-nodeport-cmbxv\naffinity-nodeport-cmbxv\naffinity-nodeport-cmbxv\naffinity-nodeport-cmbxv\naffinity-nodeport-cmbxv\naffinity-nodeport-cmbxv\naffinity-nodeport-cmbxv\naffinity-nodeport-cmbxv"
Mar 24 02:47:11.523: INFO: Received response from host: affinity-nodeport-cmbxv
Mar 24 02:47:11.523: INFO: Received response from host: affinity-nodeport-cmbxv
Mar 24 02:47:11.523: INFO: Received response from host: affinity-nodeport-cmbxv
Mar 24 02:47:11.523: INFO: Received response from host: affinity-nodeport-cmbxv
Mar 24 02:47:11.523: INFO: Received response from host: affinity-nodeport-cmbxv
Mar 24 02:47:11.523: INFO: Received response from host: affinity-nodeport-cmbxv
Mar 24 02:47:11.523: INFO: Received response from host: affinity-nodeport-cmbxv
Mar 24 02:47:11.523: INFO: Received response from host: affinity-nodeport-cmbxv
Mar 24 02:47:11.523: INFO: Received response from host: affinity-nodeport-cmbxv
Mar 24 02:47:11.523: INFO: Received response from host: affinity-nodeport-cmbxv
Mar 24 02:47:11.523: INFO: Received response from host: affinity-nodeport-cmbxv
Mar 24 02:47:11.523: INFO: Received response from host: affinity-nodeport-cmbxv
Mar 24 02:47:11.523: INFO: Received response from host: affinity-nodeport-cmbxv
Mar 24 02:47:11.523: INFO: Received response from host: affinity-nodeport-cmbxv
Mar 24 02:47:11.523: INFO: Received response from host: affinity-nodeport-cmbxv
Mar 24 02:47:11.523: INFO: Received response from host: affinity-nodeport-cmbxv
Mar 24 02:47:11.523: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5769, will wait for the garbage collector to delete the pods
Mar 24 02:47:11.589: INFO: Deleting ReplicationController affinity-nodeport took: 5.097625ms
Mar 24 02:47:11.689: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.163555ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:47:22.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5769" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

â€¢ [SLOW TEST:18.089 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":88,"skipped":1303,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:47:22.615: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3900
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 24 02:47:24.759: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:47:24.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3900" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":89,"skipped":1304,"failed":0}
SS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:47:24.776: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-9985
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:47:25.357: INFO: Checking APIGroup: apiregistration.k8s.io
Mar 24 02:47:25.358: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar 24 02:47:25.358: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Mar 24 02:47:25.358: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar 24 02:47:25.358: INFO: Checking APIGroup: apps
Mar 24 02:47:25.359: INFO: PreferredVersion.GroupVersion: apps/v1
Mar 24 02:47:25.359: INFO: Versions found [{apps/v1 v1}]
Mar 24 02:47:25.359: INFO: apps/v1 matches apps/v1
Mar 24 02:47:25.359: INFO: Checking APIGroup: events.k8s.io
Mar 24 02:47:25.359: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar 24 02:47:25.359: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Mar 24 02:47:25.359: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar 24 02:47:25.359: INFO: Checking APIGroup: authentication.k8s.io
Mar 24 02:47:25.360: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar 24 02:47:25.360: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Mar 24 02:47:25.360: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar 24 02:47:25.360: INFO: Checking APIGroup: authorization.k8s.io
Mar 24 02:47:25.361: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar 24 02:47:25.361: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Mar 24 02:47:25.361: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar 24 02:47:25.361: INFO: Checking APIGroup: autoscaling
Mar 24 02:47:25.361: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Mar 24 02:47:25.361: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Mar 24 02:47:25.361: INFO: autoscaling/v1 matches autoscaling/v1
Mar 24 02:47:25.361: INFO: Checking APIGroup: batch
Mar 24 02:47:25.362: INFO: PreferredVersion.GroupVersion: batch/v1
Mar 24 02:47:25.362: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Mar 24 02:47:25.362: INFO: batch/v1 matches batch/v1
Mar 24 02:47:25.362: INFO: Checking APIGroup: certificates.k8s.io
Mar 24 02:47:25.362: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar 24 02:47:25.362: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Mar 24 02:47:25.362: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar 24 02:47:25.362: INFO: Checking APIGroup: networking.k8s.io
Mar 24 02:47:25.363: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar 24 02:47:25.363: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Mar 24 02:47:25.363: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar 24 02:47:25.363: INFO: Checking APIGroup: extensions
Mar 24 02:47:25.363: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Mar 24 02:47:25.363: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Mar 24 02:47:25.363: INFO: extensions/v1beta1 matches extensions/v1beta1
Mar 24 02:47:25.363: INFO: Checking APIGroup: policy
Mar 24 02:47:25.364: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Mar 24 02:47:25.364: INFO: Versions found [{policy/v1beta1 v1beta1}]
Mar 24 02:47:25.364: INFO: policy/v1beta1 matches policy/v1beta1
Mar 24 02:47:25.364: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar 24 02:47:25.364: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar 24 02:47:25.364: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Mar 24 02:47:25.364: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar 24 02:47:25.364: INFO: Checking APIGroup: storage.k8s.io
Mar 24 02:47:25.365: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar 24 02:47:25.365: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar 24 02:47:25.365: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar 24 02:47:25.365: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar 24 02:47:25.366: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar 24 02:47:25.366: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Mar 24 02:47:25.366: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar 24 02:47:25.366: INFO: Checking APIGroup: apiextensions.k8s.io
Mar 24 02:47:25.366: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar 24 02:47:25.366: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Mar 24 02:47:25.366: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar 24 02:47:25.366: INFO: Checking APIGroup: scheduling.k8s.io
Mar 24 02:47:25.367: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar 24 02:47:25.367: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Mar 24 02:47:25.367: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar 24 02:47:25.367: INFO: Checking APIGroup: coordination.k8s.io
Mar 24 02:47:25.367: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar 24 02:47:25.367: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Mar 24 02:47:25.367: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar 24 02:47:25.367: INFO: Checking APIGroup: node.k8s.io
Mar 24 02:47:25.368: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar 24 02:47:25.368: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Mar 24 02:47:25.368: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar 24 02:47:25.368: INFO: Checking APIGroup: discovery.k8s.io
Mar 24 02:47:25.368: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Mar 24 02:47:25.368: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Mar 24 02:47:25.368: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Mar 24 02:47:25.368: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar 24 02:47:25.369: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Mar 24 02:47:25.369: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Mar 24 02:47:25.369: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Mar 24 02:47:25.369: INFO: Checking APIGroup: alert.alibabacloud.com
Mar 24 02:47:25.369: INFO: PreferredVersion.GroupVersion: alert.alibabacloud.com/v1beta1
Mar 24 02:47:25.369: INFO: Versions found [{alert.alibabacloud.com/v1beta1 v1beta1}]
Mar 24 02:47:25.369: INFO: alert.alibabacloud.com/v1beta1 matches alert.alibabacloud.com/v1beta1
Mar 24 02:47:25.369: INFO: Checking APIGroup: snapshot.storage.k8s.io
Mar 24 02:47:25.370: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1beta1
Mar 24 02:47:25.370: INFO: Versions found [{snapshot.storage.k8s.io/v1beta1 v1beta1}]
Mar 24 02:47:25.370: INFO: snapshot.storage.k8s.io/v1beta1 matches snapshot.storage.k8s.io/v1beta1
Mar 24 02:47:25.370: INFO: Checking APIGroup: metrics.k8s.io
Mar 24 02:47:25.370: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Mar 24 02:47:25.370: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Mar 24 02:47:25.370: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:47:25.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-9985" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":90,"skipped":1306,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:47:25.379: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:47:25.505: INFO: Creating deployment "test-recreate-deployment"
Mar 24 02:47:25.509: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 24 02:47:25.513: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Mar 24 02:47:27.521: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 24 02:47:27.522: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 24 02:47:27.530: INFO: Updating deployment test-recreate-deployment
Mar 24 02:47:27.530: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar 24 02:47:27.579: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6696  427754e4-13eb-4021-a895-2b61e6ec69f4 788312 2 2021-03-24 02:47:25 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-24 02:47:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-24 02:47:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00448aca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-03-24 02:47:27 +0000 UTC,LastTransitionTime:2021-03-24 02:47:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-03-24 02:47:27 +0000 UTC,LastTransitionTime:2021-03-24 02:47:25 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 24 02:47:27.581: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-6696  18f642a7-658a-4c92-90f4-4be07e5cc273 788311 1 2021-03-24 02:47:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 427754e4-13eb-4021-a895-2b61e6ec69f4 0xc00448b130 0xc00448b131}] []  [{kube-controller-manager Update apps/v1 2021-03-24 02:47:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"427754e4-13eb-4021-a895-2b61e6ec69f4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00448b1a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 24 02:47:27.581: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 24 02:47:27.581: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-6696  52c112f0-5011-4aab-b91b-e5ba9e28b4ed 788300 2 2021-03-24 02:47:25 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 427754e4-13eb-4021-a895-2b61e6ec69f4 0xc00448b037 0xc00448b038}] []  [{kube-controller-manager Update apps/v1 2021-03-24 02:47:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"427754e4-13eb-4021-a895-2b61e6ec69f4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00448b0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 24 02:47:27.583: INFO: Pod "test-recreate-deployment-f79dd4667-92kxt" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-92kxt test-recreate-deployment-f79dd4667- deployment-6696  7a9c2fa8-8810-4c9f-a7df-614e8e7fa731 788313 0 2021-03-24 02:47:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 18f642a7-658a-4c92-90f4-4be07e5cc273 0xc00448b5d0 0xc00448b5d1}] []  [{kube-controller-manager Update v1 2021-03-24 02:47:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18f642a7-658a-4c92-90f4-4be07e5cc273\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:47:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bmvzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bmvzv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bmvzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:47:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:47:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:47:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:47:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.14,PodIP:,StartTime:2021-03-24 02:47:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:47:27.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6696" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":91,"skipped":1308,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:47:27.590: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7828
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar 24 02:47:27.715: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:47:35.487: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:47:51.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7828" for this suite.

â€¢ [SLOW TEST:23.747 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":92,"skipped":1376,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:47:51.337: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2959
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-8xc7
STEP: Creating a pod to test atomic-volume-subpath
Mar 24 02:47:51.476: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8xc7" in namespace "subpath-2959" to be "Succeeded or Failed"
Mar 24 02:47:51.478: INFO: Pod "pod-subpath-test-secret-8xc7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.804016ms
Mar 24 02:47:53.484: INFO: Pod "pod-subpath-test-secret-8xc7": Phase="Running", Reason="", readiness=true. Elapsed: 2.007641977s
Mar 24 02:47:55.490: INFO: Pod "pod-subpath-test-secret-8xc7": Phase="Running", Reason="", readiness=true. Elapsed: 4.013759587s
Mar 24 02:47:57.497: INFO: Pod "pod-subpath-test-secret-8xc7": Phase="Running", Reason="", readiness=true. Elapsed: 6.020722898s
Mar 24 02:47:59.503: INFO: Pod "pod-subpath-test-secret-8xc7": Phase="Running", Reason="", readiness=true. Elapsed: 8.026402431s
Mar 24 02:48:01.506: INFO: Pod "pod-subpath-test-secret-8xc7": Phase="Running", Reason="", readiness=true. Elapsed: 10.029822656s
Mar 24 02:48:03.512: INFO: Pod "pod-subpath-test-secret-8xc7": Phase="Running", Reason="", readiness=true. Elapsed: 12.035874195s
Mar 24 02:48:05.518: INFO: Pod "pod-subpath-test-secret-8xc7": Phase="Running", Reason="", readiness=true. Elapsed: 14.042084772s
Mar 24 02:48:07.524: INFO: Pod "pod-subpath-test-secret-8xc7": Phase="Running", Reason="", readiness=true. Elapsed: 16.047591259s
Mar 24 02:48:09.530: INFO: Pod "pod-subpath-test-secret-8xc7": Phase="Running", Reason="", readiness=true. Elapsed: 18.053563675s
Mar 24 02:48:11.534: INFO: Pod "pod-subpath-test-secret-8xc7": Phase="Running", Reason="", readiness=true. Elapsed: 20.057970856s
Mar 24 02:48:13.540: INFO: Pod "pod-subpath-test-secret-8xc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.063970718s
STEP: Saw pod success
Mar 24 02:48:13.540: INFO: Pod "pod-subpath-test-secret-8xc7" satisfied condition "Succeeded or Failed"
Mar 24 02:48:13.542: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-subpath-test-secret-8xc7 container test-container-subpath-secret-8xc7: <nil>
STEP: delete the pod
Mar 24 02:48:13.557: INFO: Waiting for pod pod-subpath-test-secret-8xc7 to disappear
Mar 24 02:48:13.559: INFO: Pod pod-subpath-test-secret-8xc7 no longer exists
STEP: Deleting pod pod-subpath-test-secret-8xc7
Mar 24 02:48:13.559: INFO: Deleting pod "pod-subpath-test-secret-8xc7" in namespace "subpath-2959"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:48:13.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2959" for this suite.

â€¢ [SLOW TEST:22.231 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":93,"skipped":1387,"failed":0}
S
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:48:13.568: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-1992
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Mar 24 02:48:13.693: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 24 02:49:13.723: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:49:13.725: INFO: Starting informer...
STEP: Starting pods...
Mar 24 02:49:13.937: INFO: Pod1 is running on cn-hongkong.192.168.0.14. Tainting Node
Mar 24 02:49:16.152: INFO: Pod2 is running on cn-hongkong.192.168.0.14. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Mar 24 02:49:23.087: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 24 02:49:52.548: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:49:52.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-1992" for this suite.

â€¢ [SLOW TEST:99.000 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":94,"skipped":1388,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:49:52.569: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2372
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-6a53d8fa-f165-4c32-92a8-a36b48e06ead
STEP: Creating a pod to test consume secrets
Mar 24 02:49:52.705: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ffb3e39b-650b-4dbc-a1c9-574715c13343" in namespace "projected-2372" to be "Succeeded or Failed"
Mar 24 02:49:52.708: INFO: Pod "pod-projected-secrets-ffb3e39b-650b-4dbc-a1c9-574715c13343": Phase="Pending", Reason="", readiness=false. Elapsed: 3.543509ms
Mar 24 02:49:54.713: INFO: Pod "pod-projected-secrets-ffb3e39b-650b-4dbc-a1c9-574715c13343": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00867461s
STEP: Saw pod success
Mar 24 02:49:54.713: INFO: Pod "pod-projected-secrets-ffb3e39b-650b-4dbc-a1c9-574715c13343" satisfied condition "Succeeded or Failed"
Mar 24 02:49:54.715: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-projected-secrets-ffb3e39b-650b-4dbc-a1c9-574715c13343 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 24 02:49:54.735: INFO: Waiting for pod pod-projected-secrets-ffb3e39b-650b-4dbc-a1c9-574715c13343 to disappear
Mar 24 02:49:54.736: INFO: Pod pod-projected-secrets-ffb3e39b-650b-4dbc-a1c9-574715c13343 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:49:54.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2372" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":95,"skipped":1396,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:49:54.742: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6519
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-eeab0ac4-2778-48f2-be40-920a3ab034c3
STEP: Creating a pod to test consume configMaps
Mar 24 02:49:54.878: INFO: Waiting up to 5m0s for pod "pod-configmaps-cad5acb3-d8a5-480a-baa3-6e92f184f30c" in namespace "configmap-6519" to be "Succeeded or Failed"
Mar 24 02:49:54.880: INFO: Pod "pod-configmaps-cad5acb3-d8a5-480a-baa3-6e92f184f30c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.639678ms
Mar 24 02:49:56.885: INFO: Pod "pod-configmaps-cad5acb3-d8a5-480a-baa3-6e92f184f30c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006931458s
STEP: Saw pod success
Mar 24 02:49:56.885: INFO: Pod "pod-configmaps-cad5acb3-d8a5-480a-baa3-6e92f184f30c" satisfied condition "Succeeded or Failed"
Mar 24 02:49:56.887: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-configmaps-cad5acb3-d8a5-480a-baa3-6e92f184f30c container agnhost-container: <nil>
STEP: delete the pod
Mar 24 02:49:56.900: INFO: Waiting for pod pod-configmaps-cad5acb3-d8a5-480a-baa3-6e92f184f30c to disappear
Mar 24 02:49:56.902: INFO: Pod pod-configmaps-cad5acb3-d8a5-480a-baa3-6e92f184f30c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:49:56.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6519" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":96,"skipped":1405,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:49:56.908: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2722
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar 24 02:49:57.039: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:50:02.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2722" for this suite.

â€¢ [SLOW TEST:5.736 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":97,"skipped":1411,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:50:02.645: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5833
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-bcd0198a-e87a-4eb9-a956-e7de36551aec
STEP: Creating a pod to test consume secrets
Mar 24 02:50:02.779: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5f9eee01-76fe-46df-b2e8-917afb13eb9e" in namespace "projected-5833" to be "Succeeded or Failed"
Mar 24 02:50:02.781: INFO: Pod "pod-projected-secrets-5f9eee01-76fe-46df-b2e8-917afb13eb9e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.860067ms
Mar 24 02:50:04.786: INFO: Pod "pod-projected-secrets-5f9eee01-76fe-46df-b2e8-917afb13eb9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007143528s
STEP: Saw pod success
Mar 24 02:50:04.786: INFO: Pod "pod-projected-secrets-5f9eee01-76fe-46df-b2e8-917afb13eb9e" satisfied condition "Succeeded or Failed"
Mar 24 02:50:04.788: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-projected-secrets-5f9eee01-76fe-46df-b2e8-917afb13eb9e container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 24 02:50:04.802: INFO: Waiting for pod pod-projected-secrets-5f9eee01-76fe-46df-b2e8-917afb13eb9e to disappear
Mar 24 02:50:04.804: INFO: Pod pod-projected-secrets-5f9eee01-76fe-46df-b2e8-917afb13eb9e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:50:04.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5833" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":98,"skipped":1414,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:50:04.810: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7727
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar 24 02:50:04.938: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar 24 02:50:18.560: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 02:50:25.842: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:50:39.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7727" for this suite.

â€¢ [SLOW TEST:34.884 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":99,"skipped":1454,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:50:39.693: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5032
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Mar 24 02:50:39.821: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:50:59.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5032" for this suite.

â€¢ [SLOW TEST:19.575 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":100,"skipped":1468,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:50:59.269: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1853
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-1853
STEP: creating replication controller nodeport-test in namespace services-1853
I0324 02:50:59.409826      21 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-1853, replica count: 2
Mar 24 02:51:02.460: INFO: Creating new exec pod
I0324 02:51:02.460073      21 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 02:51:05.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-1853 exec execpodltpqw -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar 24 02:51:05.624: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 24 02:51:05.624: INFO: stdout: ""
Mar 24 02:51:05.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-1853 exec execpodltpqw -- /bin/sh -x -c nc -zv -t -w 2 172.16.147.42 80'
Mar 24 02:51:05.770: INFO: stderr: "+ nc -zv -t -w 2 172.16.147.42 80\nConnection to 172.16.147.42 80 port [tcp/http] succeeded!\n"
Mar 24 02:51:05.770: INFO: stdout: ""
Mar 24 02:51:05.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-1853 exec execpodltpqw -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.14 31505'
Mar 24 02:51:05.909: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.14 31505\nConnection to 192.168.0.14 31505 port [tcp/31505] succeeded!\n"
Mar 24 02:51:05.909: INFO: stdout: ""
Mar 24 02:51:05.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-1853 exec execpodltpqw -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.15 31505'
Mar 24 02:51:06.056: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.15 31505\nConnection to 192.168.0.15 31505 port [tcp/31505] succeeded!\n"
Mar 24 02:51:06.056: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:51:06.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1853" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

â€¢ [SLOW TEST:6.799 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":101,"skipped":1477,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:51:06.068: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-8267
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:51:08.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8267" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":102,"skipped":1485,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:51:08.241: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2904
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Mar 24 02:51:10.909: INFO: Successfully updated pod "annotationupdateef187eb9-b969-4f12-86bb-edd1c90a140e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:51:14.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2904" for this suite.

â€¢ [SLOW TEST:6.702 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":103,"skipped":1489,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:51:14.943: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7015
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:51:17.099: INFO: Waiting up to 5m0s for pod "client-envvars-6de198f1-3fe7-4514-9351-752ccb88e8b2" in namespace "pods-7015" to be "Succeeded or Failed"
Mar 24 02:51:17.103: INFO: Pod "client-envvars-6de198f1-3fe7-4514-9351-752ccb88e8b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.294023ms
Mar 24 02:51:19.108: INFO: Pod "client-envvars-6de198f1-3fe7-4514-9351-752ccb88e8b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009727668s
STEP: Saw pod success
Mar 24 02:51:19.108: INFO: Pod "client-envvars-6de198f1-3fe7-4514-9351-752ccb88e8b2" satisfied condition "Succeeded or Failed"
Mar 24 02:51:19.110: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod client-envvars-6de198f1-3fe7-4514-9351-752ccb88e8b2 container env3cont: <nil>
STEP: delete the pod
Mar 24 02:51:19.125: INFO: Waiting for pod client-envvars-6de198f1-3fe7-4514-9351-752ccb88e8b2 to disappear
Mar 24 02:51:19.127: INFO: Pod client-envvars-6de198f1-3fe7-4514-9351-752ccb88e8b2 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:51:19.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7015" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":104,"skipped":1495,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:51:19.134: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9384
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:51:19.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9384" for this suite.
â€¢{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":105,"skipped":1565,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:51:19.302: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1953
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar 24 02:51:19.428: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 24 02:51:19.434: INFO: Waiting for terminating namespaces to be deleted...
Mar 24 02:51:19.436: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.13 before test
Mar 24 02:51:19.441: INFO: coredns-58d46886cf-fl2z7 from kube-system started at 2021-03-23 11:44:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:51:19.441: INFO: 	Container coredns ready: true, restart count 0
Mar 24 02:51:19.441: INFO: csi-plugin-78sns from kube-system started at 2021-03-22 03:28:49 +0000 UTC (4 container statuses recorded)
Mar 24 02:51:19.441: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 24 02:51:19.441: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 24 02:51:19.441: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 24 02:51:19.441: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 24 02:51:19.441: INFO: csi-provisioner-57c8d966fb-spnmk from kube-system started at 2021-03-22 03:29:32 +0000 UTC (7 container statuses recorded)
Mar 24 02:51:19.441: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 24 02:51:19.441: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
Mar 24 02:51:19.441: INFO: 	Container external-disk-attacher ready: true, restart count 0
Mar 24 02:51:19.441: INFO: 	Container external-disk-provisioner ready: true, restart count 0
Mar 24 02:51:19.441: INFO: 	Container external-disk-resizer ready: true, restart count 0
Mar 24 02:51:19.441: INFO: 	Container external-nas-provisioner ready: true, restart count 0
Mar 24 02:51:19.441: INFO: 	Container external-snapshot-controller ready: true, restart count 0
Mar 24 02:51:19.441: INFO: kube-flannel-ds-77s6x from kube-system started at 2021-03-22 08:51:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:51:19.441: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 02:51:19.441: INFO: kube-proxy-worker-kfc4c from kube-system started at 2021-03-22 03:28:49 +0000 UTC (1 container statuses recorded)
Mar 24 02:51:19.441: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 24 02:51:19.441: INFO: metrics-server-7d6f974b9f-tglpj from kube-system started at 2021-03-22 03:29:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:51:19.441: INFO: 	Container metrics-server ready: true, restart count 2
Mar 24 02:51:19.441: INFO: nginx-ingress-controller-67bc64c7-5tcjs from kube-system started at 2021-03-23 11:44:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:51:19.441: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 24 02:51:19.441: INFO: sonobuoy-systemd-logs-daemon-set-46debedd236a484a-5dl8c from sonobuoy started at 2021-03-24 02:26:37 +0000 UTC (2 container statuses recorded)
Mar 24 02:51:19.441: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 24 02:51:19.441: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 24 02:51:19.441: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.14 before test
Mar 24 02:51:19.446: INFO: csi-plugin-sdgx9 from kube-system started at 2021-03-22 03:28:49 +0000 UTC (4 container statuses recorded)
Mar 24 02:51:19.446: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 24 02:51:19.446: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 24 02:51:19.446: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 24 02:51:19.446: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 24 02:51:19.446: INFO: kube-flannel-ds-bqvbv from kube-system started at 2021-03-22 08:51:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:51:19.446: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 02:51:19.446: INFO: kube-proxy-worker-46686 from kube-system started at 2021-03-22 03:28:49 +0000 UTC (1 container statuses recorded)
Mar 24 02:51:19.446: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 24 02:51:19.446: INFO: server-envvars-a1baac3e-638a-443c-b986-2117a021e08c from pods-7015 started at 2021-03-24 02:51:15 +0000 UTC (1 container statuses recorded)
Mar 24 02:51:19.446: INFO: 	Container srv ready: true, restart count 0
Mar 24 02:51:19.446: INFO: sonobuoy from sonobuoy started at 2021-03-24 02:26:36 +0000 UTC (1 container statuses recorded)
Mar 24 02:51:19.446: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 24 02:51:19.446: INFO: sonobuoy-systemd-logs-daemon-set-46debedd236a484a-l98ph from sonobuoy started at 2021-03-24 02:26:37 +0000 UTC (2 container statuses recorded)
Mar 24 02:51:19.446: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 24 02:51:19.446: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 24 02:51:19.446: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.15 before test
Mar 24 02:51:19.451: INFO: annotationupdateef187eb9-b969-4f12-86bb-edd1c90a140e from downward-api-2904 started at 2021-03-24 02:51:08 +0000 UTC (1 container statuses recorded)
Mar 24 02:51:19.451: INFO: 	Container client-container ready: true, restart count 0
Mar 24 02:51:19.451: INFO: coredns-58d46886cf-h7vgt from kube-system started at 2021-03-22 03:29:29 +0000 UTC (1 container statuses recorded)
Mar 24 02:51:19.451: INFO: 	Container coredns ready: true, restart count 23
Mar 24 02:51:19.451: INFO: csi-plugin-v7dd9 from kube-system started at 2021-03-22 03:28:49 +0000 UTC (4 container statuses recorded)
Mar 24 02:51:19.451: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 24 02:51:19.451: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 24 02:51:19.451: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 24 02:51:19.451: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 24 02:51:19.451: INFO: kube-flannel-ds-bswlq from kube-system started at 2021-03-22 08:51:32 +0000 UTC (1 container statuses recorded)
Mar 24 02:51:19.451: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 02:51:19.451: INFO: kube-proxy-worker-xvh5g from kube-system started at 2021-03-22 03:28:49 +0000 UTC (1 container statuses recorded)
Mar 24 02:51:19.451: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 24 02:51:19.451: INFO: nginx-ingress-controller-67bc64c7-zm4r9 from kube-system started at 2021-03-22 11:02:12 +0000 UTC (1 container statuses recorded)
Mar 24 02:51:19.451: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 24 02:51:19.451: INFO: sonobuoy-systemd-logs-daemon-set-46debedd236a484a-sb56k from sonobuoy started at 2021-03-24 02:26:37 +0000 UTC (2 container statuses recorded)
Mar 24 02:51:19.451: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 24 02:51:19.451: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-af801469-2f77-4ec8-884d-354209bc5474 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.0.14 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-af801469-2f77-4ec8-884d-354209bc5474 off the node cn-hongkong.192.168.0.14
STEP: verifying the node doesn't have the label kubernetes.io/e2e-af801469-2f77-4ec8-884d-354209bc5474
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:56:23.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1953" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

â€¢ [SLOW TEST:304.227 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":106,"skipped":1606,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:56:23.529: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5258
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-99de94dd-a304-4071-b1f0-494cec6e9523
STEP: Creating a pod to test consume secrets
Mar 24 02:56:23.664: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7afa1f22-c7c0-426e-b8dc-4a05f44357a0" in namespace "projected-5258" to be "Succeeded or Failed"
Mar 24 02:56:23.666: INFO: Pod "pod-projected-secrets-7afa1f22-c7c0-426e-b8dc-4a05f44357a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.854798ms
Mar 24 02:56:25.671: INFO: Pod "pod-projected-secrets-7afa1f22-c7c0-426e-b8dc-4a05f44357a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00650749s
STEP: Saw pod success
Mar 24 02:56:25.671: INFO: Pod "pod-projected-secrets-7afa1f22-c7c0-426e-b8dc-4a05f44357a0" satisfied condition "Succeeded or Failed"
Mar 24 02:56:25.673: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod pod-projected-secrets-7afa1f22-c7c0-426e-b8dc-4a05f44357a0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 24 02:56:25.693: INFO: Waiting for pod pod-projected-secrets-7afa1f22-c7c0-426e-b8dc-4a05f44357a0 to disappear
Mar 24 02:56:25.695: INFO: Pod pod-projected-secrets-7afa1f22-c7c0-426e-b8dc-4a05f44357a0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:56:25.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5258" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":107,"skipped":1612,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:56:25.701: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:56:25.830: INFO: Creating ReplicaSet my-hostname-basic-e63c177e-f071-4b27-b222-144e52d4567a
Mar 24 02:56:25.835: INFO: Pod name my-hostname-basic-e63c177e-f071-4b27-b222-144e52d4567a: Found 0 pods out of 1
Mar 24 02:56:30.844: INFO: Pod name my-hostname-basic-e63c177e-f071-4b27-b222-144e52d4567a: Found 1 pods out of 1
Mar 24 02:56:30.844: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e63c177e-f071-4b27-b222-144e52d4567a" is running
Mar 24 02:56:30.847: INFO: Pod "my-hostname-basic-e63c177e-f071-4b27-b222-144e52d4567a-vknv9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-24 02:56:25 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-24 02:56:27 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-24 02:56:27 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-24 02:56:25 +0000 UTC Reason: Message:}])
Mar 24 02:56:30.847: INFO: Trying to dial the pod
Mar 24 02:56:35.859: INFO: Controller my-hostname-basic-e63c177e-f071-4b27-b222-144e52d4567a: Got expected result from replica 1 [my-hostname-basic-e63c177e-f071-4b27-b222-144e52d4567a-vknv9]: "my-hostname-basic-e63c177e-f071-4b27-b222-144e52d4567a-vknv9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:56:35.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1584" for this suite.

â€¢ [SLOW TEST:10.166 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":108,"skipped":1635,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:56:35.868: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9180
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0324 02:56:37.541572      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 24 02:57:39.557: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:57:39.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9180" for this suite.

â€¢ [SLOW TEST:63.709 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":109,"skipped":1654,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:57:39.577: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-678
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar 24 02:57:42.733: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:57:43.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-678" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":110,"skipped":1669,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:57:43.753: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4462
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Mar 24 02:57:43.879: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:57:45.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4462" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":111,"skipped":1701,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:57:45.963: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4427
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-ac3e6500-2f70-49d8-956a-23cc9be62d23
STEP: Creating a pod to test consume configMaps
Mar 24 02:57:46.101: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fba2867b-a8e0-4b57-9e3a-8e1a2ec98970" in namespace "projected-4427" to be "Succeeded or Failed"
Mar 24 02:57:46.102: INFO: Pod "pod-projected-configmaps-fba2867b-a8e0-4b57-9e3a-8e1a2ec98970": Phase="Pending", Reason="", readiness=false. Elapsed: 1.637011ms
Mar 24 02:57:48.108: INFO: Pod "pod-projected-configmaps-fba2867b-a8e0-4b57-9e3a-8e1a2ec98970": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006774683s
STEP: Saw pod success
Mar 24 02:57:48.108: INFO: Pod "pod-projected-configmaps-fba2867b-a8e0-4b57-9e3a-8e1a2ec98970" satisfied condition "Succeeded or Failed"
Mar 24 02:57:48.109: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod pod-projected-configmaps-fba2867b-a8e0-4b57-9e3a-8e1a2ec98970 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 02:57:48.123: INFO: Waiting for pod pod-projected-configmaps-fba2867b-a8e0-4b57-9e3a-8e1a2ec98970 to disappear
Mar 24 02:57:48.125: INFO: Pod pod-projected-configmaps-fba2867b-a8e0-4b57-9e3a-8e1a2ec98970 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:57:48.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4427" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":112,"skipped":1713,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:57:48.131: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5282
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 02:57:48.264: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9b7a1bc-9802-4aa2-ab4a-73bdebe2f147" in namespace "downward-api-5282" to be "Succeeded or Failed"
Mar 24 02:57:48.266: INFO: Pod "downwardapi-volume-f9b7a1bc-9802-4aa2-ab4a-73bdebe2f147": Phase="Pending", Reason="", readiness=false. Elapsed: 1.681961ms
Mar 24 02:57:50.271: INFO: Pod "downwardapi-volume-f9b7a1bc-9802-4aa2-ab4a-73bdebe2f147": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006785612s
STEP: Saw pod success
Mar 24 02:57:50.271: INFO: Pod "downwardapi-volume-f9b7a1bc-9802-4aa2-ab4a-73bdebe2f147" satisfied condition "Succeeded or Failed"
Mar 24 02:57:50.273: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod downwardapi-volume-f9b7a1bc-9802-4aa2-ab4a-73bdebe2f147 container client-container: <nil>
STEP: delete the pod
Mar 24 02:57:50.286: INFO: Waiting for pod downwardapi-volume-f9b7a1bc-9802-4aa2-ab4a-73bdebe2f147 to disappear
Mar 24 02:57:50.288: INFO: Pod downwardapi-volume-f9b7a1bc-9802-4aa2-ab4a-73bdebe2f147 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:57:50.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5282" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":113,"skipped":1732,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:57:50.294: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4615
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 24 02:57:50.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-4615 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Mar 24 02:57:50.615: INFO: stderr: ""
Mar 24 02:57:50.615: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar 24 02:57:55.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-4615 get pod e2e-test-httpd-pod -o json'
Mar 24 02:57:55.720: INFO: stderr: ""
Mar 24 02:57:55.720: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"ack.privileged\"\n        },\n        \"creationTimestamp\": \"2021-03-24T02:57:50Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-24T02:57:50Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.43.1.52\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-24T02:57:52Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4615\",\n        \"resourceVersion\": \"791590\",\n        \"uid\": \"72096714-64f7-4cdf-98f1-56a9fa4ffa0d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-s2fb8\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cn-hongkong.192.168.0.15\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-s2fb8\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-s2fb8\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-24T02:57:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-24T02:57:52Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-24T02:57:52Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-24T02:57:50Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://2add6f847f907518848bb99762bb0cea3658553b63464934b10d34b9920b8db1\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-03-24T02:57:51Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.0.15\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.43.1.52\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.43.1.52\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-03-24T02:57:50Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar 24 02:57:55.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-4615 replace -f -'
Mar 24 02:57:55.918: INFO: stderr: ""
Mar 24 02:57:55.918: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Mar 24 02:57:55.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-4615 delete pods e2e-test-httpd-pod'
Mar 24 02:58:02.259: INFO: stderr: ""
Mar 24 02:58:02.259: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:58:02.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4615" for this suite.

â€¢ [SLOW TEST:11.979 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":114,"skipped":1738,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:58:02.273: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7465
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-dd53a001-48db-42b6-979a-f6a8235942b5
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:58:02.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7465" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":115,"skipped":1752,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:58:02.411: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8728
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 24 02:58:02.558: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:58:02.558: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:58:02.558: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:58:02.560: INFO: Number of nodes with available pods: 0
Mar 24 02:58:02.560: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 02:58:03.565: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:58:03.565: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:58:03.565: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:58:03.567: INFO: Number of nodes with available pods: 2
Mar 24 02:58:03.567: INFO: Node cn-hongkong.192.168.0.15 is running more than one daemon pod
Mar 24 02:58:04.565: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:58:04.565: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:58:04.565: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:58:04.568: INFO: Number of nodes with available pods: 3
Mar 24 02:58:04.568: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 24 02:58:04.581: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:58:04.581: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:58:04.581: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:58:04.584: INFO: Number of nodes with available pods: 2
Mar 24 02:58:04.584: INFO: Node cn-hongkong.192.168.0.14 is running more than one daemon pod
Mar 24 02:58:05.589: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:58:05.589: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:58:05.589: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 02:58:05.591: INFO: Number of nodes with available pods: 3
Mar 24 02:58:05.591: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8728, will wait for the garbage collector to delete the pods
Mar 24 02:58:05.653: INFO: Deleting DaemonSet.extensions daemon-set took: 6.686426ms
Mar 24 02:58:06.353: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.126781ms
Mar 24 02:58:12.565: INFO: Number of nodes with available pods: 0
Mar 24 02:58:12.565: INFO: Number of running nodes: 0, number of available pods: 0
Mar 24 02:58:12.566: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"791808"},"items":null}

Mar 24 02:58:12.569: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"791808"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:58:12.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8728" for this suite.

â€¢ [SLOW TEST:10.178 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":116,"skipped":1789,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:58:12.588: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1726
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 02:58:12.716: INFO: Creating deployment "webserver-deployment"
Mar 24 02:58:12.720: INFO: Waiting for observed generation 1
Mar 24 02:58:14.726: INFO: Waiting for all required pods to come up
Mar 24 02:58:14.729: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar 24 02:58:14.729: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 24 02:58:14.733: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 24 02:58:14.739: INFO: Updating deployment webserver-deployment
Mar 24 02:58:14.739: INFO: Waiting for observed generation 2
Mar 24 02:58:16.746: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 24 02:58:16.748: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 24 02:58:16.750: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 24 02:58:16.755: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 24 02:58:16.755: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 24 02:58:16.757: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 24 02:58:16.760: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 24 02:58:16.760: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 24 02:58:16.766: INFO: Updating deployment webserver-deployment
Mar 24 02:58:16.766: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 24 02:58:16.770: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 24 02:58:18.778: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar 24 02:58:18.782: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1726  6d665bc3-2520-4ca5-8dc5-a11090d714a0 792193 3 2021-03-24 02:58:12 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-24 02:58:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035731f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:9,UnavailableReplicas:24,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-03-24 02:58:16 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-03-24 02:58:18 +0000 UTC,LastTransitionTime:2021-03-24 02:58:12 +0000 UTC,},},ReadyReplicas:9,CollisionCount:nil,},}

Mar 24 02:58:18.784: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-1726  432d44da-e829-482d-a6a8-0c511cb4d7a3 792097 3 2021-03-24 02:58:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6d665bc3-2520-4ca5-8dc5-a11090d714a0 0xc00621cf77 0xc00621cf78}] []  [{kube-controller-manager Update apps/v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d665bc3-2520-4ca5-8dc5-a11090d714a0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00621cff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 24 02:58:18.784: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 24 02:58:18.784: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-1726  803edebf-cfbb-4425-acbc-732e09299a5d 792192 3 2021-03-24 02:58:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6d665bc3-2520-4ca5-8dc5-a11090d714a0 0xc00621d057 0xc00621d058}] []  [{kube-controller-manager Update apps/v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d665bc3-2520-4ca5-8dc5-a11090d714a0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00621d0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[]ReplicaSetCondition{},},}
Mar 24 02:58:18.787: INFO: Pod "webserver-deployment-795d758f88-2fnpd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2fnpd webserver-deployment-795d758f88- deployment-1726  1c7c1f58-a1a9-402b-b154-08c963ae9137 791973 0 2021-03-24 02:58:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 432d44da-e829-482d-a6a8-0c511cb4d7a3 0xc00621d557 0xc00621d558}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"432d44da-e829-482d-a6a8-0c511cb4d7a3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.15,PodIP:,StartTime:2021-03-24 02:58:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.787: INFO: Pod "webserver-deployment-795d758f88-5l6mv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5l6mv webserver-deployment-795d758f88- deployment-1726  ac191475-2666-49c1-a70c-5a4a0a872d08 791997 0 2021-03-24 02:58:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 432d44da-e829-482d-a6a8-0c511cb4d7a3 0xc00621d707 0xc00621d708}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"432d44da-e829-482d-a6a8-0c511cb4d7a3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.14,PodIP:,StartTime:2021-03-24 02:58:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.788: INFO: Pod "webserver-deployment-795d758f88-7b96v" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7b96v webserver-deployment-795d758f88- deployment-1726  b1947ae2-67ff-4f0d-a712-10661fb1cc55 792208 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 432d44da-e829-482d-a6a8-0c511cb4d7a3 0xc00621d8b7 0xc00621d8b8}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"432d44da-e829-482d-a6a8-0c511cb4d7a3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.14,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.788: INFO: Pod "webserver-deployment-795d758f88-7sk76" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7sk76 webserver-deployment-795d758f88- deployment-1726  e8cfaf04-fa5a-45ba-954b-67f1d32e836e 791999 0 2021-03-24 02:58:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 432d44da-e829-482d-a6a8-0c511cb4d7a3 0xc00621da67 0xc00621da68}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"432d44da-e829-482d-a6a8-0c511cb4d7a3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.15,PodIP:,StartTime:2021-03-24 02:58:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.788: INFO: Pod "webserver-deployment-795d758f88-8b65n" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8b65n webserver-deployment-795d758f88- deployment-1726  04aa07bf-7c41-4ae4-8926-947a0bcb05f6 792107 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 432d44da-e829-482d-a6a8-0c511cb4d7a3 0xc00621dc37 0xc00621dc38}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"432d44da-e829-482d-a6a8-0c511cb4d7a3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.13,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.788: INFO: Pod "webserver-deployment-795d758f88-c2wx8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-c2wx8 webserver-deployment-795d758f88- deployment-1726  6477e5a7-1560-449f-b253-56e07dc22e60 792003 0 2021-03-24 02:58:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 432d44da-e829-482d-a6a8-0c511cb4d7a3 0xc00621dde7 0xc00621dde8}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"432d44da-e829-482d-a6a8-0c511cb4d7a3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.14,PodIP:,StartTime:2021-03-24 02:58:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.788: INFO: Pod "webserver-deployment-795d758f88-fblfw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fblfw webserver-deployment-795d758f88- deployment-1726  d64cab40-e143-4dfa-ac02-29f82a17203c 792128 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 432d44da-e829-482d-a6a8-0c511cb4d7a3 0xc00621df97 0xc00621df98}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"432d44da-e829-482d-a6a8-0c511cb4d7a3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.15,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.788: INFO: Pod "webserver-deployment-795d758f88-fd256" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fd256 webserver-deployment-795d758f88- deployment-1726  212750f0-a189-4d59-b243-1b6f76506532 791975 0 2021-03-24 02:58:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 432d44da-e829-482d-a6a8-0c511cb4d7a3 0xc0039c8937 0xc0039c8938}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"432d44da-e829-482d-a6a8-0c511cb4d7a3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.13,PodIP:,StartTime:2021-03-24 02:58:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.788: INFO: Pod "webserver-deployment-795d758f88-fm87g" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fm87g webserver-deployment-795d758f88- deployment-1726  9f3dc434-9d34-4ee3-84de-8e0656327ba0 792200 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 432d44da-e829-482d-a6a8-0c511cb4d7a3 0xc0039c9b87 0xc0039c9b88}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"432d44da-e829-482d-a6a8-0c511cb4d7a3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.14,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.788: INFO: Pod "webserver-deployment-795d758f88-jm8dt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jm8dt webserver-deployment-795d758f88- deployment-1726  6b106b9c-d113-4ca8-84bb-2d3f8c518e19 792106 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 432d44da-e829-482d-a6a8-0c511cb4d7a3 0xc0039c9e07 0xc0039c9e08}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"432d44da-e829-482d-a6a8-0c511cb4d7a3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.15,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.789: INFO: Pod "webserver-deployment-795d758f88-rvtsz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rvtsz webserver-deployment-795d758f88- deployment-1726  5f82abfe-adac-49da-b9ce-833d11358a8d 792098 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 432d44da-e829-482d-a6a8-0c511cb4d7a3 0xc005fec127 0xc005fec128}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"432d44da-e829-482d-a6a8-0c511cb4d7a3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.13,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.789: INFO: Pod "webserver-deployment-795d758f88-zmxk5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zmxk5 webserver-deployment-795d758f88- deployment-1726  fa3b661a-8709-4b4d-9dd1-ebe19b2a69b5 792090 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 432d44da-e829-482d-a6a8-0c511cb4d7a3 0xc005fec2d7 0xc005fec2d8}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"432d44da-e829-482d-a6a8-0c511cb4d7a3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.15,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.789: INFO: Pod "webserver-deployment-795d758f88-zqs62" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zqs62 webserver-deployment-795d758f88- deployment-1726  7a4ed174-6130-4aa6-9fe8-0f78104ec6cc 792078 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 432d44da-e829-482d-a6a8-0c511cb4d7a3 0xc005fec487 0xc005fec488}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"432d44da-e829-482d-a6a8-0c511cb4d7a3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.14,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.789: INFO: Pod "webserver-deployment-dd94f59b7-2zg9s" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2zg9s webserver-deployment-dd94f59b7- deployment-1726  9088e03e-67a5-4e2e-8b20-209a1b718cab 792105 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc005fec637 0xc005fec638}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.13,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.789: INFO: Pod "webserver-deployment-dd94f59b7-4rsqh" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4rsqh webserver-deployment-dd94f59b7- deployment-1726  8bd95360-a710-4e74-8e25-07ad9e45bb45 792191 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc005fec7c7 0xc005fec7c8}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.43.0.217\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.13,PodIP:10.43.0.217,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 02:58:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ecf38898cbe7f65b40dfc40bcc0d9619b7bd60e86e062e42f88f36298b4674b5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.43.0.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.789: INFO: Pod "webserver-deployment-dd94f59b7-59w6s" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-59w6s webserver-deployment-dd94f59b7- deployment-1726  04079830-87d3-4a3f-86e5-a4a6542ddd05 792160 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc005fec977 0xc005fec978}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.14,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.789: INFO: Pod "webserver-deployment-dd94f59b7-6mgqp" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6mgqp webserver-deployment-dd94f59b7- deployment-1726  b1683544-5375-4e84-8302-f6c08040c6d6 791912 0 2021-03-24 02:58:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc005fecb07 0xc005fecb08}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.43.0.214\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.13,PodIP:10.43.0.214,StartTime:2021-03-24 02:58:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 02:58:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c2f93a8a0846589aa9a3d4b3d3046acd0fc03a7b022e9e268b72354a53e51293,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.43.0.214,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.789: INFO: Pod "webserver-deployment-dd94f59b7-6mrsv" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6mrsv webserver-deployment-dd94f59b7- deployment-1726  974ec39b-06e4-4732-94c7-99e3371b02dc 792108 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc005feccb7 0xc005feccb8}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.15,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.790: INFO: Pod "webserver-deployment-dd94f59b7-7bcr8" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7bcr8 webserver-deployment-dd94f59b7- deployment-1726  bb99aec1-fede-4106-ab11-38d067bbbb92 792104 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc005fece47 0xc005fece48}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.15,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.790: INFO: Pod "webserver-deployment-dd94f59b7-7hmk9" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7hmk9 webserver-deployment-dd94f59b7- deployment-1726  9297296c-d6cf-43ee-b483-8491125d9f92 791914 0 2021-03-24 02:58:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc005fecfe7 0xc005fecfe8}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.43.0.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.13,PodIP:10.43.0.215,StartTime:2021-03-24 02:58:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 02:58:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d1d744c6d7b7e40b7fe126c312b78cee36f4183f16e24838cac25deca698b627,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.43.0.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.790: INFO: Pod "webserver-deployment-dd94f59b7-7srkv" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7srkv webserver-deployment-dd94f59b7- deployment-1726  df132d83-d1f5-47ee-8a4b-57218cb05158 791919 0 2021-03-24 02:58:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc005fed197 0xc005fed198}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.43.1.70\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.14,PodIP:10.43.1.70,StartTime:2021-03-24 02:58:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 02:58:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9fe9d4badfcf47b5a3c39a40cd6c13a6189b8940c8be4de872fdbd4123008308,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.43.1.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.790: INFO: Pod "webserver-deployment-dd94f59b7-8t5dn" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8t5dn webserver-deployment-dd94f59b7- deployment-1726  c22214b9-312c-48eb-b58d-8854dabe50fb 791928 0 2021-03-24 02:58:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc005fed340 0xc005fed341}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.43.1.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.14,PodIP:10.43.1.72,StartTime:2021-03-24 02:58:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 02:58:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://3015272dce0d2ee60bfb04d78bedfbf9ab4285292774c22ef57133ee72689273,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.43.1.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.790: INFO: Pod "webserver-deployment-dd94f59b7-b5fzj" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-b5fzj webserver-deployment-dd94f59b7- deployment-1726  6bb4a2a0-0a33-4f3d-8564-146e90299444 792114 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc005fed4e0 0xc005fed4e1}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.15,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.790: INFO: Pod "webserver-deployment-dd94f59b7-cz27v" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-cz27v webserver-deployment-dd94f59b7- deployment-1726  a29ec31e-7d98-4e23-869a-09d40f2318a9 791940 0 2021-03-24 02:58:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc005fed667 0xc005fed668}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.43.1.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.15,PodIP:10.43.1.56,StartTime:2021-03-24 02:58:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 02:58:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://301b5138e4f42fb4b30b9bfde583f06c03d028c6f8c3062cd76288c4bc00eadb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.43.1.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.790: INFO: Pod "webserver-deployment-dd94f59b7-dhdpz" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dhdpz webserver-deployment-dd94f59b7- deployment-1726  72d8b188-59ef-4cd6-bf75-96d155cc9ede 792119 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc005fed810 0xc005fed811}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.13,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.790: INFO: Pod "webserver-deployment-dd94f59b7-f7wld" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-f7wld webserver-deployment-dd94f59b7- deployment-1726  e274522d-7a69-4eb2-9349-0f6da3a1a10e 792126 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc005fed997 0xc005fed998}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.14,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.790: INFO: Pod "webserver-deployment-dd94f59b7-jznkg" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jznkg webserver-deployment-dd94f59b7- deployment-1726  ab33b6aa-6d37-4c8c-8ef6-0fd8f4d61031 792039 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc005fedb27 0xc005fedb28}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.14,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.791: INFO: Pod "webserver-deployment-dd94f59b7-nhc2s" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nhc2s webserver-deployment-dd94f59b7- deployment-1726  ee42ee15-dbd0-4e2d-b8bf-f23d5fad1340 791929 0 2021-03-24 02:58:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc005fedcb7 0xc005fedcb8}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.43.1.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.15,PodIP:10.43.1.54,StartTime:2021-03-24 02:58:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 02:58:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://fb27c90b75411cba1bcfc10efef2677e459917a208095dfe306dc15763d96636,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.43.1.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.791: INFO: Pod "webserver-deployment-dd94f59b7-shm7z" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-shm7z webserver-deployment-dd94f59b7- deployment-1726  5115901d-bcd1-4a52-96b0-d99823d5dec3 791937 0 2021-03-24 02:58:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc005fede60 0xc005fede61}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.43.1.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.15,PodIP:10.43.1.55,StartTime:2021-03-24 02:58:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 02:58:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://5445c8a7af18e0606937d14f07cf4f7acf5854f699d93d730bc1b0df7146c8ae,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.43.1.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.791: INFO: Pod "webserver-deployment-dd94f59b7-spc5p" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-spc5p webserver-deployment-dd94f59b7- deployment-1726  9d040df6-4fb8-4495-9631-d01883db9397 792109 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc0043b6000 0xc0043b6001}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.13,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.13,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.791: INFO: Pod "webserver-deployment-dd94f59b7-tskrq" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-tskrq webserver-deployment-dd94f59b7- deployment-1726  109b6f2b-8e19-46ff-8061-7343a3c60bf7 791934 0 2021-03-24 02:58:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc0043b6187 0xc0043b6188}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.43.1.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.15,PodIP:10.43.1.57,StartTime:2021-03-24 02:58:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 02:58:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://48db7d1775752ace6eabef697d600b7347f2b68d5535e0a9c42c284bafd4a150,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.43.1.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.791: INFO: Pod "webserver-deployment-dd94f59b7-xrpcm" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xrpcm webserver-deployment-dd94f59b7- deployment-1726  99d6d780-a365-41c5-8eea-f55d95e16bc6 792111 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc0043b6330 0xc0043b6331}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.14,PodIP:,StartTime:2021-03-24 02:58:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 02:58:18.791: INFO: Pod "webserver-deployment-dd94f59b7-zlsfh" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zlsfh webserver-deployment-dd94f59b7- deployment-1726  d325a875-d88e-47f1-bd78-7b7f19f03e7f 792082 0 2021-03-24 02:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 803edebf-cfbb-4425-acbc-732e09299a5d 0xc0043b64b7 0xc0043b64b8}] []  [{kube-controller-manager Update v1 2021-03-24 02:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"803edebf-cfbb-4425-acbc-732e09299a5d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zc8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zc8hq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zc8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 02:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:58:18.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1726" for this suite.

â€¢ [SLOW TEST:6.210 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":117,"skipped":1809,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:58:18.799: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9446
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9446
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9446
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9446
Mar 24 02:58:18.940: INFO: Found 0 stateful pods, waiting for 1
Mar 24 02:58:28.950: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 24 02:58:28.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-9446 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 02:58:29.103: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 02:58:29.103: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 02:58:29.103: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 02:58:29.106: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 24 02:58:39.117: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 02:58:39.117: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 02:58:39.128: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999822s
Mar 24 02:58:40.132: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99814307s
Mar 24 02:58:41.137: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99395265s
Mar 24 02:58:42.141: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989315384s
Mar 24 02:58:43.144: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984831357s
Mar 24 02:58:44.148: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.981836922s
Mar 24 02:58:45.153: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.977502064s
Mar 24 02:58:46.157: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.97327983s
Mar 24 02:58:47.163: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.968416832s
Mar 24 02:58:48.166: INFO: Verifying statefulset ss doesn't scale past 1 for another 963.312159ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9446
Mar 24 02:58:49.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-9446 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 02:58:49.323: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 24 02:58:49.323: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 02:58:49.323: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 24 02:58:49.326: INFO: Found 1 stateful pods, waiting for 3
Mar 24 02:58:59.338: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 02:58:59.338: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 02:58:59.338: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar 24 02:58:59.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-9446 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 02:58:59.487: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 02:58:59.487: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 02:58:59.487: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 02:58:59.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-9446 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 02:58:59.643: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 02:58:59.643: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 02:58:59.643: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 02:58:59.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-9446 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 02:58:59.801: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 02:58:59.801: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 02:58:59.801: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 02:58:59.801: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 02:58:59.804: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 24 02:59:09.818: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 02:59:09.818: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 02:59:09.818: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 24 02:59:09.825: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999827s
Mar 24 02:59:10.829: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997953133s
Mar 24 02:59:11.833: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993661715s
Mar 24 02:59:12.838: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989528312s
Mar 24 02:59:13.842: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985399593s
Mar 24 02:59:14.846: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981251631s
Mar 24 02:59:15.850: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977167473s
Mar 24 02:59:16.854: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972921418s
Mar 24 02:59:17.859: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.968533767s
Mar 24 02:59:18.863: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.235656ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9446
Mar 24 02:59:19.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-9446 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 02:59:20.011: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 24 02:59:20.011: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 02:59:20.011: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 24 02:59:20.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-9446 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 02:59:20.152: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 24 02:59:20.152: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 02:59:20.152: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 24 02:59:20.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-9446 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 02:59:20.301: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 24 02:59:20.301: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 02:59:20.301: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 24 02:59:20.301: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 24 02:59:40.321: INFO: Deleting all statefulset in ns statefulset-9446
Mar 24 02:59:40.324: INFO: Scaling statefulset ss to 0
Mar 24 02:59:40.332: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 02:59:40.334: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 02:59:40.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9446" for this suite.

â€¢ [SLOW TEST:81.549 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":118,"skipped":1836,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 02:59:40.349: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-550
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-550
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-550
STEP: Creating statefulset with conflicting port in namespace statefulset-550
STEP: Waiting until pod test-pod will start running in namespace statefulset-550
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-550
Mar 24 02:59:44.504: INFO: Observed stateful pod in namespace: statefulset-550, name: ss-0, uid: eecc3b82-0709-4e9a-8904-bc645330e773, status phase: Pending. Waiting for statefulset controller to delete.
Mar 24 02:59:44.893: INFO: Observed stateful pod in namespace: statefulset-550, name: ss-0, uid: eecc3b82-0709-4e9a-8904-bc645330e773, status phase: Failed. Waiting for statefulset controller to delete.
Mar 24 02:59:44.900: INFO: Observed stateful pod in namespace: statefulset-550, name: ss-0, uid: eecc3b82-0709-4e9a-8904-bc645330e773, status phase: Failed. Waiting for statefulset controller to delete.
Mar 24 02:59:44.903: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-550
STEP: Removing pod with conflicting port in namespace statefulset-550
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-550 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 24 02:59:48.926: INFO: Deleting all statefulset in ns statefulset-550
Mar 24 02:59:48.928: INFO: Scaling statefulset ss to 0
Mar 24 03:00:08.945: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 03:00:08.947: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:00:08.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-550" for this suite.

â€¢ [SLOW TEST:28.615 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":119,"skipped":1839,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:00:08.964: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:00:26.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2199" for this suite.

â€¢ [SLOW TEST:17.185 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":120,"skipped":1864,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:00:26.149: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9955
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:00:26.276: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:01:27.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9955" for this suite.

â€¢ [SLOW TEST:61.331 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":121,"skipped":1867,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:01:27.480: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9446
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:01:38.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9446" for this suite.

â€¢ [SLOW TEST:11.176 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":122,"skipped":1872,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:01:38.656: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-557
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-dba6b7d6-419d-4e21-97e0-4dc202ef2064
STEP: Creating configMap with name cm-test-opt-upd-a890e557-0248-4206-99b0-c21b06ded850
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-dba6b7d6-419d-4e21-97e0-4dc202ef2064
STEP: Updating configmap cm-test-opt-upd-a890e557-0248-4206-99b0-c21b06ded850
STEP: Creating configMap with name cm-test-opt-create-9cdb9f1a-6a24-40a7-8280-62a8340eee79
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:01:42.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-557" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":123,"skipped":1873,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:01:42.876: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar 24 03:01:43.011: INFO: Waiting up to 5m0s for pod "downward-api-4b219a94-0b1b-4ca9-9874-d70fca7317eb" in namespace "downward-api-3401" to be "Succeeded or Failed"
Mar 24 03:01:43.013: INFO: Pod "downward-api-4b219a94-0b1b-4ca9-9874-d70fca7317eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041543ms
Mar 24 03:01:45.018: INFO: Pod "downward-api-4b219a94-0b1b-4ca9-9874-d70fca7317eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0069854s
STEP: Saw pod success
Mar 24 03:01:45.018: INFO: Pod "downward-api-4b219a94-0b1b-4ca9-9874-d70fca7317eb" satisfied condition "Succeeded or Failed"
Mar 24 03:01:45.020: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod downward-api-4b219a94-0b1b-4ca9-9874-d70fca7317eb container dapi-container: <nil>
STEP: delete the pod
Mar 24 03:01:45.042: INFO: Waiting for pod downward-api-4b219a94-0b1b-4ca9-9874-d70fca7317eb to disappear
Mar 24 03:01:45.044: INFO: Pod downward-api-4b219a94-0b1b-4ca9-9874-d70fca7317eb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:01:45.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3401" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":124,"skipped":1956,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:01:45.050: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4238
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4238
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4238
STEP: creating replication controller externalsvc in namespace services-4238
I0324 03:01:45.204584      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4238, replica count: 2
I0324 03:01:48.254756      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar 24 03:01:48.278: INFO: Creating new exec pod
Mar 24 03:01:50.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-4238 exec execpodfqrrt -- /bin/sh -x -c nslookup nodeport-service.services-4238.svc.cluster.local'
Mar 24 03:01:50.438: INFO: stderr: "+ nslookup nodeport-service.services-4238.svc.cluster.local\n"
Mar 24 03:01:50.438: INFO: stdout: "Server:\t\t172.16.0.10\nAddress:\t172.16.0.10#53\n\nnodeport-service.services-4238.svc.cluster.local\tcanonical name = externalsvc.services-4238.svc.cluster.local.\nName:\texternalsvc.services-4238.svc.cluster.local\nAddress: 172.16.225.234\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4238, will wait for the garbage collector to delete the pods
Mar 24 03:01:50.523: INFO: Deleting ReplicationController externalsvc took: 32.185141ms
Mar 24 03:01:51.223: INFO: Terminating ReplicationController externalsvc pods took: 700.132347ms
Mar 24 03:01:55.339: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:01:55.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4238" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

â€¢ [SLOW TEST:10.306 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":125,"skipped":1958,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:01:55.357: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9682
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar 24 03:01:55.494: INFO: Waiting up to 5m0s for pod "downward-api-54a791f7-d0fe-4350-93dd-c06ad493f4ab" in namespace "downward-api-9682" to be "Succeeded or Failed"
Mar 24 03:01:55.496: INFO: Pod "downward-api-54a791f7-d0fe-4350-93dd-c06ad493f4ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1.888035ms
Mar 24 03:01:57.502: INFO: Pod "downward-api-54a791f7-d0fe-4350-93dd-c06ad493f4ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00797592s
STEP: Saw pod success
Mar 24 03:01:57.502: INFO: Pod "downward-api-54a791f7-d0fe-4350-93dd-c06ad493f4ab" satisfied condition "Succeeded or Failed"
Mar 24 03:01:57.504: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod downward-api-54a791f7-d0fe-4350-93dd-c06ad493f4ab container dapi-container: <nil>
STEP: delete the pod
Mar 24 03:01:57.519: INFO: Waiting for pod downward-api-54a791f7-d0fe-4350-93dd-c06ad493f4ab to disappear
Mar 24 03:01:57.520: INFO: Pod downward-api-54a791f7-d0fe-4350-93dd-c06ad493f4ab no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:01:57.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9682" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":126,"skipped":1962,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:01:57.527: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3867
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0324 03:02:03.686664      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 24 03:03:05.702: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:03:05.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3867" for this suite.

â€¢ [SLOW TEST:68.186 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":127,"skipped":1981,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:03:05.713: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6791
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:03:05.846: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ebbda22f-1bae-45d1-9a05-fba09d7371d2" in namespace "downward-api-6791" to be "Succeeded or Failed"
Mar 24 03:03:05.848: INFO: Pod "downwardapi-volume-ebbda22f-1bae-45d1-9a05-fba09d7371d2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.702615ms
Mar 24 03:03:07.853: INFO: Pod "downwardapi-volume-ebbda22f-1bae-45d1-9a05-fba09d7371d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006856363s
STEP: Saw pod success
Mar 24 03:03:07.853: INFO: Pod "downwardapi-volume-ebbda22f-1bae-45d1-9a05-fba09d7371d2" satisfied condition "Succeeded or Failed"
Mar 24 03:03:07.855: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod downwardapi-volume-ebbda22f-1bae-45d1-9a05-fba09d7371d2 container client-container: <nil>
STEP: delete the pod
Mar 24 03:03:07.869: INFO: Waiting for pod downwardapi-volume-ebbda22f-1bae-45d1-9a05-fba09d7371d2 to disappear
Mar 24 03:03:07.870: INFO: Pod downwardapi-volume-ebbda22f-1bae-45d1-9a05-fba09d7371d2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:03:07.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6791" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":128,"skipped":2027,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:03:07.876: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-8500
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Mar 24 03:03:08.003: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 24 03:04:08.032: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:04:08.035: INFO: Starting informer...
STEP: Starting pod...
Mar 24 03:04:08.244: INFO: Pod is running on cn-hongkong.192.168.0.14. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Mar 24 03:04:08.255: INFO: Pod wasn't evicted. Proceeding
Mar 24 03:04:08.255: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Mar 24 03:05:23.266: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:05:23.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-8500" for this suite.

â€¢ [SLOW TEST:135.408 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":129,"skipped":2056,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:05:23.284: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4977
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-3f65dfb0-eb78-477a-97bf-6ee16612655c
STEP: Creating a pod to test consume configMaps
Mar 24 03:05:23.425: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-732b86ec-14d6-423b-8c34-edaf657548b1" in namespace "projected-4977" to be "Succeeded or Failed"
Mar 24 03:05:23.426: INFO: Pod "pod-projected-configmaps-732b86ec-14d6-423b-8c34-edaf657548b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.577361ms
Mar 24 03:05:25.431: INFO: Pod "pod-projected-configmaps-732b86ec-14d6-423b-8c34-edaf657548b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006005076s
STEP: Saw pod success
Mar 24 03:05:25.431: INFO: Pod "pod-projected-configmaps-732b86ec-14d6-423b-8c34-edaf657548b1" satisfied condition "Succeeded or Failed"
Mar 24 03:05:25.432: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod pod-projected-configmaps-732b86ec-14d6-423b-8c34-edaf657548b1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 03:05:25.451: INFO: Waiting for pod pod-projected-configmaps-732b86ec-14d6-423b-8c34-edaf657548b1 to disappear
Mar 24 03:05:25.453: INFO: Pod pod-projected-configmaps-732b86ec-14d6-423b-8c34-edaf657548b1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:05:25.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4977" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":130,"skipped":2074,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:05:25.458: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7101
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Mar 24 03:05:25.584: INFO: namespace kubectl-7101
Mar 24 03:05:25.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-7101 create -f -'
Mar 24 03:05:25.800: INFO: stderr: ""
Mar 24 03:05:25.800: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 24 03:05:26.804: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 24 03:05:26.804: INFO: Found 1 / 1
Mar 24 03:05:26.804: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 24 03:05:26.806: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 24 03:05:26.806: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 24 03:05:26.806: INFO: wait on agnhost-primary startup in kubectl-7101 
Mar 24 03:05:26.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-7101 logs agnhost-primary-lgcc2 agnhost-primary'
Mar 24 03:05:26.872: INFO: stderr: ""
Mar 24 03:05:26.872: INFO: stdout: "Paused\n"
STEP: exposing RC
Mar 24 03:05:26.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-7101 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar 24 03:05:26.946: INFO: stderr: ""
Mar 24 03:05:26.946: INFO: stdout: "service/rm2 exposed\n"
Mar 24 03:05:26.948: INFO: Service rm2 in namespace kubectl-7101 found.
STEP: exposing service
Mar 24 03:05:28.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-7101 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar 24 03:05:29.022: INFO: stderr: ""
Mar 24 03:05:29.022: INFO: stdout: "service/rm3 exposed\n"
Mar 24 03:05:29.024: INFO: Service rm3 in namespace kubectl-7101 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:05:31.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7101" for this suite.

â€¢ [SLOW TEST:5.580 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":131,"skipped":2108,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:05:31.039: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5739
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-9906
STEP: Creating secret with name secret-test-6d2855ab-1a01-45a1-a068-5863b98a245d
STEP: Creating a pod to test consume secrets
Mar 24 03:05:31.299: INFO: Waiting up to 5m0s for pod "pod-secrets-dc6c4b66-3c66-4b68-af8a-2a18f74df57f" in namespace "secrets-5739" to be "Succeeded or Failed"
Mar 24 03:05:31.300: INFO: Pod "pod-secrets-dc6c4b66-3c66-4b68-af8a-2a18f74df57f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.923967ms
Mar 24 03:05:33.306: INFO: Pod "pod-secrets-dc6c4b66-3c66-4b68-af8a-2a18f74df57f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00739329s
STEP: Saw pod success
Mar 24 03:05:33.306: INFO: Pod "pod-secrets-dc6c4b66-3c66-4b68-af8a-2a18f74df57f" satisfied condition "Succeeded or Failed"
Mar 24 03:05:33.308: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod pod-secrets-dc6c4b66-3c66-4b68-af8a-2a18f74df57f container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 03:05:33.321: INFO: Waiting for pod pod-secrets-dc6c4b66-3c66-4b68-af8a-2a18f74df57f to disappear
Mar 24 03:05:33.322: INFO: Pod pod-secrets-dc6c4b66-3c66-4b68-af8a-2a18f74df57f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:05:33.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5739" for this suite.
STEP: Destroying namespace "secret-namespace-9906" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":132,"skipped":2128,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:05:33.331: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-357
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Mar 24 03:05:33.467: INFO: Waiting up to 5m0s for pod "pod-41c5e57f-1ea7-4b6a-88a9-9e4ff429b7e5" in namespace "emptydir-357" to be "Succeeded or Failed"
Mar 24 03:05:33.470: INFO: Pod "pod-41c5e57f-1ea7-4b6a-88a9-9e4ff429b7e5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.568662ms
Mar 24 03:05:35.474: INFO: Pod "pod-41c5e57f-1ea7-4b6a-88a9-9e4ff429b7e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007461739s
STEP: Saw pod success
Mar 24 03:05:35.474: INFO: Pod "pod-41c5e57f-1ea7-4b6a-88a9-9e4ff429b7e5" satisfied condition "Succeeded or Failed"
Mar 24 03:05:35.476: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod pod-41c5e57f-1ea7-4b6a-88a9-9e4ff429b7e5 container test-container: <nil>
STEP: delete the pod
Mar 24 03:05:35.489: INFO: Waiting for pod pod-41c5e57f-1ea7-4b6a-88a9-9e4ff429b7e5 to disappear
Mar 24 03:05:35.491: INFO: Pod pod-41c5e57f-1ea7-4b6a-88a9-9e4ff429b7e5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:05:35.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-357" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":133,"skipped":2128,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:05:35.497: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2421
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:05:37.636: INFO: Deleting pod "var-expansion-0f848f88-082f-408d-8a88-d12411b98063" in namespace "var-expansion-2421"
Mar 24 03:05:37.644: INFO: Wait up to 5m0s for pod "var-expansion-0f848f88-082f-408d-8a88-d12411b98063" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:05:43.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2421" for this suite.

â€¢ [SLOW TEST:8.164 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":134,"skipped":2135,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:05:43.660: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1491
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-1491
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 24 03:05:43.788: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 24 03:05:43.810: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 24 03:05:45.815: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 03:05:47.815: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 03:05:49.815: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 03:05:51.816: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 03:05:53.815: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 03:05:55.815: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 24 03:05:55.819: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 24 03:05:57.824: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 24 03:05:59.824: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 24 03:06:01.824: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 24 03:06:01.828: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 24 03:06:03.853: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 24 03:06:03.853: INFO: Going to poll 10.43.0.225 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 24 03:06:03.854: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.43.0.225 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1491 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:06:03.854: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 03:06:04.946: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 24 03:06:04.946: INFO: Going to poll 10.43.1.99 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 24 03:06:04.949: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.43.1.99 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1491 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:06:04.949: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 03:06:06.050: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 24 03:06:06.050: INFO: Going to poll 10.43.1.18 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 24 03:06:06.055: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.43.1.18 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1491 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:06:06.055: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 03:06:07.170: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:06:07.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1491" for this suite.

â€¢ [SLOW TEST:23.521 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":135,"skipped":2137,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:06:07.181: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-106
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-53aee603-b9ec-4ba7-b842-a9a318df5b27
STEP: Creating a pod to test consume configMaps
Mar 24 03:06:07.315: INFO: Waiting up to 5m0s for pod "pod-configmaps-eb5b9f7e-550a-45be-b594-1eaf03cfffde" in namespace "configmap-106" to be "Succeeded or Failed"
Mar 24 03:06:07.317: INFO: Pod "pod-configmaps-eb5b9f7e-550a-45be-b594-1eaf03cfffde": Phase="Pending", Reason="", readiness=false. Elapsed: 1.588426ms
Mar 24 03:06:09.323: INFO: Pod "pod-configmaps-eb5b9f7e-550a-45be-b594-1eaf03cfffde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007602865s
STEP: Saw pod success
Mar 24 03:06:09.323: INFO: Pod "pod-configmaps-eb5b9f7e-550a-45be-b594-1eaf03cfffde" satisfied condition "Succeeded or Failed"
Mar 24 03:06:09.325: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod pod-configmaps-eb5b9f7e-550a-45be-b594-1eaf03cfffde container agnhost-container: <nil>
STEP: delete the pod
Mar 24 03:06:09.340: INFO: Waiting for pod pod-configmaps-eb5b9f7e-550a-45be-b594-1eaf03cfffde to disappear
Mar 24 03:06:09.341: INFO: Pod pod-configmaps-eb5b9f7e-550a-45be-b594-1eaf03cfffde no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:06:09.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-106" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":136,"skipped":2142,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:06:09.348: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2218
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:06:09.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2218" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":137,"skipped":2186,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:06:09.494: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3778
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-h28j
STEP: Creating a pod to test atomic-volume-subpath
Mar 24 03:06:09.630: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-h28j" in namespace "subpath-3778" to be "Succeeded or Failed"
Mar 24 03:06:09.631: INFO: Pod "pod-subpath-test-downwardapi-h28j": Phase="Pending", Reason="", readiness=false. Elapsed: 1.576439ms
Mar 24 03:06:11.637: INFO: Pod "pod-subpath-test-downwardapi-h28j": Phase="Running", Reason="", readiness=true. Elapsed: 2.00678698s
Mar 24 03:06:13.642: INFO: Pod "pod-subpath-test-downwardapi-h28j": Phase="Running", Reason="", readiness=true. Elapsed: 4.011786975s
Mar 24 03:06:15.646: INFO: Pod "pod-subpath-test-downwardapi-h28j": Phase="Running", Reason="", readiness=true. Elapsed: 6.016150752s
Mar 24 03:06:17.651: INFO: Pod "pod-subpath-test-downwardapi-h28j": Phase="Running", Reason="", readiness=true. Elapsed: 8.021350681s
Mar 24 03:06:19.656: INFO: Pod "pod-subpath-test-downwardapi-h28j": Phase="Running", Reason="", readiness=true. Elapsed: 10.026572471s
Mar 24 03:06:21.662: INFO: Pod "pod-subpath-test-downwardapi-h28j": Phase="Running", Reason="", readiness=true. Elapsed: 12.031828854s
Mar 24 03:06:23.667: INFO: Pod "pod-subpath-test-downwardapi-h28j": Phase="Running", Reason="", readiness=true. Elapsed: 14.036904521s
Mar 24 03:06:25.671: INFO: Pod "pod-subpath-test-downwardapi-h28j": Phase="Running", Reason="", readiness=true. Elapsed: 16.041041555s
Mar 24 03:06:27.676: INFO: Pod "pod-subpath-test-downwardapi-h28j": Phase="Running", Reason="", readiness=true. Elapsed: 18.045821307s
Mar 24 03:06:29.681: INFO: Pod "pod-subpath-test-downwardapi-h28j": Phase="Running", Reason="", readiness=true. Elapsed: 20.051114513s
Mar 24 03:06:31.686: INFO: Pod "pod-subpath-test-downwardapi-h28j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.05646279s
STEP: Saw pod success
Mar 24 03:06:31.686: INFO: Pod "pod-subpath-test-downwardapi-h28j" satisfied condition "Succeeded or Failed"
Mar 24 03:06:31.688: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod pod-subpath-test-downwardapi-h28j container test-container-subpath-downwardapi-h28j: <nil>
STEP: delete the pod
Mar 24 03:06:31.704: INFO: Waiting for pod pod-subpath-test-downwardapi-h28j to disappear
Mar 24 03:06:31.705: INFO: Pod pod-subpath-test-downwardapi-h28j no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-h28j
Mar 24 03:06:31.705: INFO: Deleting pod "pod-subpath-test-downwardapi-h28j" in namespace "subpath-3778"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:06:31.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3778" for this suite.

â€¢ [SLOW TEST:22.219 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":138,"skipped":2209,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:06:31.712: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6730
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:06:31.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6730" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":139,"skipped":2212,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:06:31.864: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8752
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:06:31.994: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b31722ed-eb73-4e0f-803b-a094accdb880" in namespace "projected-8752" to be "Succeeded or Failed"
Mar 24 03:06:31.996: INFO: Pod "downwardapi-volume-b31722ed-eb73-4e0f-803b-a094accdb880": Phase="Pending", Reason="", readiness=false. Elapsed: 1.60728ms
Mar 24 03:06:34.001: INFO: Pod "downwardapi-volume-b31722ed-eb73-4e0f-803b-a094accdb880": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00723349s
STEP: Saw pod success
Mar 24 03:06:34.001: INFO: Pod "downwardapi-volume-b31722ed-eb73-4e0f-803b-a094accdb880" satisfied condition "Succeeded or Failed"
Mar 24 03:06:34.003: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod downwardapi-volume-b31722ed-eb73-4e0f-803b-a094accdb880 container client-container: <nil>
STEP: delete the pod
Mar 24 03:06:34.021: INFO: Waiting for pod downwardapi-volume-b31722ed-eb73-4e0f-803b-a094accdb880 to disappear
Mar 24 03:06:34.023: INFO: Pod downwardapi-volume-b31722ed-eb73-4e0f-803b-a094accdb880 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:06:34.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8752" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":140,"skipped":2213,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:06:34.029: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9484
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Mar 24 03:06:34.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9484 cluster-info'
Mar 24 03:06:34.210: INFO: stderr: ""
Mar 24 03:06:34.210: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.16.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:06:34.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9484" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":141,"skipped":2214,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:06:34.218: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6966
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Mar 24 03:06:34.343: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar 24 03:06:34.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-6966 create -f -'
Mar 24 03:06:34.550: INFO: stderr: ""
Mar 24 03:06:34.550: INFO: stdout: "service/agnhost-replica created\n"
Mar 24 03:06:34.550: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar 24 03:06:34.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-6966 create -f -'
Mar 24 03:06:34.766: INFO: stderr: ""
Mar 24 03:06:34.766: INFO: stdout: "service/agnhost-primary created\n"
Mar 24 03:06:34.766: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 24 03:06:34.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-6966 create -f -'
Mar 24 03:06:34.972: INFO: stderr: ""
Mar 24 03:06:34.972: INFO: stdout: "service/frontend created\n"
Mar 24 03:06:34.972: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 24 03:06:34.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-6966 create -f -'
Mar 24 03:06:35.177: INFO: stderr: ""
Mar 24 03:06:35.177: INFO: stdout: "deployment.apps/frontend created\n"
Mar 24 03:06:35.178: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 24 03:06:35.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-6966 create -f -'
Mar 24 03:06:35.375: INFO: stderr: ""
Mar 24 03:06:35.375: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar 24 03:06:35.375: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 24 03:06:35.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-6966 create -f -'
Mar 24 03:06:35.575: INFO: stderr: ""
Mar 24 03:06:35.575: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Mar 24 03:06:35.575: INFO: Waiting for all frontend pods to be Running.
Mar 24 03:06:40.625: INFO: Waiting for frontend to serve content.
Mar 24 03:06:40.632: INFO: Trying to add a new entry to the guestbook.
Mar 24 03:06:40.638: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar 24 03:06:40.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-6966 delete --grace-period=0 --force -f -'
Mar 24 03:06:40.715: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:06:40.715: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Mar 24 03:06:40.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-6966 delete --grace-period=0 --force -f -'
Mar 24 03:06:40.791: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:06:40.791: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar 24 03:06:40.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-6966 delete --grace-period=0 --force -f -'
Mar 24 03:06:40.859: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:06:40.859: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 24 03:06:40.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-6966 delete --grace-period=0 --force -f -'
Mar 24 03:06:40.918: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:06:40.918: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 24 03:06:40.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-6966 delete --grace-period=0 --force -f -'
Mar 24 03:06:40.973: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:06:40.973: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar 24 03:06:40.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-6966 delete --grace-period=0 --force -f -'
Mar 24 03:06:41.028: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:06:41.028: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:06:41.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6966" for this suite.

â€¢ [SLOW TEST:6.817 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":142,"skipped":2237,"failed":0}
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:06:41.035: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2554
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:06:41.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2554 create -f -'
Mar 24 03:06:41.367: INFO: stderr: ""
Mar 24 03:06:41.367: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar 24 03:06:41.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2554 create -f -'
Mar 24 03:06:41.575: INFO: stderr: ""
Mar 24 03:06:41.575: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 24 03:06:42.578: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 24 03:06:42.578: INFO: Found 0 / 1
Mar 24 03:06:43.579: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 24 03:06:43.579: INFO: Found 1 / 1
Mar 24 03:06:43.579: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 24 03:06:43.581: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 24 03:06:43.581: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 24 03:06:43.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2554 describe pod agnhost-primary-s9z5l'
Mar 24 03:06:43.644: INFO: stderr: ""
Mar 24 03:06:43.644: INFO: stdout: "Name:         agnhost-primary-s9z5l\nNamespace:    kubectl-2554\nPriority:     0\nNode:         cn-hongkong.192.168.0.15/192.168.0.15\nStart Time:   Wed, 24 Mar 2021 03:06:41 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  kubernetes.io/psp: ack.privileged\nStatus:       Running\nIP:           10.43.1.23\nIPs:\n  IP:           10.43.1.23\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://79f91177c2b77cb0eff0c4f52e2f0a922841b269d404fa809a9bd722a97aeaec\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 24 Mar 2021 03:06:42 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-w7hfr (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-w7hfr:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-w7hfr\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-2554/agnhost-primary-s9z5l to cn-hongkong.192.168.0.15\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Mar 24 03:06:43.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2554 describe rc agnhost-primary'
Mar 24 03:06:43.709: INFO: stderr: ""
Mar 24 03:06:43.709: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2554\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-s9z5l\n"
Mar 24 03:06:43.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2554 describe service agnhost-primary'
Mar 24 03:06:43.766: INFO: stderr: ""
Mar 24 03:06:43.766: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2554\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                172.16.243.185\nIPs:               172.16.243.185\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.43.1.23:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 24 03:06:43.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2554 describe node cn-hongkong.192.168.0.10'
Mar 24 03:06:43.840: INFO: stderr: ""
Mar 24 03:06:43.840: INFO: stdout: "Name:               cn-hongkong.192.168.0.10\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=ecs.c6.2xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=cn-hongkong\n                    failure-domain.beta.kubernetes.io/zone=cn-hongkong-b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cn-hongkong.192.168.0.10\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=ecs.c6.2xlarge\n                    topology.diskplugin.csi.alibabacloud.com/zone=cn-hongkong-b\n                    topology.kubernetes.io/region=cn-hongkong\n                    topology.kubernetes.io/zone=cn-hongkong-b\nAnnotations:        csi.volume.kubernetes.io/nodeid:\n                      {\"diskplugin.csi.alibabacloud.com\":\"i-j6c2zb75enfgb6xb09qz\",\"nasplugin.csi.alibabacloud.com\":\"i-j6c2zb75enfgb6xb09qz\",\"ossplugin.csi.aliba...\n                    flannel.alpha.coreos.com/backend-data: null\n                    flannel.alpha.coreos.com/backend-type: \n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.0.10\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 22 Mar 2021 03:20:36 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  cn-hongkong.192.168.0.10\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 24 Mar 2021 03:06:36 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 22 Mar 2021 03:29:25 +0000   Mon, 22 Mar 2021 03:29:25 +0000   RouteCreated                 RouteController created a route\n  MemoryPressure       False   Wed, 24 Mar 2021 03:01:45 +0000   Mon, 22 Mar 2021 03:20:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 24 Mar 2021 03:01:45 +0000   Mon, 22 Mar 2021 03:20:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 24 Mar 2021 03:01:45 +0000   Mon, 22 Mar 2021 03:20:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 24 Mar 2021 03:01:45 +0000   Mon, 22 Mar 2021 03:29:10 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.0.10\n  Hostname:    cn-hongkong.192.168.0.10\nCapacity:\n  cpu:                8\n  ephemeral-storage:  123722704Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15626632Ki\n  pods:               64\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  114022843818\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             14602632Ki\n  pods:               64\nSystem Info:\n  Machine ID:                 20200529205003284907652924514599\n  System UUID:                22f304a9-3877-4819-872f-b3a56691f608\n  Boot ID:                    b201849a-9ad1-471d-a0ce-7843906c4a5d\n  Kernel Version:             4.19.91-19.1.al7.x86_64\n  OS Image:                   Aliyun Linux 2.1903 LTS (Hunting Beagle)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.5\n  Kubelet Version:            v1.20.4-aliyun.1\n  Kube-Proxy Version:         v1.20.4-aliyun.1\nPodCIDR:                      10.43.0.0/26\nPodCIDRs:                     10.43.0.0/26\nProviderID:                   cn-hongkong.i-j6c2zb75enfgb6xb09qz\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 alicloud-monitor-controller-7d46cf5f94-khtz9               0 (0%)        0 (0%)      0 (0%)           0 (0%)         47h\n  kube-system                 cloud-controller-manager-xtqh7                             200m (2%)     0 (0%)      0 (0%)           0 (0%)         47h\n  kube-system                 csi-plugin-f8dlk                                           100m (1%)     500m (6%)   128Mi (0%)       1Gi (7%)       47h\n  kube-system                 csi-provisioner-57c8d966fb-24j9p                           100m (1%)     500m (6%)   128Mi (0%)       512Mi (3%)     47h\n  kube-system                 kube-apiserver-cn-hongkong.192.168.0.10                    250m (3%)     0 (0%)      0 (0%)           0 (0%)         47h\n  kube-system                 kube-controller-manager-cn-hongkong.192.168.0.10           200m (2%)     0 (0%)      0 (0%)           0 (0%)         47h\n  kube-system                 kube-flannel-ds-2tg6j                                      100m (1%)     100m (1%)   100Mi (0%)       256Mi (1%)     42h\n  kube-system                 kube-proxy-master-8kx55                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         47h\n  kube-system                 kube-scheduler-cn-hongkong.192.168.0.10                    100m (1%)     0 (0%)      0 (0%)           0 (0%)         47h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-46debedd236a484a-48cds    0 (0%)        0 (0%)      0 (0%)           0 (0%)         40m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1050m (13%)  1100m (13%)\n  memory             356Mi (2%)   1792Mi (12%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
Mar 24 03:06:43.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2554 describe namespace kubectl-2554'
Mar 24 03:06:43.896: INFO: stderr: ""
Mar 24 03:06:43.896: INFO: stdout: "Name:         kubectl-2554\nLabels:       e2e-framework=kubectl\n              e2e-run=0e7ab19b-68f5-4fce-a9f4-03e42eb6f230\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:06:43.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2554" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":143,"skipped":2237,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:06:43.903: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8973
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-561c59e5-eef6-4a65-bac9-9233be553737
STEP: Creating a pod to test consume configMaps
Mar 24 03:06:44.038: INFO: Waiting up to 5m0s for pod "pod-configmaps-a7a5779c-ab47-406d-b7a4-01fc6d214a75" in namespace "configmap-8973" to be "Succeeded or Failed"
Mar 24 03:06:44.040: INFO: Pod "pod-configmaps-a7a5779c-ab47-406d-b7a4-01fc6d214a75": Phase="Pending", Reason="", readiness=false. Elapsed: 1.71896ms
Mar 24 03:06:46.044: INFO: Pod "pod-configmaps-a7a5779c-ab47-406d-b7a4-01fc6d214a75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006510686s
STEP: Saw pod success
Mar 24 03:06:46.044: INFO: Pod "pod-configmaps-a7a5779c-ab47-406d-b7a4-01fc6d214a75" satisfied condition "Succeeded or Failed"
Mar 24 03:06:46.046: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-configmaps-a7a5779c-ab47-406d-b7a4-01fc6d214a75 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 03:06:46.063: INFO: Waiting for pod pod-configmaps-a7a5779c-ab47-406d-b7a4-01fc6d214a75 to disappear
Mar 24 03:06:46.065: INFO: Pod pod-configmaps-a7a5779c-ab47-406d-b7a4-01fc6d214a75 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:06:46.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8973" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":144,"skipped":2244,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:06:46.071: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar 24 03:06:48.218: INFO: &Pod{ObjectMeta:{send-events-9767e927-0ace-4abe-bd23-2c65882bf3a1  events-9404  40889d9b-5414-4826-a46d-441f06bdf678 796179 0 2021-03-24 03:06:46 +0000 UTC <nil> <nil> map[name:foo time:201650824] map[kubernetes.io/psp:ack.privileged] [] []  [{e2e.test Update v1 2021-03-24 03:06:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 03:06:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.43.1.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9cftx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9cftx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9cftx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:06:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:06:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:06:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:06:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.14,PodIP:10.43.1.106,StartTime:2021-03-24 03:06:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 03:06:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://f91202ed031e05bc57a7c66bd67b824bffde1573f8bbcb148ca0e39f283b5f54,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.43.1.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar 24 03:06:50.223: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar 24 03:06:52.229: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:06:52.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9404" for this suite.

â€¢ [SLOW TEST:6.173 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":145,"skipped":2260,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:06:52.245: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-261
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Mar 24 03:06:52.373: INFO: PodSpec: initContainers in spec.initContainers
Mar 24 03:07:31.882: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b9e737c7-7a88-464e-9101-9bc8b8688fab", GenerateName:"", Namespace:"init-container-261", SelfLink:"", UID:"d953ecb7-9b6f-48a2-81c8-c46665591063", ResourceVersion:"796438", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63752152012, loc:(*time.Location)(0x797de40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"373516592"}, Annotations:map[string]string{"kubernetes.io/psp":"ack.privileged"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0038abe40), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0038abe80)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0038abec0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0038abf00)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-qpsds", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc004d1de00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qpsds", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qpsds", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qpsds", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003269508), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cn-hongkong.192.168.0.15", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0032598f0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003269580)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0032695a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0032695a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0032695ac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc003ad3560), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752152012, loc:(*time.Location)(0x797de40)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752152012, loc:(*time.Location)(0x797de40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752152012, loc:(*time.Location)(0x797de40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752152012, loc:(*time.Location)(0x797de40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.0.15", PodIP:"10.43.1.24", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.43.1.24"}}, StartTime:(*v1.Time)(0xc0038abf20), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0032599d0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003259a40)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://c2a4e7adb5f4f8c68db4a2b6d2ce1d63034e3838d876a8c9ba0b1797bb2116fc", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038abf60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038abf40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc00326964f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:07:31.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-261" for this suite.

â€¢ [SLOW TEST:39.653 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":146,"skipped":2287,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:07:31.898: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6736
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-559056f3-ee61-4d33-a5e4-5f0150996981
STEP: Creating a pod to test consume secrets
Mar 24 03:07:32.034: INFO: Waiting up to 5m0s for pod "pod-secrets-58f23aab-d484-4b3d-902f-4f64afd7978c" in namespace "secrets-6736" to be "Succeeded or Failed"
Mar 24 03:07:32.036: INFO: Pod "pod-secrets-58f23aab-d484-4b3d-902f-4f64afd7978c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.706791ms
Mar 24 03:07:34.041: INFO: Pod "pod-secrets-58f23aab-d484-4b3d-902f-4f64afd7978c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0072219s
STEP: Saw pod success
Mar 24 03:07:34.041: INFO: Pod "pod-secrets-58f23aab-d484-4b3d-902f-4f64afd7978c" satisfied condition "Succeeded or Failed"
Mar 24 03:07:34.043: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-secrets-58f23aab-d484-4b3d-902f-4f64afd7978c container secret-env-test: <nil>
STEP: delete the pod
Mar 24 03:07:34.056: INFO: Waiting for pod pod-secrets-58f23aab-d484-4b3d-902f-4f64afd7978c to disappear
Mar 24 03:07:34.058: INFO: Pod pod-secrets-58f23aab-d484-4b3d-902f-4f64afd7978c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:07:34.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6736" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":147,"skipped":2349,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:07:34.065: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9889
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Mar 24 03:07:34.191: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:07:53.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9889" for this suite.

â€¢ [SLOW TEST:19.793 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":148,"skipped":2352,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:07:53.858: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4716
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Mar 24 03:07:56.001: INFO: Pod pod-hostip-612ee694-4795-4b40-aad3-46d9b2747097 has hostIP: 192.168.0.14
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:07:56.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4716" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":149,"skipped":2377,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:07:56.009: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-7958
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-7958
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7958
STEP: Deleting pre-stop pod
Mar 24 03:08:05.176: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:08:05.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7958" for this suite.

â€¢ [SLOW TEST:9.182 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":150,"skipped":2390,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:08:05.191: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2611
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:08:07.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2611" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":151,"skipped":2404,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:08:07.348: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 03:08:07.958: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 03:08:10.976: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:08:10.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1159" for this suite.
STEP: Destroying namespace "webhook-1159-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":152,"skipped":2443,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:08:11.021: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6542
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:08:11.148: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:08:16.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6542" for this suite.

â€¢ [SLOW TEST:5.665 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":153,"skipped":2444,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:08:16.686: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1167
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-2ccd0c90-480d-4b17-95ba-ddb333cf411b in namespace container-probe-1167
Mar 24 03:08:18.859: INFO: Started pod test-webserver-2ccd0c90-480d-4b17-95ba-ddb333cf411b in namespace container-probe-1167
STEP: checking the pod's current state and verifying that restartCount is present
Mar 24 03:08:18.861: INFO: Initial restart count of pod test-webserver-2ccd0c90-480d-4b17-95ba-ddb333cf411b is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:12:19.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1167" for this suite.

â€¢ [SLOW TEST:242.852 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":154,"skipped":2451,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:12:19.538: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2632
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:12:21.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2632" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":155,"skipped":2476,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:12:21.705: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4043
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:12:21.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4043" for this suite.
â€¢{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":156,"skipped":2572,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:12:21.846: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 24 03:12:21.978: INFO: Waiting up to 5m0s for pod "pod-648d1742-09c4-42b9-a887-284007a076a9" in namespace "emptydir-2694" to be "Succeeded or Failed"
Mar 24 03:12:21.980: INFO: Pod "pod-648d1742-09c4-42b9-a887-284007a076a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.61019ms
Mar 24 03:12:23.985: INFO: Pod "pod-648d1742-09c4-42b9-a887-284007a076a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007051596s
STEP: Saw pod success
Mar 24 03:12:23.985: INFO: Pod "pod-648d1742-09c4-42b9-a887-284007a076a9" satisfied condition "Succeeded or Failed"
Mar 24 03:12:23.987: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod pod-648d1742-09c4-42b9-a887-284007a076a9 container test-container: <nil>
STEP: delete the pod
Mar 24 03:12:24.009: INFO: Waiting for pod pod-648d1742-09c4-42b9-a887-284007a076a9 to disappear
Mar 24 03:12:24.010: INFO: Pod pod-648d1742-09c4-42b9-a887-284007a076a9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:12:24.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2694" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":157,"skipped":2573,"failed":0}

------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:12:24.016: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4248
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Mar 24 03:12:24.142: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:12:27.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4248" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":158,"skipped":2573,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:12:27.375: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2043
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2043
Mar 24 03:12:29.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-2043 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 24 03:12:29.790: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 24 03:12:29.790: INFO: stdout: "ipvs"
Mar 24 03:12:29.790: INFO: proxyMode: ipvs
Mar 24 03:12:29.801: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 24 03:12:29.803: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-2043
STEP: creating replication controller affinity-nodeport-timeout in namespace services-2043
I0324 03:12:29.817162      21 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2043, replica count: 3
I0324 03:12:32.867444      21 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 03:12:32.878: INFO: Creating new exec pod
Mar 24 03:12:35.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-2043 exec execpod-affinityr7ttc -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Mar 24 03:12:36.040: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar 24 03:12:36.040: INFO: stdout: ""
Mar 24 03:12:36.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-2043 exec execpod-affinityr7ttc -- /bin/sh -x -c nc -zv -t -w 2 172.16.132.214 80'
Mar 24 03:12:36.190: INFO: stderr: "+ nc -zv -t -w 2 172.16.132.214 80\nConnection to 172.16.132.214 80 port [tcp/http] succeeded!\n"
Mar 24 03:12:36.190: INFO: stdout: ""
Mar 24 03:12:36.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-2043 exec execpod-affinityr7ttc -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.14 31278'
Mar 24 03:12:36.337: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.14 31278\nConnection to 192.168.0.14 31278 port [tcp/31278] succeeded!\n"
Mar 24 03:12:36.337: INFO: stdout: ""
Mar 24 03:12:36.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-2043 exec execpod-affinityr7ttc -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.13 31278'
Mar 24 03:12:36.480: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.13 31278\nConnection to 192.168.0.13 31278 port [tcp/31278] succeeded!\n"
Mar 24 03:12:36.480: INFO: stdout: ""
Mar 24 03:12:36.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-2043 exec execpod-affinityr7ttc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.13:31278/ ; done'
Mar 24 03:12:36.677: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n"
Mar 24 03:12:36.677: INFO: stdout: "\naffinity-nodeport-timeout-564rw\naffinity-nodeport-timeout-564rw\naffinity-nodeport-timeout-564rw\naffinity-nodeport-timeout-564rw\naffinity-nodeport-timeout-564rw\naffinity-nodeport-timeout-564rw\naffinity-nodeport-timeout-564rw\naffinity-nodeport-timeout-564rw\naffinity-nodeport-timeout-564rw\naffinity-nodeport-timeout-564rw\naffinity-nodeport-timeout-564rw\naffinity-nodeport-timeout-564rw\naffinity-nodeport-timeout-564rw\naffinity-nodeport-timeout-564rw\naffinity-nodeport-timeout-564rw\naffinity-nodeport-timeout-564rw"
Mar 24 03:12:36.677: INFO: Received response from host: affinity-nodeport-timeout-564rw
Mar 24 03:12:36.677: INFO: Received response from host: affinity-nodeport-timeout-564rw
Mar 24 03:12:36.677: INFO: Received response from host: affinity-nodeport-timeout-564rw
Mar 24 03:12:36.677: INFO: Received response from host: affinity-nodeport-timeout-564rw
Mar 24 03:12:36.677: INFO: Received response from host: affinity-nodeport-timeout-564rw
Mar 24 03:12:36.677: INFO: Received response from host: affinity-nodeport-timeout-564rw
Mar 24 03:12:36.677: INFO: Received response from host: affinity-nodeport-timeout-564rw
Mar 24 03:12:36.677: INFO: Received response from host: affinity-nodeport-timeout-564rw
Mar 24 03:12:36.677: INFO: Received response from host: affinity-nodeport-timeout-564rw
Mar 24 03:12:36.677: INFO: Received response from host: affinity-nodeport-timeout-564rw
Mar 24 03:12:36.677: INFO: Received response from host: affinity-nodeport-timeout-564rw
Mar 24 03:12:36.677: INFO: Received response from host: affinity-nodeport-timeout-564rw
Mar 24 03:12:36.677: INFO: Received response from host: affinity-nodeport-timeout-564rw
Mar 24 03:12:36.677: INFO: Received response from host: affinity-nodeport-timeout-564rw
Mar 24 03:12:36.677: INFO: Received response from host: affinity-nodeport-timeout-564rw
Mar 24 03:12:36.677: INFO: Received response from host: affinity-nodeport-timeout-564rw
Mar 24 03:12:36.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-2043 exec execpod-affinityr7ttc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.13:31278/'
Mar 24 03:12:36.817: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n"
Mar 24 03:12:36.817: INFO: stdout: "affinity-nodeport-timeout-564rw"
Mar 24 03:14:46.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-2043 exec execpod-affinityr7ttc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.0.13:31278/'
Mar 24 03:14:46.963: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.0.13:31278/\n"
Mar 24 03:14:46.963: INFO: stdout: "affinity-nodeport-timeout-nq2gn"
Mar 24 03:14:46.963: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2043, will wait for the garbage collector to delete the pods
Mar 24 03:14:47.029: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 4.772351ms
Mar 24 03:14:47.729: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 700.13492ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:15:02.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2043" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

â€¢ [SLOW TEST:155.287 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":159,"skipped":2587,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:15:02.662: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6001
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:15:02.796: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 24 03:15:02.801: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 24 03:15:07.805: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 24 03:15:07.805: INFO: Creating deployment "test-rolling-update-deployment"
Mar 24 03:15:07.810: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 24 03:15:07.814: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 24 03:15:09.821: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 24 03:15:09.823: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar 24 03:15:09.831: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6001  5c52b909-91b0-40f0-839e-2b1198605d75 798788 1 2021-03-24 03:15:07 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-03-24 03:15:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-24 03:15:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042f9828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-24 03:15:07 +0000 UTC,LastTransitionTime:2021-03-24 03:15:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-03-24 03:15:09 +0000 UTC,LastTransitionTime:2021-03-24 03:15:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 24 03:15:09.833: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-6001  55b4abfd-cf6e-4e43-8023-83c326c9ea1d 798777 1 2021-03-24 03:15:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5c52b909-91b0-40f0-839e-2b1198605d75 0xc0042f9cc7 0xc0042f9cc8}] []  [{kube-controller-manager Update apps/v1 2021-03-24 03:15:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c52b909-91b0-40f0-839e-2b1198605d75\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042f9d58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 24 03:15:09.833: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 24 03:15:09.833: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6001  2e68289b-3448-45c7-8562-20bba186b4c0 798786 2 2021-03-24 03:15:02 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5c52b909-91b0-40f0-839e-2b1198605d75 0xc0042f9bb7 0xc0042f9bb8}] []  [{e2e.test Update apps/v1 2021-03-24 03:15:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-24 03:15:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c52b909-91b0-40f0-839e-2b1198605d75\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0042f9c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 24 03:15:09.835: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-cflxt" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-cflxt test-rolling-update-deployment-6b6bf9df46- deployment-6001  136b4796-a873-4bee-8724-29ab99cb245a 798776 0 2021-03-24 03:15:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 55b4abfd-cf6e-4e43-8023-83c326c9ea1d 0xc000926177 0xc000926178}] []  [{kube-controller-manager Update v1 2021-03-24 03:15:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55b4abfd-cf6e-4e43-8023-83c326c9ea1d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 03:15:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.43.1.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-44r77,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-44r77,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-44r77,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:15:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:15:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:15:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:15:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.15,PodIP:10.43.1.31,StartTime:2021-03-24 03:15:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 03:15:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://5f272a9a79f5af63aac83997fe90001f9fa1ae9d3ec730bde973b50ac477fc21,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.43.1.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:15:09.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6001" for this suite.

â€¢ [SLOW TEST:7.179 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":160,"skipped":2589,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:15:09.841: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4845
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 03:15:10.236: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 03:15:13.253: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:15:13.257: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6944-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:15:19.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4845" for this suite.
STEP: Destroying namespace "webhook-4845-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

â€¢ [SLOW TEST:9.524 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":161,"skipped":2591,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:15:19.365: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3468
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:15:19.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3468" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":162,"skipped":2614,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:15:19.525: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8494
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-0f4ce954-bfd8-4e1e-a430-0edf03946298
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-0f4ce954-bfd8-4e1e-a430-0edf03946298
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:16:25.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8494" for this suite.

â€¢ [SLOW TEST:66.389 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":163,"skipped":2638,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:16:25.914: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3072
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0324 03:16:27.579614      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 24 03:17:29.595: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:17:29.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3072" for this suite.

â€¢ [SLOW TEST:63.694 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":164,"skipped":2670,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:17:29.608: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar 24 03:17:29.738: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 24 03:17:29.743: INFO: Waiting for terminating namespaces to be deleted...
Mar 24 03:17:29.745: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.13 before test
Mar 24 03:17:29.750: INFO: coredns-58d46886cf-fl2z7 from kube-system started at 2021-03-23 11:44:32 +0000 UTC (1 container statuses recorded)
Mar 24 03:17:29.750: INFO: 	Container coredns ready: true, restart count 0
Mar 24 03:17:29.750: INFO: csi-plugin-78sns from kube-system started at 2021-03-22 03:28:49 +0000 UTC (4 container statuses recorded)
Mar 24 03:17:29.750: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 24 03:17:29.750: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 24 03:17:29.750: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 24 03:17:29.750: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 24 03:17:29.750: INFO: csi-provisioner-57c8d966fb-spnmk from kube-system started at 2021-03-22 03:29:32 +0000 UTC (7 container statuses recorded)
Mar 24 03:17:29.750: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 24 03:17:29.750: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
Mar 24 03:17:29.750: INFO: 	Container external-disk-attacher ready: true, restart count 0
Mar 24 03:17:29.750: INFO: 	Container external-disk-provisioner ready: true, restart count 0
Mar 24 03:17:29.750: INFO: 	Container external-disk-resizer ready: true, restart count 0
Mar 24 03:17:29.750: INFO: 	Container external-nas-provisioner ready: true, restart count 0
Mar 24 03:17:29.750: INFO: 	Container external-snapshot-controller ready: true, restart count 0
Mar 24 03:17:29.750: INFO: kube-flannel-ds-77s6x from kube-system started at 2021-03-22 08:51:32 +0000 UTC (1 container statuses recorded)
Mar 24 03:17:29.750: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 03:17:29.750: INFO: kube-proxy-worker-kfc4c from kube-system started at 2021-03-22 03:28:49 +0000 UTC (1 container statuses recorded)
Mar 24 03:17:29.750: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 24 03:17:29.750: INFO: metrics-server-7d6f974b9f-tglpj from kube-system started at 2021-03-22 03:29:32 +0000 UTC (1 container statuses recorded)
Mar 24 03:17:29.750: INFO: 	Container metrics-server ready: true, restart count 2
Mar 24 03:17:29.750: INFO: nginx-ingress-controller-67bc64c7-5tcjs from kube-system started at 2021-03-23 11:44:32 +0000 UTC (1 container statuses recorded)
Mar 24 03:17:29.750: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 24 03:17:29.750: INFO: sonobuoy-systemd-logs-daemon-set-46debedd236a484a-5dl8c from sonobuoy started at 2021-03-24 02:26:37 +0000 UTC (2 container statuses recorded)
Mar 24 03:17:29.750: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 24 03:17:29.750: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 24 03:17:29.750: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.14 before test
Mar 24 03:17:29.755: INFO: simpletest.deployment-599b45cf47-sg75k from gc-3072 started at 2021-03-24 03:16:26 +0000 UTC (1 container statuses recorded)
Mar 24 03:17:29.755: INFO: 	Container nginx ready: true, restart count 0
Mar 24 03:17:29.755: INFO: csi-plugin-sdgx9 from kube-system started at 2021-03-22 03:28:49 +0000 UTC (4 container statuses recorded)
Mar 24 03:17:29.755: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 24 03:17:29.755: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 24 03:17:29.755: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 24 03:17:29.755: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 24 03:17:29.755: INFO: kube-flannel-ds-bqvbv from kube-system started at 2021-03-22 08:51:32 +0000 UTC (1 container statuses recorded)
Mar 24 03:17:29.755: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 03:17:29.755: INFO: kube-proxy-worker-46686 from kube-system started at 2021-03-22 03:28:49 +0000 UTC (1 container statuses recorded)
Mar 24 03:17:29.755: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 24 03:17:29.755: INFO: sonobuoy from sonobuoy started at 2021-03-24 02:26:36 +0000 UTC (1 container statuses recorded)
Mar 24 03:17:29.755: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 24 03:17:29.755: INFO: sonobuoy-systemd-logs-daemon-set-46debedd236a484a-l98ph from sonobuoy started at 2021-03-24 02:26:37 +0000 UTC (2 container statuses recorded)
Mar 24 03:17:29.755: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 24 03:17:29.755: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 24 03:17:29.755: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.15 before test
Mar 24 03:17:29.760: INFO: simpletest.deployment-599b45cf47-mbzfm from gc-3072 started at 2021-03-24 03:16:26 +0000 UTC (1 container statuses recorded)
Mar 24 03:17:29.760: INFO: 	Container nginx ready: true, restart count 0
Mar 24 03:17:29.760: INFO: coredns-58d46886cf-h7vgt from kube-system started at 2021-03-22 03:29:29 +0000 UTC (1 container statuses recorded)
Mar 24 03:17:29.760: INFO: 	Container coredns ready: true, restart count 23
Mar 24 03:17:29.760: INFO: csi-plugin-v7dd9 from kube-system started at 2021-03-22 03:28:49 +0000 UTC (4 container statuses recorded)
Mar 24 03:17:29.760: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 24 03:17:29.760: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 24 03:17:29.760: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 24 03:17:29.760: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 24 03:17:29.760: INFO: kube-flannel-ds-bswlq from kube-system started at 2021-03-22 08:51:32 +0000 UTC (1 container statuses recorded)
Mar 24 03:17:29.760: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 03:17:29.760: INFO: kube-proxy-worker-xvh5g from kube-system started at 2021-03-22 03:28:49 +0000 UTC (1 container statuses recorded)
Mar 24 03:17:29.760: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 24 03:17:29.760: INFO: nginx-ingress-controller-67bc64c7-zm4r9 from kube-system started at 2021-03-22 11:02:12 +0000 UTC (1 container statuses recorded)
Mar 24 03:17:29.760: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 24 03:17:29.760: INFO: sonobuoy-systemd-logs-daemon-set-46debedd236a484a-sb56k from sonobuoy started at 2021-03-24 02:26:37 +0000 UTC (2 container statuses recorded)
Mar 24 03:17:29.760: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 24 03:17:29.760: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.166f28fb01a2eee6], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.166f28fb01ea1a02], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:17:30.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7577" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":165,"skipped":2700,"failed":0}
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:17:30.789: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3253
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:17:30.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3253" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
â€¢{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":166,"skipped":2701,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:17:30.926: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-410
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-410, will wait for the garbage collector to delete the pods
Mar 24 03:17:33.119: INFO: Deleting Job.batch foo took: 5.596147ms
Mar 24 03:17:33.819: INFO: Terminating Job.batch foo pods took: 700.131998ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:18:12.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-410" for this suite.

â€¢ [SLOW TEST:41.709 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":167,"skipped":2717,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:18:12.635: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9239
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar 24 03:18:12.768: INFO: Waiting up to 5m0s for pod "downward-api-a8be8da1-566f-4941-8dd3-499d6b89a8db" in namespace "downward-api-9239" to be "Succeeded or Failed"
Mar 24 03:18:12.770: INFO: Pod "downward-api-a8be8da1-566f-4941-8dd3-499d6b89a8db": Phase="Pending", Reason="", readiness=false. Elapsed: 1.87317ms
Mar 24 03:18:14.775: INFO: Pod "downward-api-a8be8da1-566f-4941-8dd3-499d6b89a8db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006883528s
STEP: Saw pod success
Mar 24 03:18:14.775: INFO: Pod "downward-api-a8be8da1-566f-4941-8dd3-499d6b89a8db" satisfied condition "Succeeded or Failed"
Mar 24 03:18:14.777: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod downward-api-a8be8da1-566f-4941-8dd3-499d6b89a8db container dapi-container: <nil>
STEP: delete the pod
Mar 24 03:18:14.800: INFO: Waiting for pod downward-api-a8be8da1-566f-4941-8dd3-499d6b89a8db to disappear
Mar 24 03:18:14.802: INFO: Pod downward-api-a8be8da1-566f-4941-8dd3-499d6b89a8db no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:18:14.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9239" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":168,"skipped":2731,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:18:14.808: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1581
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Mar 24 03:18:17.466: INFO: Successfully updated pod "labelsupdate4ddba108-00ec-4055-9d11-77b81ad7eec5"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:18:19.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1581" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":169,"skipped":2744,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:18:19.490: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9575
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-8d321f40-8e04-403a-8602-1d339d0f9f3b
STEP: Creating secret with name s-test-opt-upd-97155dd8-49b7-4470-b07f-ea9d69aa08b6
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8d321f40-8e04-403a-8602-1d339d0f9f3b
STEP: Updating secret s-test-opt-upd-97155dd8-49b7-4470-b07f-ea9d69aa08b6
STEP: Creating secret with name s-test-opt-create-d50249fb-6c07-4915-8cab-0c1130d3ef05
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:18:25.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9575" for this suite.

â€¢ [SLOW TEST:6.234 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":170,"skipped":2779,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:18:25.724: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-459
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-c7628946-5e4b-433e-a28f-255b7545a97c in namespace container-probe-459
Mar 24 03:18:27.863: INFO: Started pod busybox-c7628946-5e4b-433e-a28f-255b7545a97c in namespace container-probe-459
STEP: checking the pod's current state and verifying that restartCount is present
Mar 24 03:18:27.865: INFO: Initial restart count of pod busybox-c7628946-5e4b-433e-a28f-255b7545a97c is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:22:28.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-459" for this suite.

â€¢ [SLOW TEST:242.761 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":171,"skipped":2812,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:22:28.486: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5699
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-5699
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 24 03:22:28.611: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 24 03:22:28.635: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 24 03:22:30.640: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 03:22:32.640: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 03:22:34.640: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 03:22:36.641: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 03:22:38.638: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 03:22:40.640: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 24 03:22:40.644: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 24 03:22:42.649: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 24 03:22:44.649: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 24 03:22:46.649: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 24 03:22:48.647: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 24 03:22:48.651: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 24 03:22:50.665: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 24 03:22:50.665: INFO: Breadth first check of 10.43.0.228 on host 192.168.0.13...
Mar 24 03:22:50.666: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.43.1.125:9080/dial?request=hostname&protocol=udp&host=10.43.0.228&port=8081&tries=1'] Namespace:pod-network-test-5699 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:22:50.666: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 03:22:50.757: INFO: Waiting for responses: map[]
Mar 24 03:22:50.757: INFO: reached 10.43.0.228 after 0/1 tries
Mar 24 03:22:50.757: INFO: Breadth first check of 10.43.1.124 on host 192.168.0.14...
Mar 24 03:22:50.759: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.43.1.125:9080/dial?request=hostname&protocol=udp&host=10.43.1.124&port=8081&tries=1'] Namespace:pod-network-test-5699 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:22:50.759: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 03:22:50.867: INFO: Waiting for responses: map[]
Mar 24 03:22:50.867: INFO: reached 10.43.1.124 after 0/1 tries
Mar 24 03:22:50.867: INFO: Breadth first check of 10.43.1.35 on host 192.168.0.15...
Mar 24 03:22:50.870: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.43.1.125:9080/dial?request=hostname&protocol=udp&host=10.43.1.35&port=8081&tries=1'] Namespace:pod-network-test-5699 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:22:50.870: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 03:22:50.990: INFO: Waiting for responses: map[]
Mar 24 03:22:50.990: INFO: reached 10.43.1.35 after 0/1 tries
Mar 24 03:22:50.990: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:22:50.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5699" for this suite.

â€¢ [SLOW TEST:22.514 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":172,"skipped":2820,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:22:51.000: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2331
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-4ba965e1-df5e-43e8-95b4-757972fd3e83
STEP: Creating a pod to test consume configMaps
Mar 24 03:22:51.139: INFO: Waiting up to 5m0s for pod "pod-configmaps-de946ff2-7fa3-468e-974b-9062d5eaaa84" in namespace "configmap-2331" to be "Succeeded or Failed"
Mar 24 03:22:51.140: INFO: Pod "pod-configmaps-de946ff2-7fa3-468e-974b-9062d5eaaa84": Phase="Pending", Reason="", readiness=false. Elapsed: 1.693024ms
Mar 24 03:22:53.146: INFO: Pod "pod-configmaps-de946ff2-7fa3-468e-974b-9062d5eaaa84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007236214s
STEP: Saw pod success
Mar 24 03:22:53.146: INFO: Pod "pod-configmaps-de946ff2-7fa3-468e-974b-9062d5eaaa84" satisfied condition "Succeeded or Failed"
Mar 24 03:22:53.148: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-configmaps-de946ff2-7fa3-468e-974b-9062d5eaaa84 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 24 03:22:53.168: INFO: Waiting for pod pod-configmaps-de946ff2-7fa3-468e-974b-9062d5eaaa84 to disappear
Mar 24 03:22:53.170: INFO: Pod pod-configmaps-de946ff2-7fa3-468e-974b-9062d5eaaa84 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:22:53.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2331" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":173,"skipped":2828,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:22:53.176: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:22:53.308: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c7c611dd-3f25-43b7-b60a-6ea610bc98ec" in namespace "projected-8154" to be "Succeeded or Failed"
Mar 24 03:22:53.310: INFO: Pod "downwardapi-volume-c7c611dd-3f25-43b7-b60a-6ea610bc98ec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.787822ms
Mar 24 03:22:55.315: INFO: Pod "downwardapi-volume-c7c611dd-3f25-43b7-b60a-6ea610bc98ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007303446s
STEP: Saw pod success
Mar 24 03:22:55.315: INFO: Pod "downwardapi-volume-c7c611dd-3f25-43b7-b60a-6ea610bc98ec" satisfied condition "Succeeded or Failed"
Mar 24 03:22:55.317: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod downwardapi-volume-c7c611dd-3f25-43b7-b60a-6ea610bc98ec container client-container: <nil>
STEP: delete the pod
Mar 24 03:22:55.335: INFO: Waiting for pod downwardapi-volume-c7c611dd-3f25-43b7-b60a-6ea610bc98ec to disappear
Mar 24 03:22:55.337: INFO: Pod downwardapi-volume-c7c611dd-3f25-43b7-b60a-6ea610bc98ec no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:22:55.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8154" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":174,"skipped":2838,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:22:55.342: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1777
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Mar 24 03:22:55.472: INFO: Waiting up to 5m0s for pod "var-expansion-4e174405-b7c2-43bd-a156-cd3fd8728d18" in namespace "var-expansion-1777" to be "Succeeded or Failed"
Mar 24 03:22:55.474: INFO: Pod "var-expansion-4e174405-b7c2-43bd-a156-cd3fd8728d18": Phase="Pending", Reason="", readiness=false. Elapsed: 1.590596ms
Mar 24 03:22:57.480: INFO: Pod "var-expansion-4e174405-b7c2-43bd-a156-cd3fd8728d18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007137823s
STEP: Saw pod success
Mar 24 03:22:57.480: INFO: Pod "var-expansion-4e174405-b7c2-43bd-a156-cd3fd8728d18" satisfied condition "Succeeded or Failed"
Mar 24 03:22:57.482: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod var-expansion-4e174405-b7c2-43bd-a156-cd3fd8728d18 container dapi-container: <nil>
STEP: delete the pod
Mar 24 03:22:57.494: INFO: Waiting for pod var-expansion-4e174405-b7c2-43bd-a156-cd3fd8728d18 to disappear
Mar 24 03:22:57.496: INFO: Pod var-expansion-4e174405-b7c2-43bd-a156-cd3fd8728d18 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:22:57.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1777" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":175,"skipped":2847,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:22:57.503: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-568
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Mar 24 03:22:57.638: INFO: observed Pod pod-test in namespace pods-568 in phase Pending conditions []
Mar 24 03:22:57.641: INFO: observed Pod pod-test in namespace pods-568 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 03:22:57 +0000 UTC  }]
Mar 24 03:22:57.654: INFO: observed Pod pod-test in namespace pods-568 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 03:22:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 03:22:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-24 03:22:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-24 03:22:57 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Mar 24 03:22:58.885: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Mar 24 03:22:58.902: INFO: observed event type ADDED
Mar 24 03:22:58.902: INFO: observed event type MODIFIED
Mar 24 03:22:58.902: INFO: observed event type MODIFIED
Mar 24 03:22:58.902: INFO: observed event type MODIFIED
Mar 24 03:22:58.902: INFO: observed event type MODIFIED
Mar 24 03:22:58.902: INFO: observed event type MODIFIED
Mar 24 03:22:58.902: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:22:58.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-568" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":176,"skipped":2856,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:22:58.908: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9261
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Mar 24 03:22:59.039: INFO: Waiting up to 5m0s for pod "var-expansion-627dfb76-b824-434d-9b12-f91fd9e73192" in namespace "var-expansion-9261" to be "Succeeded or Failed"
Mar 24 03:22:59.041: INFO: Pod "var-expansion-627dfb76-b824-434d-9b12-f91fd9e73192": Phase="Pending", Reason="", readiness=false. Elapsed: 1.867396ms
Mar 24 03:23:01.046: INFO: Pod "var-expansion-627dfb76-b824-434d-9b12-f91fd9e73192": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007126381s
STEP: Saw pod success
Mar 24 03:23:01.046: INFO: Pod "var-expansion-627dfb76-b824-434d-9b12-f91fd9e73192" satisfied condition "Succeeded or Failed"
Mar 24 03:23:01.048: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod var-expansion-627dfb76-b824-434d-9b12-f91fd9e73192 container dapi-container: <nil>
STEP: delete the pod
Mar 24 03:23:01.067: INFO: Waiting for pod var-expansion-627dfb76-b824-434d-9b12-f91fd9e73192 to disappear
Mar 24 03:23:01.068: INFO: Pod var-expansion-627dfb76-b824-434d-9b12-f91fd9e73192 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:23:01.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9261" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":177,"skipped":2866,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:23:01.075: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-2347
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:23:01.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2347" for this suite.
â€¢{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":178,"skipped":2887,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:23:01.242: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5182
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-5vkj
STEP: Creating a pod to test atomic-volume-subpath
Mar 24 03:23:01.379: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5vkj" in namespace "subpath-5182" to be "Succeeded or Failed"
Mar 24 03:23:01.381: INFO: Pod "pod-subpath-test-projected-5vkj": Phase="Pending", Reason="", readiness=false. Elapsed: 1.64223ms
Mar 24 03:23:03.387: INFO: Pod "pod-subpath-test-projected-5vkj": Phase="Running", Reason="", readiness=true. Elapsed: 2.007394168s
Mar 24 03:23:05.392: INFO: Pod "pod-subpath-test-projected-5vkj": Phase="Running", Reason="", readiness=true. Elapsed: 4.012944913s
Mar 24 03:23:07.398: INFO: Pod "pod-subpath-test-projected-5vkj": Phase="Running", Reason="", readiness=true. Elapsed: 6.018630187s
Mar 24 03:23:09.402: INFO: Pod "pod-subpath-test-projected-5vkj": Phase="Running", Reason="", readiness=true. Elapsed: 8.022691431s
Mar 24 03:23:11.407: INFO: Pod "pod-subpath-test-projected-5vkj": Phase="Running", Reason="", readiness=true. Elapsed: 10.027864536s
Mar 24 03:23:13.413: INFO: Pod "pod-subpath-test-projected-5vkj": Phase="Running", Reason="", readiness=true. Elapsed: 12.033631414s
Mar 24 03:23:15.418: INFO: Pod "pod-subpath-test-projected-5vkj": Phase="Running", Reason="", readiness=true. Elapsed: 14.039072618s
Mar 24 03:23:17.425: INFO: Pod "pod-subpath-test-projected-5vkj": Phase="Running", Reason="", readiness=true. Elapsed: 16.045093081s
Mar 24 03:23:19.429: INFO: Pod "pod-subpath-test-projected-5vkj": Phase="Running", Reason="", readiness=true. Elapsed: 18.049136513s
Mar 24 03:23:21.434: INFO: Pod "pod-subpath-test-projected-5vkj": Phase="Running", Reason="", readiness=true. Elapsed: 20.05436086s
Mar 24 03:23:23.439: INFO: Pod "pod-subpath-test-projected-5vkj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.05947877s
STEP: Saw pod success
Mar 24 03:23:23.439: INFO: Pod "pod-subpath-test-projected-5vkj" satisfied condition "Succeeded or Failed"
Mar 24 03:23:23.441: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-subpath-test-projected-5vkj container test-container-subpath-projected-5vkj: <nil>
STEP: delete the pod
Mar 24 03:23:23.463: INFO: Waiting for pod pod-subpath-test-projected-5vkj to disappear
Mar 24 03:23:23.464: INFO: Pod pod-subpath-test-projected-5vkj no longer exists
STEP: Deleting pod pod-subpath-test-projected-5vkj
Mar 24 03:23:23.465: INFO: Deleting pod "pod-subpath-test-projected-5vkj" in namespace "subpath-5182"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:23:23.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5182" for this suite.

â€¢ [SLOW TEST:22.230 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":179,"skipped":2908,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:23:23.472: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-8408
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Mar 24 03:23:23.601: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Mar 24 03:23:23.604: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 24 03:23:23.604: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Mar 24 03:23:23.609: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 24 03:23:23.609: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Mar 24 03:23:23.615: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar 24 03:23:23.615: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Mar 24 03:23:30.645: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:23:30.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-8408" for this suite.

â€¢ [SLOW TEST:7.190 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":180,"skipped":2923,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:23:30.662: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7093
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:23:30.788: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:23:32.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7093" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":181,"skipped":2950,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:23:32.894: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9906
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-9906/configmap-test-699ff6ef-173c-444c-8246-707ae0f23736
STEP: Creating a pod to test consume configMaps
Mar 24 03:23:33.031: INFO: Waiting up to 5m0s for pod "pod-configmaps-809981c0-b0fe-4f12-a455-89d1047f6202" in namespace "configmap-9906" to be "Succeeded or Failed"
Mar 24 03:23:33.032: INFO: Pod "pod-configmaps-809981c0-b0fe-4f12-a455-89d1047f6202": Phase="Pending", Reason="", readiness=false. Elapsed: 1.684408ms
Mar 24 03:23:35.038: INFO: Pod "pod-configmaps-809981c0-b0fe-4f12-a455-89d1047f6202": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007229052s
STEP: Saw pod success
Mar 24 03:23:35.038: INFO: Pod "pod-configmaps-809981c0-b0fe-4f12-a455-89d1047f6202" satisfied condition "Succeeded or Failed"
Mar 24 03:23:35.040: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-configmaps-809981c0-b0fe-4f12-a455-89d1047f6202 container env-test: <nil>
STEP: delete the pod
Mar 24 03:23:35.054: INFO: Waiting for pod pod-configmaps-809981c0-b0fe-4f12-a455-89d1047f6202 to disappear
Mar 24 03:23:35.055: INFO: Pod pod-configmaps-809981c0-b0fe-4f12-a455-89d1047f6202 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:23:35.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9906" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":182,"skipped":2955,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:23:35.061: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9653
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 24 03:23:35.192: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9653  626cbeb6-09d2-4b1a-92ec-bddba2950473 801498 0 2021-03-24 03:23:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-24 03:23:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 03:23:35.192: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9653  626cbeb6-09d2-4b1a-92ec-bddba2950473 801498 0 2021-03-24 03:23:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-24 03:23:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar 24 03:23:45.208: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9653  626cbeb6-09d2-4b1a-92ec-bddba2950473 801587 0 2021-03-24 03:23:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-24 03:23:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 03:23:45.208: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9653  626cbeb6-09d2-4b1a-92ec-bddba2950473 801587 0 2021-03-24 03:23:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-24 03:23:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 24 03:23:55.222: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9653  626cbeb6-09d2-4b1a-92ec-bddba2950473 801627 0 2021-03-24 03:23:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-24 03:23:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 03:23:55.222: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9653  626cbeb6-09d2-4b1a-92ec-bddba2950473 801627 0 2021-03-24 03:23:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-24 03:23:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar 24 03:24:05.236: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9653  626cbeb6-09d2-4b1a-92ec-bddba2950473 801665 0 2021-03-24 03:23:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-24 03:23:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 03:24:05.236: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9653  626cbeb6-09d2-4b1a-92ec-bddba2950473 801665 0 2021-03-24 03:23:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-24 03:23:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 24 03:24:15.257: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9653  6c91cd58-ce2f-49e8-9fb8-421d19e4654c 801705 0 2021-03-24 03:24:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-24 03:24:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 03:24:15.257: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9653  6c91cd58-ce2f-49e8-9fb8-421d19e4654c 801705 0 2021-03-24 03:24:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-24 03:24:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar 24 03:24:25.272: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9653  6c91cd58-ce2f-49e8-9fb8-421d19e4654c 801746 0 2021-03-24 03:24:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-24 03:24:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 03:24:25.272: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9653  6c91cd58-ce2f-49e8-9fb8-421d19e4654c 801746 0 2021-03-24 03:24:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-24 03:24:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:24:35.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9653" for this suite.

â€¢ [SLOW TEST:60.229 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":183,"skipped":2996,"failed":0}
S
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:24:35.290: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-146
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:24:35.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-146" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":184,"skipped":2997,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:24:35.443: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4823
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-4823
STEP: creating service affinity-nodeport-transition in namespace services-4823
STEP: creating replication controller affinity-nodeport-transition in namespace services-4823
I0324 03:24:35.583719      21 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-4823, replica count: 3
I0324 03:24:38.633949      21 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 03:24:38.644: INFO: Creating new exec pod
Mar 24 03:24:41.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-4823 exec execpod-affinity2bt7p -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Mar 24 03:24:41.945: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar 24 03:24:41.945: INFO: stdout: ""
Mar 24 03:24:41.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-4823 exec execpod-affinity2bt7p -- /bin/sh -x -c nc -zv -t -w 2 172.16.119.172 80'
Mar 24 03:24:42.091: INFO: stderr: "+ nc -zv -t -w 2 172.16.119.172 80\nConnection to 172.16.119.172 80 port [tcp/http] succeeded!\n"
Mar 24 03:24:42.092: INFO: stdout: ""
Mar 24 03:24:42.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-4823 exec execpod-affinity2bt7p -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.13 31286'
Mar 24 03:24:42.244: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.13 31286\nConnection to 192.168.0.13 31286 port [tcp/31286] succeeded!\n"
Mar 24 03:24:42.244: INFO: stdout: ""
Mar 24 03:24:42.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-4823 exec execpod-affinity2bt7p -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.15 31286'
Mar 24 03:24:42.384: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.15 31286\nConnection to 192.168.0.15 31286 port [tcp/31286] succeeded!\n"
Mar 24 03:24:42.384: INFO: stdout: ""
Mar 24 03:24:42.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-4823 exec execpod-affinity2bt7p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.13:31286/ ; done'
Mar 24 03:24:42.593: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n"
Mar 24 03:24:42.593: INFO: stdout: "\naffinity-nodeport-transition-47l9n\naffinity-nodeport-transition-xk6sf\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-47l9n\naffinity-nodeport-transition-xk6sf\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-47l9n\naffinity-nodeport-transition-xk6sf\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-47l9n\naffinity-nodeport-transition-xk6sf\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-47l9n\naffinity-nodeport-transition-xk6sf\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-47l9n"
Mar 24 03:24:42.593: INFO: Received response from host: affinity-nodeport-transition-47l9n
Mar 24 03:24:42.593: INFO: Received response from host: affinity-nodeport-transition-xk6sf
Mar 24 03:24:42.593: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.593: INFO: Received response from host: affinity-nodeport-transition-47l9n
Mar 24 03:24:42.593: INFO: Received response from host: affinity-nodeport-transition-xk6sf
Mar 24 03:24:42.593: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.593: INFO: Received response from host: affinity-nodeport-transition-47l9n
Mar 24 03:24:42.593: INFO: Received response from host: affinity-nodeport-transition-xk6sf
Mar 24 03:24:42.593: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.593: INFO: Received response from host: affinity-nodeport-transition-47l9n
Mar 24 03:24:42.593: INFO: Received response from host: affinity-nodeport-transition-xk6sf
Mar 24 03:24:42.593: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.593: INFO: Received response from host: affinity-nodeport-transition-47l9n
Mar 24 03:24:42.593: INFO: Received response from host: affinity-nodeport-transition-xk6sf
Mar 24 03:24:42.593: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.593: INFO: Received response from host: affinity-nodeport-transition-47l9n
Mar 24 03:24:42.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-4823 exec execpod-affinity2bt7p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.13:31286/ ; done'
Mar 24 03:24:42.803: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.13:31286/\n"
Mar 24 03:24:42.803: INFO: stdout: "\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-zqxld\naffinity-nodeport-transition-zqxld"
Mar 24 03:24:42.803: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.803: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.803: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.803: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.803: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.803: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.803: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.803: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.803: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.803: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.803: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.803: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.803: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.803: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.803: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.803: INFO: Received response from host: affinity-nodeport-transition-zqxld
Mar 24 03:24:42.803: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4823, will wait for the garbage collector to delete the pods
Mar 24 03:24:42.871: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.673818ms
Mar 24 03:24:43.571: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 700.147359ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:24:52.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4823" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

â€¢ [SLOW TEST:17.155 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":185,"skipped":3014,"failed":0}
S
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:24:52.598: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1177
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-skm8f in namespace proxy-1177
I0324 03:24:52.736853      21 runners.go:190] Created replication controller with name: proxy-service-skm8f, namespace: proxy-1177, replica count: 1
I0324 03:24:53.787077      21 runners.go:190] proxy-service-skm8f Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0324 03:24:54.787201      21 runners.go:190] proxy-service-skm8f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 03:24:55.787332      21 runners.go:190] proxy-service-skm8f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 03:24:56.787461      21 runners.go:190] proxy-service-skm8f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 03:24:57.787586      21 runners.go:190] proxy-service-skm8f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 03:24:58.787712      21 runners.go:190] proxy-service-skm8f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0324 03:24:59.787838      21 runners.go:190] proxy-service-skm8f Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 03:24:59.791: INFO: setup took 7.065560148s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar 24 03:24:59.794: INFO: (0) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.766524ms)
Mar 24 03:24:59.794: INFO: (0) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 2.820209ms)
Mar 24 03:24:59.794: INFO: (0) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 3.010424ms)
Mar 24 03:24:59.794: INFO: (0) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 3.037448ms)
Mar 24 03:24:59.794: INFO: (0) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 3.506624ms)
Mar 24 03:24:59.794: INFO: (0) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 3.445884ms)
Mar 24 03:24:59.794: INFO: (0) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 3.455493ms)
Mar 24 03:24:59.796: INFO: (0) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 5.406423ms)
Mar 24 03:24:59.798: INFO: (0) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 7.048881ms)
Mar 24 03:24:59.798: INFO: (0) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 7.594063ms)
Mar 24 03:24:59.798: INFO: (0) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 7.582329ms)
Mar 24 03:24:59.798: INFO: (0) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 7.696714ms)
Mar 24 03:24:59.799: INFO: (0) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 8.531633ms)
Mar 24 03:24:59.799: INFO: (0) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 8.601194ms)
Mar 24 03:24:59.799: INFO: (0) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 8.843226ms)
Mar 24 03:24:59.799: INFO: (0) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 8.825337ms)
Mar 24 03:24:59.801: INFO: (1) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 1.869452ms)
Mar 24 03:24:59.802: INFO: (1) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 2.214173ms)
Mar 24 03:24:59.802: INFO: (1) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 2.392372ms)
Mar 24 03:24:59.802: INFO: (1) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.422338ms)
Mar 24 03:24:59.802: INFO: (1) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 2.89465ms)
Mar 24 03:24:59.803: INFO: (1) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.992428ms)
Mar 24 03:24:59.803: INFO: (1) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 3.036211ms)
Mar 24 03:24:59.803: INFO: (1) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 2.998267ms)
Mar 24 03:24:59.803: INFO: (1) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 3.007463ms)
Mar 24 03:24:59.803: INFO: (1) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 3.081699ms)
Mar 24 03:24:59.803: INFO: (1) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 3.418511ms)
Mar 24 03:24:59.803: INFO: (1) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 3.54341ms)
Mar 24 03:24:59.803: INFO: (1) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 3.662091ms)
Mar 24 03:24:59.803: INFO: (1) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 3.722879ms)
Mar 24 03:24:59.803: INFO: (1) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 3.808349ms)
Mar 24 03:24:59.804: INFO: (1) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 4.073548ms)
Mar 24 03:24:59.806: INFO: (2) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 1.861689ms)
Mar 24 03:24:59.806: INFO: (2) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 2.040524ms)
Mar 24 03:24:59.806: INFO: (2) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 2.209763ms)
Mar 24 03:24:59.806: INFO: (2) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.306619ms)
Mar 24 03:24:59.806: INFO: (2) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 2.469407ms)
Mar 24 03:24:59.806: INFO: (2) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.568238ms)
Mar 24 03:24:59.806: INFO: (2) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.704803ms)
Mar 24 03:24:59.807: INFO: (2) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.763426ms)
Mar 24 03:24:59.807: INFO: (2) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 3.195979ms)
Mar 24 03:24:59.807: INFO: (2) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 3.290159ms)
Mar 24 03:24:59.807: INFO: (2) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 3.421194ms)
Mar 24 03:24:59.807: INFO: (2) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 3.418773ms)
Mar 24 03:24:59.807: INFO: (2) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 3.454956ms)
Mar 24 03:24:59.807: INFO: (2) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 3.444575ms)
Mar 24 03:24:59.807: INFO: (2) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 3.57093ms)
Mar 24 03:24:59.808: INFO: (2) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 3.953499ms)
Mar 24 03:24:59.810: INFO: (3) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 1.775162ms)
Mar 24 03:24:59.810: INFO: (3) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 2.03919ms)
Mar 24 03:24:59.810: INFO: (3) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 2.020042ms)
Mar 24 03:24:59.810: INFO: (3) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.186509ms)
Mar 24 03:24:59.810: INFO: (3) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.520393ms)
Mar 24 03:24:59.810: INFO: (3) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.614759ms)
Mar 24 03:24:59.811: INFO: (3) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 2.869785ms)
Mar 24 03:24:59.811: INFO: (3) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 3.024818ms)
Mar 24 03:24:59.811: INFO: (3) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 3.199633ms)
Mar 24 03:24:59.811: INFO: (3) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 3.210386ms)
Mar 24 03:24:59.811: INFO: (3) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 3.271811ms)
Mar 24 03:24:59.811: INFO: (3) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 3.316244ms)
Mar 24 03:24:59.811: INFO: (3) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 3.364377ms)
Mar 24 03:24:59.811: INFO: (3) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 3.479786ms)
Mar 24 03:24:59.812: INFO: (3) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 3.692029ms)
Mar 24 03:24:59.812: INFO: (3) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 3.957234ms)
Mar 24 03:24:59.814: INFO: (4) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 1.824083ms)
Mar 24 03:24:59.814: INFO: (4) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 1.883251ms)
Mar 24 03:24:59.814: INFO: (4) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 1.962091ms)
Mar 24 03:24:59.814: INFO: (4) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 2.091346ms)
Mar 24 03:24:59.814: INFO: (4) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 2.408515ms)
Mar 24 03:24:59.815: INFO: (4) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.738295ms)
Mar 24 03:24:59.815: INFO: (4) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 2.719278ms)
Mar 24 03:24:59.815: INFO: (4) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 2.730476ms)
Mar 24 03:24:59.815: INFO: (4) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 3.012528ms)
Mar 24 03:24:59.815: INFO: (4) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 3.385791ms)
Mar 24 03:24:59.815: INFO: (4) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 3.454366ms)
Mar 24 03:24:59.815: INFO: (4) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 3.393433ms)
Mar 24 03:24:59.815: INFO: (4) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 3.521583ms)
Mar 24 03:24:59.816: INFO: (4) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 4.033512ms)
Mar 24 03:24:59.816: INFO: (4) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 4.064123ms)
Mar 24 03:24:59.816: INFO: (4) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 4.552507ms)
Mar 24 03:24:59.818: INFO: (5) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 1.828131ms)
Mar 24 03:24:59.818: INFO: (5) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 1.972998ms)
Mar 24 03:24:59.819: INFO: (5) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.123919ms)
Mar 24 03:24:59.819: INFO: (5) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 2.47529ms)
Mar 24 03:24:59.819: INFO: (5) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 2.653993ms)
Mar 24 03:24:59.819: INFO: (5) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.681318ms)
Mar 24 03:24:59.819: INFO: (5) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.739992ms)
Mar 24 03:24:59.819: INFO: (5) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 2.842889ms)
Mar 24 03:24:59.819: INFO: (5) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 2.915121ms)
Mar 24 03:24:59.819: INFO: (5) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 3.073868ms)
Mar 24 03:24:59.819: INFO: (5) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 3.125371ms)
Mar 24 03:24:59.820: INFO: (5) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 3.061142ms)
Mar 24 03:24:59.820: INFO: (5) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 3.343647ms)
Mar 24 03:24:59.820: INFO: (5) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 3.586983ms)
Mar 24 03:24:59.820: INFO: (5) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 3.544987ms)
Mar 24 03:24:59.820: INFO: (5) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 3.92023ms)
Mar 24 03:24:59.823: INFO: (6) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 2.434625ms)
Mar 24 03:24:59.823: INFO: (6) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.376504ms)
Mar 24 03:24:59.823: INFO: (6) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 2.521704ms)
Mar 24 03:24:59.823: INFO: (6) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.556405ms)
Mar 24 03:24:59.823: INFO: (6) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 2.711803ms)
Mar 24 03:24:59.823: INFO: (6) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 2.638925ms)
Mar 24 03:24:59.823: INFO: (6) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 2.727814ms)
Mar 24 03:24:59.823: INFO: (6) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.845318ms)
Mar 24 03:24:59.823: INFO: (6) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.856654ms)
Mar 24 03:24:59.823: INFO: (6) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 2.941546ms)
Mar 24 03:24:59.824: INFO: (6) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 3.059816ms)
Mar 24 03:24:59.824: INFO: (6) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 3.540715ms)
Mar 24 03:24:59.824: INFO: (6) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 3.762089ms)
Mar 24 03:24:59.824: INFO: (6) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 3.826607ms)
Mar 24 03:24:59.824: INFO: (6) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 3.774991ms)
Mar 24 03:24:59.825: INFO: (6) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 4.285551ms)
Mar 24 03:24:59.827: INFO: (7) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 1.925716ms)
Mar 24 03:24:59.827: INFO: (7) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 2.088046ms)
Mar 24 03:24:59.827: INFO: (7) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 2.160499ms)
Mar 24 03:24:59.827: INFO: (7) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 2.404526ms)
Mar 24 03:24:59.827: INFO: (7) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.647426ms)
Mar 24 03:24:59.827: INFO: (7) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.61818ms)
Mar 24 03:24:59.827: INFO: (7) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 2.730735ms)
Mar 24 03:24:59.828: INFO: (7) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.92076ms)
Mar 24 03:24:59.828: INFO: (7) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 3.055374ms)
Mar 24 03:24:59.828: INFO: (7) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 3.064613ms)
Mar 24 03:24:59.828: INFO: (7) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 3.15263ms)
Mar 24 03:24:59.828: INFO: (7) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 3.219671ms)
Mar 24 03:24:59.828: INFO: (7) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 3.515726ms)
Mar 24 03:24:59.828: INFO: (7) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 3.536299ms)
Mar 24 03:24:59.829: INFO: (7) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 3.854172ms)
Mar 24 03:24:59.829: INFO: (7) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 3.900649ms)
Mar 24 03:24:59.830: INFO: (8) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 1.762669ms)
Mar 24 03:24:59.831: INFO: (8) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 1.937302ms)
Mar 24 03:24:59.831: INFO: (8) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 1.950362ms)
Mar 24 03:24:59.831: INFO: (8) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 2.0894ms)
Mar 24 03:24:59.831: INFO: (8) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.226232ms)
Mar 24 03:24:59.831: INFO: (8) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 2.317177ms)
Mar 24 03:24:59.831: INFO: (8) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.574633ms)
Mar 24 03:24:59.831: INFO: (8) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 2.581929ms)
Mar 24 03:24:59.832: INFO: (8) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 3.099513ms)
Mar 24 03:24:59.832: INFO: (8) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 3.126827ms)
Mar 24 03:24:59.832: INFO: (8) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 3.320161ms)
Mar 24 03:24:59.832: INFO: (8) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 3.312327ms)
Mar 24 03:24:59.832: INFO: (8) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 3.610481ms)
Mar 24 03:24:59.832: INFO: (8) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 3.681273ms)
Mar 24 03:24:59.833: INFO: (8) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 3.800817ms)
Mar 24 03:24:59.833: INFO: (8) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 3.894608ms)
Mar 24 03:24:59.835: INFO: (9) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 1.80919ms)
Mar 24 03:24:59.835: INFO: (9) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.017203ms)
Mar 24 03:24:59.835: INFO: (9) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 2.221899ms)
Mar 24 03:24:59.836: INFO: (9) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 2.810617ms)
Mar 24 03:24:59.836: INFO: (9) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.821521ms)
Mar 24 03:24:59.836: INFO: (9) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 2.785954ms)
Mar 24 03:24:59.836: INFO: (9) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 3.082452ms)
Mar 24 03:24:59.836: INFO: (9) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 3.091258ms)
Mar 24 03:24:59.836: INFO: (9) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 3.148782ms)
Mar 24 03:24:59.836: INFO: (9) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 3.115447ms)
Mar 24 03:24:59.836: INFO: (9) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 3.321301ms)
Mar 24 03:24:59.836: INFO: (9) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 3.313774ms)
Mar 24 03:24:59.836: INFO: (9) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 3.384968ms)
Mar 24 03:24:59.836: INFO: (9) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 3.54442ms)
Mar 24 03:24:59.836: INFO: (9) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 3.723353ms)
Mar 24 03:24:59.837: INFO: (9) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 3.975631ms)
Mar 24 03:24:59.839: INFO: (10) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 1.731533ms)
Mar 24 03:24:59.839: INFO: (10) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 2.145751ms)
Mar 24 03:24:59.839: INFO: (10) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 2.180141ms)
Mar 24 03:24:59.839: INFO: (10) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.381072ms)
Mar 24 03:24:59.839: INFO: (10) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 2.469556ms)
Mar 24 03:24:59.839: INFO: (10) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 2.540667ms)
Mar 24 03:24:59.839: INFO: (10) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 2.597224ms)
Mar 24 03:24:59.839: INFO: (10) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 2.702598ms)
Mar 24 03:24:59.840: INFO: (10) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.963478ms)
Mar 24 03:24:59.840: INFO: (10) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 3.006592ms)
Mar 24 03:24:59.840: INFO: (10) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 3.122174ms)
Mar 24 03:24:59.840: INFO: (10) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 3.233461ms)
Mar 24 03:24:59.840: INFO: (10) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 3.320271ms)
Mar 24 03:24:59.840: INFO: (10) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 3.309193ms)
Mar 24 03:24:59.841: INFO: (10) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 3.918546ms)
Mar 24 03:24:59.841: INFO: (10) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 3.926723ms)
Mar 24 03:24:59.843: INFO: (11) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 1.911709ms)
Mar 24 03:24:59.843: INFO: (11) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 1.844254ms)
Mar 24 03:24:59.843: INFO: (11) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 1.985673ms)
Mar 24 03:24:59.843: INFO: (11) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 2.393994ms)
Mar 24 03:24:59.843: INFO: (11) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 2.368469ms)
Mar 24 03:24:59.843: INFO: (11) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.544428ms)
Mar 24 03:24:59.844: INFO: (11) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.703939ms)
Mar 24 03:24:59.844: INFO: (11) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.888941ms)
Mar 24 03:24:59.844: INFO: (11) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 3.111468ms)
Mar 24 03:24:59.844: INFO: (11) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 3.050074ms)
Mar 24 03:24:59.844: INFO: (11) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 3.160227ms)
Mar 24 03:24:59.844: INFO: (11) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 3.276557ms)
Mar 24 03:24:59.844: INFO: (11) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 3.329889ms)
Mar 24 03:24:59.844: INFO: (11) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 3.54009ms)
Mar 24 03:24:59.845: INFO: (11) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 3.743203ms)
Mar 24 03:24:59.845: INFO: (11) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 3.832719ms)
Mar 24 03:24:59.847: INFO: (12) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 1.849131ms)
Mar 24 03:24:59.847: INFO: (12) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 1.895228ms)
Mar 24 03:24:59.847: INFO: (12) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 2.095931ms)
Mar 24 03:24:59.847: INFO: (12) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 2.141303ms)
Mar 24 03:24:59.847: INFO: (12) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 2.411595ms)
Mar 24 03:24:59.847: INFO: (12) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.501309ms)
Mar 24 03:24:59.848: INFO: (12) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.953672ms)
Mar 24 03:24:59.848: INFO: (12) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 3.094458ms)
Mar 24 03:24:59.848: INFO: (12) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 3.166292ms)
Mar 24 03:24:59.848: INFO: (12) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 3.1707ms)
Mar 24 03:24:59.848: INFO: (12) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 3.272236ms)
Mar 24 03:24:59.848: INFO: (12) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 3.365981ms)
Mar 24 03:24:59.848: INFO: (12) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 3.281814ms)
Mar 24 03:24:59.848: INFO: (12) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 3.319867ms)
Mar 24 03:24:59.848: INFO: (12) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 3.571637ms)
Mar 24 03:24:59.849: INFO: (12) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 3.937412ms)
Mar 24 03:24:59.851: INFO: (13) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 2.117006ms)
Mar 24 03:24:59.851: INFO: (13) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.347203ms)
Mar 24 03:24:59.851: INFO: (13) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.587321ms)
Mar 24 03:24:59.851: INFO: (13) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 2.537049ms)
Mar 24 03:24:59.851: INFO: (13) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 2.585183ms)
Mar 24 03:24:59.852: INFO: (13) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 2.873286ms)
Mar 24 03:24:59.852: INFO: (13) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 2.962139ms)
Mar 24 03:24:59.852: INFO: (13) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.964059ms)
Mar 24 03:24:59.852: INFO: (13) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 2.995547ms)
Mar 24 03:24:59.852: INFO: (13) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 3.246105ms)
Mar 24 03:24:59.852: INFO: (13) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 3.384676ms)
Mar 24 03:24:59.853: INFO: (13) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 3.756979ms)
Mar 24 03:24:59.853: INFO: (13) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 3.858446ms)
Mar 24 03:24:59.853: INFO: (13) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 4.331195ms)
Mar 24 03:24:59.853: INFO: (13) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 4.317761ms)
Mar 24 03:24:59.853: INFO: (13) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 4.64582ms)
Mar 24 03:24:59.855: INFO: (14) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 1.760274ms)
Mar 24 03:24:59.856: INFO: (14) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.06872ms)
Mar 24 03:24:59.856: INFO: (14) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 2.066424ms)
Mar 24 03:24:59.856: INFO: (14) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 2.380498ms)
Mar 24 03:24:59.856: INFO: (14) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 2.521318ms)
Mar 24 03:24:59.856: INFO: (14) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.51528ms)
Mar 24 03:24:59.856: INFO: (14) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.718837ms)
Mar 24 03:24:59.856: INFO: (14) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 2.889642ms)
Mar 24 03:24:59.857: INFO: (14) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 3.192477ms)
Mar 24 03:24:59.857: INFO: (14) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 3.385508ms)
Mar 24 03:24:59.857: INFO: (14) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 3.619991ms)
Mar 24 03:24:59.857: INFO: (14) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 3.832281ms)
Mar 24 03:24:59.857: INFO: (14) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 3.824414ms)
Mar 24 03:24:59.857: INFO: (14) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 3.994003ms)
Mar 24 03:24:59.858: INFO: (14) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 4.374077ms)
Mar 24 03:24:59.858: INFO: (14) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 4.492496ms)
Mar 24 03:24:59.860: INFO: (15) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 1.807366ms)
Mar 24 03:24:59.860: INFO: (15) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 2.060139ms)
Mar 24 03:24:59.860: INFO: (15) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 2.15286ms)
Mar 24 03:24:59.860: INFO: (15) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 2.256151ms)
Mar 24 03:24:59.861: INFO: (15) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 2.586671ms)
Mar 24 03:24:59.861: INFO: (15) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.709851ms)
Mar 24 03:24:59.861: INFO: (15) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 3.081399ms)
Mar 24 03:24:59.864: INFO: (15) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 5.878002ms)
Mar 24 03:24:59.864: INFO: (15) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 5.897195ms)
Mar 24 03:24:59.864: INFO: (15) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 5.964675ms)
Mar 24 03:24:59.864: INFO: (15) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 6.115812ms)
Mar 24 03:24:59.864: INFO: (15) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 6.141883ms)
Mar 24 03:24:59.864: INFO: (15) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 6.138087ms)
Mar 24 03:24:59.864: INFO: (15) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 6.124192ms)
Mar 24 03:24:59.864: INFO: (15) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 6.262554ms)
Mar 24 03:24:59.864: INFO: (15) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 6.310226ms)
Mar 24 03:24:59.869: INFO: (16) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 4.691747ms)
Mar 24 03:24:59.869: INFO: (16) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 4.847832ms)
Mar 24 03:24:59.869: INFO: (16) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 4.825559ms)
Mar 24 03:24:59.869: INFO: (16) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 4.925917ms)
Mar 24 03:24:59.870: INFO: (16) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 5.2699ms)
Mar 24 03:24:59.870: INFO: (16) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 5.346524ms)
Mar 24 03:24:59.870: INFO: (16) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 5.320958ms)
Mar 24 03:24:59.870: INFO: (16) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 5.512066ms)
Mar 24 03:24:59.870: INFO: (16) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 5.76243ms)
Mar 24 03:24:59.871: INFO: (16) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 6.009667ms)
Mar 24 03:24:59.872: INFO: (16) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 7.722942ms)
Mar 24 03:24:59.872: INFO: (16) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 7.721704ms)
Mar 24 03:24:59.872: INFO: (16) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 7.734917ms)
Mar 24 03:24:59.872: INFO: (16) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 7.834055ms)
Mar 24 03:24:59.873: INFO: (16) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 8.589617ms)
Mar 24 03:24:59.873: INFO: (16) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 8.83995ms)
Mar 24 03:24:59.875: INFO: (17) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 1.939285ms)
Mar 24 03:24:59.876: INFO: (17) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.131544ms)
Mar 24 03:24:59.876: INFO: (17) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 2.135366ms)
Mar 24 03:24:59.876: INFO: (17) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 2.210252ms)
Mar 24 03:24:59.876: INFO: (17) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 2.596072ms)
Mar 24 03:24:59.876: INFO: (17) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 2.655574ms)
Mar 24 03:24:59.876: INFO: (17) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 2.64725ms)
Mar 24 03:24:59.877: INFO: (17) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 3.261874ms)
Mar 24 03:24:59.877: INFO: (17) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 3.50523ms)
Mar 24 03:24:59.877: INFO: (17) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 3.415984ms)
Mar 24 03:24:59.877: INFO: (17) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 3.467429ms)
Mar 24 03:24:59.877: INFO: (17) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 3.659421ms)
Mar 24 03:24:59.877: INFO: (17) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 3.67618ms)
Mar 24 03:24:59.877: INFO: (17) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 3.774318ms)
Mar 24 03:24:59.877: INFO: (17) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 3.822246ms)
Mar 24 03:24:59.878: INFO: (17) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 4.136154ms)
Mar 24 03:24:59.880: INFO: (18) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 1.847486ms)
Mar 24 03:24:59.880: INFO: (18) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 2.01921ms)
Mar 24 03:24:59.880: INFO: (18) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 2.167857ms)
Mar 24 03:24:59.880: INFO: (18) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 2.248742ms)
Mar 24 03:24:59.880: INFO: (18) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 2.365956ms)
Mar 24 03:24:59.880: INFO: (18) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.51633ms)
Mar 24 03:24:59.880: INFO: (18) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.604455ms)
Mar 24 03:24:59.880: INFO: (18) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.652969ms)
Mar 24 03:24:59.881: INFO: (18) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 3.06688ms)
Mar 24 03:24:59.881: INFO: (18) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 3.061908ms)
Mar 24 03:24:59.881: INFO: (18) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 3.174225ms)
Mar 24 03:24:59.881: INFO: (18) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 3.396035ms)
Mar 24 03:24:59.881: INFO: (18) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 3.487504ms)
Mar 24 03:24:59.881: INFO: (18) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 3.505085ms)
Mar 24 03:24:59.882: INFO: (18) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 3.9644ms)
Mar 24 03:24:59.882: INFO: (18) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 3.932175ms)
Mar 24 03:24:59.884: INFO: (19) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:462/proxy/: tls qux (200; 2.000555ms)
Mar 24 03:24:59.884: INFO: (19) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 2.042051ms)
Mar 24 03:24:59.884: INFO: (19) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:460/proxy/: tls baz (200; 2.04069ms)
Mar 24 03:24:59.884: INFO: (19) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.332934ms)
Mar 24 03:24:59.884: INFO: (19) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">test<... (200; 2.356802ms)
Mar 24 03:24:59.884: INFO: (19) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd/proxy/rewriteme">test</a> (200; 2.576891ms)
Mar 24 03:24:59.884: INFO: (19) /api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/https:proxy-service-skm8f-lnlkd:443/proxy/tlsrewritem... (200; 2.602672ms)
Mar 24 03:24:59.885: INFO: (19) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname2/proxy/: bar (200; 2.790082ms)
Mar 24 03:24:59.885: INFO: (19) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:162/proxy/: bar (200; 2.968375ms)
Mar 24 03:24:59.885: INFO: (19) /api/v1/namespaces/proxy-1177/pods/proxy-service-skm8f-lnlkd:160/proxy/: foo (200; 3.269612ms)
Mar 24 03:24:59.885: INFO: (19) /api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1177/pods/http:proxy-service-skm8f-lnlkd:1080/proxy/rewriteme">... (200; 3.326546ms)
Mar 24 03:24:59.885: INFO: (19) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname1/proxy/: tls baz (200; 3.319964ms)
Mar 24 03:24:59.885: INFO: (19) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname2/proxy/: bar (200; 3.492612ms)
Mar 24 03:24:59.885: INFO: (19) /api/v1/namespaces/proxy-1177/services/proxy-service-skm8f:portname1/proxy/: foo (200; 3.646891ms)
Mar 24 03:24:59.886: INFO: (19) /api/v1/namespaces/proxy-1177/services/http:proxy-service-skm8f:portname1/proxy/: foo (200; 4.088341ms)
Mar 24 03:24:59.886: INFO: (19) /api/v1/namespaces/proxy-1177/services/https:proxy-service-skm8f:tlsportname2/proxy/: tls qux (200; 4.309539ms)
STEP: deleting ReplicationController proxy-service-skm8f in namespace proxy-1177, will wait for the garbage collector to delete the pods
Mar 24 03:24:59.944: INFO: Deleting ReplicationController proxy-service-skm8f took: 6.292754ms
Mar 24 03:25:00.044: INFO: Terminating ReplicationController proxy-service-skm8f pods took: 100.13081ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:25:12.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1177" for this suite.

â€¢ [SLOW TEST:19.961 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":186,"skipped":3015,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:25:12.560: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6678
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:25:37.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6678" for this suite.

â€¢ [SLOW TEST:25.318 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":187,"skipped":3044,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:25:37.878: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1056
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1056.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1056.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1056.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1056.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1056.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1056.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 03:25:40.031: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:25:40.033: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:25:40.038: INFO: Unable to read jessie_udp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:25:40.040: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:25:40.040: INFO: Lookups using dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:25:45.048: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:25:45.049: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:25:45.055: INFO: Unable to read jessie_udp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:25:45.057: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:25:45.057: INFO: Lookups using dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:25:50.047: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:25:50.054: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:25:50.061: INFO: Unable to read jessie_udp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:25:50.063: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:25:50.063: INFO: Lookups using dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:25:55.048: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:25:55.050: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:25:55.055: INFO: Unable to read jessie_udp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:25:55.057: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:25:55.057: INFO: Lookups using dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:26:00.050: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:26:00.052: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:26:00.057: INFO: Unable to read jessie_udp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:26:00.059: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:26:00.059: INFO: Lookups using dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:26:05.048: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:26:05.049: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:26:05.055: INFO: Unable to read jessie_udp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:26:05.057: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d: the server could not find the requested resource (get pods dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d)
Mar 24 03:26:05.057: INFO: Lookups using dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:26:10.057: INFO: DNS probes using dns-1056/dns-test-71e55522-f82e-4b46-a6c5-1f1a31560d3d succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:26:10.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1056" for this suite.

â€¢ [SLOW TEST:32.208 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":188,"skipped":3070,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:26:10.086: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5075
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:26:12.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5075" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":189,"skipped":3086,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:26:12.245: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8254
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:26:12.376: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 24 03:26:17.385: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 24 03:26:17.385: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 24 03:26:19.388: INFO: Creating deployment "test-rollover-deployment"
Mar 24 03:26:19.394: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 24 03:26:21.402: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 24 03:26:21.406: INFO: Ensure that both replica sets have 1 created replica
Mar 24 03:26:21.410: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 24 03:26:21.416: INFO: Updating deployment test-rollover-deployment
Mar 24 03:26:21.416: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 24 03:26:23.423: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 24 03:26:23.427: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 24 03:26:23.431: INFO: all replica sets need to contain the pod-template-hash label
Mar 24 03:26:23.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153179, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153179, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153182, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153179, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:26:25.439: INFO: all replica sets need to contain the pod-template-hash label
Mar 24 03:26:25.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153179, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153179, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153182, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153179, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:26:27.439: INFO: all replica sets need to contain the pod-template-hash label
Mar 24 03:26:27.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153179, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153179, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153182, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153179, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:26:29.435: INFO: all replica sets need to contain the pod-template-hash label
Mar 24 03:26:29.435: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153179, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153179, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153182, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153179, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:26:31.439: INFO: all replica sets need to contain the pod-template-hash label
Mar 24 03:26:31.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153179, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153179, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153182, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153179, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:26:33.438: INFO: 
Mar 24 03:26:33.438: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar 24 03:26:33.443: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8254  4ebd6a66-5cd0-4416-8e7f-9174766e6ceb 802679 2 2021-03-24 03:26:19 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-24 03:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-24 03:26:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031a4498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-24 03:26:19 +0000 UTC,LastTransitionTime:2021-03-24 03:26:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-03-24 03:26:32 +0000 UTC,LastTransitionTime:2021-03-24 03:26:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 24 03:26:33.445: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-8254  3e45442a-a058-4e6c-a609-be57a385b99c 802669 2 2021-03-24 03:26:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 4ebd6a66-5cd0-4416-8e7f-9174766e6ceb 0xc0031a4927 0xc0031a4928}] []  [{kube-controller-manager Update apps/v1 2021-03-24 03:26:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ebd6a66-5cd0-4416-8e7f-9174766e6ceb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031a49b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 24 03:26:33.445: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 24 03:26:33.445: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8254  c91c3b7a-dfbd-464c-9eb8-22922f7bc7b2 802677 2 2021-03-24 03:26:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 4ebd6a66-5cd0-4416-8e7f-9174766e6ceb 0xc0031a4807 0xc0031a4808}] []  [{e2e.test Update apps/v1 2021-03-24 03:26:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-24 03:26:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ebd6a66-5cd0-4416-8e7f-9174766e6ceb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0031a48b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 24 03:26:33.445: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-8254  50932fee-1978-42c7-86ee-25639838087d 802610 2 2021-03-24 03:26:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 4ebd6a66-5cd0-4416-8e7f-9174766e6ceb 0xc0031a4a27 0xc0031a4a28}] []  [{kube-controller-manager Update apps/v1 2021-03-24 03:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ebd6a66-5cd0-4416-8e7f-9174766e6ceb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031a4ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 24 03:26:33.447: INFO: Pod "test-rollover-deployment-668db69979-jxn46" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-jxn46 test-rollover-deployment-668db69979- deployment-8254  782a8003-c93a-4f9b-9e7d-42cabcd058bc 802628 0 2021-03-24 03:26:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 3e45442a-a058-4e6c-a609-be57a385b99c 0xc005fed0e7 0xc005fed0e8}] []  [{kube-controller-manager Update v1 2021-03-24 03:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3e45442a-a058-4e6c-a609-be57a385b99c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 03:26:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.43.1.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zqpx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zqpx9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zqpx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:26:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:26:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:26:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:26:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.14,PodIP:10.43.1.81,StartTime:2021-03-24 03:26:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 03:26:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://094f82ac919ab945e0f77e97fc8c138818ae3e1717af0e8d40eb8ed6094b8a2e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.43.1.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:26:33.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8254" for this suite.

â€¢ [SLOW TEST:21.210 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":190,"skipped":3087,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:26:33.456: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5783
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:27:33.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5783" for this suite.

â€¢ [SLOW TEST:60.143 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":191,"skipped":3138,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:27:33.599: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3101
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 03:27:34.322: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 24 03:27:36.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153254, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153254, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153254, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153254, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 03:27:39.350: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:27:39.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3101" for this suite.
STEP: Destroying namespace "webhook-3101-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

â€¢ [SLOW TEST:5.833 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":192,"skipped":3142,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:27:39.432: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7720
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:27:39.566: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20715dd3-6968-4ca4-bad1-1bc6223b955a" in namespace "projected-7720" to be "Succeeded or Failed"
Mar 24 03:27:39.569: INFO: Pod "downwardapi-volume-20715dd3-6968-4ca4-bad1-1bc6223b955a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.311926ms
Mar 24 03:27:41.574: INFO: Pod "downwardapi-volume-20715dd3-6968-4ca4-bad1-1bc6223b955a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008310171s
STEP: Saw pod success
Mar 24 03:27:41.574: INFO: Pod "downwardapi-volume-20715dd3-6968-4ca4-bad1-1bc6223b955a" satisfied condition "Succeeded or Failed"
Mar 24 03:27:41.576: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod downwardapi-volume-20715dd3-6968-4ca4-bad1-1bc6223b955a container client-container: <nil>
STEP: delete the pod
Mar 24 03:27:41.590: INFO: Waiting for pod downwardapi-volume-20715dd3-6968-4ca4-bad1-1bc6223b955a to disappear
Mar 24 03:27:41.592: INFO: Pod downwardapi-volume-20715dd3-6968-4ca4-bad1-1bc6223b955a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:27:41.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7720" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":193,"skipped":3145,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:27:41.599: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6842
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:27:45.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6842" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":194,"skipped":3190,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:27:45.752: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2505
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar 24 03:27:45.885: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 24 03:27:50.890: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:27:51.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2505" for this suite.

â€¢ [SLOW TEST:6.160 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":195,"skipped":3215,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:27:51.912: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9980
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 24 03:27:56.085: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 24 03:27:56.087: INFO: Pod pod-with-poststart-http-hook still exists
Mar 24 03:27:58.087: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 24 03:27:58.092: INFO: Pod pod-with-poststart-http-hook still exists
Mar 24 03:28:00.087: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 24 03:28:00.091: INFO: Pod pod-with-poststart-http-hook still exists
Mar 24 03:28:02.087: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 24 03:28:02.092: INFO: Pod pod-with-poststart-http-hook still exists
Mar 24 03:28:04.087: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 24 03:28:04.092: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:28:04.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9980" for this suite.

â€¢ [SLOW TEST:12.187 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":196,"skipped":3242,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:28:04.099: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2070
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:28:04.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2070" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":197,"skipped":3264,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:28:04.253: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2528
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Mar 24 03:28:04.380: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2528 proxy --unix-socket=/tmp/kubectl-proxy-unix040455316/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:28:04.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2528" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":198,"skipped":3271,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:28:04.427: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3582
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:28:04.558: INFO: Waiting up to 5m0s for pod "downwardapi-volume-338f47e5-116b-47b0-be22-36c534381a7b" in namespace "downward-api-3582" to be "Succeeded or Failed"
Mar 24 03:28:04.560: INFO: Pod "downwardapi-volume-338f47e5-116b-47b0-be22-36c534381a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.92118ms
Mar 24 03:28:06.565: INFO: Pod "downwardapi-volume-338f47e5-116b-47b0-be22-36c534381a7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006776458s
STEP: Saw pod success
Mar 24 03:28:06.565: INFO: Pod "downwardapi-volume-338f47e5-116b-47b0-be22-36c534381a7b" satisfied condition "Succeeded or Failed"
Mar 24 03:28:06.568: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod downwardapi-volume-338f47e5-116b-47b0-be22-36c534381a7b container client-container: <nil>
STEP: delete the pod
Mar 24 03:28:06.581: INFO: Waiting for pod downwardapi-volume-338f47e5-116b-47b0-be22-36c534381a7b to disappear
Mar 24 03:28:06.582: INFO: Pod downwardapi-volume-338f47e5-116b-47b0-be22-36c534381a7b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:28:06.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3582" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":199,"skipped":3277,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:28:06.588: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:28:06.719: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f954aa80-1df6-49d1-a177-db211e73c263" in namespace "projected-3541" to be "Succeeded or Failed"
Mar 24 03:28:06.721: INFO: Pod "downwardapi-volume-f954aa80-1df6-49d1-a177-db211e73c263": Phase="Pending", Reason="", readiness=false. Elapsed: 1.680216ms
Mar 24 03:28:08.726: INFO: Pod "downwardapi-volume-f954aa80-1df6-49d1-a177-db211e73c263": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007041341s
STEP: Saw pod success
Mar 24 03:28:08.726: INFO: Pod "downwardapi-volume-f954aa80-1df6-49d1-a177-db211e73c263" satisfied condition "Succeeded or Failed"
Mar 24 03:28:08.728: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod downwardapi-volume-f954aa80-1df6-49d1-a177-db211e73c263 container client-container: <nil>
STEP: delete the pod
Mar 24 03:28:08.742: INFO: Waiting for pod downwardapi-volume-f954aa80-1df6-49d1-a177-db211e73c263 to disappear
Mar 24 03:28:08.744: INFO: Pod downwardapi-volume-f954aa80-1df6-49d1-a177-db211e73c263 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:28:08.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3541" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":200,"skipped":3292,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:28:08.750: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4017
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-37106479-0d5a-444d-8986-1e4b952e916c
STEP: Creating a pod to test consume configMaps
Mar 24 03:28:08.884: INFO: Waiting up to 5m0s for pod "pod-configmaps-9e49885f-f36a-42b2-8b67-5691651c8636" in namespace "configmap-4017" to be "Succeeded or Failed"
Mar 24 03:28:08.886: INFO: Pod "pod-configmaps-9e49885f-f36a-42b2-8b67-5691651c8636": Phase="Pending", Reason="", readiness=false. Elapsed: 1.729061ms
Mar 24 03:28:10.890: INFO: Pod "pod-configmaps-9e49885f-f36a-42b2-8b67-5691651c8636": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005506241s
STEP: Saw pod success
Mar 24 03:28:10.890: INFO: Pod "pod-configmaps-9e49885f-f36a-42b2-8b67-5691651c8636" satisfied condition "Succeeded or Failed"
Mar 24 03:28:10.891: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-configmaps-9e49885f-f36a-42b2-8b67-5691651c8636 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 03:28:10.905: INFO: Waiting for pod pod-configmaps-9e49885f-f36a-42b2-8b67-5691651c8636 to disappear
Mar 24 03:28:10.907: INFO: Pod pod-configmaps-9e49885f-f36a-42b2-8b67-5691651c8636 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:28:10.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4017" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":201,"skipped":3307,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:28:10.912: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7679
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 24 03:28:15.086: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 24 03:28:15.088: INFO: Pod pod-with-prestop-http-hook still exists
Mar 24 03:28:17.088: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 24 03:28:17.092: INFO: Pod pod-with-prestop-http-hook still exists
Mar 24 03:28:19.088: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 24 03:28:19.093: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:28:19.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7679" for this suite.

â€¢ [SLOW TEST:8.193 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":202,"skipped":3326,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:28:19.105: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-96
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:28:19.240: INFO: The status of Pod test-webserver-711d3b54-a408-4f63-99cc-037abe190279 is Pending, waiting for it to be Running (with Ready = true)
Mar 24 03:28:21.246: INFO: The status of Pod test-webserver-711d3b54-a408-4f63-99cc-037abe190279 is Running (Ready = false)
Mar 24 03:28:23.246: INFO: The status of Pod test-webserver-711d3b54-a408-4f63-99cc-037abe190279 is Running (Ready = false)
Mar 24 03:28:25.245: INFO: The status of Pod test-webserver-711d3b54-a408-4f63-99cc-037abe190279 is Running (Ready = false)
Mar 24 03:28:27.246: INFO: The status of Pod test-webserver-711d3b54-a408-4f63-99cc-037abe190279 is Running (Ready = false)
Mar 24 03:28:29.246: INFO: The status of Pod test-webserver-711d3b54-a408-4f63-99cc-037abe190279 is Running (Ready = false)
Mar 24 03:28:31.246: INFO: The status of Pod test-webserver-711d3b54-a408-4f63-99cc-037abe190279 is Running (Ready = false)
Mar 24 03:28:33.246: INFO: The status of Pod test-webserver-711d3b54-a408-4f63-99cc-037abe190279 is Running (Ready = false)
Mar 24 03:28:35.246: INFO: The status of Pod test-webserver-711d3b54-a408-4f63-99cc-037abe190279 is Running (Ready = false)
Mar 24 03:28:37.245: INFO: The status of Pod test-webserver-711d3b54-a408-4f63-99cc-037abe190279 is Running (Ready = false)
Mar 24 03:28:39.246: INFO: The status of Pod test-webserver-711d3b54-a408-4f63-99cc-037abe190279 is Running (Ready = false)
Mar 24 03:28:41.246: INFO: The status of Pod test-webserver-711d3b54-a408-4f63-99cc-037abe190279 is Running (Ready = true)
Mar 24 03:28:41.248: INFO: Container started at 2021-03-24 03:28:19 +0000 UTC, pod became ready at 2021-03-24 03:28:39 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:28:41.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-96" for this suite.

â€¢ [SLOW TEST:22.150 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":203,"skipped":3328,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:28:41.256: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3341
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Mar 24 03:28:41.391: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 24 03:28:41.391: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 24 03:28:41.398: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 24 03:28:41.398: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 24 03:28:41.407: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 24 03:28:41.407: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 24 03:28:41.421: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 24 03:28:41.421: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 24 03:28:43.006: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 24 03:28:43.006: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 24 03:28:43.056: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Mar 24 03:28:43.063: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Mar 24 03:28:43.064: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 0
Mar 24 03:28:43.064: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 0
Mar 24 03:28:43.064: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 0
Mar 24 03:28:43.064: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 0
Mar 24 03:28:43.064: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 0
Mar 24 03:28:43.064: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 0
Mar 24 03:28:43.064: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 0
Mar 24 03:28:43.064: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 0
Mar 24 03:28:43.064: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1
Mar 24 03:28:43.064: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1
Mar 24 03:28:43.064: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 2
Mar 24 03:28:43.064: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 2
Mar 24 03:28:43.064: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 2
Mar 24 03:28:43.064: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 2
Mar 24 03:28:43.071: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 2
Mar 24 03:28:43.071: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 2
Mar 24 03:28:43.091: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 2
Mar 24 03:28:43.091: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 2
Mar 24 03:28:43.096: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1
STEP: listing Deployments
Mar 24 03:28:43.099: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Mar 24 03:28:43.107: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Mar 24 03:28:43.111: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 24 03:28:43.124: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 24 03:28:43.133: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 24 03:28:43.147: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 24 03:28:43.155: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 24 03:28:43.162: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 24 03:28:43.165: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 24 03:28:43.169: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Mar 24 03:28:44.087: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1
Mar 24 03:28:44.087: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1
Mar 24 03:28:44.087: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1
Mar 24 03:28:44.087: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1
Mar 24 03:28:44.087: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1
Mar 24 03:28:44.087: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1
Mar 24 03:28:44.087: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1
Mar 24 03:28:44.087: INFO: observed Deployment test-deployment in namespace deployment-3341 with ReadyReplicas 1
STEP: deleting the Deployment
Mar 24 03:28:44.096: INFO: observed event type MODIFIED
Mar 24 03:28:44.096: INFO: observed event type MODIFIED
Mar 24 03:28:44.096: INFO: observed event type MODIFIED
Mar 24 03:28:44.096: INFO: observed event type MODIFIED
Mar 24 03:28:44.096: INFO: observed event type MODIFIED
Mar 24 03:28:44.096: INFO: observed event type MODIFIED
Mar 24 03:28:44.096: INFO: observed event type MODIFIED
Mar 24 03:28:44.096: INFO: observed event type MODIFIED
Mar 24 03:28:44.096: INFO: observed event type MODIFIED
Mar 24 03:28:44.096: INFO: observed event type MODIFIED
Mar 24 03:28:44.097: INFO: observed event type MODIFIED
Mar 24 03:28:44.097: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar 24 03:28:44.103: INFO: Log out all the ReplicaSets if there is no deployment created
Mar 24 03:28:44.105: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-3341  6f4bcd18-2451-4aa5-9c45-4074dc66d577 804058 3 2021-03-24 03:28:43 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment f5c261a1-cd5b-4c52-be78-af270527bd64 0xc003fbb287 0xc003fbb288}] []  [{kube-controller-manager Update apps/v1 2021-03-24 03:28:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c261a1-cd5b-4c52-be78-af270527bd64\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003fbb2f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Mar 24 03:28:44.107: INFO: pod: "test-deployment-768947d6f5-4cpp4":
&Pod{ObjectMeta:{test-deployment-768947d6f5-4cpp4 test-deployment-768947d6f5- deployment-3341  392e4130-3c0d-4857-ba08-1e6bff1f84bb 804061 0 2021-03-24 03:28:44 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet test-deployment-768947d6f5 6f4bcd18-2451-4aa5-9c45-4074dc66d577 0xc003617517 0xc003617518}] []  [{kube-controller-manager Update v1 2021-03-24 03:28:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f4bcd18-2451-4aa5-9c45-4074dc66d577\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 03:28:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-knmxm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-knmxm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-knmxm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:28:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:28:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:28:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:28:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.14,PodIP:,StartTime:2021-03-24 03:28:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 24 03:28:44.107: INFO: pod: "test-deployment-768947d6f5-r84z9":
&Pod{ObjectMeta:{test-deployment-768947d6f5-r84z9 test-deployment-768947d6f5- deployment-3341  bf7de97b-8e44-4f18-8f59-d46e22684988 804036 0 2021-03-24 03:28:43 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet test-deployment-768947d6f5 6f4bcd18-2451-4aa5-9c45-4074dc66d577 0xc0036176a7 0xc0036176a8}] []  [{kube-controller-manager Update v1 2021-03-24 03:28:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f4bcd18-2451-4aa5-9c45-4074dc66d577\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 03:28:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.43.1.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-knmxm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-knmxm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-knmxm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:28:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:28:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:28:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:28:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.15,PodIP:10.43.1.48,StartTime:2021-03-24 03:28:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 03:28:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://4a56f9c6ebb1cbfdd5379aadde71cd873740286af6bcfdacaacb7c2d32a2dd4e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.43.1.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 24 03:28:44.107: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-3341  e52cf9ea-47ca-4269-81d5-586fde0555aa 804052 4 2021-03-24 03:28:43 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment f5c261a1-cd5b-4c52-be78-af270527bd64 0xc003fbb357 0xc003fbb358}] []  [{kube-controller-manager Update apps/v1 2021-03-24 03:28:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c261a1-cd5b-4c52-be78-af270527bd64\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003fbb3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar 24 03:28:44.109: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-3341  0a36e237-fffd-4db9-aa19-f484c27deba9 803995 2 2021-03-24 03:28:41 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment f5c261a1-cd5b-4c52-be78-af270527bd64 0xc003fbb437 0xc003fbb438}] []  [{kube-controller-manager Update apps/v1 2021-03-24 03:28:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c261a1-cd5b-4c52-be78-af270527bd64\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003fbb4a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Mar 24 03:28:44.111: INFO: pod: "test-deployment-8b6954bfb-7krvv":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-7krvv test-deployment-8b6954bfb- deployment-3341  9b045817-4228-4265-bb48-12bdfed020fe 803964 0 2021-03-24 03:28:41 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet test-deployment-8b6954bfb 0a36e237-fffd-4db9-aa19-f484c27deba9 0xc003fbb707 0xc003fbb708}] []  [{kube-controller-manager Update v1 2021-03-24 03:28:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a36e237-fffd-4db9-aa19-f484c27deba9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 03:28:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.43.1.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-knmxm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-knmxm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-knmxm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.14,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:28:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:28:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:28:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:28:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.14,PodIP:10.43.1.92,StartTime:2021-03-24 03:28:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 03:28:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://24b3c24a3b9abd6c9957ae7f0806a916dbde557ee79d8e0c03a4501a8d7c021d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.43.1.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:28:44.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3341" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":204,"skipped":3369,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:28:44.118: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6487
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Mar 24 03:28:44.248: INFO: Waiting up to 5m0s for pod "client-containers-63592b8f-b8f3-40b3-a89f-2d767696eb98" in namespace "containers-6487" to be "Succeeded or Failed"
Mar 24 03:28:44.250: INFO: Pod "client-containers-63592b8f-b8f3-40b3-a89f-2d767696eb98": Phase="Pending", Reason="", readiness=false. Elapsed: 1.820074ms
Mar 24 03:28:46.256: INFO: Pod "client-containers-63592b8f-b8f3-40b3-a89f-2d767696eb98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007530553s
STEP: Saw pod success
Mar 24 03:28:46.256: INFO: Pod "client-containers-63592b8f-b8f3-40b3-a89f-2d767696eb98" satisfied condition "Succeeded or Failed"
Mar 24 03:28:46.258: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod client-containers-63592b8f-b8f3-40b3-a89f-2d767696eb98 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 03:28:46.275: INFO: Waiting for pod client-containers-63592b8f-b8f3-40b3-a89f-2d767696eb98 to disappear
Mar 24 03:28:46.277: INFO: Pod client-containers-63592b8f-b8f3-40b3-a89f-2d767696eb98 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:28:46.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6487" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":205,"skipped":3370,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:28:46.284: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1797
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 24 03:28:46.417: INFO: Waiting up to 5m0s for pod "pod-cc7901f0-86e8-47b2-bff2-367b19ee7aff" in namespace "emptydir-1797" to be "Succeeded or Failed"
Mar 24 03:28:46.419: INFO: Pod "pod-cc7901f0-86e8-47b2-bff2-367b19ee7aff": Phase="Pending", Reason="", readiness=false. Elapsed: 1.887744ms
Mar 24 03:28:48.425: INFO: Pod "pod-cc7901f0-86e8-47b2-bff2-367b19ee7aff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007620191s
STEP: Saw pod success
Mar 24 03:28:48.425: INFO: Pod "pod-cc7901f0-86e8-47b2-bff2-367b19ee7aff" satisfied condition "Succeeded or Failed"
Mar 24 03:28:48.427: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-cc7901f0-86e8-47b2-bff2-367b19ee7aff container test-container: <nil>
STEP: delete the pod
Mar 24 03:28:48.444: INFO: Waiting for pod pod-cc7901f0-86e8-47b2-bff2-367b19ee7aff to disappear
Mar 24 03:28:48.446: INFO: Pod pod-cc7901f0-86e8-47b2-bff2-367b19ee7aff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:28:48.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1797" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":206,"skipped":3411,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:28:48.451: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7467
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar 24 03:28:48.585: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7467  3ba67309-5c10-438a-834b-69ccd235a3ca 804151 0 2021-03-24 03:28:48 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:ack.privileged] [] []  [{e2e.test Update v1 2021-03-24 03:28:48 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6bk5h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6bk5h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6bk5h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 24 03:28:48.586: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar 24 03:28:50.590: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Mar 24 03:28:50.590: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7467 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:28:50.590: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Verifying customized DNS server is configured on pod...
Mar 24 03:28:50.685: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7467 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:28:50.685: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 03:28:50.782: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:28:50.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7467" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":207,"skipped":3420,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:28:50.798: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1272
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1272.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1272.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1272.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1272.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1272.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1272.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1272.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1272.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1272.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1272.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 03:28:52.950: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:52.952: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:52.955: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:52.957: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:52.958: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:52.961: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:52.962: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:52.964: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:52.966: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:52.968: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:52.970: INFO: Unable to read jessie_udp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:52.972: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:52.972: INFO: Lookups using dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1272.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1272.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local jessie_udp@dns-test-service-2.dns-1272.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1272.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:28:57.975: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:57.977: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:57.979: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:57.981: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:57.983: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:57.985: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:57.987: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:57.989: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:57.991: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:57.993: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:57.995: INFO: Unable to read jessie_udp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:57.996: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:28:57.996: INFO: Lookups using dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1272.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1272.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local jessie_udp@dns-test-service-2.dns-1272.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1272.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:29:02.975: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:02.977: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:02.979: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:02.981: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:02.983: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:02.985: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:02.987: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:02.989: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:02.991: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:02.993: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:02.995: INFO: Unable to read jessie_udp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:02.997: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:02.997: INFO: Lookups using dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1272.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1272.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local jessie_udp@dns-test-service-2.dns-1272.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1272.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:29:07.975: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:07.977: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:07.979: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:07.981: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:07.983: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:07.985: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:07.987: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:07.989: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:07.991: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:07.992: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:07.994: INFO: Unable to read jessie_udp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:07.996: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:07.996: INFO: Lookups using dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1272.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1272.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local jessie_udp@dns-test-service-2.dns-1272.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1272.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:29:12.975: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:12.978: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:12.980: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:12.982: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:12.984: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:12.985: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:12.987: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:12.989: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:12.991: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:12.993: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:12.995: INFO: Unable to read jessie_udp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:12.997: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:12.997: INFO: Lookups using dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1272.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1272.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local jessie_udp@dns-test-service-2.dns-1272.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1272.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:29:17.975: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:17.977: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:17.979: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:17.981: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:17.983: INFO: Unable to read wheezy_udp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:17.985: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:17.987: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:17.989: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:17.991: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:17.993: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1272.svc.cluster.local from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:17.995: INFO: Unable to read jessie_udp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:17.996: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de: the server could not find the requested resource (get pods dns-test-b922ed2e-7034-4470-9e97-bd120537a2de)
Mar 24 03:29:17.996: INFO: Lookups using dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1272.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1272.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1272.svc.cluster.local jessie_udp@dns-test-service-2.dns-1272.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1272.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:29:22.996: INFO: DNS probes using dns-1272/dns-test-b922ed2e-7034-4470-9e97-bd120537a2de succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:29:23.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1272" for this suite.

â€¢ [SLOW TEST:32.236 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":208,"skipped":3423,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:29:23.034: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7685
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar 24 03:29:23.165: INFO: Waiting up to 5m0s for pod "downward-api-4d1d6fb4-02df-482e-aa6a-ff083c30c3ef" in namespace "downward-api-7685" to be "Succeeded or Failed"
Mar 24 03:29:23.167: INFO: Pod "downward-api-4d1d6fb4-02df-482e-aa6a-ff083c30c3ef": Phase="Pending", Reason="", readiness=false. Elapsed: 1.875947ms
Mar 24 03:29:25.174: INFO: Pod "downward-api-4d1d6fb4-02df-482e-aa6a-ff083c30c3ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009012308s
STEP: Saw pod success
Mar 24 03:29:25.174: INFO: Pod "downward-api-4d1d6fb4-02df-482e-aa6a-ff083c30c3ef" satisfied condition "Succeeded or Failed"
Mar 24 03:29:25.176: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod downward-api-4d1d6fb4-02df-482e-aa6a-ff083c30c3ef container dapi-container: <nil>
STEP: delete the pod
Mar 24 03:29:25.189: INFO: Waiting for pod downward-api-4d1d6fb4-02df-482e-aa6a-ff083c30c3ef to disappear
Mar 24 03:29:25.190: INFO: Pod downward-api-4d1d6fb4-02df-482e-aa6a-ff083c30c3ef no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:29:25.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7685" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":209,"skipped":3431,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:29:25.196: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7386
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:29:29.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7386" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":210,"skipped":3435,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:29:29.952: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6936
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Mar 24 03:29:30.598: INFO: created pod pod-service-account-defaultsa
Mar 24 03:29:30.598: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 24 03:29:30.603: INFO: created pod pod-service-account-mountsa
Mar 24 03:29:30.603: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 24 03:29:30.607: INFO: created pod pod-service-account-nomountsa
Mar 24 03:29:30.607: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 24 03:29:30.611: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 24 03:29:30.611: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 24 03:29:30.616: INFO: created pod pod-service-account-mountsa-mountspec
Mar 24 03:29:30.616: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 24 03:29:30.623: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 24 03:29:30.623: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 24 03:29:30.626: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 24 03:29:30.626: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 24 03:29:30.629: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 24 03:29:30.629: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 24 03:29:30.634: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 24 03:29:30.634: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:29:30.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6936" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":211,"skipped":3453,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:29:30.645: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4128
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 24 03:29:30.778: INFO: Waiting up to 5m0s for pod "pod-2325705a-2df0-4f44-b14a-192f4e72066a" in namespace "emptydir-4128" to be "Succeeded or Failed"
Mar 24 03:29:30.780: INFO: Pod "pod-2325705a-2df0-4f44-b14a-192f4e72066a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.831703ms
Mar 24 03:29:32.785: INFO: Pod "pod-2325705a-2df0-4f44-b14a-192f4e72066a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007251169s
STEP: Saw pod success
Mar 24 03:29:32.785: INFO: Pod "pod-2325705a-2df0-4f44-b14a-192f4e72066a" satisfied condition "Succeeded or Failed"
Mar 24 03:29:32.787: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod pod-2325705a-2df0-4f44-b14a-192f4e72066a container test-container: <nil>
STEP: delete the pod
Mar 24 03:29:32.799: INFO: Waiting for pod pod-2325705a-2df0-4f44-b14a-192f4e72066a to disappear
Mar 24 03:29:32.801: INFO: Pod pod-2325705a-2df0-4f44-b14a-192f4e72066a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:29:32.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4128" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":212,"skipped":3514,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:29:32.807: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1625
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-41f28136-00ad-41ac-ac6e-27c68faedca8
STEP: Creating a pod to test consume secrets
Mar 24 03:29:32.950: INFO: Waiting up to 5m0s for pod "pod-secrets-235ea12b-e3c6-44db-bada-e3b1be4f3212" in namespace "secrets-1625" to be "Succeeded or Failed"
Mar 24 03:29:32.951: INFO: Pod "pod-secrets-235ea12b-e3c6-44db-bada-e3b1be4f3212": Phase="Pending", Reason="", readiness=false. Elapsed: 1.5659ms
Mar 24 03:29:34.956: INFO: Pod "pod-secrets-235ea12b-e3c6-44db-bada-e3b1be4f3212": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006657718s
STEP: Saw pod success
Mar 24 03:29:34.956: INFO: Pod "pod-secrets-235ea12b-e3c6-44db-bada-e3b1be4f3212" satisfied condition "Succeeded or Failed"
Mar 24 03:29:34.958: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-secrets-235ea12b-e3c6-44db-bada-e3b1be4f3212 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 03:29:34.971: INFO: Waiting for pod pod-secrets-235ea12b-e3c6-44db-bada-e3b1be4f3212 to disappear
Mar 24 03:29:34.973: INFO: Pod pod-secrets-235ea12b-e3c6-44db-bada-e3b1be4f3212 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:29:34.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1625" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":213,"skipped":3572,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:29:34.979: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2315
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar 24 03:29:35.115: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2315  9cce3d6a-9c54-42fc-a059-dc7cfef7a8bb 804755 0 2021-03-24 03:29:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-24 03:29:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 03:29:35.115: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2315  9cce3d6a-9c54-42fc-a059-dc7cfef7a8bb 804756 0 2021-03-24 03:29:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-24 03:29:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 03:29:35.115: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2315  9cce3d6a-9c54-42fc-a059-dc7cfef7a8bb 804757 0 2021-03-24 03:29:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-24 03:29:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar 24 03:29:45.142: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2315  9cce3d6a-9c54-42fc-a059-dc7cfef7a8bb 804890 0 2021-03-24 03:29:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-24 03:29:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 03:29:45.142: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2315  9cce3d6a-9c54-42fc-a059-dc7cfef7a8bb 804891 0 2021-03-24 03:29:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-24 03:29:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 03:29:45.142: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2315  9cce3d6a-9c54-42fc-a059-dc7cfef7a8bb 804892 0 2021-03-24 03:29:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-24 03:29:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:29:45.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2315" for this suite.

â€¢ [SLOW TEST:10.169 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":214,"skipped":3625,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:29:45.149: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2244
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Mar 24 03:29:45.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2244 create -f -'
Mar 24 03:29:45.479: INFO: stderr: ""
Mar 24 03:29:45.479: INFO: stdout: "pod/pause created\n"
Mar 24 03:29:45.479: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 24 03:29:45.479: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2244" to be "running and ready"
Mar 24 03:29:45.482: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.819742ms
Mar 24 03:29:47.487: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.008002988s
Mar 24 03:29:47.487: INFO: Pod "pause" satisfied condition "running and ready"
Mar 24 03:29:47.487: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Mar 24 03:29:47.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2244 label pods pause testing-label=testing-label-value'
Mar 24 03:29:47.556: INFO: stderr: ""
Mar 24 03:29:47.556: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar 24 03:29:47.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2244 get pod pause -L testing-label'
Mar 24 03:29:47.606: INFO: stderr: ""
Mar 24 03:29:47.606: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar 24 03:29:47.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2244 label pods pause testing-label-'
Mar 24 03:29:47.667: INFO: stderr: ""
Mar 24 03:29:47.667: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar 24 03:29:47.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2244 get pod pause -L testing-label'
Mar 24 03:29:47.718: INFO: stderr: ""
Mar 24 03:29:47.718: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Mar 24 03:29:47.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2244 delete --grace-period=0 --force -f -'
Mar 24 03:29:47.776: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:29:47.776: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 24 03:29:47.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2244 get rc,svc -l name=pause --no-headers'
Mar 24 03:29:47.828: INFO: stderr: "No resources found in kubectl-2244 namespace.\n"
Mar 24 03:29:47.828: INFO: stdout: ""
Mar 24 03:29:47.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2244 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 24 03:29:47.877: INFO: stderr: ""
Mar 24 03:29:47.877: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:29:47.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2244" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":215,"skipped":3630,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:29:47.885: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2058
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2058.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2058.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2058.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2058.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 03:29:50.033: INFO: DNS probes using dns-test-78041353-a65d-4ca1-be24-c800029e049f succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2058.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2058.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2058.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2058.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 03:29:52.063: INFO: File wheezy_udp@dns-test-service-3.dns-2058.svc.cluster.local from pod  dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:29:52.065: INFO: File jessie_udp@dns-test-service-3.dns-2058.svc.cluster.local from pod  dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:29:52.065: INFO: Lookups using dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b failed for: [wheezy_udp@dns-test-service-3.dns-2058.svc.cluster.local jessie_udp@dns-test-service-3.dns-2058.svc.cluster.local]

Mar 24 03:29:57.068: INFO: File wheezy_udp@dns-test-service-3.dns-2058.svc.cluster.local from pod  dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:29:57.071: INFO: File jessie_udp@dns-test-service-3.dns-2058.svc.cluster.local from pod  dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:29:57.071: INFO: Lookups using dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b failed for: [wheezy_udp@dns-test-service-3.dns-2058.svc.cluster.local jessie_udp@dns-test-service-3.dns-2058.svc.cluster.local]

Mar 24 03:30:02.068: INFO: File wheezy_udp@dns-test-service-3.dns-2058.svc.cluster.local from pod  dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:30:02.071: INFO: File jessie_udp@dns-test-service-3.dns-2058.svc.cluster.local from pod  dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:30:02.071: INFO: Lookups using dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b failed for: [wheezy_udp@dns-test-service-3.dns-2058.svc.cluster.local jessie_udp@dns-test-service-3.dns-2058.svc.cluster.local]

Mar 24 03:30:07.070: INFO: File wheezy_udp@dns-test-service-3.dns-2058.svc.cluster.local from pod  dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:30:07.072: INFO: File jessie_udp@dns-test-service-3.dns-2058.svc.cluster.local from pod  dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:30:07.072: INFO: Lookups using dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b failed for: [wheezy_udp@dns-test-service-3.dns-2058.svc.cluster.local jessie_udp@dns-test-service-3.dns-2058.svc.cluster.local]

Mar 24 03:30:12.068: INFO: File wheezy_udp@dns-test-service-3.dns-2058.svc.cluster.local from pod  dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:30:12.071: INFO: File jessie_udp@dns-test-service-3.dns-2058.svc.cluster.local from pod  dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:30:12.071: INFO: Lookups using dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b failed for: [wheezy_udp@dns-test-service-3.dns-2058.svc.cluster.local jessie_udp@dns-test-service-3.dns-2058.svc.cluster.local]

Mar 24 03:30:17.068: INFO: File wheezy_udp@dns-test-service-3.dns-2058.svc.cluster.local from pod  dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:30:17.070: INFO: File jessie_udp@dns-test-service-3.dns-2058.svc.cluster.local from pod  dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 24 03:30:17.070: INFO: Lookups using dns-2058/dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b failed for: [wheezy_udp@dns-test-service-3.dns-2058.svc.cluster.local jessie_udp@dns-test-service-3.dns-2058.svc.cluster.local]

Mar 24 03:30:22.071: INFO: DNS probes using dns-test-85b46cc3-747d-46a2-acd6-459ab8dfc53b succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2058.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2058.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2058.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2058.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 03:30:24.119: INFO: DNS probes using dns-test-413c8ffc-2099-4da1-ae05-3c369acce790 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:30:24.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2058" for this suite.

â€¢ [SLOW TEST:36.259 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":216,"skipped":3671,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:30:24.144: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-555
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 03:30:24.721: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 24 03:30:26.730: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153424, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153424, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153424, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153424, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 03:30:29.748: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar 24 03:30:31.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=webhook-555 attach --namespace=webhook-555 to-be-attached-pod -i -c=container1'
Mar 24 03:30:31.840: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:30:31.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-555" for this suite.
STEP: Destroying namespace "webhook-555-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

â€¢ [SLOW TEST:7.740 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":217,"skipped":3718,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:30:31.884: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-7513
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar 24 03:30:32.020: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 24 03:31:32.052: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Mar 24 03:31:32.070: INFO: Created pod: pod0-sched-preemption-low-priority
Mar 24 03:31:32.081: INFO: Created pod: pod1-sched-preemption-medium-priority
Mar 24 03:31:32.092: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:31:52.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7513" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

â€¢ [SLOW TEST:80.286 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":218,"skipped":3758,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:31:52.170: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7439
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:31:52.303: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a9e5b62-1e95-4903-84c1-90978122dff1" in namespace "projected-7439" to be "Succeeded or Failed"
Mar 24 03:31:52.306: INFO: Pod "downwardapi-volume-1a9e5b62-1e95-4903-84c1-90978122dff1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.991533ms
Mar 24 03:31:54.311: INFO: Pod "downwardapi-volume-1a9e5b62-1e95-4903-84c1-90978122dff1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008409513s
STEP: Saw pod success
Mar 24 03:31:54.311: INFO: Pod "downwardapi-volume-1a9e5b62-1e95-4903-84c1-90978122dff1" satisfied condition "Succeeded or Failed"
Mar 24 03:31:54.313: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod downwardapi-volume-1a9e5b62-1e95-4903-84c1-90978122dff1 container client-container: <nil>
STEP: delete the pod
Mar 24 03:31:54.331: INFO: Waiting for pod downwardapi-volume-1a9e5b62-1e95-4903-84c1-90978122dff1 to disappear
Mar 24 03:31:54.333: INFO: Pod downwardapi-volume-1a9e5b62-1e95-4903-84c1-90978122dff1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:31:54.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7439" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":219,"skipped":3811,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:31:54.340: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4717
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4717.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4717.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 03:31:56.490: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:31:56.492: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:31:56.497: INFO: Unable to read jessie_udp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:31:56.499: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:31:56.499: INFO: Lookups using dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:32:01.507: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:01.509: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:01.514: INFO: Unable to read jessie_udp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:01.516: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:01.516: INFO: Lookups using dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:32:06.507: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:06.509: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:06.514: INFO: Unable to read jessie_udp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:06.516: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:06.516: INFO: Lookups using dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:32:11.507: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:11.509: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:11.516: INFO: Unable to read jessie_udp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:11.518: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:11.518: INFO: Lookups using dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:32:16.507: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:16.508: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:16.514: INFO: Unable to read jessie_udp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:16.516: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:16.516: INFO: Lookups using dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:32:21.507: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:21.509: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:21.515: INFO: Unable to read jessie_udp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:21.516: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49: the server could not find the requested resource (get pods dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49)
Mar 24 03:32:21.516: INFO: Lookups using dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:32:26.516: INFO: DNS probes using dns-4717/dns-test-89957f19-c9a5-4e44-98c1-d5e485f0ad49 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:32:26.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4717" for this suite.

â€¢ [SLOW TEST:32.199 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":220,"skipped":3872,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:32:26.539: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5499
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:32:26.672: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a034a093-f5a8-497d-9b7e-786fe2e8f8ac" in namespace "projected-5499" to be "Succeeded or Failed"
Mar 24 03:32:26.674: INFO: Pod "downwardapi-volume-a034a093-f5a8-497d-9b7e-786fe2e8f8ac": Phase="Pending", Reason="", readiness=false. Elapsed: 1.994718ms
Mar 24 03:32:28.680: INFO: Pod "downwardapi-volume-a034a093-f5a8-497d-9b7e-786fe2e8f8ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007213505s
STEP: Saw pod success
Mar 24 03:32:28.680: INFO: Pod "downwardapi-volume-a034a093-f5a8-497d-9b7e-786fe2e8f8ac" satisfied condition "Succeeded or Failed"
Mar 24 03:32:28.682: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod downwardapi-volume-a034a093-f5a8-497d-9b7e-786fe2e8f8ac container client-container: <nil>
STEP: delete the pod
Mar 24 03:32:28.696: INFO: Waiting for pod downwardapi-volume-a034a093-f5a8-497d-9b7e-786fe2e8f8ac to disappear
Mar 24 03:32:28.698: INFO: Pod downwardapi-volume-a034a093-f5a8-497d-9b7e-786fe2e8f8ac no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:32:28.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5499" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":221,"skipped":3873,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:32:28.704: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3169
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 24 03:32:28.835: INFO: Waiting up to 5m0s for pod "pod-d14789ac-47b5-4aac-9831-94de0e29a9f6" in namespace "emptydir-3169" to be "Succeeded or Failed"
Mar 24 03:32:28.837: INFO: Pod "pod-d14789ac-47b5-4aac-9831-94de0e29a9f6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.727853ms
Mar 24 03:32:30.840: INFO: Pod "pod-d14789ac-47b5-4aac-9831-94de0e29a9f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005357594s
STEP: Saw pod success
Mar 24 03:32:30.840: INFO: Pod "pod-d14789ac-47b5-4aac-9831-94de0e29a9f6" satisfied condition "Succeeded or Failed"
Mar 24 03:32:30.842: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-d14789ac-47b5-4aac-9831-94de0e29a9f6 container test-container: <nil>
STEP: delete the pod
Mar 24 03:32:30.855: INFO: Waiting for pod pod-d14789ac-47b5-4aac-9831-94de0e29a9f6 to disappear
Mar 24 03:32:30.857: INFO: Pod pod-d14789ac-47b5-4aac-9831-94de0e29a9f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:32:30.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3169" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":222,"skipped":3879,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:32:30.863: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-772
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 24 03:32:31.409: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 03:32:34.427: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:32:34.431: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:32:40.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-772" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:9.678 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":223,"skipped":3893,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:32:40.541: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4933
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-d69f1c00-23fa-482e-b623-3b99999ec342
STEP: Creating a pod to test consume configMaps
Mar 24 03:32:40.677: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-91dc3807-f0c9-4eba-ad33-62112fcdd80d" in namespace "projected-4933" to be "Succeeded or Failed"
Mar 24 03:32:40.679: INFO: Pod "pod-projected-configmaps-91dc3807-f0c9-4eba-ad33-62112fcdd80d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.888464ms
Mar 24 03:32:42.684: INFO: Pod "pod-projected-configmaps-91dc3807-f0c9-4eba-ad33-62112fcdd80d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006893928s
STEP: Saw pod success
Mar 24 03:32:42.684: INFO: Pod "pod-projected-configmaps-91dc3807-f0c9-4eba-ad33-62112fcdd80d" satisfied condition "Succeeded or Failed"
Mar 24 03:32:42.686: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-projected-configmaps-91dc3807-f0c9-4eba-ad33-62112fcdd80d container agnhost-container: <nil>
STEP: delete the pod
Mar 24 03:32:42.701: INFO: Waiting for pod pod-projected-configmaps-91dc3807-f0c9-4eba-ad33-62112fcdd80d to disappear
Mar 24 03:32:42.703: INFO: Pod pod-projected-configmaps-91dc3807-f0c9-4eba-ad33-62112fcdd80d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:32:42.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4933" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":224,"skipped":3906,"failed":0}

------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:32:42.708: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3407
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Mar 24 03:32:42.841: INFO: Waiting up to 5m0s for pod "client-containers-069e53d7-3bf3-4bb5-b894-90b76f8d6074" in namespace "containers-3407" to be "Succeeded or Failed"
Mar 24 03:32:42.842: INFO: Pod "client-containers-069e53d7-3bf3-4bb5-b894-90b76f8d6074": Phase="Pending", Reason="", readiness=false. Elapsed: 1.677753ms
Mar 24 03:32:44.848: INFO: Pod "client-containers-069e53d7-3bf3-4bb5-b894-90b76f8d6074": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007331982s
STEP: Saw pod success
Mar 24 03:32:44.848: INFO: Pod "client-containers-069e53d7-3bf3-4bb5-b894-90b76f8d6074" satisfied condition "Succeeded or Failed"
Mar 24 03:32:44.850: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod client-containers-069e53d7-3bf3-4bb5-b894-90b76f8d6074 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 03:32:44.864: INFO: Waiting for pod client-containers-069e53d7-3bf3-4bb5-b894-90b76f8d6074 to disappear
Mar 24 03:32:44.866: INFO: Pod client-containers-069e53d7-3bf3-4bb5-b894-90b76f8d6074 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:32:44.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3407" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":225,"skipped":3906,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:32:44.872: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6224
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Mar 24 03:32:47.528: INFO: Successfully updated pod "annotationupdate26fe2f6f-7437-453b-b178-120ae3024ed3"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:32:51.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6224" for this suite.

â€¢ [SLOW TEST:6.683 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":226,"skipped":3929,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:32:51.555: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-243
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:32:51.685: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 24 03:32:59.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-243 --namespace=crd-publish-openapi-243 create -f -'
Mar 24 03:32:59.821: INFO: stderr: ""
Mar 24 03:32:59.821: INFO: stdout: "e2e-test-crd-publish-openapi-357-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 24 03:32:59.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-243 --namespace=crd-publish-openapi-243 delete e2e-test-crd-publish-openapi-357-crds test-cr'
Mar 24 03:32:59.910: INFO: stderr: ""
Mar 24 03:32:59.910: INFO: stdout: "e2e-test-crd-publish-openapi-357-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 24 03:32:59.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-243 --namespace=crd-publish-openapi-243 apply -f -'
Mar 24 03:33:00.119: INFO: stderr: ""
Mar 24 03:33:00.119: INFO: stdout: "e2e-test-crd-publish-openapi-357-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 24 03:33:00.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-243 --namespace=crd-publish-openapi-243 delete e2e-test-crd-publish-openapi-357-crds test-cr'
Mar 24 03:33:00.177: INFO: stderr: ""
Mar 24 03:33:00.177: INFO: stdout: "e2e-test-crd-publish-openapi-357-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar 24 03:33:00.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-243 explain e2e-test-crd-publish-openapi-357-crds'
Mar 24 03:33:00.384: INFO: stderr: ""
Mar 24 03:33:00.384: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-357-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:33:03.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-243" for this suite.

â€¢ [SLOW TEST:11.597 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":227,"skipped":3941,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:33:03.152: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6873
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6873 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6873;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6873 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6873;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6873.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6873.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6873.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6873.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6873.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6873.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6873.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6873.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6873.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6873.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6873.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6873.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6873.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 195.74.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.74.195_udp@PTR;check="$$(dig +tcp +noall +answer +search 195.74.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.74.195_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6873 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6873;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6873 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6873;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6873.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6873.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6873.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6873.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6873.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6873.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6873.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6873.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6873.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6873.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6873.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6873.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6873.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 195.74.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.74.195_udp@PTR;check="$$(dig +tcp +noall +answer +search 195.74.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.74.195_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 03:33:05.316: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.318: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.320: INFO: Unable to read wheezy_udp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.322: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.324: INFO: Unable to read wheezy_udp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.325: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.327: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.329: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.335: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.337: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.342: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.344: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.346: INFO: Unable to read jessie_udp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.348: INFO: Unable to read jessie_tcp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.350: INFO: Unable to read jessie_udp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.352: INFO: Unable to read jessie_tcp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.354: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.355: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.361: INFO: Unable to read jessie_udp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.363: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:05.367: INFO: Lookups using dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6873 wheezy_tcp@dns-test-service.dns-6873 wheezy_udp@dns-test-service.dns-6873.svc wheezy_tcp@dns-test-service.dns-6873.svc wheezy_udp@_http._tcp.dns-test-service.dns-6873.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6873.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6873 jessie_tcp@dns-test-service.dns-6873 jessie_udp@dns-test-service.dns-6873.svc jessie_tcp@dns-test-service.dns-6873.svc jessie_udp@_http._tcp.dns-test-service.dns-6873.svc jessie_tcp@_http._tcp.dns-test-service.dns-6873.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:33:10.370: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.372: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.374: INFO: Unable to read wheezy_udp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.376: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.378: INFO: Unable to read wheezy_udp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.380: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.382: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.383: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.389: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.391: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.397: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.399: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.400: INFO: Unable to read jessie_udp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.402: INFO: Unable to read jessie_tcp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.405: INFO: Unable to read jessie_udp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.406: INFO: Unable to read jessie_tcp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.408: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.410: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.416: INFO: Unable to read jessie_udp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.418: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:10.423: INFO: Lookups using dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6873 wheezy_tcp@dns-test-service.dns-6873 wheezy_udp@dns-test-service.dns-6873.svc wheezy_tcp@dns-test-service.dns-6873.svc wheezy_udp@_http._tcp.dns-test-service.dns-6873.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6873.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6873 jessie_tcp@dns-test-service.dns-6873 jessie_udp@dns-test-service.dns-6873.svc jessie_tcp@dns-test-service.dns-6873.svc jessie_udp@_http._tcp.dns-test-service.dns-6873.svc jessie_tcp@_http._tcp.dns-test-service.dns-6873.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:33:15.370: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.372: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.374: INFO: Unable to read wheezy_udp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.376: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.378: INFO: Unable to read wheezy_udp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.380: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.381: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.383: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.389: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.391: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.396: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.398: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.400: INFO: Unable to read jessie_udp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.402: INFO: Unable to read jessie_tcp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.404: INFO: Unable to read jessie_udp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.405: INFO: Unable to read jessie_tcp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.407: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.409: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.414: INFO: Unable to read jessie_udp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.416: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:15.420: INFO: Lookups using dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6873 wheezy_tcp@dns-test-service.dns-6873 wheezy_udp@dns-test-service.dns-6873.svc wheezy_tcp@dns-test-service.dns-6873.svc wheezy_udp@_http._tcp.dns-test-service.dns-6873.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6873.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6873 jessie_tcp@dns-test-service.dns-6873 jessie_udp@dns-test-service.dns-6873.svc jessie_tcp@dns-test-service.dns-6873.svc jessie_udp@_http._tcp.dns-test-service.dns-6873.svc jessie_tcp@_http._tcp.dns-test-service.dns-6873.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:33:20.370: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.372: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.374: INFO: Unable to read wheezy_udp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.376: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.379: INFO: Unable to read wheezy_udp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.380: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.382: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.384: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.390: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.392: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.397: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.399: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.401: INFO: Unable to read jessie_udp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.403: INFO: Unable to read jessie_tcp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.404: INFO: Unable to read jessie_udp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.406: INFO: Unable to read jessie_tcp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.408: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.410: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.416: INFO: Unable to read jessie_udp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.417: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:20.421: INFO: Lookups using dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6873 wheezy_tcp@dns-test-service.dns-6873 wheezy_udp@dns-test-service.dns-6873.svc wheezy_tcp@dns-test-service.dns-6873.svc wheezy_udp@_http._tcp.dns-test-service.dns-6873.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6873.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6873 jessie_tcp@dns-test-service.dns-6873 jessie_udp@dns-test-service.dns-6873.svc jessie_tcp@dns-test-service.dns-6873.svc jessie_udp@_http._tcp.dns-test-service.dns-6873.svc jessie_tcp@_http._tcp.dns-test-service.dns-6873.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:33:25.370: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.372: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.374: INFO: Unable to read wheezy_udp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.376: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.378: INFO: Unable to read wheezy_udp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.380: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.382: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.384: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.389: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.391: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.397: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.398: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.400: INFO: Unable to read jessie_udp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.402: INFO: Unable to read jessie_tcp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.404: INFO: Unable to read jessie_udp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.405: INFO: Unable to read jessie_tcp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.407: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.409: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.415: INFO: Unable to read jessie_udp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.416: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:25.420: INFO: Lookups using dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6873 wheezy_tcp@dns-test-service.dns-6873 wheezy_udp@dns-test-service.dns-6873.svc wheezy_tcp@dns-test-service.dns-6873.svc wheezy_udp@_http._tcp.dns-test-service.dns-6873.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6873.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6873 jessie_tcp@dns-test-service.dns-6873 jessie_udp@dns-test-service.dns-6873.svc jessie_tcp@dns-test-service.dns-6873.svc jessie_udp@_http._tcp.dns-test-service.dns-6873.svc jessie_tcp@_http._tcp.dns-test-service.dns-6873.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:33:30.369: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.372: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.374: INFO: Unable to read wheezy_udp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.376: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.378: INFO: Unable to read wheezy_udp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.380: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.382: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.384: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.389: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.391: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.396: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.398: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.400: INFO: Unable to read jessie_udp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.402: INFO: Unable to read jessie_tcp@dns-test-service.dns-6873 from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.404: INFO: Unable to read jessie_udp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.406: INFO: Unable to read jessie_tcp@dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.407: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.409: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6873.svc from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.415: INFO: Unable to read jessie_udp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.417: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27: the server could not find the requested resource (get pods dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27)
Mar 24 03:33:30.420: INFO: Lookups using dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6873 wheezy_tcp@dns-test-service.dns-6873 wheezy_udp@dns-test-service.dns-6873.svc wheezy_tcp@dns-test-service.dns-6873.svc wheezy_udp@_http._tcp.dns-test-service.dns-6873.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6873.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6873 jessie_tcp@dns-test-service.dns-6873 jessie_udp@dns-test-service.dns-6873.svc jessie_tcp@dns-test-service.dns-6873.svc jessie_udp@_http._tcp.dns-test-service.dns-6873.svc jessie_tcp@_http._tcp.dns-test-service.dns-6873.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:33:35.421: INFO: DNS probes using dns-6873/dns-test-efc28ab8-b835-4bbd-aaf6-dc1fa3920a27 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:33:35.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6873" for this suite.

â€¢ [SLOW TEST:32.320 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":228,"skipped":3947,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:33:35.472: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1869
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1869
I0324 03:33:35.622569      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-1869, replica count: 2
Mar 24 03:33:38.672: INFO: Creating new exec pod
I0324 03:33:38.672790      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 03:33:41.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-1869 exec execpodcr7ns -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 24 03:33:41.828: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 24 03:33:41.828: INFO: stdout: ""
Mar 24 03:33:41.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-1869 exec execpodcr7ns -- /bin/sh -x -c nc -zv -t -w 2 172.16.246.11 80'
Mar 24 03:33:41.970: INFO: stderr: "+ nc -zv -t -w 2 172.16.246.11 80\nConnection to 172.16.246.11 80 port [tcp/http] succeeded!\n"
Mar 24 03:33:41.970: INFO: stdout: ""
Mar 24 03:33:41.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-1869 exec execpodcr7ns -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.15 32728'
Mar 24 03:33:42.113: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.15 32728\nConnection to 192.168.0.15 32728 port [tcp/32728] succeeded!\n"
Mar 24 03:33:42.113: INFO: stdout: ""
Mar 24 03:33:42.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-1869 exec execpodcr7ns -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.13 32728'
Mar 24 03:33:42.256: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.13 32728\nConnection to 192.168.0.13 32728 port [tcp/32728] succeeded!\n"
Mar 24 03:33:42.256: INFO: stdout: ""
Mar 24 03:33:42.256: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:33:42.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1869" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

â€¢ [SLOW TEST:6.812 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":229,"skipped":3991,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:33:42.284: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8689
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:33:42.417: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d29d6f0-b7d7-44f3-8234-1602f73745ec" in namespace "projected-8689" to be "Succeeded or Failed"
Mar 24 03:33:42.421: INFO: Pod "downwardapi-volume-0d29d6f0-b7d7-44f3-8234-1602f73745ec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.94216ms
Mar 24 03:33:44.426: INFO: Pod "downwardapi-volume-0d29d6f0-b7d7-44f3-8234-1602f73745ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009312909s
STEP: Saw pod success
Mar 24 03:33:44.426: INFO: Pod "downwardapi-volume-0d29d6f0-b7d7-44f3-8234-1602f73745ec" satisfied condition "Succeeded or Failed"
Mar 24 03:33:44.428: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod downwardapi-volume-0d29d6f0-b7d7-44f3-8234-1602f73745ec container client-container: <nil>
STEP: delete the pod
Mar 24 03:33:44.448: INFO: Waiting for pod downwardapi-volume-0d29d6f0-b7d7-44f3-8234-1602f73745ec to disappear
Mar 24 03:33:44.450: INFO: Pod downwardapi-volume-0d29d6f0-b7d7-44f3-8234-1602f73745ec no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:33:44.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8689" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":230,"skipped":3995,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:33:44.456: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6906
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-67lj
STEP: Creating a pod to test atomic-volume-subpath
Mar 24 03:33:44.593: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-67lj" in namespace "subpath-6906" to be "Succeeded or Failed"
Mar 24 03:33:44.595: INFO: Pod "pod-subpath-test-configmap-67lj": Phase="Pending", Reason="", readiness=false. Elapsed: 1.751484ms
Mar 24 03:33:46.600: INFO: Pod "pod-subpath-test-configmap-67lj": Phase="Running", Reason="", readiness=true. Elapsed: 2.00667436s
Mar 24 03:33:48.605: INFO: Pod "pod-subpath-test-configmap-67lj": Phase="Running", Reason="", readiness=true. Elapsed: 4.011539377s
Mar 24 03:33:50.610: INFO: Pod "pod-subpath-test-configmap-67lj": Phase="Running", Reason="", readiness=true. Elapsed: 6.016786654s
Mar 24 03:33:52.615: INFO: Pod "pod-subpath-test-configmap-67lj": Phase="Running", Reason="", readiness=true. Elapsed: 8.021764842s
Mar 24 03:33:54.620: INFO: Pod "pod-subpath-test-configmap-67lj": Phase="Running", Reason="", readiness=true. Elapsed: 10.026730718s
Mar 24 03:33:56.625: INFO: Pod "pod-subpath-test-configmap-67lj": Phase="Running", Reason="", readiness=true. Elapsed: 12.031990417s
Mar 24 03:33:58.631: INFO: Pod "pod-subpath-test-configmap-67lj": Phase="Running", Reason="", readiness=true. Elapsed: 14.0370433s
Mar 24 03:34:00.636: INFO: Pod "pod-subpath-test-configmap-67lj": Phase="Running", Reason="", readiness=true. Elapsed: 16.042217725s
Mar 24 03:34:02.641: INFO: Pod "pod-subpath-test-configmap-67lj": Phase="Running", Reason="", readiness=true. Elapsed: 18.047359363s
Mar 24 03:34:04.646: INFO: Pod "pod-subpath-test-configmap-67lj": Phase="Running", Reason="", readiness=true. Elapsed: 20.052656337s
Mar 24 03:34:06.652: INFO: Pod "pod-subpath-test-configmap-67lj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.058469067s
STEP: Saw pod success
Mar 24 03:34:06.652: INFO: Pod "pod-subpath-test-configmap-67lj" satisfied condition "Succeeded or Failed"
Mar 24 03:34:06.654: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod pod-subpath-test-configmap-67lj container test-container-subpath-configmap-67lj: <nil>
STEP: delete the pod
Mar 24 03:34:06.667: INFO: Waiting for pod pod-subpath-test-configmap-67lj to disappear
Mar 24 03:34:06.670: INFO: Pod pod-subpath-test-configmap-67lj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-67lj
Mar 24 03:34:06.670: INFO: Deleting pod "pod-subpath-test-configmap-67lj" in namespace "subpath-6906"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:34:06.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6906" for this suite.

â€¢ [SLOW TEST:22.221 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":231,"skipped":4020,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:34:06.678: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8272
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Mar 24 03:34:06.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-8272 create -f -'
Mar 24 03:34:07.004: INFO: stderr: ""
Mar 24 03:34:07.004: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Mar 24 03:34:07.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-8272 diff -f -'
Mar 24 03:34:07.312: INFO: rc: 1
Mar 24 03:34:07.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-8272 delete -f -'
Mar 24 03:34:07.379: INFO: stderr: ""
Mar 24 03:34:07.379: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:34:07.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8272" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":232,"skipped":4021,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:34:07.395: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6795
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:34:23.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6795" for this suite.

â€¢ [SLOW TEST:16.221 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":233,"skipped":4046,"failed":0}
SSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:34:23.616: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-3269
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Mar 24 03:34:23.746: INFO: Major version: 1
STEP: Confirm minor version
Mar 24 03:34:23.746: INFO: cleanMinorVersion: 20
Mar 24 03:34:23.746: INFO: Minor version: 20+
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:34:23.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-3269" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":234,"skipped":4049,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:34:23.753: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9080
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0324 03:34:33.916943      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 24 03:35:35.932: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:35:35.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9080" for this suite.

â€¢ [SLOW TEST:72.194 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":235,"skipped":4057,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:35:35.947: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-4042
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 24 03:35:36.097: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar 24 03:35:36.099: INFO: starting watch
STEP: patching
STEP: updating
Mar 24 03:35:36.108: INFO: waiting for watch events with expected annotations
Mar 24 03:35:36.108: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:35:36.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-4042" for this suite.
â€¢{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":236,"skipped":4076,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:35:36.150: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3411
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:35:36.276: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:35:38.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3411" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":237,"skipped":4104,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:35:38.317: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-555
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Mar 24 03:35:40.458: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-555 PodName:pod-sharedvolume-ab3537c6-c39c-4e8b-9cba-551c96fccdc4 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:35:40.459: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 03:35:40.552: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:35:40.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-555" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":238,"skipped":4110,"failed":0}

------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:35:40.561: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2832
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:35:40.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2832" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
â€¢{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":239,"skipped":4110,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:35:40.720: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Mar 24 03:35:40.844: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:35:43.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7736" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":240,"skipped":4120,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:35:43.878: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7004
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:35:44.010: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 24 03:35:49.019: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 24 03:35:49.019: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar 24 03:35:49.032: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7004  47aa26ee-155b-4132-a1c7-c668ca44c763 807564 1 2021-03-24 03:35:49 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-03-24 03:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004562cd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar 24 03:35:49.035: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Mar 24 03:35:49.035: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar 24 03:35:49.035: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7004  ea327de9-5fed-418d-9ab5-81c31ea29f7d 807565 1 2021-03-24 03:35:44 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 47aa26ee-155b-4132-a1c7-c668ca44c763 0xc004562fe7 0xc004562fe8}] []  [{e2e.test Update apps/v1 2021-03-24 03:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-24 03:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"47aa26ee-155b-4132-a1c7-c668ca44c763\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004563088 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 24 03:35:49.039: INFO: Pod "test-cleanup-controller-75xv8" is available:
&Pod{ObjectMeta:{test-cleanup-controller-75xv8 test-cleanup-controller- deployment-7004  983eddff-9d02-47aa-ab4b-dc714f601201 807506 0 2021-03-24 03:35:44 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[kubernetes.io/psp:ack.privileged] [{apps/v1 ReplicaSet test-cleanup-controller ea327de9-5fed-418d-9ab5-81c31ea29f7d 0xc004563357 0xc004563358}] []  [{kube-controller-manager Update v1 2021-03-24 03:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea327de9-5fed-418d-9ab5-81c31ea29f7d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-24 03:35:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.43.1.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rg78d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rg78d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rg78d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cn-hongkong.192.168.0.15,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:35:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:35:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:35:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-24 03:35:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.15,PodIP:10.43.1.3,StartTime:2021-03-24 03:35:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-24 03:35:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://74ccda07e8e11941fe1089fc5b170f244b9135a106e033bb70b1c5a8c38b9361,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.43.1.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:35:49.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7004" for this suite.

â€¢ [SLOW TEST:5.168 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":241,"skipped":4147,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:35:49.047: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2316
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:35:49.180: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe6f2a8d-cd9f-48f4-a242-d759d4c12272" in namespace "downward-api-2316" to be "Succeeded or Failed"
Mar 24 03:35:49.181: INFO: Pod "downwardapi-volume-fe6f2a8d-cd9f-48f4-a242-d759d4c12272": Phase="Pending", Reason="", readiness=false. Elapsed: 1.839813ms
Mar 24 03:35:51.184: INFO: Pod "downwardapi-volume-fe6f2a8d-cd9f-48f4-a242-d759d4c12272": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004691175s
STEP: Saw pod success
Mar 24 03:35:51.184: INFO: Pod "downwardapi-volume-fe6f2a8d-cd9f-48f4-a242-d759d4c12272" satisfied condition "Succeeded or Failed"
Mar 24 03:35:51.186: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod downwardapi-volume-fe6f2a8d-cd9f-48f4-a242-d759d4c12272 container client-container: <nil>
STEP: delete the pod
Mar 24 03:35:51.204: INFO: Waiting for pod downwardapi-volume-fe6f2a8d-cd9f-48f4-a242-d759d4c12272 to disappear
Mar 24 03:35:51.205: INFO: Pod downwardapi-volume-fe6f2a8d-cd9f-48f4-a242-d759d4c12272 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:35:51.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2316" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":242,"skipped":4166,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:35:51.212: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar 24 03:35:51.371: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8694  cc848170-739c-4a9a-a16e-ae0cf210e042 807629 0 2021-03-24 03:35:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-24 03:35:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 03:35:51.371: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8694  cc848170-739c-4a9a-a16e-ae0cf210e042 807630 0 2021-03-24 03:35:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-24 03:35:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 24 03:35:51.384: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8694  cc848170-739c-4a9a-a16e-ae0cf210e042 807631 0 2021-03-24 03:35:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-24 03:35:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 24 03:35:51.384: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8694  cc848170-739c-4a9a-a16e-ae0cf210e042 807632 0 2021-03-24 03:35:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-24 03:35:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:35:51.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8694" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":243,"skipped":4197,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:35:51.390: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1024
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1024
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Mar 24 03:35:51.525: INFO: Found 0 stateful pods, waiting for 3
Mar 24 03:36:01.529: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 03:36:01.529: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 03:36:01.529: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 24 03:36:01.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-1024 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 03:36:01.678: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 03:36:01.678: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 03:36:01.678: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 24 03:36:11.706: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar 24 03:36:21.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-1024 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 03:36:21.858: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 24 03:36:21.858: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 03:36:21.858: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Mar 24 03:36:41.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-1024 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 24 03:36:42.014: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 24 03:36:42.014: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 24 03:36:42.014: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 24 03:36:52.041: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar 24 03:37:02.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=statefulset-1024 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 24 03:37:02.194: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 24 03:37:02.194: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 24 03:37:02.194: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 24 03:37:12.209: INFO: Waiting for StatefulSet statefulset-1024/ss2 to complete update
Mar 24 03:37:12.209: INFO: Waiting for Pod statefulset-1024/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 24 03:37:22.215: INFO: Deleting all statefulset in ns statefulset-1024
Mar 24 03:37:22.217: INFO: Scaling statefulset ss2 to 0
Mar 24 03:37:52.231: INFO: Waiting for statefulset status.replicas updated to 0
Mar 24 03:37:52.233: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:37:52.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1024" for this suite.

â€¢ [SLOW TEST:120.858 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":244,"skipped":4207,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:37:52.248: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6870
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 03:37:52.861: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 24 03:37:54.870: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153872, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153872, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153872, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153872, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 03:37:57.886: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:37:57.890: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-644-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:38:04.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6870" for this suite.
STEP: Destroying namespace "webhook-6870-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

â€¢ [SLOW TEST:11.805 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":245,"skipped":4210,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:38:04.054: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-96e7e4ce-35a6-4c48-8840-516ca8403f85
STEP: Creating a pod to test consume secrets
Mar 24 03:38:04.191: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4add2724-f0b8-4313-84d4-ba74bae71042" in namespace "projected-6381" to be "Succeeded or Failed"
Mar 24 03:38:04.193: INFO: Pod "pod-projected-secrets-4add2724-f0b8-4313-84d4-ba74bae71042": Phase="Pending", Reason="", readiness=false. Elapsed: 1.894438ms
Mar 24 03:38:06.199: INFO: Pod "pod-projected-secrets-4add2724-f0b8-4313-84d4-ba74bae71042": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007319816s
STEP: Saw pod success
Mar 24 03:38:06.199: INFO: Pod "pod-projected-secrets-4add2724-f0b8-4313-84d4-ba74bae71042" satisfied condition "Succeeded or Failed"
Mar 24 03:38:06.201: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod pod-projected-secrets-4add2724-f0b8-4313-84d4-ba74bae71042 container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 03:38:06.219: INFO: Waiting for pod pod-projected-secrets-4add2724-f0b8-4313-84d4-ba74bae71042 to disappear
Mar 24 03:38:06.220: INFO: Pod pod-projected-secrets-4add2724-f0b8-4313-84d4-ba74bae71042 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:38:06.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6381" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":246,"skipped":4231,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:38:06.226: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3458
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 03:38:06.587: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 24 03:38:08.596: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153886, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153886, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153886, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153886, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 03:38:11.608: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:38:11.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3458" for this suite.
STEP: Destroying namespace "webhook-3458-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

â€¢ [SLOW TEST:5.475 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":247,"skipped":4248,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:38:11.701: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4034
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4034
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4034
I0324 03:38:11.851210      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4034, replica count: 2
I0324 03:38:14.901484      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 03:38:14.901: INFO: Creating new exec pod
Mar 24 03:38:17.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-4034 exec execpodxg6jc -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 24 03:38:18.061: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 24 03:38:18.061: INFO: stdout: ""
Mar 24 03:38:18.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-4034 exec execpodxg6jc -- /bin/sh -x -c nc -zv -t -w 2 172.16.214.4 80'
Mar 24 03:38:18.201: INFO: stderr: "+ nc -zv -t -w 2 172.16.214.4 80\nConnection to 172.16.214.4 80 port [tcp/http] succeeded!\n"
Mar 24 03:38:18.201: INFO: stdout: ""
Mar 24 03:38:18.201: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:38:18.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4034" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

â€¢ [SLOW TEST:6.521 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":248,"skipped":4251,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:38:18.222: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5666
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 03:38:18.731: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 24 03:38:20.740: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153898, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153898, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153898, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153898, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 03:38:23.752: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:38:23.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5666" for this suite.
STEP: Destroying namespace "webhook-5666-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

â€¢ [SLOW TEST:5.697 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":249,"skipped":4255,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:38:23.919: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7455
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 24 03:38:24.642: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 24 03:38:26.652: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153904, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153904, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153904, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752153904, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 03:38:29.666: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:38:41.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7455" for this suite.
STEP: Destroying namespace "webhook-7455-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

â€¢ [SLOW TEST:17.879 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":250,"skipped":4258,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:38:41.798: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-1763
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Mar 24 03:38:41.936: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Mar 24 03:38:41.949: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:38:41.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1763" for this suite.
â€¢{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":251,"skipped":4272,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:38:41.973: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8388
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:38:42.107: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec95994c-243b-48eb-8490-de8f54d7775e" in namespace "downward-api-8388" to be "Succeeded or Failed"
Mar 24 03:38:42.109: INFO: Pod "downwardapi-volume-ec95994c-243b-48eb-8490-de8f54d7775e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.869465ms
Mar 24 03:38:44.115: INFO: Pod "downwardapi-volume-ec95994c-243b-48eb-8490-de8f54d7775e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007262183s
STEP: Saw pod success
Mar 24 03:38:44.115: INFO: Pod "downwardapi-volume-ec95994c-243b-48eb-8490-de8f54d7775e" satisfied condition "Succeeded or Failed"
Mar 24 03:38:44.117: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod downwardapi-volume-ec95994c-243b-48eb-8490-de8f54d7775e container client-container: <nil>
STEP: delete the pod
Mar 24 03:38:44.131: INFO: Waiting for pod downwardapi-volume-ec95994c-243b-48eb-8490-de8f54d7775e to disappear
Mar 24 03:38:44.132: INFO: Pod downwardapi-volume-ec95994c-243b-48eb-8490-de8f54d7775e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:38:44.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8388" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":252,"skipped":4277,"failed":0}
SS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:38:44.138: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5703
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-c1500565-b9fb-457e-b55e-ab938c434083
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:38:44.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5703" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":253,"skipped":4279,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:38:44.273: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-3057
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar 24 03:38:44.677: INFO: Pod name wrapped-volume-race-c0089fe4-de25-4d87-8d74-ea55ec3f787a: Found 3 pods out of 5
Mar 24 03:38:49.690: INFO: Pod name wrapped-volume-race-c0089fe4-de25-4d87-8d74-ea55ec3f787a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c0089fe4-de25-4d87-8d74-ea55ec3f787a in namespace emptydir-wrapper-3057, will wait for the garbage collector to delete the pods
Mar 24 03:38:59.766: INFO: Deleting ReplicationController wrapped-volume-race-c0089fe4-de25-4d87-8d74-ea55ec3f787a took: 8.285555ms
Mar 24 03:38:59.866: INFO: Terminating ReplicationController wrapped-volume-race-c0089fe4-de25-4d87-8d74-ea55ec3f787a pods took: 100.145447ms
STEP: Creating RC which spawns configmap-volume pods
Mar 24 03:39:03.381: INFO: Pod name wrapped-volume-race-6eb9d393-246c-4fcc-bb34-448cc87e9288: Found 0 pods out of 5
Mar 24 03:39:08.392: INFO: Pod name wrapped-volume-race-6eb9d393-246c-4fcc-bb34-448cc87e9288: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6eb9d393-246c-4fcc-bb34-448cc87e9288 in namespace emptydir-wrapper-3057, will wait for the garbage collector to delete the pods
Mar 24 03:39:18.466: INFO: Deleting ReplicationController wrapped-volume-race-6eb9d393-246c-4fcc-bb34-448cc87e9288 took: 5.893389ms
Mar 24 03:39:19.166: INFO: Terminating ReplicationController wrapped-volume-race-6eb9d393-246c-4fcc-bb34-448cc87e9288 pods took: 700.142239ms
STEP: Creating RC which spawns configmap-volume pods
Mar 24 03:39:32.582: INFO: Pod name wrapped-volume-race-878038dc-fc78-426f-b56f-bd58d484d696: Found 0 pods out of 5
Mar 24 03:39:37.592: INFO: Pod name wrapped-volume-race-878038dc-fc78-426f-b56f-bd58d484d696: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-878038dc-fc78-426f-b56f-bd58d484d696 in namespace emptydir-wrapper-3057, will wait for the garbage collector to delete the pods
Mar 24 03:39:47.665: INFO: Deleting ReplicationController wrapped-volume-race-878038dc-fc78-426f-b56f-bd58d484d696 took: 5.91845ms
Mar 24 03:39:48.365: INFO: Terminating ReplicationController wrapped-volume-race-878038dc-fc78-426f-b56f-bd58d484d696 pods took: 700.129091ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:40:02.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3057" for this suite.

â€¢ [SLOW TEST:77.998 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":254,"skipped":4292,"failed":0}
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:40:02.272: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7334
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-c78db584-b272-4a22-933a-8c85ef410d4a
Mar 24 03:40:02.403: INFO: Pod name my-hostname-basic-c78db584-b272-4a22-933a-8c85ef410d4a: Found 0 pods out of 1
Mar 24 03:40:07.412: INFO: Pod name my-hostname-basic-c78db584-b272-4a22-933a-8c85ef410d4a: Found 1 pods out of 1
Mar 24 03:40:07.412: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c78db584-b272-4a22-933a-8c85ef410d4a" are running
Mar 24 03:40:07.414: INFO: Pod "my-hostname-basic-c78db584-b272-4a22-933a-8c85ef410d4a-lssx8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-24 03:40:02 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-24 03:40:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-24 03:40:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-24 03:40:02 +0000 UTC Reason: Message:}])
Mar 24 03:40:07.414: INFO: Trying to dial the pod
Mar 24 03:40:12.423: INFO: Controller my-hostname-basic-c78db584-b272-4a22-933a-8c85ef410d4a: Got expected result from replica 1 [my-hostname-basic-c78db584-b272-4a22-933a-8c85ef410d4a-lssx8]: "my-hostname-basic-c78db584-b272-4a22-933a-8c85ef410d4a-lssx8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:40:12.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7334" for this suite.

â€¢ [SLOW TEST:10.157 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":255,"skipped":4292,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:40:12.429: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2691
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-09fa2474-b3e6-4922-a2ec-e677d428745e
STEP: Creating a pod to test consume secrets
Mar 24 03:40:12.566: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1f643f36-a415-43cc-8312-360b9bc7db41" in namespace "projected-2691" to be "Succeeded or Failed"
Mar 24 03:40:12.570: INFO: Pod "pod-projected-secrets-1f643f36-a415-43cc-8312-360b9bc7db41": Phase="Pending", Reason="", readiness=false. Elapsed: 3.963379ms
Mar 24 03:40:14.574: INFO: Pod "pod-projected-secrets-1f643f36-a415-43cc-8312-360b9bc7db41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00855224s
STEP: Saw pod success
Mar 24 03:40:14.574: INFO: Pod "pod-projected-secrets-1f643f36-a415-43cc-8312-360b9bc7db41" satisfied condition "Succeeded or Failed"
Mar 24 03:40:14.578: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-projected-secrets-1f643f36-a415-43cc-8312-360b9bc7db41 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 24 03:40:14.597: INFO: Waiting for pod pod-projected-secrets-1f643f36-a415-43cc-8312-360b9bc7db41 to disappear
Mar 24 03:40:14.599: INFO: Pod pod-projected-secrets-1f643f36-a415-43cc-8312-360b9bc7db41 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:40:14.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2691" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":256,"skipped":4293,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:40:14.605: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4206
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Mar 24 03:40:14.738: INFO: Waiting up to 5m0s for pod "var-expansion-f0b18065-1196-4c84-a7ea-107ba3829fbb" in namespace "var-expansion-4206" to be "Succeeded or Failed"
Mar 24 03:40:14.740: INFO: Pod "var-expansion-f0b18065-1196-4c84-a7ea-107ba3829fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.681812ms
Mar 24 03:40:16.745: INFO: Pod "var-expansion-f0b18065-1196-4c84-a7ea-107ba3829fbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006924422s
STEP: Saw pod success
Mar 24 03:40:16.745: INFO: Pod "var-expansion-f0b18065-1196-4c84-a7ea-107ba3829fbb" satisfied condition "Succeeded or Failed"
Mar 24 03:40:16.747: INFO: Trying to get logs from node cn-hongkong.192.168.0.15 pod var-expansion-f0b18065-1196-4c84-a7ea-107ba3829fbb container dapi-container: <nil>
STEP: delete the pod
Mar 24 03:40:16.769: INFO: Waiting for pod var-expansion-f0b18065-1196-4c84-a7ea-107ba3829fbb to disappear
Mar 24 03:40:16.770: INFO: Pod var-expansion-f0b18065-1196-4c84-a7ea-107ba3829fbb no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:40:16.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4206" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":257,"skipped":4305,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:40:16.777: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9246
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar 24 03:40:16.907: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 24 03:40:16.912: INFO: Waiting for terminating namespaces to be deleted...
Mar 24 03:40:16.914: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.13 before test
Mar 24 03:40:16.919: INFO: coredns-58d46886cf-fl2z7 from kube-system started at 2021-03-23 11:44:32 +0000 UTC (1 container statuses recorded)
Mar 24 03:40:16.919: INFO: 	Container coredns ready: true, restart count 0
Mar 24 03:40:16.919: INFO: csi-plugin-78sns from kube-system started at 2021-03-22 03:28:49 +0000 UTC (4 container statuses recorded)
Mar 24 03:40:16.919: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 24 03:40:16.919: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 24 03:40:16.919: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 24 03:40:16.919: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 24 03:40:16.919: INFO: csi-provisioner-57c8d966fb-spnmk from kube-system started at 2021-03-22 03:29:32 +0000 UTC (7 container statuses recorded)
Mar 24 03:40:16.919: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 24 03:40:16.919: INFO: 	Container external-csi-snapshotter ready: true, restart count 0
Mar 24 03:40:16.919: INFO: 	Container external-disk-attacher ready: true, restart count 0
Mar 24 03:40:16.919: INFO: 	Container external-disk-provisioner ready: true, restart count 0
Mar 24 03:40:16.919: INFO: 	Container external-disk-resizer ready: true, restart count 0
Mar 24 03:40:16.919: INFO: 	Container external-nas-provisioner ready: true, restart count 0
Mar 24 03:40:16.919: INFO: 	Container external-snapshot-controller ready: true, restart count 0
Mar 24 03:40:16.919: INFO: kube-flannel-ds-77s6x from kube-system started at 2021-03-22 08:51:32 +0000 UTC (1 container statuses recorded)
Mar 24 03:40:16.919: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 03:40:16.919: INFO: kube-proxy-worker-kfc4c from kube-system started at 2021-03-22 03:28:49 +0000 UTC (1 container statuses recorded)
Mar 24 03:40:16.919: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 24 03:40:16.919: INFO: metrics-server-7d6f974b9f-tglpj from kube-system started at 2021-03-22 03:29:32 +0000 UTC (1 container statuses recorded)
Mar 24 03:40:16.919: INFO: 	Container metrics-server ready: true, restart count 2
Mar 24 03:40:16.919: INFO: nginx-ingress-controller-67bc64c7-5tcjs from kube-system started at 2021-03-23 11:44:32 +0000 UTC (1 container statuses recorded)
Mar 24 03:40:16.919: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 24 03:40:16.919: INFO: sonobuoy-systemd-logs-daemon-set-46debedd236a484a-5dl8c from sonobuoy started at 2021-03-24 02:26:37 +0000 UTC (2 container statuses recorded)
Mar 24 03:40:16.919: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Mar 24 03:40:16.919: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 24 03:40:16.919: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.14 before test
Mar 24 03:40:16.925: INFO: csi-plugin-sdgx9 from kube-system started at 2021-03-22 03:28:49 +0000 UTC (4 container statuses recorded)
Mar 24 03:40:16.926: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 24 03:40:16.926: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 24 03:40:16.926: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 24 03:40:16.926: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 24 03:40:16.926: INFO: kube-flannel-ds-bqvbv from kube-system started at 2021-03-22 08:51:32 +0000 UTC (1 container statuses recorded)
Mar 24 03:40:16.926: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 03:40:16.926: INFO: kube-proxy-worker-46686 from kube-system started at 2021-03-22 03:28:49 +0000 UTC (1 container statuses recorded)
Mar 24 03:40:16.926: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 24 03:40:16.926: INFO: my-hostname-basic-c78db584-b272-4a22-933a-8c85ef410d4a-lssx8 from replication-controller-7334 started at 2021-03-24 03:40:02 +0000 UTC (1 container statuses recorded)
Mar 24 03:40:16.926: INFO: 	Container my-hostname-basic-c78db584-b272-4a22-933a-8c85ef410d4a ready: true, restart count 0
Mar 24 03:40:16.926: INFO: sonobuoy from sonobuoy started at 2021-03-24 02:26:36 +0000 UTC (1 container statuses recorded)
Mar 24 03:40:16.926: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 24 03:40:16.926: INFO: sonobuoy-systemd-logs-daemon-set-46debedd236a484a-l98ph from sonobuoy started at 2021-03-24 02:26:37 +0000 UTC (2 container statuses recorded)
Mar 24 03:40:16.926: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Mar 24 03:40:16.926: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 24 03:40:16.926: INFO: 
Logging pods the apiserver thinks is on node cn-hongkong.192.168.0.15 before test
Mar 24 03:40:16.931: INFO: coredns-58d46886cf-h7vgt from kube-system started at 2021-03-22 03:29:29 +0000 UTC (1 container statuses recorded)
Mar 24 03:40:16.931: INFO: 	Container coredns ready: true, restart count 23
Mar 24 03:40:16.931: INFO: csi-plugin-v7dd9 from kube-system started at 2021-03-22 03:28:49 +0000 UTC (4 container statuses recorded)
Mar 24 03:40:16.931: INFO: 	Container csi-plugin ready: true, restart count 0
Mar 24 03:40:16.931: INFO: 	Container disk-driver-registrar ready: true, restart count 0
Mar 24 03:40:16.931: INFO: 	Container nas-driver-registrar ready: true, restart count 0
Mar 24 03:40:16.931: INFO: 	Container oss-driver-registrar ready: true, restart count 0
Mar 24 03:40:16.931: INFO: kube-flannel-ds-bswlq from kube-system started at 2021-03-22 08:51:32 +0000 UTC (1 container statuses recorded)
Mar 24 03:40:16.931: INFO: 	Container kube-flannel ready: true, restart count 0
Mar 24 03:40:16.931: INFO: kube-proxy-worker-xvh5g from kube-system started at 2021-03-22 03:28:49 +0000 UTC (1 container statuses recorded)
Mar 24 03:40:16.931: INFO: 	Container kube-proxy-worker ready: true, restart count 0
Mar 24 03:40:16.931: INFO: nginx-ingress-controller-67bc64c7-zm4r9 from kube-system started at 2021-03-22 11:02:12 +0000 UTC (1 container statuses recorded)
Mar 24 03:40:16.931: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Mar 24 03:40:16.931: INFO: sonobuoy-systemd-logs-daemon-set-46debedd236a484a-sb56k from sonobuoy started at 2021-03-24 02:26:37 +0000 UTC (2 container statuses recorded)
Mar 24 03:40:16.931: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Mar 24 03:40:16.931: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ad326c19-7baf-4d23-b728-eb1b3dc722d8 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 192.168.0.14 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 192.168.0.14 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar 24 03:40:27.019: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.0.14 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:40:27.019: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.14, port: 54321
Mar 24 03:40:27.109: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.0.14:54321/hostname] Namespace:sched-pred-9246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:40:27.110: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.14, port: 54321 UDP
Mar 24 03:40:27.221: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.0.14 54321] Namespace:sched-pred-9246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:40:27.221: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar 24 03:40:32.323: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.0.14 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:40:32.323: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.14, port: 54321
Mar 24 03:40:32.433: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.0.14:54321/hostname] Namespace:sched-pred-9246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:40:32.433: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.14, port: 54321 UDP
Mar 24 03:40:32.549: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.0.14 54321] Namespace:sched-pred-9246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:40:32.549: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar 24 03:40:37.677: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.0.14 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:40:37.677: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.14, port: 54321
Mar 24 03:40:37.784: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.0.14:54321/hostname] Namespace:sched-pred-9246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:40:37.784: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.14, port: 54321 UDP
Mar 24 03:40:37.905: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.0.14 54321] Namespace:sched-pred-9246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:40:37.905: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar 24 03:40:43.011: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.0.14 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:40:43.011: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.14, port: 54321
Mar 24 03:40:43.125: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.0.14:54321/hostname] Namespace:sched-pred-9246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:40:43.125: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.14, port: 54321 UDP
Mar 24 03:40:43.229: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.0.14 54321] Namespace:sched-pred-9246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:40:43.229: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar 24 03:40:48.332: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.0.14 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:40:48.332: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.14, port: 54321
Mar 24 03:40:48.442: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.0.14:54321/hostname] Namespace:sched-pred-9246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:40:48.442: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.14, port: 54321 UDP
Mar 24 03:40:48.567: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.0.14 54321] Namespace:sched-pred-9246 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:40:48.567: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: removing the label kubernetes.io/e2e-ad326c19-7baf-4d23-b728-eb1b3dc722d8 off the node cn-hongkong.192.168.0.14
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ad326c19-7baf-4d23-b728-eb1b3dc722d8
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:40:53.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9246" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

â€¢ [SLOW TEST:36.918 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":258,"skipped":4325,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:40:53.695: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-7293
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 24 03:40:54.067: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 24 03:40:57.083: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:40:57.088: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:41:03.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7293" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:9.551 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":259,"skipped":4353,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:41:03.247: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7626
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Mar 24 03:41:03.388: INFO: Waiting up to 5m0s for pod "var-expansion-ac3ae43e-fa58-45d5-8ff7-a118370c40dd" in namespace "var-expansion-7626" to be "Succeeded or Failed"
Mar 24 03:41:03.390: INFO: Pod "var-expansion-ac3ae43e-fa58-45d5-8ff7-a118370c40dd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.685142ms
Mar 24 03:41:05.396: INFO: Pod "var-expansion-ac3ae43e-fa58-45d5-8ff7-a118370c40dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007760359s
STEP: Saw pod success
Mar 24 03:41:05.396: INFO: Pod "var-expansion-ac3ae43e-fa58-45d5-8ff7-a118370c40dd" satisfied condition "Succeeded or Failed"
Mar 24 03:41:05.398: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod var-expansion-ac3ae43e-fa58-45d5-8ff7-a118370c40dd container dapi-container: <nil>
STEP: delete the pod
Mar 24 03:41:05.416: INFO: Waiting for pod var-expansion-ac3ae43e-fa58-45d5-8ff7-a118370c40dd to disappear
Mar 24 03:41:05.418: INFO: Pod var-expansion-ac3ae43e-fa58-45d5-8ff7-a118370c40dd no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:41:05.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7626" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":260,"skipped":4395,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:41:05.424: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-8846
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Mar 24 03:41:05.566: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:41:05.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8846" for this suite.
â€¢{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":261,"skipped":4427,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:41:05.586: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8999
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:41:05.710: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 24 03:41:12.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-8999 --namespace=crd-publish-openapi-8999 create -f -'
Mar 24 03:41:12.843: INFO: stderr: ""
Mar 24 03:41:12.843: INFO: stdout: "e2e-test-crd-publish-openapi-2079-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 24 03:41:12.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-8999 --namespace=crd-publish-openapi-8999 delete e2e-test-crd-publish-openapi-2079-crds test-cr'
Mar 24 03:41:12.926: INFO: stderr: ""
Mar 24 03:41:12.926: INFO: stdout: "e2e-test-crd-publish-openapi-2079-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 24 03:41:12.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-8999 --namespace=crd-publish-openapi-8999 apply -f -'
Mar 24 03:41:13.130: INFO: stderr: ""
Mar 24 03:41:13.130: INFO: stdout: "e2e-test-crd-publish-openapi-2079-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 24 03:41:13.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-8999 --namespace=crd-publish-openapi-8999 delete e2e-test-crd-publish-openapi-2079-crds test-cr'
Mar 24 03:41:13.186: INFO: stderr: ""
Mar 24 03:41:13.186: INFO: stdout: "e2e-test-crd-publish-openapi-2079-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 24 03:41:13.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-8999 explain e2e-test-crd-publish-openapi-2079-crds'
Mar 24 03:41:13.375: INFO: stderr: ""
Mar 24 03:41:13.375: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2079-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:41:15.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8999" for this suite.

â€¢ [SLOW TEST:9.568 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":262,"skipped":4442,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:41:15.154: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2661
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:41:15.299: INFO: Create a RollingUpdate DaemonSet
Mar 24 03:41:15.303: INFO: Check that daemon pods launch on every node of the cluster
Mar 24 03:41:15.305: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:41:15.305: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:41:15.305: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:41:15.308: INFO: Number of nodes with available pods: 0
Mar 24 03:41:15.308: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:41:16.313: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:41:16.313: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:41:16.313: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:41:16.316: INFO: Number of nodes with available pods: 1
Mar 24 03:41:16.316: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:41:17.312: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:41:17.312: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:41:17.312: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:41:17.314: INFO: Number of nodes with available pods: 3
Mar 24 03:41:17.314: INFO: Number of running nodes: 3, number of available pods: 3
Mar 24 03:41:17.314: INFO: Update the DaemonSet to trigger a rollout
Mar 24 03:41:17.319: INFO: Updating DaemonSet daemon-set
Mar 24 03:41:21.331: INFO: Roll back the DaemonSet before rollout is complete
Mar 24 03:41:21.338: INFO: Updating DaemonSet daemon-set
Mar 24 03:41:21.338: INFO: Make sure DaemonSet rollback is complete
Mar 24 03:41:21.340: INFO: Wrong image for pod: daemon-set-nltv6. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 24 03:41:21.340: INFO: Pod daemon-set-nltv6 is not available
Mar 24 03:41:21.344: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:41:21.344: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:41:21.344: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:41:22.347: INFO: Wrong image for pod: daemon-set-nltv6. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 24 03:41:22.347: INFO: Pod daemon-set-nltv6 is not available
Mar 24 03:41:22.349: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:41:22.349: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:41:22.349: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:41:23.349: INFO: Pod daemon-set-t2snl is not available
Mar 24 03:41:23.351: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:41:23.351: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:41:23.351: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2661, will wait for the garbage collector to delete the pods
Mar 24 03:41:23.413: INFO: Deleting DaemonSet.extensions daemon-set took: 6.020463ms
Mar 24 03:41:23.513: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.117366ms
Mar 24 03:41:32.127: INFO: Number of nodes with available pods: 0
Mar 24 03:41:32.127: INFO: Number of running nodes: 0, number of available pods: 0
Mar 24 03:41:32.129: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"811248"},"items":null}

Mar 24 03:41:32.131: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"811248"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:41:32.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2661" for this suite.

â€¢ [SLOW TEST:16.996 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":263,"skipped":4501,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:41:32.150: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6317
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:41:32.282: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-c3e0ba29-bfb9-49c5-afbd-1f506a3127c3" in namespace "security-context-test-6317" to be "Succeeded or Failed"
Mar 24 03:41:32.284: INFO: Pod "busybox-readonly-false-c3e0ba29-bfb9-49c5-afbd-1f506a3127c3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.705774ms
Mar 24 03:41:34.289: INFO: Pod "busybox-readonly-false-c3e0ba29-bfb9-49c5-afbd-1f506a3127c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007189717s
Mar 24 03:41:34.289: INFO: Pod "busybox-readonly-false-c3e0ba29-bfb9-49c5-afbd-1f506a3127c3" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:41:34.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6317" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":264,"skipped":4519,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:41:34.297: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5253
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-5253/secret-test-9b52580a-e45a-42b0-8799-f882fa22733a
STEP: Creating a pod to test consume secrets
Mar 24 03:41:34.433: INFO: Waiting up to 5m0s for pod "pod-configmaps-93fc4adb-35b7-4ebe-ab95-6fdc2033bc46" in namespace "secrets-5253" to be "Succeeded or Failed"
Mar 24 03:41:34.435: INFO: Pod "pod-configmaps-93fc4adb-35b7-4ebe-ab95-6fdc2033bc46": Phase="Pending", Reason="", readiness=false. Elapsed: 1.682494ms
Mar 24 03:41:36.441: INFO: Pod "pod-configmaps-93fc4adb-35b7-4ebe-ab95-6fdc2033bc46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00760735s
STEP: Saw pod success
Mar 24 03:41:36.441: INFO: Pod "pod-configmaps-93fc4adb-35b7-4ebe-ab95-6fdc2033bc46" satisfied condition "Succeeded or Failed"
Mar 24 03:41:36.443: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-configmaps-93fc4adb-35b7-4ebe-ab95-6fdc2033bc46 container env-test: <nil>
STEP: delete the pod
Mar 24 03:41:36.458: INFO: Waiting for pod pod-configmaps-93fc4adb-35b7-4ebe-ab95-6fdc2033bc46 to disappear
Mar 24 03:41:36.459: INFO: Pod pod-configmaps-93fc4adb-35b7-4ebe-ab95-6fdc2033bc46 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:41:36.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5253" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":265,"skipped":4531,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:41:36.465: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9413
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar 24 03:41:36.596: INFO: Waiting up to 5m0s for pod "pod-601d21d0-60e5-4ae9-ba9b-a6bd8179f288" in namespace "emptydir-9413" to be "Succeeded or Failed"
Mar 24 03:41:36.598: INFO: Pod "pod-601d21d0-60e5-4ae9-ba9b-a6bd8179f288": Phase="Pending", Reason="", readiness=false. Elapsed: 1.798981ms
Mar 24 03:41:38.603: INFO: Pod "pod-601d21d0-60e5-4ae9-ba9b-a6bd8179f288": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006855804s
STEP: Saw pod success
Mar 24 03:41:38.603: INFO: Pod "pod-601d21d0-60e5-4ae9-ba9b-a6bd8179f288" satisfied condition "Succeeded or Failed"
Mar 24 03:41:38.605: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-601d21d0-60e5-4ae9-ba9b-a6bd8179f288 container test-container: <nil>
STEP: delete the pod
Mar 24 03:41:38.623: INFO: Waiting for pod pod-601d21d0-60e5-4ae9-ba9b-a6bd8179f288 to disappear
Mar 24 03:41:38.625: INFO: Pod pod-601d21d0-60e5-4ae9-ba9b-a6bd8179f288 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:41:38.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9413" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":266,"skipped":4533,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:41:38.631: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7494
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7494
Mar 24 03:41:40.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-7494 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 24 03:41:40.913: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 24 03:41:40.913: INFO: stdout: "ipvs"
Mar 24 03:41:40.913: INFO: proxyMode: ipvs
Mar 24 03:41:40.921: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 24 03:41:40.923: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-7494
STEP: creating replication controller affinity-clusterip-timeout in namespace services-7494
I0324 03:41:40.934870      21 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-7494, replica count: 3
I0324 03:41:43.985118      21 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 03:41:43.991: INFO: Creating new exec pod
Mar 24 03:41:47.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-7494 exec execpod-affinitybnc49 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Mar 24 03:41:47.151: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar 24 03:41:47.151: INFO: stdout: ""
Mar 24 03:41:47.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-7494 exec execpod-affinitybnc49 -- /bin/sh -x -c nc -zv -t -w 2 172.16.242.222 80'
Mar 24 03:41:47.294: INFO: stderr: "+ nc -zv -t -w 2 172.16.242.222 80\nConnection to 172.16.242.222 80 port [tcp/http] succeeded!\n"
Mar 24 03:41:47.294: INFO: stdout: ""
Mar 24 03:41:47.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-7494 exec execpod-affinitybnc49 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.242.222:80/ ; done'
Mar 24 03:41:47.479: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n"
Mar 24 03:41:47.479: INFO: stdout: "\naffinity-clusterip-timeout-grthj\naffinity-clusterip-timeout-grthj\naffinity-clusterip-timeout-grthj\naffinity-clusterip-timeout-grthj\naffinity-clusterip-timeout-grthj\naffinity-clusterip-timeout-grthj\naffinity-clusterip-timeout-grthj\naffinity-clusterip-timeout-grthj\naffinity-clusterip-timeout-grthj\naffinity-clusterip-timeout-grthj\naffinity-clusterip-timeout-grthj\naffinity-clusterip-timeout-grthj\naffinity-clusterip-timeout-grthj\naffinity-clusterip-timeout-grthj\naffinity-clusterip-timeout-grthj\naffinity-clusterip-timeout-grthj"
Mar 24 03:41:47.479: INFO: Received response from host: affinity-clusterip-timeout-grthj
Mar 24 03:41:47.479: INFO: Received response from host: affinity-clusterip-timeout-grthj
Mar 24 03:41:47.479: INFO: Received response from host: affinity-clusterip-timeout-grthj
Mar 24 03:41:47.479: INFO: Received response from host: affinity-clusterip-timeout-grthj
Mar 24 03:41:47.479: INFO: Received response from host: affinity-clusterip-timeout-grthj
Mar 24 03:41:47.479: INFO: Received response from host: affinity-clusterip-timeout-grthj
Mar 24 03:41:47.479: INFO: Received response from host: affinity-clusterip-timeout-grthj
Mar 24 03:41:47.479: INFO: Received response from host: affinity-clusterip-timeout-grthj
Mar 24 03:41:47.479: INFO: Received response from host: affinity-clusterip-timeout-grthj
Mar 24 03:41:47.479: INFO: Received response from host: affinity-clusterip-timeout-grthj
Mar 24 03:41:47.479: INFO: Received response from host: affinity-clusterip-timeout-grthj
Mar 24 03:41:47.479: INFO: Received response from host: affinity-clusterip-timeout-grthj
Mar 24 03:41:47.479: INFO: Received response from host: affinity-clusterip-timeout-grthj
Mar 24 03:41:47.479: INFO: Received response from host: affinity-clusterip-timeout-grthj
Mar 24 03:41:47.479: INFO: Received response from host: affinity-clusterip-timeout-grthj
Mar 24 03:41:47.479: INFO: Received response from host: affinity-clusterip-timeout-grthj
Mar 24 03:41:47.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-7494 exec execpod-affinitybnc49 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.16.242.222:80/'
Mar 24 03:41:47.618: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n"
Mar 24 03:41:47.618: INFO: stdout: "affinity-clusterip-timeout-grthj"
Mar 24 03:43:57.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-7494 exec execpod-affinitybnc49 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.16.242.222:80/'
Mar 24 03:43:57.771: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.16.242.222:80/\n"
Mar 24 03:43:57.771: INFO: stdout: "affinity-clusterip-timeout-56b9d"
Mar 24 03:43:57.771: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-7494, will wait for the garbage collector to delete the pods
Mar 24 03:43:57.850: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 4.685393ms
Mar 24 03:43:58.550: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 700.126277ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:44:12.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7494" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

â€¢ [SLOW TEST:153.953 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":267,"skipped":4551,"failed":0}
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:44:12.584: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8566
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-415c10a1-0314-4f0d-9f3f-872e3db7db41
STEP: Creating secret with name secret-projected-all-test-volume-da227d82-9c87-4503-83ab-3c9aa603bb86
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar 24 03:44:12.722: INFO: Waiting up to 5m0s for pod "projected-volume-a08113e8-0aa1-41a9-bb88-abf22636649d" in namespace "projected-8566" to be "Succeeded or Failed"
Mar 24 03:44:12.724: INFO: Pod "projected-volume-a08113e8-0aa1-41a9-bb88-abf22636649d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.896596ms
Mar 24 03:44:14.729: INFO: Pod "projected-volume-a08113e8-0aa1-41a9-bb88-abf22636649d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006863516s
STEP: Saw pod success
Mar 24 03:44:14.729: INFO: Pod "projected-volume-a08113e8-0aa1-41a9-bb88-abf22636649d" satisfied condition "Succeeded or Failed"
Mar 24 03:44:14.731: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod projected-volume-a08113e8-0aa1-41a9-bb88-abf22636649d container projected-all-volume-test: <nil>
STEP: delete the pod
Mar 24 03:44:14.750: INFO: Waiting for pod projected-volume-a08113e8-0aa1-41a9-bb88-abf22636649d to disappear
Mar 24 03:44:14.752: INFO: Pod projected-volume-a08113e8-0aa1-41a9-bb88-abf22636649d no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:44:14.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8566" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":268,"skipped":4552,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:44:14.758: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6072
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Mar 24 03:44:14.890: INFO: Waiting up to 5m0s for pod "client-containers-4194911f-000f-4be3-a387-610a1f258c8f" in namespace "containers-6072" to be "Succeeded or Failed"
Mar 24 03:44:14.892: INFO: Pod "client-containers-4194911f-000f-4be3-a387-610a1f258c8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.19827ms
Mar 24 03:44:16.897: INFO: Pod "client-containers-4194911f-000f-4be3-a387-610a1f258c8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007255739s
STEP: Saw pod success
Mar 24 03:44:16.897: INFO: Pod "client-containers-4194911f-000f-4be3-a387-610a1f258c8f" satisfied condition "Succeeded or Failed"
Mar 24 03:44:16.899: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod client-containers-4194911f-000f-4be3-a387-610a1f258c8f container agnhost-container: <nil>
STEP: delete the pod
Mar 24 03:44:16.914: INFO: Waiting for pod client-containers-4194911f-000f-4be3-a387-610a1f258c8f to disappear
Mar 24 03:44:16.915: INFO: Pod client-containers-4194911f-000f-4be3-a387-610a1f258c8f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:44:16.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6072" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":269,"skipped":4578,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:44:16.925: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6314
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar 24 03:44:19.576: INFO: Successfully updated pod "adopt-release-25dcb"
STEP: Checking that the Job readopts the Pod
Mar 24 03:44:19.576: INFO: Waiting up to 15m0s for pod "adopt-release-25dcb" in namespace "job-6314" to be "adopted"
Mar 24 03:44:19.578: INFO: Pod "adopt-release-25dcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.072169ms
Mar 24 03:44:21.582: INFO: Pod "adopt-release-25dcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.006684666s
Mar 24 03:44:21.582: INFO: Pod "adopt-release-25dcb" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar 24 03:44:22.095: INFO: Successfully updated pod "adopt-release-25dcb"
STEP: Checking that the Job releases the Pod
Mar 24 03:44:22.095: INFO: Waiting up to 15m0s for pod "adopt-release-25dcb" in namespace "job-6314" to be "released"
Mar 24 03:44:22.096: INFO: Pod "adopt-release-25dcb": Phase="Running", Reason="", readiness=true. Elapsed: 1.730389ms
Mar 24 03:44:24.101: INFO: Pod "adopt-release-25dcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.006297631s
Mar 24 03:44:24.101: INFO: Pod "adopt-release-25dcb" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:44:24.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6314" for this suite.

â€¢ [SLOW TEST:7.184 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":270,"skipped":4583,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:44:24.109: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6400
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-23b4c789-879c-41d9-a5da-8c77884981b8
STEP: Creating a pod to test consume secrets
Mar 24 03:44:24.246: INFO: Waiting up to 5m0s for pod "pod-secrets-4bb1feca-980d-4f09-9bed-e9c804b9ac2e" in namespace "secrets-6400" to be "Succeeded or Failed"
Mar 24 03:44:24.248: INFO: Pod "pod-secrets-4bb1feca-980d-4f09-9bed-e9c804b9ac2e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.656246ms
Mar 24 03:44:26.253: INFO: Pod "pod-secrets-4bb1feca-980d-4f09-9bed-e9c804b9ac2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006821392s
STEP: Saw pod success
Mar 24 03:44:26.253: INFO: Pod "pod-secrets-4bb1feca-980d-4f09-9bed-e9c804b9ac2e" satisfied condition "Succeeded or Failed"
Mar 24 03:44:26.255: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-secrets-4bb1feca-980d-4f09-9bed-e9c804b9ac2e container secret-volume-test: <nil>
STEP: delete the pod
Mar 24 03:44:26.269: INFO: Waiting for pod pod-secrets-4bb1feca-980d-4f09-9bed-e9c804b9ac2e to disappear
Mar 24 03:44:26.271: INFO: Pod pod-secrets-4bb1feca-980d-4f09-9bed-e9c804b9ac2e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:44:26.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6400" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":271,"skipped":4620,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:44:26.277: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9074
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Mar 24 03:44:26.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 create -f -'
Mar 24 03:44:26.601: INFO: stderr: ""
Mar 24 03:44:26.601: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 24 03:44:26.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 03:44:26.663: INFO: stderr: ""
Mar 24 03:44:26.663: INFO: stdout: "update-demo-nautilus-gzxwl update-demo-nautilus-hscmr "
Mar 24 03:44:26.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods update-demo-nautilus-gzxwl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 03:44:26.714: INFO: stderr: ""
Mar 24 03:44:26.714: INFO: stdout: ""
Mar 24 03:44:26.714: INFO: update-demo-nautilus-gzxwl is created but not running
Mar 24 03:44:31.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 03:44:31.774: INFO: stderr: ""
Mar 24 03:44:31.774: INFO: stdout: "update-demo-nautilus-gzxwl update-demo-nautilus-hscmr "
Mar 24 03:44:31.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods update-demo-nautilus-gzxwl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 03:44:31.830: INFO: stderr: ""
Mar 24 03:44:31.830: INFO: stdout: "true"
Mar 24 03:44:31.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods update-demo-nautilus-gzxwl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 24 03:44:31.886: INFO: stderr: ""
Mar 24 03:44:31.886: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 03:44:31.886: INFO: validating pod update-demo-nautilus-gzxwl
Mar 24 03:44:31.889: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 03:44:31.889: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 03:44:31.889: INFO: update-demo-nautilus-gzxwl is verified up and running
Mar 24 03:44:31.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods update-demo-nautilus-hscmr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 03:44:31.940: INFO: stderr: ""
Mar 24 03:44:31.940: INFO: stdout: "true"
Mar 24 03:44:31.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods update-demo-nautilus-hscmr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 24 03:44:31.990: INFO: stderr: ""
Mar 24 03:44:31.990: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 03:44:31.990: INFO: validating pod update-demo-nautilus-hscmr
Mar 24 03:44:31.993: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 03:44:31.993: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 03:44:31.993: INFO: update-demo-nautilus-hscmr is verified up and running
STEP: scaling down the replication controller
Mar 24 03:44:31.994: INFO: scanned /root for discovery docs: <nil>
Mar 24 03:44:31.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar 24 03:44:33.064: INFO: stderr: ""
Mar 24 03:44:33.064: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 24 03:44:33.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 03:44:33.117: INFO: stderr: ""
Mar 24 03:44:33.117: INFO: stdout: "update-demo-nautilus-gzxwl update-demo-nautilus-hscmr "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 24 03:44:38.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 03:44:38.179: INFO: stderr: ""
Mar 24 03:44:38.179: INFO: stdout: "update-demo-nautilus-gzxwl update-demo-nautilus-hscmr "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 24 03:44:43.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 03:44:43.232: INFO: stderr: ""
Mar 24 03:44:43.232: INFO: stdout: "update-demo-nautilus-hscmr "
Mar 24 03:44:43.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods update-demo-nautilus-hscmr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 03:44:43.287: INFO: stderr: ""
Mar 24 03:44:43.287: INFO: stdout: "true"
Mar 24 03:44:43.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods update-demo-nautilus-hscmr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 24 03:44:43.336: INFO: stderr: ""
Mar 24 03:44:43.336: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 03:44:43.336: INFO: validating pod update-demo-nautilus-hscmr
Mar 24 03:44:43.339: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 03:44:43.339: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 03:44:43.339: INFO: update-demo-nautilus-hscmr is verified up and running
STEP: scaling up the replication controller
Mar 24 03:44:43.340: INFO: scanned /root for discovery docs: <nil>
Mar 24 03:44:43.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar 24 03:44:44.404: INFO: stderr: ""
Mar 24 03:44:44.404: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 24 03:44:44.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 03:44:44.459: INFO: stderr: ""
Mar 24 03:44:44.459: INFO: stdout: "update-demo-nautilus-bl7df update-demo-nautilus-hscmr "
Mar 24 03:44:44.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods update-demo-nautilus-bl7df -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 03:44:44.512: INFO: stderr: ""
Mar 24 03:44:44.512: INFO: stdout: "true"
Mar 24 03:44:44.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods update-demo-nautilus-bl7df -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 24 03:44:44.562: INFO: stderr: ""
Mar 24 03:44:44.562: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 03:44:44.562: INFO: validating pod update-demo-nautilus-bl7df
Mar 24 03:44:44.567: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 03:44:44.567: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 03:44:44.567: INFO: update-demo-nautilus-bl7df is verified up and running
Mar 24 03:44:44.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods update-demo-nautilus-hscmr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 03:44:44.617: INFO: stderr: ""
Mar 24 03:44:44.617: INFO: stdout: "true"
Mar 24 03:44:44.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods update-demo-nautilus-hscmr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 24 03:44:44.668: INFO: stderr: ""
Mar 24 03:44:44.668: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 03:44:44.668: INFO: validating pod update-demo-nautilus-hscmr
Mar 24 03:44:44.670: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 03:44:44.670: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 03:44:44.670: INFO: update-demo-nautilus-hscmr is verified up and running
STEP: using delete to clean up resources
Mar 24 03:44:44.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 delete --grace-period=0 --force -f -'
Mar 24 03:44:44.727: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:44:44.727: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 24 03:44:44.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get rc,svc -l name=update-demo --no-headers'
Mar 24 03:44:44.779: INFO: stderr: "No resources found in kubectl-9074 namespace.\n"
Mar 24 03:44:44.779: INFO: stdout: ""
Mar 24 03:44:44.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 24 03:44:44.831: INFO: stderr: ""
Mar 24 03:44:44.831: INFO: stdout: "update-demo-nautilus-bl7df\nupdate-demo-nautilus-hscmr\n"
Mar 24 03:44:45.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get rc,svc -l name=update-demo --no-headers'
Mar 24 03:44:45.387: INFO: stderr: "No resources found in kubectl-9074 namespace.\n"
Mar 24 03:44:45.387: INFO: stdout: ""
Mar 24 03:44:45.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-9074 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 24 03:44:45.440: INFO: stderr: ""
Mar 24 03:44:45.440: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:44:45.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9074" for this suite.

â€¢ [SLOW TEST:19.171 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":272,"skipped":4629,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:44:45.448: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-927
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar 24 03:44:45.574: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Mar 24 03:44:46.272: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Mar 24 03:44:48.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:44:50.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:44:52.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:44:54.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:44:56.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:44:58.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:45:00.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63752154286, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 24 03:45:02.925: INFO: Waited 614.314873ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:45:03.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-927" for this suite.

â€¢ [SLOW TEST:18.370 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":273,"skipped":4641,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:45:03.818: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-579
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-8qmq
STEP: Creating a pod to test atomic-volume-subpath
Mar 24 03:45:03.957: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8qmq" in namespace "subpath-579" to be "Succeeded or Failed"
Mar 24 03:45:03.959: INFO: Pod "pod-subpath-test-configmap-8qmq": Phase="Pending", Reason="", readiness=false. Elapsed: 1.850925ms
Mar 24 03:45:05.965: INFO: Pod "pod-subpath-test-configmap-8qmq": Phase="Running", Reason="", readiness=true. Elapsed: 2.007794358s
Mar 24 03:45:07.970: INFO: Pod "pod-subpath-test-configmap-8qmq": Phase="Running", Reason="", readiness=true. Elapsed: 4.01267119s
Mar 24 03:45:09.975: INFO: Pod "pod-subpath-test-configmap-8qmq": Phase="Running", Reason="", readiness=true. Elapsed: 6.017387689s
Mar 24 03:45:11.980: INFO: Pod "pod-subpath-test-configmap-8qmq": Phase="Running", Reason="", readiness=true. Elapsed: 8.022740758s
Mar 24 03:45:13.983: INFO: Pod "pod-subpath-test-configmap-8qmq": Phase="Running", Reason="", readiness=true. Elapsed: 10.026317464s
Mar 24 03:45:15.997: INFO: Pod "pod-subpath-test-configmap-8qmq": Phase="Running", Reason="", readiness=true. Elapsed: 12.039455546s
Mar 24 03:45:18.002: INFO: Pod "pod-subpath-test-configmap-8qmq": Phase="Running", Reason="", readiness=true. Elapsed: 14.04504437s
Mar 24 03:45:20.007: INFO: Pod "pod-subpath-test-configmap-8qmq": Phase="Running", Reason="", readiness=true. Elapsed: 16.049891536s
Mar 24 03:45:22.013: INFO: Pod "pod-subpath-test-configmap-8qmq": Phase="Running", Reason="", readiness=true. Elapsed: 18.055358685s
Mar 24 03:45:24.017: INFO: Pod "pod-subpath-test-configmap-8qmq": Phase="Running", Reason="", readiness=true. Elapsed: 20.059356125s
Mar 24 03:45:26.022: INFO: Pod "pod-subpath-test-configmap-8qmq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.064388888s
STEP: Saw pod success
Mar 24 03:45:26.022: INFO: Pod "pod-subpath-test-configmap-8qmq" satisfied condition "Succeeded or Failed"
Mar 24 03:45:26.023: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-subpath-test-configmap-8qmq container test-container-subpath-configmap-8qmq: <nil>
STEP: delete the pod
Mar 24 03:45:26.036: INFO: Waiting for pod pod-subpath-test-configmap-8qmq to disappear
Mar 24 03:45:26.038: INFO: Pod pod-subpath-test-configmap-8qmq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8qmq
Mar 24 03:45:26.038: INFO: Deleting pod "pod-subpath-test-configmap-8qmq" in namespace "subpath-579"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:45:26.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-579" for this suite.

â€¢ [SLOW TEST:22.228 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":274,"skipped":4667,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:45:26.046: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9183
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:45:26.189: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar 24 03:45:26.195: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:26.195: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:26.195: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:26.197: INFO: Number of nodes with available pods: 0
Mar 24 03:45:26.197: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:45:27.202: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:27.202: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:27.202: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:27.204: INFO: Number of nodes with available pods: 0
Mar 24 03:45:27.204: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:45:28.201: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:28.201: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:28.201: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:28.203: INFO: Number of nodes with available pods: 3
Mar 24 03:45:28.203: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar 24 03:45:28.219: INFO: Wrong image for pod: daemon-set-29b2z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:28.219: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:28.219: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:28.221: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:28.221: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:28.221: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:29.226: INFO: Wrong image for pod: daemon-set-29b2z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:29.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:29.226: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:29.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:29.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:29.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:30.225: INFO: Wrong image for pod: daemon-set-29b2z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:30.225: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:30.225: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:30.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:30.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:30.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:31.226: INFO: Wrong image for pod: daemon-set-29b2z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:31.226: INFO: Pod daemon-set-29b2z is not available
Mar 24 03:45:31.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:31.226: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:31.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:31.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:31.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:32.226: INFO: Wrong image for pod: daemon-set-29b2z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:32.226: INFO: Pod daemon-set-29b2z is not available
Mar 24 03:45:32.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:32.226: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:32.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:32.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:32.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:33.223: INFO: Wrong image for pod: daemon-set-29b2z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:33.223: INFO: Pod daemon-set-29b2z is not available
Mar 24 03:45:33.223: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:33.223: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:33.226: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:33.226: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:33.226: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:34.226: INFO: Wrong image for pod: daemon-set-29b2z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:34.226: INFO: Pod daemon-set-29b2z is not available
Mar 24 03:45:34.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:34.226: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:34.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:34.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:34.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:35.226: INFO: Wrong image for pod: daemon-set-29b2z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:35.226: INFO: Pod daemon-set-29b2z is not available
Mar 24 03:45:35.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:35.226: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:35.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:35.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:35.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:36.226: INFO: Wrong image for pod: daemon-set-29b2z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:36.226: INFO: Pod daemon-set-29b2z is not available
Mar 24 03:45:36.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:36.226: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:36.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:36.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:36.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:37.226: INFO: Wrong image for pod: daemon-set-29b2z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:37.226: INFO: Pod daemon-set-29b2z is not available
Mar 24 03:45:37.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:37.226: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:37.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:37.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:37.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:38.226: INFO: Wrong image for pod: daemon-set-29b2z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:38.226: INFO: Pod daemon-set-29b2z is not available
Mar 24 03:45:38.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:38.226: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:38.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:38.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:38.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:39.226: INFO: Wrong image for pod: daemon-set-29b2z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:39.226: INFO: Pod daemon-set-29b2z is not available
Mar 24 03:45:39.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:39.226: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:39.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:39.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:39.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:40.225: INFO: Wrong image for pod: daemon-set-29b2z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:40.225: INFO: Pod daemon-set-29b2z is not available
Mar 24 03:45:40.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:40.226: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:40.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:40.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:40.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:41.225: INFO: Wrong image for pod: daemon-set-29b2z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:41.225: INFO: Pod daemon-set-29b2z is not available
Mar 24 03:45:41.225: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:41.225: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:41.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:41.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:41.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:42.226: INFO: Pod daemon-set-45d58 is not available
Mar 24 03:45:42.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:42.226: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:42.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:42.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:42.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:43.252: INFO: Pod daemon-set-45d58 is not available
Mar 24 03:45:43.252: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:43.252: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:43.255: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:43.255: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:43.256: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:44.225: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:44.225: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:44.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:44.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:44.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:45.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:45.226: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:45.226: INFO: Pod daemon-set-d58r5 is not available
Mar 24 03:45:45.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:45.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:45.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:46.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:46.226: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:46.226: INFO: Pod daemon-set-d58r5 is not available
Mar 24 03:45:46.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:46.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:46.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:47.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:47.226: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:47.226: INFO: Pod daemon-set-d58r5 is not available
Mar 24 03:45:47.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:47.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:47.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:48.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:48.226: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:48.226: INFO: Pod daemon-set-d58r5 is not available
Mar 24 03:45:48.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:48.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:48.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:49.225: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:49.225: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:49.225: INFO: Pod daemon-set-d58r5 is not available
Mar 24 03:45:49.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:49.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:49.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:50.225: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:50.225: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:50.225: INFO: Pod daemon-set-d58r5 is not available
Mar 24 03:45:50.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:50.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:50.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:51.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:51.226: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:51.226: INFO: Pod daemon-set-d58r5 is not available
Mar 24 03:45:51.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:51.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:51.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:52.225: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:52.225: INFO: Wrong image for pod: daemon-set-d58r5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:52.225: INFO: Pod daemon-set-d58r5 is not available
Mar 24 03:45:52.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:52.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:52.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:53.225: INFO: Pod daemon-set-7x4c4 is not available
Mar 24 03:45:53.225: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:53.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:53.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:53.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:54.225: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:54.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:54.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:54.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:55.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:55.226: INFO: Pod daemon-set-b9ljx is not available
Mar 24 03:45:55.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:55.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:55.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:56.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:56.226: INFO: Pod daemon-set-b9ljx is not available
Mar 24 03:45:56.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:56.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:56.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:57.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:57.226: INFO: Pod daemon-set-b9ljx is not available
Mar 24 03:45:57.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:57.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:57.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:58.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:58.226: INFO: Pod daemon-set-b9ljx is not available
Mar 24 03:45:58.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:58.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:58.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:59.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:45:59.226: INFO: Pod daemon-set-b9ljx is not available
Mar 24 03:45:59.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:59.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:45:59.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:00.225: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:46:00.225: INFO: Pod daemon-set-b9ljx is not available
Mar 24 03:46:00.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:00.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:00.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:01.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:46:01.226: INFO: Pod daemon-set-b9ljx is not available
Mar 24 03:46:01.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:01.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:01.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:02.226: INFO: Wrong image for pod: daemon-set-b9ljx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar 24 03:46:02.226: INFO: Pod daemon-set-b9ljx is not available
Mar 24 03:46:02.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:02.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:02.229: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:03.225: INFO: Pod daemon-set-zcmzz is not available
Mar 24 03:46:03.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:03.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:03.228: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar 24 03:46:03.230: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:03.230: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:03.230: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:03.232: INFO: Number of nodes with available pods: 2
Mar 24 03:46:03.232: INFO: Node cn-hongkong.192.168.0.15 is running more than one daemon pod
Mar 24 03:46:04.237: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:04.237: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:04.237: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:46:04.239: INFO: Number of nodes with available pods: 3
Mar 24 03:46:04.239: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9183, will wait for the garbage collector to delete the pods
Mar 24 03:46:04.306: INFO: Deleting DaemonSet.extensions daemon-set took: 5.517696ms
Mar 24 03:46:05.006: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.113805ms
Mar 24 03:46:12.318: INFO: Number of nodes with available pods: 0
Mar 24 03:46:12.318: INFO: Number of running nodes: 0, number of available pods: 0
Mar 24 03:46:12.320: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"813097"},"items":null}

Mar 24 03:46:12.321: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"813097"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:46:12.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9183" for this suite.

â€¢ [SLOW TEST:46.289 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":275,"skipped":4676,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:46:12.335: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4493
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 24 03:46:14.990: INFO: Successfully updated pod "pod-update-6535e886-d5c2-43f5-962b-ca0e545e6d57"
STEP: verifying the updated pod is in kubernetes
Mar 24 03:46:14.993: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:46:14.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4493" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":276,"skipped":4714,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:46:15.000: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8087
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 24 03:46:19.161: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:46:19.163: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:46:21.163: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:46:21.169: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:46:23.163: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:46:23.169: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:46:25.163: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:46:25.168: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:46:27.163: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:46:27.169: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:46:29.163: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:46:29.169: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:46:31.163: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:46:31.169: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 24 03:46:33.163: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 24 03:46:33.168: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:46:33.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8087" for this suite.

â€¢ [SLOW TEST:18.205 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":277,"skipped":4792,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:46:33.205: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6069
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-36805f3d-ca37-45ab-a3f8-a11af8cbd6be
STEP: Creating a pod to test consume configMaps
Mar 24 03:46:33.340: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a8bd1156-6350-490d-9b03-9a8c4845df3f" in namespace "projected-6069" to be "Succeeded or Failed"
Mar 24 03:46:33.341: INFO: Pod "pod-projected-configmaps-a8bd1156-6350-490d-9b03-9a8c4845df3f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.758255ms
Mar 24 03:46:35.348: INFO: Pod "pod-projected-configmaps-a8bd1156-6350-490d-9b03-9a8c4845df3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007836627s
STEP: Saw pod success
Mar 24 03:46:35.348: INFO: Pod "pod-projected-configmaps-a8bd1156-6350-490d-9b03-9a8c4845df3f" satisfied condition "Succeeded or Failed"
Mar 24 03:46:35.349: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-projected-configmaps-a8bd1156-6350-490d-9b03-9a8c4845df3f container agnhost-container: <nil>
STEP: delete the pod
Mar 24 03:46:35.368: INFO: Waiting for pod pod-projected-configmaps-a8bd1156-6350-490d-9b03-9a8c4845df3f to disappear
Mar 24 03:46:35.369: INFO: Pod pod-projected-configmaps-a8bd1156-6350-490d-9b03-9a8c4845df3f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:46:35.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6069" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":278,"skipped":4805,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:46:35.376: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 24 03:46:35.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-8466 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Mar 24 03:46:35.565: INFO: stderr: ""
Mar 24 03:46:35.565: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Mar 24 03:46:35.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-8466 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Mar 24 03:46:35.761: INFO: stderr: ""
Mar 24 03:46:35.761: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Mar 24 03:46:35.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-8466 delete pods e2e-test-httpd-pod'
Mar 24 03:46:42.542: INFO: stderr: ""
Mar 24 03:46:42.542: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:46:42.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8466" for this suite.

â€¢ [SLOW TEST:7.184 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:909
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":279,"skipped":4861,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:46:42.560: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8716
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-8716
STEP: creating service affinity-clusterip-transition in namespace services-8716
STEP: creating replication controller affinity-clusterip-transition in namespace services-8716
I0324 03:46:42.701377      21 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-8716, replica count: 3
I0324 03:46:45.751576      21 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 24 03:46:45.759: INFO: Creating new exec pod
Mar 24 03:46:48.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-8716 exec execpod-affinityf6nld -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Mar 24 03:46:48.929: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar 24 03:46:48.929: INFO: stdout: ""
Mar 24 03:46:48.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-8716 exec execpod-affinityf6nld -- /bin/sh -x -c nc -zv -t -w 2 172.16.171.147 80'
Mar 24 03:46:49.071: INFO: stderr: "+ nc -zv -t -w 2 172.16.171.147 80\nConnection to 172.16.171.147 80 port [tcp/http] succeeded!\n"
Mar 24 03:46:49.071: INFO: stdout: ""
Mar 24 03:46:49.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-8716 exec execpod-affinityf6nld -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.171.147:80/ ; done'
Mar 24 03:46:49.275: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n"
Mar 24 03:46:49.275: INFO: stdout: "\naffinity-clusterip-transition-4dz2z\naffinity-clusterip-transition-n8z9q\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4dz2z\naffinity-clusterip-transition-n8z9q\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4dz2z\naffinity-clusterip-transition-n8z9q\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4dz2z\naffinity-clusterip-transition-n8z9q\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4dz2z\naffinity-clusterip-transition-n8z9q\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4dz2z"
Mar 24 03:46:49.275: INFO: Received response from host: affinity-clusterip-transition-4dz2z
Mar 24 03:46:49.275: INFO: Received response from host: affinity-clusterip-transition-n8z9q
Mar 24 03:46:49.275: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.275: INFO: Received response from host: affinity-clusterip-transition-4dz2z
Mar 24 03:46:49.275: INFO: Received response from host: affinity-clusterip-transition-n8z9q
Mar 24 03:46:49.275: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.275: INFO: Received response from host: affinity-clusterip-transition-4dz2z
Mar 24 03:46:49.275: INFO: Received response from host: affinity-clusterip-transition-n8z9q
Mar 24 03:46:49.275: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.275: INFO: Received response from host: affinity-clusterip-transition-4dz2z
Mar 24 03:46:49.275: INFO: Received response from host: affinity-clusterip-transition-n8z9q
Mar 24 03:46:49.275: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.275: INFO: Received response from host: affinity-clusterip-transition-4dz2z
Mar 24 03:46:49.275: INFO: Received response from host: affinity-clusterip-transition-n8z9q
Mar 24 03:46:49.275: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.275: INFO: Received response from host: affinity-clusterip-transition-4dz2z
Mar 24 03:46:49.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=services-8716 exec execpod-affinityf6nld -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.171.147:80/ ; done'
Mar 24 03:46:49.478: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.171.147:80/\n"
Mar 24 03:46:49.478: INFO: stdout: "\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4lpf9\naffinity-clusterip-transition-4lpf9"
Mar 24 03:46:49.478: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.478: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.478: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.478: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.478: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.478: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.478: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.478: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.478: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.478: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.478: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.478: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.478: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.478: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.478: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.478: INFO: Received response from host: affinity-clusterip-transition-4lpf9
Mar 24 03:46:49.478: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8716, will wait for the garbage collector to delete the pods
Mar 24 03:46:49.545: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.303676ms
Mar 24 03:46:50.245: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 700.121359ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:47:02.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8716" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

â€¢ [SLOW TEST:20.118 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":280,"skipped":4863,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:47:02.678: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-4968
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar 24 03:47:02.815: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 24 03:48:02.848: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:48:02.852: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-8738
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:48:02.989: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Mar 24 03:48:02.991: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:48:03.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8738" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:48:03.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4968" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

â€¢ [SLOW TEST:60.376 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":281,"skipped":4901,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:48:03.054: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1759
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:48:03.184: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar 24 03:48:10.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-1759 --namespace=crd-publish-openapi-1759 create -f -'
Mar 24 03:48:11.308: INFO: stderr: ""
Mar 24 03:48:11.308: INFO: stdout: "e2e-test-crd-publish-openapi-4628-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 24 03:48:11.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-1759 --namespace=crd-publish-openapi-1759 delete e2e-test-crd-publish-openapi-4628-crds test-foo'
Mar 24 03:48:11.398: INFO: stderr: ""
Mar 24 03:48:11.398: INFO: stdout: "e2e-test-crd-publish-openapi-4628-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 24 03:48:11.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-1759 --namespace=crd-publish-openapi-1759 apply -f -'
Mar 24 03:48:11.629: INFO: stderr: ""
Mar 24 03:48:11.629: INFO: stdout: "e2e-test-crd-publish-openapi-4628-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 24 03:48:11.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-1759 --namespace=crd-publish-openapi-1759 delete e2e-test-crd-publish-openapi-4628-crds test-foo'
Mar 24 03:48:11.693: INFO: stderr: ""
Mar 24 03:48:11.693: INFO: stdout: "e2e-test-crd-publish-openapi-4628-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar 24 03:48:11.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-1759 --namespace=crd-publish-openapi-1759 create -f -'
Mar 24 03:48:11.882: INFO: rc: 1
Mar 24 03:48:11.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-1759 --namespace=crd-publish-openapi-1759 apply -f -'
Mar 24 03:48:12.072: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar 24 03:48:12.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-1759 --namespace=crd-publish-openapi-1759 create -f -'
Mar 24 03:48:12.265: INFO: rc: 1
Mar 24 03:48:12.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-1759 --namespace=crd-publish-openapi-1759 apply -f -'
Mar 24 03:48:12.459: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar 24 03:48:12.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-1759 explain e2e-test-crd-publish-openapi-4628-crds'
Mar 24 03:48:12.657: INFO: stderr: ""
Mar 24 03:48:12.657: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4628-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar 24 03:48:12.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-1759 explain e2e-test-crd-publish-openapi-4628-crds.metadata'
Mar 24 03:48:12.849: INFO: stderr: ""
Mar 24 03:48:12.849: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4628-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 24 03:48:12.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-1759 explain e2e-test-crd-publish-openapi-4628-crds.spec'
Mar 24 03:48:13.042: INFO: stderr: ""
Mar 24 03:48:13.042: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4628-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 24 03:48:13.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-1759 explain e2e-test-crd-publish-openapi-4628-crds.spec.bars'
Mar 24 03:48:13.233: INFO: stderr: ""
Mar 24 03:48:13.233: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4628-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar 24 03:48:13.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=crd-publish-openapi-1759 explain e2e-test-crd-publish-openapi-4628-crds.spec.bars2'
Mar 24 03:48:13.425: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:48:15.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1759" for this suite.

â€¢ [SLOW TEST:12.154 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":282,"skipped":4903,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:48:15.209: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2554
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 24 03:48:15.353: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:15.353: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:15.353: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:15.354: INFO: Number of nodes with available pods: 0
Mar 24 03:48:15.354: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:16.360: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:16.360: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:16.360: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:16.362: INFO: Number of nodes with available pods: 0
Mar 24 03:48:16.362: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:17.360: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:17.360: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:17.360: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:17.362: INFO: Number of nodes with available pods: 3
Mar 24 03:48:17.362: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar 24 03:48:17.373: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:17.373: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:17.373: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:17.375: INFO: Number of nodes with available pods: 2
Mar 24 03:48:17.375: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:18.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:18.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:18.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:18.383: INFO: Number of nodes with available pods: 2
Mar 24 03:48:18.383: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:19.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:19.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:19.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:19.383: INFO: Number of nodes with available pods: 2
Mar 24 03:48:19.383: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:20.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:20.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:20.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:20.382: INFO: Number of nodes with available pods: 2
Mar 24 03:48:20.382: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:21.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:21.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:21.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:21.382: INFO: Number of nodes with available pods: 2
Mar 24 03:48:21.382: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:22.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:22.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:22.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:22.383: INFO: Number of nodes with available pods: 2
Mar 24 03:48:22.383: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:23.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:23.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:23.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:23.382: INFO: Number of nodes with available pods: 2
Mar 24 03:48:23.382: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:24.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:24.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:24.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:24.383: INFO: Number of nodes with available pods: 2
Mar 24 03:48:24.383: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:25.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:25.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:25.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:25.383: INFO: Number of nodes with available pods: 2
Mar 24 03:48:25.383: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:26.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:26.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:26.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:26.382: INFO: Number of nodes with available pods: 2
Mar 24 03:48:26.382: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:27.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:27.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:27.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:27.382: INFO: Number of nodes with available pods: 2
Mar 24 03:48:27.382: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:28.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:28.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:28.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:28.383: INFO: Number of nodes with available pods: 2
Mar 24 03:48:28.383: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:29.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:29.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:29.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:29.382: INFO: Number of nodes with available pods: 2
Mar 24 03:48:29.382: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:30.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:30.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:30.380: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:30.383: INFO: Number of nodes with available pods: 2
Mar 24 03:48:30.383: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:31.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:31.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:31.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:31.383: INFO: Number of nodes with available pods: 2
Mar 24 03:48:31.383: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:32.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:32.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:32.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:32.383: INFO: Number of nodes with available pods: 2
Mar 24 03:48:32.383: INFO: Node cn-hongkong.192.168.0.13 is running more than one daemon pod
Mar 24 03:48:33.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.10 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:33.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.11 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:33.381: INFO: DaemonSet pods can't tolerate node cn-hongkong.192.168.0.12 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 24 03:48:33.383: INFO: Number of nodes with available pods: 3
Mar 24 03:48:33.383: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2554, will wait for the garbage collector to delete the pods
Mar 24 03:48:33.447: INFO: Deleting DaemonSet.extensions daemon-set took: 10.527906ms
Mar 24 03:48:34.147: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.145338ms
Mar 24 03:48:42.561: INFO: Number of nodes with available pods: 0
Mar 24 03:48:42.561: INFO: Number of running nodes: 0, number of available pods: 0
Mar 24 03:48:42.563: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"814152"},"items":null}

Mar 24 03:48:42.566: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"814152"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:48:42.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2554" for this suite.

â€¢ [SLOW TEST:27.373 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":283,"skipped":4904,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:48:42.582: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7451
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:48:55.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7451" for this suite.

â€¢ [SLOW TEST:13.197 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":284,"skipped":4918,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:48:55.779: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-16
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:48:55.912: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25b4a802-1c00-48e7-92dc-6455d90e6764" in namespace "downward-api-16" to be "Succeeded or Failed"
Mar 24 03:48:55.914: INFO: Pod "downwardapi-volume-25b4a802-1c00-48e7-92dc-6455d90e6764": Phase="Pending", Reason="", readiness=false. Elapsed: 1.865928ms
Mar 24 03:48:57.919: INFO: Pod "downwardapi-volume-25b4a802-1c00-48e7-92dc-6455d90e6764": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007300566s
STEP: Saw pod success
Mar 24 03:48:57.919: INFO: Pod "downwardapi-volume-25b4a802-1c00-48e7-92dc-6455d90e6764" satisfied condition "Succeeded or Failed"
Mar 24 03:48:57.921: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod downwardapi-volume-25b4a802-1c00-48e7-92dc-6455d90e6764 container client-container: <nil>
STEP: delete the pod
Mar 24 03:48:57.941: INFO: Waiting for pod downwardapi-volume-25b4a802-1c00-48e7-92dc-6455d90e6764 to disappear
Mar 24 03:48:57.943: INFO: Pod downwardapi-volume-25b4a802-1c00-48e7-92dc-6455d90e6764 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:48:57.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-16" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":285,"skipped":4921,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:48:57.949: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8740
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:48:58.076: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:49:04.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8740" for this suite.

â€¢ [SLOW TEST:6.238 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":286,"skipped":4948,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:49:04.188: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3659
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:49:07.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3659" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":287,"skipped":4949,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:49:07.548: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2323
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Mar 24 03:49:07.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2323 create -f -'
Mar 24 03:49:07.878: INFO: stderr: ""
Mar 24 03:49:07.878: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 24 03:49:07.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2323 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 03:49:07.934: INFO: stderr: ""
Mar 24 03:49:07.934: INFO: stdout: "update-demo-nautilus-hmnm6 update-demo-nautilus-nn95f "
Mar 24 03:49:07.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2323 get pods update-demo-nautilus-hmnm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 03:49:07.989: INFO: stderr: ""
Mar 24 03:49:07.990: INFO: stdout: ""
Mar 24 03:49:07.990: INFO: update-demo-nautilus-hmnm6 is created but not running
Mar 24 03:49:12.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2323 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 24 03:49:13.048: INFO: stderr: ""
Mar 24 03:49:13.048: INFO: stdout: "update-demo-nautilus-hmnm6 update-demo-nautilus-nn95f "
Mar 24 03:49:13.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2323 get pods update-demo-nautilus-hmnm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 03:49:13.106: INFO: stderr: ""
Mar 24 03:49:13.106: INFO: stdout: "true"
Mar 24 03:49:13.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2323 get pods update-demo-nautilus-hmnm6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 24 03:49:13.160: INFO: stderr: ""
Mar 24 03:49:13.160: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 03:49:13.160: INFO: validating pod update-demo-nautilus-hmnm6
Mar 24 03:49:13.163: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 03:49:13.163: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 03:49:13.163: INFO: update-demo-nautilus-hmnm6 is verified up and running
Mar 24 03:49:13.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2323 get pods update-demo-nautilus-nn95f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 24 03:49:13.215: INFO: stderr: ""
Mar 24 03:49:13.215: INFO: stdout: "true"
Mar 24 03:49:13.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2323 get pods update-demo-nautilus-nn95f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 24 03:49:13.265: INFO: stderr: ""
Mar 24 03:49:13.265: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 24 03:49:13.265: INFO: validating pod update-demo-nautilus-nn95f
Mar 24 03:49:13.268: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 24 03:49:13.268: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 24 03:49:13.268: INFO: update-demo-nautilus-nn95f is verified up and running
STEP: using delete to clean up resources
Mar 24 03:49:13.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2323 delete --grace-period=0 --force -f -'
Mar 24 03:49:13.322: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 24 03:49:13.322: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 24 03:49:13.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2323 get rc,svc -l name=update-demo --no-headers'
Mar 24 03:49:13.383: INFO: stderr: "No resources found in kubectl-2323 namespace.\n"
Mar 24 03:49:13.383: INFO: stdout: ""
Mar 24 03:49:13.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-543228437 --namespace=kubectl-2323 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 24 03:49:13.438: INFO: stderr: ""
Mar 24 03:49:13.438: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:49:13.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2323" for this suite.

â€¢ [SLOW TEST:5.896 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":288,"skipped":4968,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:49:13.444: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6778
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-0ae4a285-00d8-4c97-b664-f8f8c2226f87-5299
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:49:13.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6778" for this suite.
STEP: Destroying namespace "nspatchtest-0ae4a285-00d8-4c97-b664-f8f8c2226f87-5299" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":289,"skipped":4976,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:49:13.707: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7322
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-a078a781-bf4c-4986-9146-c76e1fdc8e3c in namespace container-probe-7322
Mar 24 03:49:15.844: INFO: Started pod liveness-a078a781-bf4c-4986-9146-c76e1fdc8e3c in namespace container-probe-7322
STEP: checking the pod's current state and verifying that restartCount is present
Mar 24 03:49:15.846: INFO: Initial restart count of pod liveness-a078a781-bf4c-4986-9146-c76e1fdc8e3c is 0
Mar 24 03:49:31.914: INFO: Restart count of pod container-probe-7322/liveness-a078a781-bf4c-4986-9146-c76e1fdc8e3c is now 1 (16.067882169s elapsed)
Mar 24 03:49:51.992: INFO: Restart count of pod container-probe-7322/liveness-a078a781-bf4c-4986-9146-c76e1fdc8e3c is now 2 (36.146211625s elapsed)
Mar 24 03:50:12.061: INFO: Restart count of pod container-probe-7322/liveness-a078a781-bf4c-4986-9146-c76e1fdc8e3c is now 3 (56.214899664s elapsed)
Mar 24 03:50:32.138: INFO: Restart count of pod container-probe-7322/liveness-a078a781-bf4c-4986-9146-c76e1fdc8e3c is now 4 (1m16.292187876s elapsed)
Mar 24 03:51:32.349: INFO: Restart count of pod container-probe-7322/liveness-a078a781-bf4c-4986-9146-c76e1fdc8e3c is now 5 (2m16.50345391s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:51:32.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7322" for this suite.

â€¢ [SLOW TEST:138.659 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":290,"skipped":4983,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:51:32.366: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8829
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:51:48.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8829" for this suite.

â€¢ [SLOW TEST:16.215 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":291,"skipped":4997,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:51:48.582: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7272
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 24 03:51:48.717: INFO: Waiting up to 5m0s for pod "pod-9c41950a-bf0a-415a-b4cf-bdffacf8a446" in namespace "emptydir-7272" to be "Succeeded or Failed"
Mar 24 03:51:48.719: INFO: Pod "pod-9c41950a-bf0a-415a-b4cf-bdffacf8a446": Phase="Pending", Reason="", readiness=false. Elapsed: 1.782215ms
Mar 24 03:51:50.724: INFO: Pod "pod-9c41950a-bf0a-415a-b4cf-bdffacf8a446": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007075596s
STEP: Saw pod success
Mar 24 03:51:50.724: INFO: Pod "pod-9c41950a-bf0a-415a-b4cf-bdffacf8a446" satisfied condition "Succeeded or Failed"
Mar 24 03:51:50.726: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-9c41950a-bf0a-415a-b4cf-bdffacf8a446 container test-container: <nil>
STEP: delete the pod
Mar 24 03:51:50.749: INFO: Waiting for pod pod-9c41950a-bf0a-415a-b4cf-bdffacf8a446 to disappear
Mar 24 03:51:50.750: INFO: Pod pod-9c41950a-bf0a-415a-b4cf-bdffacf8a446 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:51:50.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7272" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":292,"skipped":5003,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:51:50.757: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6242
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-d027b7d7-96f3-49cb-a0a7-8045297ba7eb
STEP: Creating configMap with name cm-test-opt-upd-4bb49f53-a2da-472f-b371-23f13e22d89e
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-d027b7d7-96f3-49cb-a0a7-8045297ba7eb
STEP: Updating configmap cm-test-opt-upd-4bb49f53-a2da-472f-b371-23f13e22d89e
STEP: Creating configMap with name cm-test-opt-create-83c9e9c4-ddb7-4469-9bce-69f569da0672
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:51:54.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6242" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":293,"skipped":5023,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:51:54.961: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7279
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:51:55.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7279" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":294,"skipped":5033,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:51:55.099: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-1713
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Mar 24 03:51:55.229: INFO: created test-podtemplate-1
Mar 24 03:51:55.232: INFO: created test-podtemplate-2
Mar 24 03:51:55.235: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Mar 24 03:51:55.237: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Mar 24 03:51:55.248: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:51:55.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1713" for this suite.
â€¢{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":295,"skipped":5071,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:51:55.255: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4705
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 24 03:51:57.398: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:51:57.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4705" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":296,"skipped":5081,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:51:57.414: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3277
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:51:59.554: INFO: Deleting pod "var-expansion-ee826890-3b8d-4690-ae8d-48043a854fce" in namespace "var-expansion-3277"
Mar 24 03:51:59.559: INFO: Wait up to 5m0s for pod "var-expansion-ee826890-3b8d-4690-ae8d-48043a854fce" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:52:01.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3277" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":297,"skipped":5105,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:52:01.576: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3610
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-1d8f0778-5763-49e3-8b79-96f332268d8d
STEP: Creating a pod to test consume configMaps
Mar 24 03:52:01.712: INFO: Waiting up to 5m0s for pod "pod-configmaps-83cea003-15dc-451e-801b-72d875a15591" in namespace "configmap-3610" to be "Succeeded or Failed"
Mar 24 03:52:01.714: INFO: Pod "pod-configmaps-83cea003-15dc-451e-801b-72d875a15591": Phase="Pending", Reason="", readiness=false. Elapsed: 1.66163ms
Mar 24 03:52:03.719: INFO: Pod "pod-configmaps-83cea003-15dc-451e-801b-72d875a15591": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006703836s
STEP: Saw pod success
Mar 24 03:52:03.719: INFO: Pod "pod-configmaps-83cea003-15dc-451e-801b-72d875a15591" satisfied condition "Succeeded or Failed"
Mar 24 03:52:03.721: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-configmaps-83cea003-15dc-451e-801b-72d875a15591 container agnhost-container: <nil>
STEP: delete the pod
Mar 24 03:52:03.739: INFO: Waiting for pod pod-configmaps-83cea003-15dc-451e-801b-72d875a15591 to disappear
Mar 24 03:52:03.741: INFO: Pod pod-configmaps-83cea003-15dc-451e-801b-72d875a15591 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:52:03.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3610" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":298,"skipped":5130,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:52:03.749: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6864
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0324 03:52:13.946575      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 24 03:53:15.961: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Mar 24 03:53:15.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rh9s" in namespace "gc-6864"
Mar 24 03:53:15.971: INFO: Deleting pod "simpletest-rc-to-be-deleted-8twx5" in namespace "gc-6864"
Mar 24 03:53:15.980: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8skv" in namespace "gc-6864"
Mar 24 03:53:15.989: INFO: Deleting pod "simpletest-rc-to-be-deleted-dc7gb" in namespace "gc-6864"
Mar 24 03:53:15.996: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbjtp" in namespace "gc-6864"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:53:16.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6864" for this suite.

â€¢ [SLOW TEST:72.264 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":299,"skipped":5141,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:53:16.013: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4982
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:53:27.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4982" for this suite.

â€¢ [SLOW TEST:11.176 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":300,"skipped":5147,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:53:27.189: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2762
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 24 03:53:27.322: INFO: Waiting up to 5m0s for pod "pod-b94d1ca7-562e-4e96-b889-9382324a032f" in namespace "emptydir-2762" to be "Succeeded or Failed"
Mar 24 03:53:27.324: INFO: Pod "pod-b94d1ca7-562e-4e96-b889-9382324a032f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.866352ms
Mar 24 03:53:29.329: INFO: Pod "pod-b94d1ca7-562e-4e96-b889-9382324a032f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007167242s
STEP: Saw pod success
Mar 24 03:53:29.329: INFO: Pod "pod-b94d1ca7-562e-4e96-b889-9382324a032f" satisfied condition "Succeeded or Failed"
Mar 24 03:53:29.331: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-b94d1ca7-562e-4e96-b889-9382324a032f container test-container: <nil>
STEP: delete the pod
Mar 24 03:53:29.345: INFO: Waiting for pod pod-b94d1ca7-562e-4e96-b889-9382324a032f to disappear
Mar 24 03:53:29.347: INFO: Pod pod-b94d1ca7-562e-4e96-b889-9382324a032f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:53:29.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2762" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":301,"skipped":5156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:53:29.352: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1508
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Mar 24 03:53:31.495: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1508 PodName:var-expansion-dfd53f2d-9c23-4b82-987f-2ea579aa9f56 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:53:31.495: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: test for file in mounted path
Mar 24 03:53:31.585: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1508 PodName:var-expansion-dfd53f2d-9c23-4b82-987f-2ea579aa9f56 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:53:31.585: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: updating the annotation value
Mar 24 03:53:32.205: INFO: Successfully updated pod "var-expansion-dfd53f2d-9c23-4b82-987f-2ea579aa9f56"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Mar 24 03:53:32.207: INFO: Deleting pod "var-expansion-dfd53f2d-9c23-4b82-987f-2ea579aa9f56" in namespace "var-expansion-1508"
Mar 24 03:53:32.212: INFO: Wait up to 5m0s for pod "var-expansion-dfd53f2d-9c23-4b82-987f-2ea579aa9f56" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:54:14.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1508" for this suite.

â€¢ [SLOW TEST:44.873 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":302,"skipped":5186,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:54:14.226: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-351
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-b16b059c-70c7-4684-845b-15f2cd89eca1 in namespace container-probe-351
Mar 24 03:54:16.365: INFO: Started pod busybox-b16b059c-70c7-4684-845b-15f2cd89eca1 in namespace container-probe-351
STEP: checking the pod's current state and verifying that restartCount is present
Mar 24 03:54:16.367: INFO: Initial restart count of pod busybox-b16b059c-70c7-4684-845b-15f2cd89eca1 is 0
Mar 24 03:55:02.501: INFO: Restart count of pod container-probe-351/busybox-b16b059c-70c7-4684-845b-15f2cd89eca1 is now 1 (46.133376527s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:55:02.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-351" for this suite.

â€¢ [SLOW TEST:48.292 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":303,"skipped":5190,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:55:02.518: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9676
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar 24 03:55:02.652: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b04f0159-f932-4110-9f85-9078ea083f96" in namespace "downward-api-9676" to be "Succeeded or Failed"
Mar 24 03:55:02.653: INFO: Pod "downwardapi-volume-b04f0159-f932-4110-9f85-9078ea083f96": Phase="Pending", Reason="", readiness=false. Elapsed: 1.804931ms
Mar 24 03:55:04.658: INFO: Pod "downwardapi-volume-b04f0159-f932-4110-9f85-9078ea083f96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006776673s
STEP: Saw pod success
Mar 24 03:55:04.658: INFO: Pod "downwardapi-volume-b04f0159-f932-4110-9f85-9078ea083f96" satisfied condition "Succeeded or Failed"
Mar 24 03:55:04.660: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod downwardapi-volume-b04f0159-f932-4110-9f85-9078ea083f96 container client-container: <nil>
STEP: delete the pod
Mar 24 03:55:04.679: INFO: Waiting for pod downwardapi-volume-b04f0159-f932-4110-9f85-9078ea083f96 to disappear
Mar 24 03:55:04.681: INFO: Pod downwardapi-volume-b04f0159-f932-4110-9f85-9078ea083f96 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:55:04.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9676" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":304,"skipped":5243,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:55:04.687: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-6338
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:55:04.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6338" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":305,"skipped":5251,"failed":0}

------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:55:04.822: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7165
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7165.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7165.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7165.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7165.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7165.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7165.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7165.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7165.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7165.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7165.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7165.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 242.194.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.194.242_udp@PTR;check="$$(dig +tcp +noall +answer +search 242.194.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.194.242_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7165.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7165.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7165.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7165.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7165.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7165.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7165.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7165.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7165.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7165.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7165.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 242.194.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.194.242_udp@PTR;check="$$(dig +tcp +noall +answer +search 242.194.16.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.16.194.242_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 24 03:55:06.981: INFO: Unable to read wheezy_udp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:06.983: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:06.985: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:06.987: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:06.993: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:06.995: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:07.001: INFO: Unable to read jessie_udp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:07.004: INFO: Unable to read jessie_tcp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:07.005: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:07.007: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:07.013: INFO: Unable to read jessie_udp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:07.015: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:07.018: INFO: Lookups using dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290 failed for: [wheezy_udp@dns-test-service.dns-7165.svc.cluster.local wheezy_tcp@dns-test-service.dns-7165.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-7165.svc.cluster.local jessie_tcp@dns-test-service.dns-7165.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:55:12.021: INFO: Unable to read wheezy_udp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:12.024: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:12.026: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:12.028: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:12.034: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:12.035: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:12.041: INFO: Unable to read jessie_udp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:12.043: INFO: Unable to read jessie_tcp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:12.045: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:12.046: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:12.052: INFO: Unable to read jessie_udp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:12.053: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:12.057: INFO: Lookups using dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290 failed for: [wheezy_udp@dns-test-service.dns-7165.svc.cluster.local wheezy_tcp@dns-test-service.dns-7165.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-7165.svc.cluster.local jessie_tcp@dns-test-service.dns-7165.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:55:17.022: INFO: Unable to read wheezy_udp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:17.024: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:17.026: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:17.028: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:17.034: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:17.036: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:17.041: INFO: Unable to read jessie_udp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:17.043: INFO: Unable to read jessie_tcp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:17.045: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:17.047: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:17.053: INFO: Unable to read jessie_udp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:17.054: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:17.058: INFO: Lookups using dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290 failed for: [wheezy_udp@dns-test-service.dns-7165.svc.cluster.local wheezy_tcp@dns-test-service.dns-7165.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-7165.svc.cluster.local jessie_tcp@dns-test-service.dns-7165.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:55:22.021: INFO: Unable to read wheezy_udp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:22.023: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:22.026: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:22.027: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:22.033: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:22.035: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:22.041: INFO: Unable to read jessie_udp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:22.043: INFO: Unable to read jessie_tcp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:22.044: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:22.046: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:22.051: INFO: Unable to read jessie_udp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:22.053: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:22.057: INFO: Lookups using dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290 failed for: [wheezy_udp@dns-test-service.dns-7165.svc.cluster.local wheezy_tcp@dns-test-service.dns-7165.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-7165.svc.cluster.local jessie_tcp@dns-test-service.dns-7165.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:55:27.021: INFO: Unable to read wheezy_udp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:27.024: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:27.026: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:27.028: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:27.034: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:27.035: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:27.041: INFO: Unable to read jessie_udp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:27.043: INFO: Unable to read jessie_tcp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:27.045: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:27.047: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:27.052: INFO: Unable to read jessie_udp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:27.054: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:27.058: INFO: Lookups using dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290 failed for: [wheezy_udp@dns-test-service.dns-7165.svc.cluster.local wheezy_tcp@dns-test-service.dns-7165.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-7165.svc.cluster.local jessie_tcp@dns-test-service.dns-7165.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:55:32.021: INFO: Unable to read wheezy_udp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:32.024: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:32.026: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:32.028: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:32.033: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:32.035: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:32.041: INFO: Unable to read jessie_udp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:32.043: INFO: Unable to read jessie_tcp@dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:32.045: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:32.047: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:32.052: INFO: Unable to read jessie_udp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:32.054: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290: the server could not find the requested resource (get pods dns-test-93303ae1-ae08-4e04-b28e-234292cce290)
Mar 24 03:55:32.058: INFO: Lookups using dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290 failed for: [wheezy_udp@dns-test-service.dns-7165.svc.cluster.local wheezy_tcp@dns-test-service.dns-7165.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-7165.svc.cluster.local jessie_tcp@dns-test-service.dns-7165.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7165.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 24 03:55:37.057: INFO: DNS probes using dns-7165/dns-test-93303ae1-ae08-4e04-b28e-234292cce290 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:55:37.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7165" for this suite.

â€¢ [SLOW TEST:32.282 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":306,"skipped":5251,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:55:37.104: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2680
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-2680
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 24 03:55:37.230: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 24 03:55:37.252: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 24 03:55:39.257: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 03:55:41.257: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 03:55:43.258: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 03:55:45.255: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 03:55:47.257: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 03:55:49.257: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 24 03:55:51.258: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 24 03:55:51.261: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 24 03:55:51.265: INFO: The status of Pod netserver-2 is Running (Ready = false)
Mar 24 03:55:53.270: INFO: The status of Pod netserver-2 is Running (Ready = false)
Mar 24 03:55:55.267: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 24 03:55:57.291: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 24 03:55:57.291: INFO: Going to poll 10.43.0.254 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Mar 24 03:55:57.293: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.43.0.254:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2680 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:55:57.293: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 03:55:57.383: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 24 03:55:57.383: INFO: Going to poll 10.43.1.71 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Mar 24 03:55:57.386: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.43.1.71:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2680 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:55:57.386: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 03:55:57.495: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 24 03:55:57.495: INFO: Going to poll 10.43.1.35 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Mar 24 03:55:57.498: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.43.1.35:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2680 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 24 03:55:57.498: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
Mar 24 03:55:57.600: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:55:57.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2680" for this suite.

â€¢ [SLOW TEST:20.503 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":307,"skipped":5266,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:55:57.607: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1088
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Mar 24 03:57:58.263: INFO: Successfully updated pod "var-expansion-6c0ae474-4bb4-473b-852f-28729d74a1ac"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Mar 24 03:58:00.271: INFO: Deleting pod "var-expansion-6c0ae474-4bb4-473b-852f-28729d74a1ac" in namespace "var-expansion-1088"
Mar 24 03:58:00.278: INFO: Wait up to 5m0s for pod "var-expansion-6c0ae474-4bb4-473b-852f-28729d74a1ac" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:58:44.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1088" for this suite.

â€¢ [SLOW TEST:166.686 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":308,"skipped":5286,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:58:44.293: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-37
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar 24 03:58:44.441: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ecc99111-63fd-4442-8296-1b5d7988a892", Controller:(*bool)(0xc0048a9ab2), BlockOwnerDeletion:(*bool)(0xc0048a9ab3)}}
Mar 24 03:58:44.452: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b78bad3f-289d-4c4d-9470-c8304cf9c5c8", Controller:(*bool)(0xc003272722), BlockOwnerDeletion:(*bool)(0xc003272723)}}
Mar 24 03:58:44.456: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"582d74e2-fc41-43b7-b82f-788baa00a906", Controller:(*bool)(0xc003fae2b2), BlockOwnerDeletion:(*bool)(0xc003fae2b3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:58:49.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-37" for this suite.

â€¢ [SLOW TEST:5.183 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":309,"skipped":5313,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:58:49.476: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7987
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 24 03:58:49.608: INFO: Waiting up to 5m0s for pod "pod-9c6a3023-12b2-4a8c-94f3-c00514f898e8" in namespace "emptydir-7987" to be "Succeeded or Failed"
Mar 24 03:58:49.610: INFO: Pod "pod-9c6a3023-12b2-4a8c-94f3-c00514f898e8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.864721ms
Mar 24 03:58:51.615: INFO: Pod "pod-9c6a3023-12b2-4a8c-94f3-c00514f898e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007091097s
STEP: Saw pod success
Mar 24 03:58:51.615: INFO: Pod "pod-9c6a3023-12b2-4a8c-94f3-c00514f898e8" satisfied condition "Succeeded or Failed"
Mar 24 03:58:51.617: INFO: Trying to get logs from node cn-hongkong.192.168.0.14 pod pod-9c6a3023-12b2-4a8c-94f3-c00514f898e8 container test-container: <nil>
STEP: delete the pod
Mar 24 03:58:51.640: INFO: Waiting for pod pod-9c6a3023-12b2-4a8c-94f3-c00514f898e8 to disappear
Mar 24 03:58:51.641: INFO: Pod pod-9c6a3023-12b2-4a8c-94f3-c00514f898e8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:58:51.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7987" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":310,"skipped":5314,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 24 03:58:51.649: INFO: >>> kubeConfig: /tmp/kubeconfig-543228437
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1127
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9081
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2785
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 24 03:59:05.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1127" for this suite.
STEP: Destroying namespace "nsdeletetest-9081" for this suite.
Mar 24 03:59:05.061: INFO: Namespace nsdeletetest-9081 was already deleted
STEP: Destroying namespace "nsdeletetest-2785" for this suite.

â€¢ [SLOW TEST:13.415 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":311,"skipped":5346,"failed":0}
SSSSSSSSSSMar 24 03:59:05.064: INFO: Running AfterSuite actions on all nodes
Mar 24 03:59:05.064: INFO: Running AfterSuite actions on node 1
Mar 24 03:59:05.064: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 5541.633 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
PASS

Ginkgo ran 1 suite in 1h32m22.760461194s
Test Suite Passed
