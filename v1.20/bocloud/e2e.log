I1223 13:24:50.446460      20 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-248507315
I1223 13:24:50.446509      20 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I1223 13:24:50.446760      20 e2e.go:129] Starting e2e run "aad65d35-1944-4c91-b392-f02573ae4d26" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1608729888 - Will randomize all specs
Will run 311 of 5667 specs

Dec 23 13:24:50.464: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 13:24:50.467: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 23 13:24:50.495: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 23 13:24:50.567: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 23 13:24:50.568: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Dec 23 13:24:50.568: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 23 13:24:50.580: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec 23 13:24:50.580: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec 23 13:24:50.580: INFO: e2e test version: v1.20.0
Dec 23 13:24:50.583: INFO: kube-apiserver version: v1.20.1
Dec 23 13:24:50.583: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 13:24:50.590: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:24:50.590: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename events
Dec 23 13:24:50.653: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:24:50.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8156" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":1,"skipped":32,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:24:50.708: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Dec 23 13:24:50.762: INFO: Waiting up to 5m0s for pod "test-pod-227af9c6-5f60-4922-9447-3e03a5d8ae56" in namespace "svcaccounts-8800" to be "Succeeded or Failed"
Dec 23 13:24:50.771: INFO: Pod "test-pod-227af9c6-5f60-4922-9447-3e03a5d8ae56": Phase="Pending", Reason="", readiness=false. Elapsed: 9.575065ms
Dec 23 13:24:52.780: INFO: Pod "test-pod-227af9c6-5f60-4922-9447-3e03a5d8ae56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018173444s
Dec 23 13:24:54.811: INFO: Pod "test-pod-227af9c6-5f60-4922-9447-3e03a5d8ae56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049783044s
STEP: Saw pod success
Dec 23 13:24:54.812: INFO: Pod "test-pod-227af9c6-5f60-4922-9447-3e03a5d8ae56" satisfied condition "Succeeded or Failed"
Dec 23 13:24:54.814: INFO: Trying to get logs from node wt-k8s-3.novalocal pod test-pod-227af9c6-5f60-4922-9447-3e03a5d8ae56 container agnhost-container: <nil>
STEP: delete the pod
Dec 23 13:24:55.306: INFO: Waiting for pod test-pod-227af9c6-5f60-4922-9447-3e03a5d8ae56 to disappear
Dec 23 13:24:55.311: INFO: Pod test-pod-227af9c6-5f60-4922-9447-3e03a5d8ae56 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:24:55.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8800" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":2,"skipped":59,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:24:55.319: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 23 13:24:55.654: INFO: Pod name wrapped-volume-race-6c99b5e7-c655-4c9e-98b6-fe3af9a04067: Found 3 pods out of 5
Dec 23 13:25:00.671: INFO: Pod name wrapped-volume-race-6c99b5e7-c655-4c9e-98b6-fe3af9a04067: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6c99b5e7-c655-4c9e-98b6-fe3af9a04067 in namespace emptydir-wrapper-3815, will wait for the garbage collector to delete the pods
Dec 23 13:25:12.762: INFO: Deleting ReplicationController wrapped-volume-race-6c99b5e7-c655-4c9e-98b6-fe3af9a04067 took: 7.642553ms
Dec 23 13:25:13.362: INFO: Terminating ReplicationController wrapped-volume-race-6c99b5e7-c655-4c9e-98b6-fe3af9a04067 pods took: 600.459878ms
STEP: Creating RC which spawns configmap-volume pods
Dec 23 13:26:09.394: INFO: Pod name wrapped-volume-race-42d2fb99-e623-4507-b19e-de11ebb5f87f: Found 0 pods out of 5
Dec 23 13:26:14.409: INFO: Pod name wrapped-volume-race-42d2fb99-e623-4507-b19e-de11ebb5f87f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-42d2fb99-e623-4507-b19e-de11ebb5f87f in namespace emptydir-wrapper-3815, will wait for the garbage collector to delete the pods
Dec 23 13:26:26.494: INFO: Deleting ReplicationController wrapped-volume-race-42d2fb99-e623-4507-b19e-de11ebb5f87f took: 6.874163ms
Dec 23 13:26:27.094: INFO: Terminating ReplicationController wrapped-volume-race-42d2fb99-e623-4507-b19e-de11ebb5f87f pods took: 600.243806ms
STEP: Creating RC which spawns configmap-volume pods
Dec 23 13:26:58.019: INFO: Pod name wrapped-volume-race-b37c3a03-9725-4ce0-a65c-06c0fabb3d85: Found 0 pods out of 5
Dec 23 13:27:03.039: INFO: Pod name wrapped-volume-race-b37c3a03-9725-4ce0-a65c-06c0fabb3d85: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b37c3a03-9725-4ce0-a65c-06c0fabb3d85 in namespace emptydir-wrapper-3815, will wait for the garbage collector to delete the pods
Dec 23 13:27:15.130: INFO: Deleting ReplicationController wrapped-volume-race-b37c3a03-9725-4ce0-a65c-06c0fabb3d85 took: 6.744754ms
Dec 23 13:27:15.731: INFO: Terminating ReplicationController wrapped-volume-race-b37c3a03-9725-4ce0-a65c-06c0fabb3d85 pods took: 600.865636ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:28:09.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3815" for this suite.

• [SLOW TEST:194.263 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":3,"skipped":110,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:28:09.583: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-7498
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7498 to expose endpoints map[]
Dec 23 13:28:09.713: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec 23 13:28:10.726: INFO: successfully validated that service endpoint-test2 in namespace services-7498 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7498
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7498 to expose endpoints map[pod1:[80]]
Dec 23 13:28:12.801: INFO: successfully validated that service endpoint-test2 in namespace services-7498 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-7498
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7498 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 23 13:28:14.839: INFO: successfully validated that service endpoint-test2 in namespace services-7498 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-7498
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7498 to expose endpoints map[pod2:[80]]
Dec 23 13:28:14.884: INFO: successfully validated that service endpoint-test2 in namespace services-7498 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-7498
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7498 to expose endpoints map[]
Dec 23 13:28:14.969: INFO: successfully validated that service endpoint-test2 in namespace services-7498 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:28:15.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7498" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:5.496 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":4,"skipped":137,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:28:15.081: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Dec 23 13:28:15.200: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 23 13:28:15.200: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 23 13:28:15.208: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 23 13:28:15.208: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 23 13:28:15.295: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 23 13:28:15.295: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 23 13:28:15.338: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 23 13:28:15.338: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 23 13:28:17.420: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec 23 13:28:17.420: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec 23 13:28:17.437: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Dec 23 13:28:17.448: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Dec 23 13:28:17.451: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 0
Dec 23 13:28:17.451: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 0
Dec 23 13:28:17.451: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 0
Dec 23 13:28:17.452: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 0
Dec 23 13:28:17.452: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 0
Dec 23 13:28:17.452: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 0
Dec 23 13:28:17.452: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 0
Dec 23 13:28:17.452: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 0
Dec 23 13:28:17.452: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1
Dec 23 13:28:17.452: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1
Dec 23 13:28:17.452: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 2
Dec 23 13:28:17.452: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 2
Dec 23 13:28:17.452: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 2
Dec 23 13:28:17.452: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 2
Dec 23 13:28:17.464: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 2
Dec 23 13:28:17.464: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 2
Dec 23 13:28:17.507: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 2
Dec 23 13:28:17.507: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 2
Dec 23 13:28:17.551: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 2
Dec 23 13:28:17.551: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 2
Dec 23 13:28:17.560: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1
STEP: listing Deployments
Dec 23 13:28:17.567: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Dec 23 13:28:17.587: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Dec 23 13:28:17.600: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 23 13:28:17.609: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 23 13:28:17.648: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 23 13:28:17.670: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 23 13:28:17.688: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 23 13:28:17.715: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 23 13:28:17.754: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 23 13:28:17.766: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Dec 23 13:28:20.043: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1
Dec 23 13:28:20.043: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1
Dec 23 13:28:20.044: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1
Dec 23 13:28:20.044: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1
Dec 23 13:28:20.044: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1
Dec 23 13:28:20.044: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1
Dec 23 13:28:20.044: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1
Dec 23 13:28:20.044: INFO: observed Deployment test-deployment in namespace deployment-7349 with ReadyReplicas 1
STEP: deleting the Deployment
Dec 23 13:28:20.080: INFO: observed event type MODIFIED
Dec 23 13:28:20.080: INFO: observed event type MODIFIED
Dec 23 13:28:20.080: INFO: observed event type MODIFIED
Dec 23 13:28:20.080: INFO: observed event type MODIFIED
Dec 23 13:28:20.080: INFO: observed event type MODIFIED
Dec 23 13:28:20.080: INFO: observed event type MODIFIED
Dec 23 13:28:20.085: INFO: observed event type MODIFIED
Dec 23 13:28:20.085: INFO: observed event type MODIFIED
Dec 23 13:28:20.085: INFO: observed event type MODIFIED
Dec 23 13:28:20.085: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Dec 23 13:28:20.099: INFO: Log out all the ReplicaSets if there is no deployment created
Dec 23 13:28:20.110: INFO: ReplicaSet "test-deployment-54c74f55cd":
&ReplicaSet{ObjectMeta:{test-deployment-54c74f55cd  deployment-7349  85ba48c1-d3da-40d6-b4a0-643cda6c69ea 78871 3 2020-12-23 13:28:03 +0000 UTC <nil> <nil> map[pod-template-hash:54c74f55cd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment cc094f24-3a92-464c-8d12-52fea81ad424 0xc001d45867 0xc001d45868}] []  [{kube-controller-manager Update apps/v1 2020-12-23 13:28:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc094f24-3a92-464c-8d12-52fea81ad424\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54c74f55cd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54c74f55cd test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment abcsys.cn:5000/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001d458d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Dec 23 13:28:20.127: INFO: pod: "test-deployment-54c74f55cd-ds27k":
&Pod{ObjectMeta:{test-deployment-54c74f55cd-ds27k test-deployment-54c74f55cd- deployment-7349  e97c9cde-081a-4f93-8514-2246077b39e9 78850 0 2020-12-23 13:28:03 +0000 UTC <nil> <nil> map[pod-template-hash:54c74f55cd test-deployment-static:true] map[cni.projectcalico.org/podIP:10.244.201.200/32 cni.projectcalico.org/podIPs:10.244.201.200/32] [{apps/v1 ReplicaSet test-deployment-54c74f55cd 85ba48c1-d3da-40d6-b4a0-643cda6c69ea 0xc0020e2617 0xc0020e2618}] []  [{kube-controller-manager Update v1 2020-12-23 13:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ba48c1-d3da-40d6-b4a0-643cda6c69ea\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-23 13:28:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-23 13:28:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.201.200\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wxxm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wxxm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wxxm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 13:28:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 13:28:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 13:28:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 13:28:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.25,PodIP:10.244.201.200,StartTime:2020-12-23 13:28:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-23 13:28:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,ImageID:docker-pullable://abcsys.cn:5000/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://5eb8a1711aec04cd976654e7dfa6c116cf746e7689829d420a82fbf635e7c7ca,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.201.200,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 23 13:28:20.127: INFO: pod: "test-deployment-54c74f55cd-dxc7w":
&Pod{ObjectMeta:{test-deployment-54c74f55cd-dxc7w test-deployment-54c74f55cd- deployment-7349  7df5fd59-614d-4e60-af82-6c7b1a3dabcd 78867 0 2020-12-23 13:28:06 +0000 UTC <nil> <nil> map[pod-template-hash:54c74f55cd test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54c74f55cd 85ba48c1-d3da-40d6-b4a0-643cda6c69ea 0xc0020e27d7 0xc0020e27d8}] []  [{kube-controller-manager Update v1 2020-12-23 13:28:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ba48c1-d3da-40d6-b4a0-643cda6c69ea\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-23 13:28:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wxxm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wxxm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wxxm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 13:28:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 13:28:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 13:28:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 13:28:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.25,PodIP:,StartTime:2020-12-23 13:28:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 23 13:28:20.127: INFO: ReplicaSet "test-deployment-7d458fdf74":
&ReplicaSet{ObjectMeta:{test-deployment-7d458fdf74  deployment-7349  2e465e3c-38ba-4360-a276-016cb6ab28be 78868 4 2020-12-23 13:28:03 +0000 UTC <nil> <nil> map[pod-template-hash:7d458fdf74 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment cc094f24-3a92-464c-8d12-52fea81ad424 0xc001d45937 0xc001d45938}] []  [{kube-controller-manager Update apps/v1 2020-12-23 13:28:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc094f24-3a92-464c-8d12-52fea81ad424\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7d458fdf74,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7d458fdf74 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment abcsys.cn:5000/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001d459b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:28:20.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7349" for this suite.

• [SLOW TEST:5.099 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":5,"skipped":170,"failed":0}
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:28:20.180: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-d793e6c1-5311-437a-b1a0-3a61caa6f886
STEP: Creating a pod to test consume secrets
Dec 23 13:28:20.266: INFO: Waiting up to 5m0s for pod "pod-secrets-77a918b3-7f57-4ae1-bd60-bfc94a65bef6" in namespace "secrets-6869" to be "Succeeded or Failed"
Dec 23 13:28:20.271: INFO: Pod "pod-secrets-77a918b3-7f57-4ae1-bd60-bfc94a65bef6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.372137ms
Dec 23 13:28:22.279: INFO: Pod "pod-secrets-77a918b3-7f57-4ae1-bd60-bfc94a65bef6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013131121s
STEP: Saw pod success
Dec 23 13:28:22.279: INFO: Pod "pod-secrets-77a918b3-7f57-4ae1-bd60-bfc94a65bef6" satisfied condition "Succeeded or Failed"
Dec 23 13:28:22.282: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-secrets-77a918b3-7f57-4ae1-bd60-bfc94a65bef6 container secret-env-test: <nil>
STEP: delete the pod
Dec 23 13:28:22.320: INFO: Waiting for pod pod-secrets-77a918b3-7f57-4ae1-bd60-bfc94a65bef6 to disappear
Dec 23 13:28:22.333: INFO: Pod pod-secrets-77a918b3-7f57-4ae1-bd60-bfc94a65bef6 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:28:22.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6869" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":6,"skipped":171,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:28:22.345: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:28:22.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2878" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":7,"skipped":196,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:28:22.451: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:28:35.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4728" for this suite.

• [SLOW TEST:13.136 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":8,"skipped":221,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:28:35.587: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1223 13:28:45.766022      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 23 13:29:47.784: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Dec 23 13:29:47.784: INFO: Deleting pod "simpletest-rc-to-be-deleted-2nj9t" in namespace "gc-2537"
Dec 23 13:29:47.805: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5722" in namespace "gc-2537"
Dec 23 13:29:47.830: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkb28" in namespace "gc-2537"
Dec 23 13:29:47.853: INFO: Deleting pod "simpletest-rc-to-be-deleted-gbckx" in namespace "gc-2537"
Dec 23 13:29:47.874: INFO: Deleting pod "simpletest-rc-to-be-deleted-hk4m7" in namespace "gc-2537"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:29:47.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2537" for this suite.

• [SLOW TEST:72.325 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":9,"skipped":223,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:29:47.913: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:29:59.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5015" for this suite.

• [SLOW TEST:11.278 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":10,"skipped":227,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:29:59.191: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:30:16.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4144" for this suite.

• [SLOW TEST:17.114 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":11,"skipped":237,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:30:16.305: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:30:16.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5728" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":12,"skipped":244,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:30:16.378: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Dec 23 13:30:16.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-312 run logs-generator --image=abcsys.cn:5000/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 23 13:30:17.028: INFO: stderr: ""
Dec 23 13:30:17.028: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Dec 23 13:30:17.028: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 23 13:30:17.029: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-312" to be "running and ready, or succeeded"
Dec 23 13:30:17.037: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008296ms
Dec 23 13:30:19.046: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017881093s
Dec 23 13:30:21.055: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.026174675s
Dec 23 13:30:21.055: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 23 13:30:21.055: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Dec 23 13:30:21.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-312 logs logs-generator logs-generator'
Dec 23 13:30:21.211: INFO: stderr: ""
Dec 23 13:30:21.211: INFO: stdout: "I1223 13:30:04.780497       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/ptn5 509\nI1223 13:30:04.980456       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/fx5h 446\nI1223 13:30:05.180514       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/4jm 299\nI1223 13:30:05.380499       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/84r 582\nI1223 13:30:05.580517       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/fhtr 200\nI1223 13:30:05.780485       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/n6r 329\nI1223 13:30:05.980461       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/w5x 534\nI1223 13:30:06.180470       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/j94k 533\nI1223 13:30:06.380512       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/z7gh 208\nI1223 13:30:06.580480       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/rbcp 422\nI1223 13:30:06.780376       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/b6dd 366\nI1223 13:30:06.980487       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/grf 553\nI1223 13:30:07.180489       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/55zw 423\nI1223 13:30:07.380481       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/2d8 405\n"
STEP: limiting log lines
Dec 23 13:30:21.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-312 logs logs-generator logs-generator --tail=1'
Dec 23 13:30:21.350: INFO: stderr: ""
Dec 23 13:30:21.350: INFO: stdout: "I1223 13:30:07.580488       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/gwnz 272\n"
Dec 23 13:30:21.350: INFO: got output "I1223 13:30:07.580488       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/gwnz 272\n"
STEP: limiting log bytes
Dec 23 13:30:21.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-312 logs logs-generator logs-generator --limit-bytes=1'
Dec 23 13:30:21.638: INFO: stderr: ""
Dec 23 13:30:21.638: INFO: stdout: "I"
Dec 23 13:30:21.638: INFO: got output "I"
STEP: exposing timestamps
Dec 23 13:30:21.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-312 logs logs-generator logs-generator --tail=1 --timestamps'
Dec 23 13:30:21.775: INFO: stderr: ""
Dec 23 13:30:21.775: INFO: stdout: "2020-12-23T13:30:07.980632631Z I1223 13:30:07.980393       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/gv28 591\n"
Dec 23 13:30:21.775: INFO: got output "2020-12-23T13:30:07.980632631Z I1223 13:30:07.980393       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/gv28 591\n"
STEP: restricting to a time range
Dec 23 13:30:24.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-312 logs logs-generator logs-generator --since=1s'
Dec 23 13:30:24.475: INFO: stderr: ""
Dec 23 13:30:24.475: INFO: stdout: "I1223 13:30:09.780476       1 logs_generator.go:76] 25 GET /api/v1/namespaces/ns/pods/8kf 425\nI1223 13:30:09.980468       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/jb8z 305\nI1223 13:30:10.180524       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/xbnp 330\nI1223 13:30:10.380627       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/2pm 232\nI1223 13:30:10.581351       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/qklc 357\n"
Dec 23 13:30:24.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-312 logs logs-generator logs-generator --since=24h'
Dec 23 13:30:24.603: INFO: stderr: ""
Dec 23 13:30:24.603: INFO: stdout: "I1223 13:30:04.780497       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/ptn5 509\nI1223 13:30:04.980456       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/fx5h 446\nI1223 13:30:05.180514       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/4jm 299\nI1223 13:30:05.380499       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/84r 582\nI1223 13:30:05.580517       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/fhtr 200\nI1223 13:30:05.780485       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/n6r 329\nI1223 13:30:05.980461       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/w5x 534\nI1223 13:30:06.180470       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/j94k 533\nI1223 13:30:06.380512       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/z7gh 208\nI1223 13:30:06.580480       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/rbcp 422\nI1223 13:30:06.780376       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/b6dd 366\nI1223 13:30:06.980487       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/grf 553\nI1223 13:30:07.180489       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/55zw 423\nI1223 13:30:07.380481       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/2d8 405\nI1223 13:30:07.580488       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/gwnz 272\nI1223 13:30:07.780557       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/5l9 404\nI1223 13:30:07.980393       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/gv28 591\nI1223 13:30:08.180506       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/pfc 421\nI1223 13:30:08.380540       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/mcd 549\nI1223 13:30:08.580393       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/wz9v 483\nI1223 13:30:08.780493       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/q9n 361\nI1223 13:30:08.980759       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/pfhs 300\nI1223 13:30:09.180501       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/4j8 419\nI1223 13:30:09.382356       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/grt5 500\nI1223 13:30:09.580484       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/plqx 264\nI1223 13:30:09.780476       1 logs_generator.go:76] 25 GET /api/v1/namespaces/ns/pods/8kf 425\nI1223 13:30:09.980468       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/jb8z 305\nI1223 13:30:10.180524       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/xbnp 330\nI1223 13:30:10.380627       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/2pm 232\nI1223 13:30:10.581351       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/qklc 357\nI1223 13:30:10.780485       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/ns/pods/nr27 206\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Dec 23 13:30:24.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-312 delete pod logs-generator'
Dec 23 13:31:09.260: INFO: stderr: ""
Dec 23 13:31:09.261: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:31:09.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-312" for this suite.

• [SLOW TEST:52.892 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":13,"skipped":262,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:31:09.270: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Dec 23 13:33:09.856: INFO: Successfully updated pod "var-expansion-45045034-136d-4bcd-ab57-943c4c8cc8e3"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Dec 23 13:33:11.920: INFO: Deleting pod "var-expansion-45045034-136d-4bcd-ab57-943c4c8cc8e3" in namespace "var-expansion-8066"
Dec 23 13:33:11.926: INFO: Wait up to 5m0s for pod "var-expansion-45045034-136d-4bcd-ab57-943c4c8cc8e3" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:33:49.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8066" for this suite.

• [SLOW TEST:160.677 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":14,"skipped":266,"failed":0}
SS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:33:49.948: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Dec 23 13:33:49.984: INFO: Major version: 1
STEP: Confirm minor version
Dec 23 13:33:49.984: INFO: cleanMinorVersion: 20
Dec 23 13:33:49.984: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:33:49.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8617" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":15,"skipped":268,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:33:49.992: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec 23 13:33:54.580: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4828 pod-service-account-60159b5b-955c-4ef6-9459-a7d51367e433 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 23 13:33:54.802: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4828 pod-service-account-60159b5b-955c-4ef6-9459-a7d51367e433 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 23 13:33:55.050: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4828 pod-service-account-60159b5b-955c-4ef6-9459-a7d51367e433 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:33:55.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4828" for this suite.

• [SLOW TEST:5.294 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":16,"skipped":282,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:33:55.286: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 23 13:33:55.321: INFO: Waiting up to 5m0s for pod "pod-0e8014c3-ded0-41b0-9010-58e4c4ea3a8c" in namespace "emptydir-4483" to be "Succeeded or Failed"
Dec 23 13:33:55.325: INFO: Pod "pod-0e8014c3-ded0-41b0-9010-58e4c4ea3a8c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.802831ms
Dec 23 13:33:57.336: INFO: Pod "pod-0e8014c3-ded0-41b0-9010-58e4c4ea3a8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0149813s
STEP: Saw pod success
Dec 23 13:33:57.336: INFO: Pod "pod-0e8014c3-ded0-41b0-9010-58e4c4ea3a8c" satisfied condition "Succeeded or Failed"
Dec 23 13:33:57.340: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-0e8014c3-ded0-41b0-9010-58e4c4ea3a8c container test-container: <nil>
STEP: delete the pod
Dec 23 13:33:57.375: INFO: Waiting for pod pod-0e8014c3-ded0-41b0-9010-58e4c4ea3a8c to disappear
Dec 23 13:33:57.377: INFO: Pod pod-0e8014c3-ded0-41b0-9010-58e4c4ea3a8c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:33:57.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4483" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":17,"skipped":294,"failed":0}
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:33:57.386: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-b13b3188-17c2-4d5a-891e-5eeabb448614
STEP: Creating secret with name secret-projected-all-test-volume-8f23c8c7-4b4b-4057-8c4b-365b01cae322
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 23 13:33:57.426: INFO: Waiting up to 5m0s for pod "projected-volume-0c0f59ec-f313-486d-961a-4a499a5bb74c" in namespace "projected-9209" to be "Succeeded or Failed"
Dec 23 13:33:57.429: INFO: Pod "projected-volume-0c0f59ec-f313-486d-961a-4a499a5bb74c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.193923ms
Dec 23 13:33:59.435: INFO: Pod "projected-volume-0c0f59ec-f313-486d-961a-4a499a5bb74c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008647381s
STEP: Saw pod success
Dec 23 13:33:59.435: INFO: Pod "projected-volume-0c0f59ec-f313-486d-961a-4a499a5bb74c" satisfied condition "Succeeded or Failed"
Dec 23 13:33:59.437: INFO: Trying to get logs from node wt-k8s-3.novalocal pod projected-volume-0c0f59ec-f313-486d-961a-4a499a5bb74c container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 23 13:33:59.460: INFO: Waiting for pod projected-volume-0c0f59ec-f313-486d-961a-4a499a5bb74c to disappear
Dec 23 13:33:59.468: INFO: Pod projected-volume-0c0f59ec-f313-486d-961a-4a499a5bb74c no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:33:59.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9209" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":18,"skipped":295,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:33:59.483: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
Dec 23 13:34:00.517: INFO: role binding webhook-auth-reader already exists
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 23 13:34:00.562: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 23 13:34:02.578: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744327226, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744327226, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744327226, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744327226, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-76bbb7fdd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 13:34:05.666: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
Dec 23 13:34:05.825: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:05.985: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:06.083: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:06.184: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:06.284: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:06.385: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:06.487: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:06.594: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:06.685: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:06.798: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:06.884: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:06.982: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:07.127: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:07.315: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:07.389: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:07.486: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:07.590: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:07.699: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:07.787: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:07.884: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:07.981: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:08.081: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:08.182: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:08.282: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:08.392: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:08.486: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:08.586: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:08.690: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:08.797: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:08.898: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:08.979: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:09.087: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:09.183: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:09.287: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:09.395: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:09.479: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:09.580: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:09.686: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:09.831: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:09.989: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:10.079: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:10.183: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:10.284: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:10.402: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:10.500: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:10.582: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:10.688: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:10.785: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:10.881: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:10.981: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:11.083: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:11.183: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:11.286: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:11.412: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:11.483: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:11.589: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:11.681: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:11.794: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:11.896: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:11.983: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:12.097: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:12.187: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:12.285: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:12.384: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:12.480: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:12.586: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:12.698: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:12.787: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:34:12.880: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:34:13.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5478" for this suite.
STEP: Destroying namespace "webhook-5478-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:13.654 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":19,"skipped":296,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:34:13.137: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9242.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9242.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9242.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9242.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9242.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9242.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9242.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9242.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9242.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9242.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 23 13:34:17.266: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:17.268: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:17.271: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:17.274: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:17.282: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:17.285: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:17.287: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:17.290: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:17.295: INFO: Lookups using dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9242.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9242.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local jessie_udp@dns-test-service-2.dns-9242.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9242.svc.cluster.local]

Dec 23 13:34:22.301: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:22.304: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:22.306: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:22.309: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:22.316: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:22.319: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:22.322: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:22.324: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:22.330: INFO: Lookups using dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9242.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9242.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local jessie_udp@dns-test-service-2.dns-9242.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9242.svc.cluster.local]

Dec 23 13:34:27.300: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:27.304: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:27.307: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:27.312: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:27.322: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:27.326: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:27.329: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:27.332: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:27.338: INFO: Lookups using dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9242.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9242.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local jessie_udp@dns-test-service-2.dns-9242.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9242.svc.cluster.local]

Dec 23 13:34:32.300: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:32.303: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:32.305: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:32.308: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:32.316: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:32.319: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:32.322: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:32.324: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:32.329: INFO: Lookups using dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9242.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9242.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local jessie_udp@dns-test-service-2.dns-9242.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9242.svc.cluster.local]

Dec 23 13:34:37.300: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:37.303: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:37.306: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:37.313: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:37.324: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:37.327: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:37.331: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:37.334: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:37.340: INFO: Lookups using dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9242.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9242.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local jessie_udp@dns-test-service-2.dns-9242.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9242.svc.cluster.local]

Dec 23 13:34:42.301: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:42.304: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:42.307: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:42.311: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:42.322: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:42.325: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:42.328: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:42.331: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9242.svc.cluster.local from pod dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655: the server could not find the requested resource (get pods dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655)
Dec 23 13:34:42.338: INFO: Lookups using dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9242.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9242.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9242.svc.cluster.local jessie_udp@dns-test-service-2.dns-9242.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9242.svc.cluster.local]

Dec 23 13:34:47.333: INFO: DNS probes using dns-9242/dns-test-2d5827ec-5dfc-43f6-af81-6b8808b0f655 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:34:47.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9242" for this suite.

• [SLOW TEST:34.297 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":20,"skipped":322,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:34:47.434: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Dec 23 13:34:47.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-6873 api-versions'
Dec 23 13:34:47.597: INFO: stderr: ""
Dec 23 13:34:47.597: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:34:47.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6873" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":21,"skipped":326,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:34:47.609: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 13:34:47.668: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 23 13:34:52.731: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 23 13:34:52.731: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Dec 23 13:34:52.779: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-891  d0571650-1637-4227-8708-90622a4cfac0 80490 1 2020-12-23 13:34:38 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2020-12-23 13:34:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost abcsys.cn:5000/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b72498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Dec 23 13:34:52.806: INFO: New ReplicaSet "test-cleanup-deployment-f9457b7f7" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-f9457b7f7  deployment-891  cfa0b25b-62d4-4dfa-902f-944e2aaf9f8f 80497 1 2020-12-23 13:34:38 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:f9457b7f7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment d0571650-1637-4227-8708-90622a4cfac0 0xc0020e2a80 0xc0020e2a81}] []  [{kube-controller-manager Update apps/v1 2020-12-23 13:34:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0571650-1637-4227-8708-90622a4cfac0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: f9457b7f7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:f9457b7f7] map[] [] []  []} {[] [] [{agnhost abcsys.cn:5000/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0020e2af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 23 13:34:52.806: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Dec 23 13:34:52.806: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-891  ccad194b-3647-4c12-bee6-24aa3cb87ba7 80491 1 2020-12-23 13:34:33 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment d0571650-1637-4227-8708-90622a4cfac0 0xc0020e2977 0xc0020e2978}] []  [{e2e.test Update apps/v1 2020-12-23 13:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-23 13:34:38 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"d0571650-1637-4227-8708-90622a4cfac0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd abcsys.cn:5000/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0020e2a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 23 13:34:52.818: INFO: Pod "test-cleanup-controller-6s5j2" is available:
&Pod{ObjectMeta:{test-cleanup-controller-6s5j2 test-cleanup-controller- deployment-891  bb34c129-7a18-4a89-9a8b-dfb6909098fa 80469 0 2020-12-23 13:34:33 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.244.201.222/32 cni.projectcalico.org/podIPs:10.244.201.222/32] [{apps/v1 ReplicaSet test-cleanup-controller ccad194b-3647-4c12-bee6-24aa3cb87ba7 0xc002b728c7 0xc002b728c8}] []  [{kube-controller-manager Update v1 2020-12-23 13:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccad194b-3647-4c12-bee6-24aa3cb87ba7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-23 13:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-23 13:34:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.201.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lpd9p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lpd9p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lpd9p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 13:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 13:34:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 13:34:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 13:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.25,PodIP:10.244.201.222,StartTime:2020-12-23 13:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-23 13:34:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,ImageID:docker-pullable://abcsys.cn:5000/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://a11eb862715e2f92304ceea374eaf8899adb4a22fb2f3e22ae7269694636d978,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.201.222,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 13:34:52.819: INFO: Pod "test-cleanup-deployment-f9457b7f7-dzzls" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-f9457b7f7-dzzls test-cleanup-deployment-f9457b7f7- deployment-891  d9de2148-4665-45a6-80e9-6d65a157fe21 80499 0 2020-12-23 13:34:38 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:f9457b7f7] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-f9457b7f7 cfa0b25b-62d4-4dfa-902f-944e2aaf9f8f 0xc002b72a80 0xc002b72a81}] []  [{kube-controller-manager Update v1 2020-12-23 13:34:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cfa0b25b-62d4-4dfa-902f-944e2aaf9f8f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lpd9p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lpd9p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:abcsys.cn:5000/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lpd9p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 13:34:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:34:52.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-891" for this suite.

• [SLOW TEST:5.237 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":22,"skipped":380,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:34:52.847: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Dec 23 13:34:52.915: INFO: Waiting up to 5m0s for pod "client-containers-2b03f72e-c302-451d-a2e7-4dc49bb83605" in namespace "containers-873" to be "Succeeded or Failed"
Dec 23 13:34:52.919: INFO: Pod "client-containers-2b03f72e-c302-451d-a2e7-4dc49bb83605": Phase="Pending", Reason="", readiness=false. Elapsed: 3.509348ms
Dec 23 13:34:54.927: INFO: Pod "client-containers-2b03f72e-c302-451d-a2e7-4dc49bb83605": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011860729s
Dec 23 13:34:56.935: INFO: Pod "client-containers-2b03f72e-c302-451d-a2e7-4dc49bb83605": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020263997s
STEP: Saw pod success
Dec 23 13:34:56.935: INFO: Pod "client-containers-2b03f72e-c302-451d-a2e7-4dc49bb83605" satisfied condition "Succeeded or Failed"
Dec 23 13:34:56.938: INFO: Trying to get logs from node wt-k8s-3.novalocal pod client-containers-2b03f72e-c302-451d-a2e7-4dc49bb83605 container agnhost-container: <nil>
STEP: delete the pod
Dec 23 13:34:56.959: INFO: Waiting for pod client-containers-2b03f72e-c302-451d-a2e7-4dc49bb83605 to disappear
Dec 23 13:34:56.962: INFO: Pod client-containers-2b03f72e-c302-451d-a2e7-4dc49bb83605 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:34:56.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-873" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":23,"skipped":391,"failed":0}

------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:34:56.969: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Dec 23 13:34:57.006: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-511 proxy --unix-socket=/tmp/kubectl-proxy-unix748947722/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:34:57.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-511" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":24,"skipped":391,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:34:57.099: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Dec 23 13:34:57.143: INFO: Waiting up to 5m0s for pod "client-containers-3568092a-7b2b-413a-b7cf-2de5a3718810" in namespace "containers-7225" to be "Succeeded or Failed"
Dec 23 13:34:57.148: INFO: Pod "client-containers-3568092a-7b2b-413a-b7cf-2de5a3718810": Phase="Pending", Reason="", readiness=false. Elapsed: 5.560794ms
Dec 23 13:34:59.156: INFO: Pod "client-containers-3568092a-7b2b-413a-b7cf-2de5a3718810": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012895648s
Dec 23 13:35:01.163: INFO: Pod "client-containers-3568092a-7b2b-413a-b7cf-2de5a3718810": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020408318s
STEP: Saw pod success
Dec 23 13:35:01.163: INFO: Pod "client-containers-3568092a-7b2b-413a-b7cf-2de5a3718810" satisfied condition "Succeeded or Failed"
Dec 23 13:35:01.165: INFO: Trying to get logs from node wt-k8s-3.novalocal pod client-containers-3568092a-7b2b-413a-b7cf-2de5a3718810 container agnhost-container: <nil>
STEP: delete the pod
Dec 23 13:35:01.184: INFO: Waiting for pod client-containers-3568092a-7b2b-413a-b7cf-2de5a3718810 to disappear
Dec 23 13:35:01.187: INFO: Pod client-containers-3568092a-7b2b-413a-b7cf-2de5a3718810 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:35:01.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7225" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":25,"skipped":455,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:35:01.198: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 23 13:35:01.246: INFO: Waiting up to 5m0s for pod "pod-6a6c2b2c-1d1b-4ad5-a481-e350f5e6b1c9" in namespace "emptydir-2038" to be "Succeeded or Failed"
Dec 23 13:35:01.249: INFO: Pod "pod-6a6c2b2c-1d1b-4ad5-a481-e350f5e6b1c9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.047235ms
Dec 23 13:35:03.255: INFO: Pod "pod-6a6c2b2c-1d1b-4ad5-a481-e350f5e6b1c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008670851s
Dec 23 13:35:05.262: INFO: Pod "pod-6a6c2b2c-1d1b-4ad5-a481-e350f5e6b1c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016325674s
STEP: Saw pod success
Dec 23 13:35:05.262: INFO: Pod "pod-6a6c2b2c-1d1b-4ad5-a481-e350f5e6b1c9" satisfied condition "Succeeded or Failed"
Dec 23 13:35:05.264: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-6a6c2b2c-1d1b-4ad5-a481-e350f5e6b1c9 container test-container: <nil>
STEP: delete the pod
Dec 23 13:35:05.290: INFO: Waiting for pod pod-6a6c2b2c-1d1b-4ad5-a481-e350f5e6b1c9 to disappear
Dec 23 13:35:05.293: INFO: Pod pod-6a6c2b2c-1d1b-4ad5-a481-e350f5e6b1c9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:35:05.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2038" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":26,"skipped":551,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:35:05.301: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-c244efa5-a1b9-44ed-ba8f-242c8a0f7bb9 in namespace container-probe-3998
Dec 23 13:35:09.364: INFO: Started pod test-webserver-c244efa5-a1b9-44ed-ba8f-242c8a0f7bb9 in namespace container-probe-3998
STEP: checking the pod's current state and verifying that restartCount is present
Dec 23 13:35:09.366: INFO: Initial restart count of pod test-webserver-c244efa5-a1b9-44ed-ba8f-242c8a0f7bb9 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:39:10.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3998" for this suite.

• [SLOW TEST:245.163 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":27,"skipped":553,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:39:10.471: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 23 13:39:11.232: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 23 13:39:13.245: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744327537, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744327537, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744327537, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744327537, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-76bbb7fdd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 13:39:16.298: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
Dec 23 13:39:16.327: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:16.440: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:16.541: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:16.643: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:16.743: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:16.841: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:16.943: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:17.041: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:17.142: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:17.242: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:17.342: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:17.443: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:17.545: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:17.652: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:17.743: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:17.843: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:17.942: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:18.041: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:18.142: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:18.242: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:18.341: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:18.468: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:18.541: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:18.649: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:18.751: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:18.843: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:18.942: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:19.041: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:19.143: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:19.244: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:19.343: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:19.442: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:19.541: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:19.642: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:19.745: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:19.840: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:19.943: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:20.041: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:20.143: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:20.240: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:20.342: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:20.442: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:20.542: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:20.643: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:20.744: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:20.844: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:20.942: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:21.044: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:21.143: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:21.242: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:21.342: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:21.441: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:21.544: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:21.642: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:21.741: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:21.841: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:21.941: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:22.043: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:22.142: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:22.241: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:22.343: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:22.445: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:22.542: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:22.645: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:22.745: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:22.843: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:22.942: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:23.041: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:23.144: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:23.242: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:23.341: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:23.441: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:23.540: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:23.645: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:23.742: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:39:23.842: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:39:23.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3698" for this suite.
STEP: Destroying namespace "webhook-3698-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:13.594 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":28,"skipped":558,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:39:24.066: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Dec 23 13:39:28.721: INFO: Successfully updated pod "annotationupdate1d8c4d93-6bf4-4515-a8c2-9fd0ccb61629"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:39:30.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3919" for this suite.

• [SLOW TEST:6.695 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":29,"skipped":587,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:39:30.761: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-6313
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 23 13:39:30.810: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 23 13:39:30.853: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 23 13:39:32.861: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 13:39:34.861: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 13:39:36.860: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 13:39:38.858: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 13:39:40.860: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 13:39:42.862: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 13:39:44.862: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 13:39:46.862: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 13:39:48.858: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 13:39:50.861: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 13:39:52.862: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 23 13:39:52.867: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Dec 23 13:39:56.922: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Dec 23 13:39:56.922: INFO: Going to poll 10.244.1.26 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Dec 23 13:39:56.924: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.26 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6313 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 13:39:56.925: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 13:39:58.040: INFO: Found all 1 expected endpoints: [netserver-0]
Dec 23 13:39:58.040: INFO: Going to poll 10.244.201.228 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Dec 23 13:39:58.051: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.201.228 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6313 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 13:39:58.051: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 13:39:59.146: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:39:59.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6313" for this suite.

• [SLOW TEST:28.396 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":30,"skipped":596,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:39:59.158: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Dec 23 13:39:59.209: INFO: Waiting up to 5m0s for pod "var-expansion-20242ce3-b61f-4fcb-9425-763696f572bf" in namespace "var-expansion-2821" to be "Succeeded or Failed"
Dec 23 13:39:59.214: INFO: Pod "var-expansion-20242ce3-b61f-4fcb-9425-763696f572bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.818747ms
Dec 23 13:40:01.222: INFO: Pod "var-expansion-20242ce3-b61f-4fcb-9425-763696f572bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012600433s
STEP: Saw pod success
Dec 23 13:40:01.222: INFO: Pod "var-expansion-20242ce3-b61f-4fcb-9425-763696f572bf" satisfied condition "Succeeded or Failed"
Dec 23 13:40:01.224: INFO: Trying to get logs from node wt-k8s-3.novalocal pod var-expansion-20242ce3-b61f-4fcb-9425-763696f572bf container dapi-container: <nil>
STEP: delete the pod
Dec 23 13:40:01.245: INFO: Waiting for pod var-expansion-20242ce3-b61f-4fcb-9425-763696f572bf to disappear
Dec 23 13:40:01.248: INFO: Pod var-expansion-20242ce3-b61f-4fcb-9425-763696f572bf no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:40:01.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2821" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":31,"skipped":597,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:40:01.263: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-04a858c6-6760-420e-a231-5ea809740968
STEP: Creating a pod to test consume secrets
Dec 23 13:40:01.310: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-26e7b881-074e-47c0-8fbb-2cd7bdde0ec4" in namespace "projected-6634" to be "Succeeded or Failed"
Dec 23 13:40:01.316: INFO: Pod "pod-projected-secrets-26e7b881-074e-47c0-8fbb-2cd7bdde0ec4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.99392ms
Dec 23 13:40:03.330: INFO: Pod "pod-projected-secrets-26e7b881-074e-47c0-8fbb-2cd7bdde0ec4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019536597s
STEP: Saw pod success
Dec 23 13:40:03.330: INFO: Pod "pod-projected-secrets-26e7b881-074e-47c0-8fbb-2cd7bdde0ec4" satisfied condition "Succeeded or Failed"
Dec 23 13:40:03.334: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-projected-secrets-26e7b881-074e-47c0-8fbb-2cd7bdde0ec4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 23 13:40:03.363: INFO: Waiting for pod pod-projected-secrets-26e7b881-074e-47c0-8fbb-2cd7bdde0ec4 to disappear
Dec 23 13:40:03.368: INFO: Pod pod-projected-secrets-26e7b881-074e-47c0-8fbb-2cd7bdde0ec4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:40:03.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6634" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":32,"skipped":599,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:40:03.378: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 23 13:40:03.416: INFO: Waiting up to 5m0s for pod "pod-6cdd91a0-0b61-45b5-bfc2-70ff9122d7df" in namespace "emptydir-7010" to be "Succeeded or Failed"
Dec 23 13:40:03.423: INFO: Pod "pod-6cdd91a0-0b61-45b5-bfc2-70ff9122d7df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.37599ms
Dec 23 13:40:05.434: INFO: Pod "pod-6cdd91a0-0b61-45b5-bfc2-70ff9122d7df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018123225s
STEP: Saw pod success
Dec 23 13:40:05.435: INFO: Pod "pod-6cdd91a0-0b61-45b5-bfc2-70ff9122d7df" satisfied condition "Succeeded or Failed"
Dec 23 13:40:05.440: INFO: Trying to get logs from node wt-k8s-2 pod pod-6cdd91a0-0b61-45b5-bfc2-70ff9122d7df container test-container: <nil>
STEP: delete the pod
Dec 23 13:40:05.483: INFO: Waiting for pod pod-6cdd91a0-0b61-45b5-bfc2-70ff9122d7df to disappear
Dec 23 13:40:05.487: INFO: Pod pod-6cdd91a0-0b61-45b5-bfc2-70ff9122d7df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:40:05.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7010" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":33,"skipped":614,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:40:05.499: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-1376
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1376
STEP: Deleting pre-stop pod
Dec 23 13:40:14.598: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:40:14.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1376" for this suite.

• [SLOW TEST:9.138 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":34,"skipped":655,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:40:14.637: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7082, will wait for the garbage collector to delete the pods
Dec 23 13:40:18.754: INFO: Deleting Job.batch foo took: 6.754198ms
Dec 23 13:40:19.355: INFO: Terminating Job.batch foo pods took: 600.230344ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:40:59.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7082" for this suite.

• [SLOW TEST:44.731 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":35,"skipped":659,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:40:59.368: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Dec 23 13:40:59.419: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:40:59.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4144" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":36,"skipped":674,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:40:59.445: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 13:40:59.519: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e703ed50-b4d6-4e2f-a41a-4ac8e2f6c01a", Controller:(*bool)(0xc0023d8a8e), BlockOwnerDeletion:(*bool)(0xc0023d8a8f)}}
Dec 23 13:40:59.526: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4edbc3ef-97ff-464c-b002-59d564dd04fc", Controller:(*bool)(0xc0006f8e16), BlockOwnerDeletion:(*bool)(0xc0006f8e17)}}
Dec 23 13:40:59.534: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"605316b2-2ea8-47f8-90c3-e08ee4604bf3", Controller:(*bool)(0xc0023d8fb6), BlockOwnerDeletion:(*bool)(0xc0023d8fb7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:41:04.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6074" for this suite.

• [SLOW TEST:5.199 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":37,"skipped":682,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:41:04.644: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 23 13:41:05.380: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 23 13:41:07.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744327651, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744327651, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744327651, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744327651, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-76bbb7fdd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 13:41:10.414: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
Dec 23 13:41:10.443: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:10.557: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:10.658: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:10.758: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:10.864: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:10.956: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:11.121: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:11.157: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:11.257: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:11.366: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:11.461: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:11.559: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:11.657: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:11.758: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:11.858: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:11.958: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:12.060: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:12.163: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:12.257: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:12.360: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:12.458: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:12.557: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:12.660: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:12.759: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:12.858: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:12.956: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:13.058: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:13.165: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:13.258: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:13.363: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:13.456: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:13.560: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:13.658: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:13.759: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:13.858: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:13.957: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:14.058: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:14.158: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:14.259: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:14.370: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:14.460: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:14.560: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:14.659: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:14.759: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:14.858: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:14.957: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:15.058: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:15.158: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:15.262: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:15.364: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:15.458: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:15.580: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:15.661: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:15.758: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:15.857: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:15.959: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:16.057: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:16.159: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:16.257: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:16.361: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:16.458: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:16.558: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:16.658: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:16.759: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:16.857: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:16.960: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:17.058: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:17.159: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:17.258: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:17.365: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:17.461: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:17.563: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:17.659: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:17.761: INFO: Waiting for webhook configuration to be ready...
Dec 23 13:41:17.857: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:41:17.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1035" for this suite.
STEP: Destroying namespace "webhook-1035-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:13.409 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":38,"skipped":697,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:41:18.055: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 23 13:41:18.188: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:41:18.192: INFO: Number of nodes with available pods: 0
Dec 23 13:41:18.192: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:41:19.226: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:41:19.230: INFO: Number of nodes with available pods: 0
Dec 23 13:41:19.230: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:41:20.198: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:41:20.201: INFO: Number of nodes with available pods: 2
Dec 23 13:41:20.201: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 23 13:41:20.225: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:41:20.228: INFO: Number of nodes with available pods: 1
Dec 23 13:41:20.228: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:41:21.235: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:41:21.238: INFO: Number of nodes with available pods: 1
Dec 23 13:41:21.238: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:41:22.236: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:41:22.240: INFO: Number of nodes with available pods: 1
Dec 23 13:41:22.240: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:41:23.236: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:41:23.240: INFO: Number of nodes with available pods: 1
Dec 23 13:41:23.240: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:41:24.234: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:41:24.241: INFO: Number of nodes with available pods: 1
Dec 23 13:41:24.241: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:41:25.237: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:41:25.240: INFO: Number of nodes with available pods: 1
Dec 23 13:41:25.240: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:41:26.236: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:41:26.238: INFO: Number of nodes with available pods: 1
Dec 23 13:41:26.238: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:41:27.237: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:41:27.241: INFO: Number of nodes with available pods: 1
Dec 23 13:41:27.241: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:41:28.238: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:41:28.242: INFO: Number of nodes with available pods: 1
Dec 23 13:41:28.242: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:41:29.235: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:41:29.239: INFO: Number of nodes with available pods: 1
Dec 23 13:41:29.239: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:41:30.234: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:41:30.237: INFO: Number of nodes with available pods: 2
Dec 23 13:41:30.237: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5567, will wait for the garbage collector to delete the pods
Dec 23 13:41:30.298: INFO: Deleting DaemonSet.extensions daemon-set took: 6.651959ms
Dec 23 13:41:30.898: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.221728ms
Dec 23 13:42:09.303: INFO: Number of nodes with available pods: 0
Dec 23 13:42:09.303: INFO: Number of running nodes: 0, number of available pods: 0
Dec 23 13:42:09.312: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"82265"},"items":null}

Dec 23 13:42:09.315: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"82265"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:42:09.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5567" for this suite.

• [SLOW TEST:51.275 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":39,"skipped":765,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:42:09.333: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:42:09.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6799" for this suite.
STEP: Destroying namespace "nspatchtest-2936070c-aedb-41cc-ad4c-490082cf6bec-1066" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":40,"skipped":897,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:42:09.435: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 23 13:42:09.491: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:42:09.495: INFO: Number of nodes with available pods: 0
Dec 23 13:42:09.495: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:42:10.501: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:42:10.504: INFO: Number of nodes with available pods: 0
Dec 23 13:42:10.504: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:42:11.502: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:42:11.506: INFO: Number of nodes with available pods: 0
Dec 23 13:42:11.506: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:42:12.502: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:42:12.505: INFO: Number of nodes with available pods: 2
Dec 23 13:42:12.505: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 23 13:42:12.533: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:42:12.541: INFO: Number of nodes with available pods: 1
Dec 23 13:42:12.541: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:42:13.546: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:42:13.549: INFO: Number of nodes with available pods: 1
Dec 23 13:42:13.549: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:42:14.549: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:42:14.552: INFO: Number of nodes with available pods: 1
Dec 23 13:42:14.552: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 13:42:15.549: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 13:42:15.551: INFO: Number of nodes with available pods: 2
Dec 23 13:42:15.551: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3869, will wait for the garbage collector to delete the pods
Dec 23 13:42:15.615: INFO: Deleting DaemonSet.extensions daemon-set took: 8.258794ms
Dec 23 13:42:16.216: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.219906ms
Dec 23 13:42:27.921: INFO: Number of nodes with available pods: 0
Dec 23 13:42:27.921: INFO: Number of running nodes: 0, number of available pods: 0
Dec 23 13:42:27.923: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"82418"},"items":null}

Dec 23 13:42:27.925: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"82418"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:42:27.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3869" for this suite.

• [SLOW TEST:18.503 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":41,"skipped":908,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:42:27.939: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-591aca59-2378-4f5a-b868-be2f17e81d7f
STEP: Creating a pod to test consume configMaps
Dec 23 13:42:27.997: INFO: Waiting up to 5m0s for pod "pod-configmaps-303f7815-9395-44b5-b854-b561f4e15cf3" in namespace "configmap-6257" to be "Succeeded or Failed"
Dec 23 13:42:28.003: INFO: Pod "pod-configmaps-303f7815-9395-44b5-b854-b561f4e15cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.513536ms
Dec 23 13:42:30.006: INFO: Pod "pod-configmaps-303f7815-9395-44b5-b854-b561f4e15cf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008850412s
STEP: Saw pod success
Dec 23 13:42:30.006: INFO: Pod "pod-configmaps-303f7815-9395-44b5-b854-b561f4e15cf3" satisfied condition "Succeeded or Failed"
Dec 23 13:42:30.008: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-configmaps-303f7815-9395-44b5-b854-b561f4e15cf3 container agnhost-container: <nil>
STEP: delete the pod
Dec 23 13:42:30.039: INFO: Waiting for pod pod-configmaps-303f7815-9395-44b5-b854-b561f4e15cf3 to disappear
Dec 23 13:42:30.042: INFO: Pod pod-configmaps-303f7815-9395-44b5-b854-b561f4e15cf3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:42:30.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6257" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":42,"skipped":923,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:42:30.054: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 13:42:30.086: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:42:30.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7431" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":43,"skipped":970,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:42:30.664: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Dec 23 13:42:30.780: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 23 13:43:30.804: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 13:43:30.807: INFO: Starting informer...
STEP: Starting pod...
Dec 23 13:43:31.025: INFO: Pod is running on wt-k8s-3.novalocal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Dec 23 13:43:31.048: INFO: Pod wasn't evicted. Proceeding
Dec 23 13:43:31.048: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Dec 23 13:44:46.077: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:44:46.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4596" for this suite.

• [SLOW TEST:135.430 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":44,"skipped":1014,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:44:46.095: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Dec 23 13:44:48.735: INFO: Successfully updated pod "labelsupdate88557bcd-dc46-4092-91c4-f0e07497f3af"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:44:50.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1212" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":45,"skipped":1038,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:44:50.783: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 13:44:51.595: INFO: Checking APIGroup: apiregistration.k8s.io
Dec 23 13:44:51.596: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Dec 23 13:44:51.596: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Dec 23 13:44:51.596: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Dec 23 13:44:51.596: INFO: Checking APIGroup: apps
Dec 23 13:44:51.597: INFO: PreferredVersion.GroupVersion: apps/v1
Dec 23 13:44:51.597: INFO: Versions found [{apps/v1 v1}]
Dec 23 13:44:51.597: INFO: apps/v1 matches apps/v1
Dec 23 13:44:51.597: INFO: Checking APIGroup: events.k8s.io
Dec 23 13:44:51.598: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Dec 23 13:44:51.598: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Dec 23 13:44:51.598: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Dec 23 13:44:51.598: INFO: Checking APIGroup: authentication.k8s.io
Dec 23 13:44:51.599: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Dec 23 13:44:51.599: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Dec 23 13:44:51.599: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Dec 23 13:44:51.599: INFO: Checking APIGroup: authorization.k8s.io
Dec 23 13:44:51.600: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Dec 23 13:44:51.600: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Dec 23 13:44:51.600: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Dec 23 13:44:51.600: INFO: Checking APIGroup: autoscaling
Dec 23 13:44:51.602: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Dec 23 13:44:51.602: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Dec 23 13:44:51.602: INFO: autoscaling/v1 matches autoscaling/v1
Dec 23 13:44:51.602: INFO: Checking APIGroup: batch
Dec 23 13:44:51.603: INFO: PreferredVersion.GroupVersion: batch/v1
Dec 23 13:44:51.603: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Dec 23 13:44:51.603: INFO: batch/v1 matches batch/v1
Dec 23 13:44:51.603: INFO: Checking APIGroup: certificates.k8s.io
Dec 23 13:44:51.604: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Dec 23 13:44:51.604: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Dec 23 13:44:51.604: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Dec 23 13:44:51.604: INFO: Checking APIGroup: networking.k8s.io
Dec 23 13:44:51.605: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Dec 23 13:44:51.605: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Dec 23 13:44:51.605: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Dec 23 13:44:51.605: INFO: Checking APIGroup: extensions
Dec 23 13:44:51.606: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Dec 23 13:44:51.606: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Dec 23 13:44:51.606: INFO: extensions/v1beta1 matches extensions/v1beta1
Dec 23 13:44:51.606: INFO: Checking APIGroup: policy
Dec 23 13:44:51.607: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Dec 23 13:44:51.607: INFO: Versions found [{policy/v1beta1 v1beta1}]
Dec 23 13:44:51.607: INFO: policy/v1beta1 matches policy/v1beta1
Dec 23 13:44:51.607: INFO: Checking APIGroup: rbac.authorization.k8s.io
Dec 23 13:44:51.608: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Dec 23 13:44:51.608: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Dec 23 13:44:51.608: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Dec 23 13:44:51.608: INFO: Checking APIGroup: storage.k8s.io
Dec 23 13:44:51.608: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Dec 23 13:44:51.609: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Dec 23 13:44:51.609: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Dec 23 13:44:51.609: INFO: Checking APIGroup: admissionregistration.k8s.io
Dec 23 13:44:51.610: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Dec 23 13:44:51.610: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Dec 23 13:44:51.610: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Dec 23 13:44:51.610: INFO: Checking APIGroup: apiextensions.k8s.io
Dec 23 13:44:51.611: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Dec 23 13:44:51.611: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Dec 23 13:44:51.611: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Dec 23 13:44:51.611: INFO: Checking APIGroup: scheduling.k8s.io
Dec 23 13:44:51.612: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Dec 23 13:44:51.612: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Dec 23 13:44:51.612: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Dec 23 13:44:51.612: INFO: Checking APIGroup: coordination.k8s.io
Dec 23 13:44:51.613: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Dec 23 13:44:51.613: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Dec 23 13:44:51.613: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Dec 23 13:44:51.613: INFO: Checking APIGroup: node.k8s.io
Dec 23 13:44:51.614: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Dec 23 13:44:51.614: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Dec 23 13:44:51.614: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Dec 23 13:44:51.614: INFO: Checking APIGroup: discovery.k8s.io
Dec 23 13:44:51.615: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Dec 23 13:44:51.615: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Dec 23 13:44:51.615: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Dec 23 13:44:51.615: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Dec 23 13:44:51.616: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Dec 23 13:44:51.616: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Dec 23 13:44:51.616: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Dec 23 13:44:51.616: INFO: Checking APIGroup: crd.projectcalico.org
Dec 23 13:44:51.617: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Dec 23 13:44:51.617: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Dec 23 13:44:51.617: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:44:51.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-9791" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":46,"skipped":1042,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:44:51.629: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image abcsys.cn:5000/library/httpd:2.4.38-alpine
Dec 23 13:44:51.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-4313 run e2e-test-httpd-pod --image=abcsys.cn:5000/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Dec 23 13:44:52.778: INFO: stderr: ""
Dec 23 13:44:52.778: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Dec 23 13:44:52.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-4313 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "abcsys.cn:5000/library/busybox:1.29"}]}} --dry-run=server'
Dec 23 13:44:53.389: INFO: stderr: ""
Dec 23 13:44:53.389: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image abcsys.cn:5000/library/httpd:2.4.38-alpine
Dec 23 13:44:53.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-4313 delete pods e2e-test-httpd-pod'
Dec 23 13:45:05.904: INFO: stderr: ""
Dec 23 13:45:05.904: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:45:05.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4313" for this suite.

• [SLOW TEST:14.297 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:909
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":47,"skipped":1042,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:45:05.927: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Dec 23 13:45:09.975: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3844 PodName:pod-sharedvolume-0bb4aae2-b9c0-450e-aae6-0c0f01b8f26d ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 13:45:09.975: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 13:45:10.065: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:45:10.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3844" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":48,"skipped":1055,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:45:10.074: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-5505096e-6826-43f2-b4d9-a68c49389934
STEP: Creating a pod to test consume secrets
Dec 23 13:45:10.150: INFO: Waiting up to 5m0s for pod "pod-secrets-f3f86386-e1fd-488a-a9f2-25c710c60c85" in namespace "secrets-5838" to be "Succeeded or Failed"
Dec 23 13:45:10.155: INFO: Pod "pod-secrets-f3f86386-e1fd-488a-a9f2-25c710c60c85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.936382ms
Dec 23 13:45:12.165: INFO: Pod "pod-secrets-f3f86386-e1fd-488a-a9f2-25c710c60c85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015235102s
Dec 23 13:45:14.174: INFO: Pod "pod-secrets-f3f86386-e1fd-488a-a9f2-25c710c60c85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023493737s
STEP: Saw pod success
Dec 23 13:45:14.174: INFO: Pod "pod-secrets-f3f86386-e1fd-488a-a9f2-25c710c60c85" satisfied condition "Succeeded or Failed"
Dec 23 13:45:14.176: INFO: Trying to get logs from node wt-k8s-2 pod pod-secrets-f3f86386-e1fd-488a-a9f2-25c710c60c85 container secret-volume-test: <nil>
STEP: delete the pod
Dec 23 13:45:14.210: INFO: Waiting for pod pod-secrets-f3f86386-e1fd-488a-a9f2-25c710c60c85 to disappear
Dec 23 13:45:14.213: INFO: Pod pod-secrets-f3f86386-e1fd-488a-a9f2-25c710c60c85 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:45:14.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5838" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":49,"skipped":1062,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:45:14.271: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 23 13:45:17.427: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:45:17.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-78" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":50,"skipped":1092,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:45:17.452: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 13:45:19.516: INFO: Deleting pod "var-expansion-a70ace1c-4294-4c5a-b493-0bdf5b5505ea" in namespace "var-expansion-6123"
Dec 23 13:45:19.519: INFO: Wait up to 5m0s for pod "var-expansion-a70ace1c-4294-4c5a-b493-0bdf5b5505ea" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:46:09.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6123" for this suite.

• [SLOW TEST:52.088 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":51,"skipped":1094,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:46:09.541: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-4c8d274b-cb6b-4d10-ae89-5dc43fd366f6
STEP: Creating a pod to test consume secrets
Dec 23 13:46:09.642: INFO: Waiting up to 5m0s for pod "pod-secrets-21e19956-8d56-4054-b78f-7c403d6dd10d" in namespace "secrets-5099" to be "Succeeded or Failed"
Dec 23 13:46:09.644: INFO: Pod "pod-secrets-21e19956-8d56-4054-b78f-7c403d6dd10d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.604242ms
Dec 23 13:46:11.654: INFO: Pod "pod-secrets-21e19956-8d56-4054-b78f-7c403d6dd10d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011885058s
Dec 23 13:46:13.659: INFO: Pod "pod-secrets-21e19956-8d56-4054-b78f-7c403d6dd10d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017430818s
STEP: Saw pod success
Dec 23 13:46:13.659: INFO: Pod "pod-secrets-21e19956-8d56-4054-b78f-7c403d6dd10d" satisfied condition "Succeeded or Failed"
Dec 23 13:46:13.661: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-secrets-21e19956-8d56-4054-b78f-7c403d6dd10d container secret-volume-test: <nil>
STEP: delete the pod
Dec 23 13:46:13.690: INFO: Waiting for pod pod-secrets-21e19956-8d56-4054-b78f-7c403d6dd10d to disappear
Dec 23 13:46:13.693: INFO: Pod pod-secrets-21e19956-8d56-4054-b78f-7c403d6dd10d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:46:13.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5099" for this suite.
STEP: Destroying namespace "secret-namespace-275" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":52,"skipped":1108,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:46:13.709: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1223 13:46:53.807264      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 23 13:47:55.828: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Dec 23 13:47:55.829: INFO: Deleting pod "simpletest.rc-7z5dj" in namespace "gc-3070"
Dec 23 13:47:55.914: INFO: Deleting pod "simpletest.rc-hwd98" in namespace "gc-3070"
Dec 23 13:47:55.945: INFO: Deleting pod "simpletest.rc-jwkrc" in namespace "gc-3070"
Dec 23 13:47:55.982: INFO: Deleting pod "simpletest.rc-m2b5m" in namespace "gc-3070"
Dec 23 13:47:56.002: INFO: Deleting pod "simpletest.rc-n4wnp" in namespace "gc-3070"
Dec 23 13:47:56.024: INFO: Deleting pod "simpletest.rc-spmwt" in namespace "gc-3070"
Dec 23 13:47:56.049: INFO: Deleting pod "simpletest.rc-xgc4f" in namespace "gc-3070"
Dec 23 13:47:56.084: INFO: Deleting pod "simpletest.rc-xt4rd" in namespace "gc-3070"
Dec 23 13:47:56.112: INFO: Deleting pod "simpletest.rc-zp652" in namespace "gc-3070"
Dec 23 13:47:56.134: INFO: Deleting pod "simpletest.rc-zznp2" in namespace "gc-3070"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:47:56.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3070" for this suite.

• [SLOW TEST:102.475 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":53,"skipped":1129,"failed":0}
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:47:56.184: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Dec 23 13:47:56.278: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:48:01.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9786" for this suite.

• [SLOW TEST:5.105 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":54,"skipped":1136,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:48:01.290: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Dec 23 13:48:01.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 create -f -'
Dec 23 13:48:01.927: INFO: stderr: ""
Dec 23 13:48:01.927: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 23 13:48:01.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 23 13:48:02.026: INFO: stderr: ""
Dec 23 13:48:02.026: INFO: stdout: "update-demo-nautilus-dcc8s update-demo-nautilus-sr56w "
Dec 23 13:48:02.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods update-demo-nautilus-dcc8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 23 13:48:02.129: INFO: stderr: ""
Dec 23 13:48:02.129: INFO: stdout: ""
Dec 23 13:48:02.129: INFO: update-demo-nautilus-dcc8s is created but not running
Dec 23 13:48:07.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 23 13:48:07.245: INFO: stderr: ""
Dec 23 13:48:07.245: INFO: stdout: "update-demo-nautilus-dcc8s update-demo-nautilus-sr56w "
Dec 23 13:48:07.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods update-demo-nautilus-dcc8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 23 13:48:07.342: INFO: stderr: ""
Dec 23 13:48:07.342: INFO: stdout: "true"
Dec 23 13:48:07.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods update-demo-nautilus-dcc8s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 23 13:48:07.458: INFO: stderr: ""
Dec 23 13:48:07.458: INFO: stdout: "abcsys.cn:5000/kubernetes-e2e-test-images/nautilus:1.0"
Dec 23 13:48:07.458: INFO: validating pod update-demo-nautilus-dcc8s
Dec 23 13:48:07.463: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 23 13:48:07.463: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 23 13:48:07.463: INFO: update-demo-nautilus-dcc8s is verified up and running
Dec 23 13:48:07.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods update-demo-nautilus-sr56w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 23 13:48:07.565: INFO: stderr: ""
Dec 23 13:48:07.565: INFO: stdout: "true"
Dec 23 13:48:07.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods update-demo-nautilus-sr56w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 23 13:48:07.672: INFO: stderr: ""
Dec 23 13:48:07.672: INFO: stdout: "abcsys.cn:5000/kubernetes-e2e-test-images/nautilus:1.0"
Dec 23 13:48:07.672: INFO: validating pod update-demo-nautilus-sr56w
Dec 23 13:48:07.676: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 23 13:48:07.677: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 23 13:48:07.677: INFO: update-demo-nautilus-sr56w is verified up and running
STEP: scaling down the replication controller
Dec 23 13:48:07.679: INFO: scanned /root for discovery docs: <nil>
Dec 23 13:48:07.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Dec 23 13:48:08.822: INFO: stderr: ""
Dec 23 13:48:08.822: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 23 13:48:08.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 23 13:48:08.947: INFO: stderr: ""
Dec 23 13:48:08.947: INFO: stdout: "update-demo-nautilus-dcc8s update-demo-nautilus-sr56w "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 23 13:48:13.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 23 13:48:14.056: INFO: stderr: ""
Dec 23 13:48:14.056: INFO: stdout: "update-demo-nautilus-dcc8s update-demo-nautilus-sr56w "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 23 13:48:19.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 23 13:48:19.175: INFO: stderr: ""
Dec 23 13:48:19.175: INFO: stdout: "update-demo-nautilus-dcc8s update-demo-nautilus-sr56w "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 23 13:48:24.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 23 13:48:24.309: INFO: stderr: ""
Dec 23 13:48:24.309: INFO: stdout: "update-demo-nautilus-dcc8s "
Dec 23 13:48:24.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods update-demo-nautilus-dcc8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 23 13:48:24.421: INFO: stderr: ""
Dec 23 13:48:24.421: INFO: stdout: "true"
Dec 23 13:48:24.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods update-demo-nautilus-dcc8s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 23 13:48:24.526: INFO: stderr: ""
Dec 23 13:48:24.526: INFO: stdout: "abcsys.cn:5000/kubernetes-e2e-test-images/nautilus:1.0"
Dec 23 13:48:24.526: INFO: validating pod update-demo-nautilus-dcc8s
Dec 23 13:48:24.530: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 23 13:48:24.530: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 23 13:48:24.530: INFO: update-demo-nautilus-dcc8s is verified up and running
STEP: scaling up the replication controller
Dec 23 13:48:24.533: INFO: scanned /root for discovery docs: <nil>
Dec 23 13:48:24.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Dec 23 13:48:25.675: INFO: stderr: ""
Dec 23 13:48:25.675: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 23 13:48:25.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 23 13:48:25.821: INFO: stderr: ""
Dec 23 13:48:25.822: INFO: stdout: "update-demo-nautilus-dcc8s update-demo-nautilus-qdjt7 "
Dec 23 13:48:25.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods update-demo-nautilus-dcc8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 23 13:48:25.928: INFO: stderr: ""
Dec 23 13:48:25.928: INFO: stdout: "true"
Dec 23 13:48:25.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods update-demo-nautilus-dcc8s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 23 13:48:26.024: INFO: stderr: ""
Dec 23 13:48:26.024: INFO: stdout: "abcsys.cn:5000/kubernetes-e2e-test-images/nautilus:1.0"
Dec 23 13:48:26.024: INFO: validating pod update-demo-nautilus-dcc8s
Dec 23 13:48:26.027: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 23 13:48:26.027: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 23 13:48:26.027: INFO: update-demo-nautilus-dcc8s is verified up and running
Dec 23 13:48:26.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods update-demo-nautilus-qdjt7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 23 13:48:26.138: INFO: stderr: ""
Dec 23 13:48:26.138: INFO: stdout: ""
Dec 23 13:48:26.138: INFO: update-demo-nautilus-qdjt7 is created but not running
Dec 23 13:48:31.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 23 13:48:31.247: INFO: stderr: ""
Dec 23 13:48:31.247: INFO: stdout: "update-demo-nautilus-dcc8s update-demo-nautilus-qdjt7 "
Dec 23 13:48:31.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods update-demo-nautilus-dcc8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 23 13:48:31.365: INFO: stderr: ""
Dec 23 13:48:31.365: INFO: stdout: "true"
Dec 23 13:48:31.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods update-demo-nautilus-dcc8s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 23 13:48:31.500: INFO: stderr: ""
Dec 23 13:48:31.500: INFO: stdout: "abcsys.cn:5000/kubernetes-e2e-test-images/nautilus:1.0"
Dec 23 13:48:31.500: INFO: validating pod update-demo-nautilus-dcc8s
Dec 23 13:48:31.503: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 23 13:48:31.504: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 23 13:48:31.504: INFO: update-demo-nautilus-dcc8s is verified up and running
Dec 23 13:48:31.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods update-demo-nautilus-qdjt7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 23 13:48:31.607: INFO: stderr: ""
Dec 23 13:48:31.607: INFO: stdout: "true"
Dec 23 13:48:31.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods update-demo-nautilus-qdjt7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 23 13:48:31.722: INFO: stderr: ""
Dec 23 13:48:31.722: INFO: stdout: "abcsys.cn:5000/kubernetes-e2e-test-images/nautilus:1.0"
Dec 23 13:48:31.722: INFO: validating pod update-demo-nautilus-qdjt7
Dec 23 13:48:31.727: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 23 13:48:31.727: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 23 13:48:31.727: INFO: update-demo-nautilus-qdjt7 is verified up and running
STEP: using delete to clean up resources
Dec 23 13:48:31.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 delete --grace-period=0 --force -f -'
Dec 23 13:48:31.830: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 13:48:31.830: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 23 13:48:31.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get rc,svc -l name=update-demo --no-headers'
Dec 23 13:48:31.942: INFO: stderr: "No resources found in kubectl-920 namespace.\n"
Dec 23 13:48:31.942: INFO: stdout: ""
Dec 23 13:48:31.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 23 13:48:32.047: INFO: stderr: ""
Dec 23 13:48:32.047: INFO: stdout: "update-demo-nautilus-dcc8s\nupdate-demo-nautilus-qdjt7\n"
Dec 23 13:48:32.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get rc,svc -l name=update-demo --no-headers'
Dec 23 13:48:32.664: INFO: stderr: "No resources found in kubectl-920 namespace.\n"
Dec 23 13:48:32.664: INFO: stdout: ""
Dec 23 13:48:32.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-920 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 23 13:48:32.782: INFO: stderr: ""
Dec 23 13:48:32.782: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:48:32.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-920" for this suite.

• [SLOW TEST:31.505 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":55,"skipped":1149,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:48:32.797: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 23 13:48:32.838: INFO: Waiting up to 5m0s for pod "pod-f61293d2-548b-4d89-a92e-49bda94ab392" in namespace "emptydir-6322" to be "Succeeded or Failed"
Dec 23 13:48:32.844: INFO: Pod "pod-f61293d2-548b-4d89-a92e-49bda94ab392": Phase="Pending", Reason="", readiness=false. Elapsed: 5.785486ms
Dec 23 13:48:34.854: INFO: Pod "pod-f61293d2-548b-4d89-a92e-49bda94ab392": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016147257s
STEP: Saw pod success
Dec 23 13:48:34.854: INFO: Pod "pod-f61293d2-548b-4d89-a92e-49bda94ab392" satisfied condition "Succeeded or Failed"
Dec 23 13:48:34.859: INFO: Trying to get logs from node wt-k8s-2 pod pod-f61293d2-548b-4d89-a92e-49bda94ab392 container test-container: <nil>
STEP: delete the pod
Dec 23 13:48:34.893: INFO: Waiting for pod pod-f61293d2-548b-4d89-a92e-49bda94ab392 to disappear
Dec 23 13:48:34.895: INFO: Pod pod-f61293d2-548b-4d89-a92e-49bda94ab392 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:48:34.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6322" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":56,"skipped":1184,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:48:34.907: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-4add7977-a7b4-40ef-9f4f-f0f399662759
Dec 23 13:48:34.962: INFO: Pod name my-hostname-basic-4add7977-a7b4-40ef-9f4f-f0f399662759: Found 1 pods out of 1
Dec 23 13:48:34.962: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-4add7977-a7b4-40ef-9f4f-f0f399662759" are running
Dec 23 13:48:36.983: INFO: Pod "my-hostname-basic-4add7977-a7b4-40ef-9f4f-f0f399662759-8jcz5" is running (conditions: [])
Dec 23 13:48:36.985: INFO: Trying to dial the pod
Dec 23 13:48:41.999: INFO: Controller my-hostname-basic-4add7977-a7b4-40ef-9f4f-f0f399662759: Got expected result from replica 1 [my-hostname-basic-4add7977-a7b4-40ef-9f4f-f0f399662759-8jcz5]: "my-hostname-basic-4add7977-a7b4-40ef-9f4f-f0f399662759-8jcz5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:48:41.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-853" for this suite.

• [SLOW TEST:7.105 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":57,"skipped":1217,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:48:42.013: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Dec 23 13:48:42.055: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 23 13:48:42.064: INFO: Waiting for terminating namespaces to be deleted...
Dec 23 13:48:42.066: INFO: 
Logging pods the apiserver thinks is on node wt-k8s-2 before test
Dec 23 13:48:42.073: INFO: calico-node-5czbz from kube-system started at 2020-12-23 08:19:38 +0000 UTC (1 container statuses recorded)
Dec 23 13:48:42.073: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 13:48:42.073: INFO: kube-proxy-55qvq from kube-system started at 2020-12-23 08:19:38 +0000 UTC (1 container statuses recorded)
Dec 23 13:48:42.073: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 23 13:48:42.073: INFO: my-hostname-basic-4add7977-a7b4-40ef-9f4f-f0f399662759-8jcz5 from replication-controller-853 started at 2020-12-23 13:48:34 +0000 UTC (1 container statuses recorded)
Dec 23 13:48:42.073: INFO: 	Container my-hostname-basic-4add7977-a7b4-40ef-9f4f-f0f399662759 ready: true, restart count 0
Dec 23 13:48:42.073: INFO: sonobuoy from sonobuoy started at 2020-12-23 13:24:44 +0000 UTC (1 container statuses recorded)
Dec 23 13:48:42.073: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 23 13:48:42.073: INFO: sonobuoy-e2e-job-381779db02e94c8b from sonobuoy started at 2020-12-23 13:24:46 +0000 UTC (2 container statuses recorded)
Dec 23 13:48:42.073: INFO: 	Container e2e ready: true, restart count 0
Dec 23 13:48:42.073: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 13:48:42.073: INFO: sonobuoy-systemd-logs-daemon-set-a3aba41d66fb4680-2x864 from sonobuoy started at 2020-12-23 13:24:46 +0000 UTC (2 container statuses recorded)
Dec 23 13:48:42.073: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 13:48:42.073: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 13:48:42.073: INFO: 
Logging pods the apiserver thinks is on node wt-k8s-3.novalocal before test
Dec 23 13:48:42.079: INFO: calico-kube-controllers-744cfdf676-wbsnz from kube-system started at 2020-12-23 13:43:17 +0000 UTC (1 container statuses recorded)
Dec 23 13:48:42.079: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 23 13:48:42.079: INFO: calico-node-lz86h from kube-system started at 2020-12-23 08:18:23 +0000 UTC (1 container statuses recorded)
Dec 23 13:48:42.079: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 13:48:42.079: INFO: kube-proxy-pnq9z from kube-system started at 2020-12-23 08:14:45 +0000 UTC (1 container statuses recorded)
Dec 23 13:48:42.079: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 23 13:48:42.079: INFO: update-demo-nautilus-dcc8s from kubectl-920 started at 2020-12-23 13:47:48 +0000 UTC (1 container statuses recorded)
Dec 23 13:48:42.079: INFO: 	Container update-demo ready: false, restart count 0
Dec 23 13:48:42.079: INFO: update-demo-nautilus-qdjt7 from kubectl-920 started at 2020-12-23 13:48:10 +0000 UTC (1 container statuses recorded)
Dec 23 13:48:42.079: INFO: 	Container update-demo ready: false, restart count 0
Dec 23 13:48:42.079: INFO: sonobuoy-systemd-logs-daemon-set-a3aba41d66fb4680-n5vgr from sonobuoy started at 2020-12-23 13:24:32 +0000 UTC (2 container statuses recorded)
Dec 23 13:48:42.079: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Dec 23 13:48:42.079: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-9127e872-120d-4de0-b4f3-961100db65e5 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.22.19.25 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-9127e872-120d-4de0-b4f3-961100db65e5 off the node wt-k8s-3.novalocal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9127e872-120d-4de0-b4f3-961100db65e5
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:53:50.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-542" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:308.237 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":58,"skipped":1235,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:53:50.250: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 13:53:50.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-4651 version'
Dec 23 13:53:50.422: INFO: stderr: ""
Dec 23 13:53:50.422: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.0\", GitCommit:\"af46c47ce925f4c4ad5cc8d1fca46c7b77d13b38\", GitTreeState:\"clean\", BuildDate:\"2020-12-08T17:59:43Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.1\", GitCommit:\"c4d752765b3bbac2237bf87cf0b1c2e307844666\", GitTreeState:\"clean\", BuildDate:\"2020-12-18T12:00:47Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:53:50.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4651" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":59,"skipped":1238,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:53:50.433: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 13:53:50.491: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 23 13:53:50.499: INFO: Number of nodes with available pods: 0
Dec 23 13:53:50.500: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec 23 13:53:50.525: INFO: Number of nodes with available pods: 0
Dec 23 13:53:50.525: INFO: Node wt-k8s-3.novalocal is running more than one daemon pod
Dec 23 13:53:51.530: INFO: Number of nodes with available pods: 0
Dec 23 13:53:51.530: INFO: Node wt-k8s-3.novalocal is running more than one daemon pod
Dec 23 13:53:52.532: INFO: Number of nodes with available pods: 1
Dec 23 13:53:52.532: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 23 13:53:52.553: INFO: Number of nodes with available pods: 1
Dec 23 13:53:52.553: INFO: Number of running nodes: 0, number of available pods: 1
Dec 23 13:53:53.560: INFO: Number of nodes with available pods: 0
Dec 23 13:53:53.560: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 23 13:53:53.578: INFO: Number of nodes with available pods: 0
Dec 23 13:53:53.578: INFO: Node wt-k8s-3.novalocal is running more than one daemon pod
Dec 23 13:53:54.585: INFO: Number of nodes with available pods: 0
Dec 23 13:53:54.585: INFO: Node wt-k8s-3.novalocal is running more than one daemon pod
Dec 23 13:53:55.584: INFO: Number of nodes with available pods: 0
Dec 23 13:53:55.584: INFO: Node wt-k8s-3.novalocal is running more than one daemon pod
Dec 23 13:53:56.582: INFO: Number of nodes with available pods: 0
Dec 23 13:53:56.582: INFO: Node wt-k8s-3.novalocal is running more than one daemon pod
Dec 23 13:53:57.585: INFO: Number of nodes with available pods: 0
Dec 23 13:53:57.585: INFO: Node wt-k8s-3.novalocal is running more than one daemon pod
Dec 23 13:53:58.584: INFO: Number of nodes with available pods: 0
Dec 23 13:53:58.584: INFO: Node wt-k8s-3.novalocal is running more than one daemon pod
Dec 23 13:53:59.584: INFO: Number of nodes with available pods: 0
Dec 23 13:53:59.584: INFO: Node wt-k8s-3.novalocal is running more than one daemon pod
Dec 23 13:54:00.583: INFO: Number of nodes with available pods: 0
Dec 23 13:54:00.583: INFO: Node wt-k8s-3.novalocal is running more than one daemon pod
Dec 23 13:54:01.584: INFO: Number of nodes with available pods: 1
Dec 23 13:54:01.584: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3422, will wait for the garbage collector to delete the pods
Dec 23 13:54:01.646: INFO: Deleting DaemonSet.extensions daemon-set took: 5.579327ms
Dec 23 13:54:02.246: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.190947ms
Dec 23 13:54:09.354: INFO: Number of nodes with available pods: 0
Dec 23 13:54:09.354: INFO: Number of running nodes: 0, number of available pods: 0
Dec 23 13:54:09.358: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"84505"},"items":null}

Dec 23 13:54:09.360: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"84505"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:54:09.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3422" for this suite.

• [SLOW TEST:18.960 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":60,"skipped":1266,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:54:09.394: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-a9ce9d75-bb30-4b0e-9790-878ac413e9f4
STEP: Creating a pod to test consume secrets
Dec 23 13:54:09.452: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f67ad58f-879d-47b2-b06c-979a13ba9040" in namespace "projected-7244" to be "Succeeded or Failed"
Dec 23 13:54:09.459: INFO: Pod "pod-projected-secrets-f67ad58f-879d-47b2-b06c-979a13ba9040": Phase="Pending", Reason="", readiness=false. Elapsed: 7.124997ms
Dec 23 13:54:11.467: INFO: Pod "pod-projected-secrets-f67ad58f-879d-47b2-b06c-979a13ba9040": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015636638s
Dec 23 13:54:13.505: INFO: Pod "pod-projected-secrets-f67ad58f-879d-47b2-b06c-979a13ba9040": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053165321s
STEP: Saw pod success
Dec 23 13:54:13.505: INFO: Pod "pod-projected-secrets-f67ad58f-879d-47b2-b06c-979a13ba9040" satisfied condition "Succeeded or Failed"
Dec 23 13:54:13.510: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-projected-secrets-f67ad58f-879d-47b2-b06c-979a13ba9040 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 23 13:54:13.614: INFO: Waiting for pod pod-projected-secrets-f67ad58f-879d-47b2-b06c-979a13ba9040 to disappear
Dec 23 13:54:13.623: INFO: Pod pod-projected-secrets-f67ad58f-879d-47b2-b06c-979a13ba9040 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:54:13.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7244" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":61,"skipped":1317,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:54:13.653: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Dec 23 13:54:13.689: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 23 13:54:13.696: INFO: Waiting for terminating namespaces to be deleted...
Dec 23 13:54:13.699: INFO: 
Logging pods the apiserver thinks is on node wt-k8s-2 before test
Dec 23 13:54:13.706: INFO: calico-node-5czbz from kube-system started at 2020-12-23 08:19:38 +0000 UTC (1 container statuses recorded)
Dec 23 13:54:13.706: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 13:54:13.706: INFO: kube-proxy-55qvq from kube-system started at 2020-12-23 08:19:38 +0000 UTC (1 container statuses recorded)
Dec 23 13:54:13.706: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 23 13:54:13.706: INFO: sonobuoy from sonobuoy started at 2020-12-23 13:24:44 +0000 UTC (1 container statuses recorded)
Dec 23 13:54:13.706: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 23 13:54:13.706: INFO: sonobuoy-e2e-job-381779db02e94c8b from sonobuoy started at 2020-12-23 13:24:46 +0000 UTC (2 container statuses recorded)
Dec 23 13:54:13.706: INFO: 	Container e2e ready: true, restart count 0
Dec 23 13:54:13.706: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 13:54:13.706: INFO: sonobuoy-systemd-logs-daemon-set-a3aba41d66fb4680-2x864 from sonobuoy started at 2020-12-23 13:24:46 +0000 UTC (2 container statuses recorded)
Dec 23 13:54:13.706: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 13:54:13.706: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 13:54:13.706: INFO: 
Logging pods the apiserver thinks is on node wt-k8s-3.novalocal before test
Dec 23 13:54:13.713: INFO: calico-kube-controllers-744cfdf676-wbsnz from kube-system started at 2020-12-23 13:43:17 +0000 UTC (1 container statuses recorded)
Dec 23 13:54:13.713: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 23 13:54:13.713: INFO: calico-node-lz86h from kube-system started at 2020-12-23 08:18:23 +0000 UTC (1 container statuses recorded)
Dec 23 13:54:13.713: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 13:54:13.713: INFO: kube-proxy-pnq9z from kube-system started at 2020-12-23 08:14:45 +0000 UTC (1 container statuses recorded)
Dec 23 13:54:13.713: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 23 13:54:13.713: INFO: sonobuoy-systemd-logs-daemon-set-a3aba41d66fb4680-n5vgr from sonobuoy started at 2020-12-23 13:24:32 +0000 UTC (2 container statuses recorded)
Dec 23 13:54:13.713: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Dec 23 13:54:13.713: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16535ce72982259e], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:54:14.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6566" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":62,"skipped":1323,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:54:14.750: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 13:54:14.798: INFO: Waiting up to 5m0s for pod "downwardapi-volume-862f967b-8db7-4d22-8ed0-81c8b383a17e" in namespace "projected-7500" to be "Succeeded or Failed"
Dec 23 13:54:14.801: INFO: Pod "downwardapi-volume-862f967b-8db7-4d22-8ed0-81c8b383a17e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.261257ms
Dec 23 13:54:16.810: INFO: Pod "downwardapi-volume-862f967b-8db7-4d22-8ed0-81c8b383a17e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011621086s
STEP: Saw pod success
Dec 23 13:54:16.810: INFO: Pod "downwardapi-volume-862f967b-8db7-4d22-8ed0-81c8b383a17e" satisfied condition "Succeeded or Failed"
Dec 23 13:54:16.814: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-862f967b-8db7-4d22-8ed0-81c8b383a17e container client-container: <nil>
STEP: delete the pod
Dec 23 13:54:16.839: INFO: Waiting for pod downwardapi-volume-862f967b-8db7-4d22-8ed0-81c8b383a17e to disappear
Dec 23 13:54:16.843: INFO: Pod downwardapi-volume-862f967b-8db7-4d22-8ed0-81c8b383a17e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:54:16.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7500" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":63,"skipped":1325,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:54:16.852: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Dec 23 13:54:19.445: INFO: Successfully updated pod "labelsupdate924ea695-5fd8-4575-846d-6485c9bebddf"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:54:21.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9440" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":64,"skipped":1340,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:54:21.478: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:54:46.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9587" for this suite.

• [SLOW TEST:25.331 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":65,"skipped":1373,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:54:46.810: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Dec 23 13:54:46.849: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-5044 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:54:46.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5044" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":66,"skipped":1415,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:54:46.958: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Dec 23 13:54:47.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-5937 create -f -'
Dec 23 13:54:47.471: INFO: stderr: ""
Dec 23 13:54:47.471: INFO: stdout: "pod/pause created\n"
Dec 23 13:54:47.471: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 23 13:54:47.471: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5937" to be "running and ready"
Dec 23 13:54:47.477: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.959563ms
Dec 23 13:54:49.482: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011173622s
Dec 23 13:54:51.495: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.024158761s
Dec 23 13:54:51.495: INFO: Pod "pause" satisfied condition "running and ready"
Dec 23 13:54:51.495: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 23 13:54:51.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-5937 label pods pause testing-label=testing-label-value'
Dec 23 13:54:51.619: INFO: stderr: ""
Dec 23 13:54:51.619: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 23 13:54:51.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-5937 get pod pause -L testing-label'
Dec 23 13:54:51.732: INFO: stderr: ""
Dec 23 13:54:51.732: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 23 13:54:51.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-5937 label pods pause testing-label-'
Dec 23 13:54:51.851: INFO: stderr: ""
Dec 23 13:54:51.851: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 23 13:54:51.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-5937 get pod pause -L testing-label'
Dec 23 13:54:52.072: INFO: stderr: ""
Dec 23 13:54:52.072: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Dec 23 13:54:52.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-5937 delete --grace-period=0 --force -f -'
Dec 23 13:54:52.216: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 13:54:52.216: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 23 13:54:52.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-5937 get rc,svc -l name=pause --no-headers'
Dec 23 13:54:52.368: INFO: stderr: "No resources found in kubectl-5937 namespace.\n"
Dec 23 13:54:52.368: INFO: stdout: ""
Dec 23 13:54:52.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-5937 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 23 13:54:52.692: INFO: stderr: ""
Dec 23 13:54:52.692: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:54:52.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5937" for this suite.

• [SLOW TEST:5.745 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":67,"skipped":1431,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:54:52.704: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-4c9r
STEP: Creating a pod to test atomic-volume-subpath
Dec 23 13:54:52.763: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4c9r" in namespace "subpath-8984" to be "Succeeded or Failed"
Dec 23 13:54:52.769: INFO: Pod "pod-subpath-test-configmap-4c9r": Phase="Pending", Reason="", readiness=false. Elapsed: 6.614862ms
Dec 23 13:54:54.778: INFO: Pod "pod-subpath-test-configmap-4c9r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015423106s
Dec 23 13:54:56.787: INFO: Pod "pod-subpath-test-configmap-4c9r": Phase="Running", Reason="", readiness=true. Elapsed: 4.023761952s
Dec 23 13:54:58.795: INFO: Pod "pod-subpath-test-configmap-4c9r": Phase="Running", Reason="", readiness=true. Elapsed: 6.032541526s
Dec 23 13:55:00.803: INFO: Pod "pod-subpath-test-configmap-4c9r": Phase="Running", Reason="", readiness=true. Elapsed: 8.040580006s
Dec 23 13:55:02.808: INFO: Pod "pod-subpath-test-configmap-4c9r": Phase="Running", Reason="", readiness=true. Elapsed: 10.045420039s
Dec 23 13:55:04.814: INFO: Pod "pod-subpath-test-configmap-4c9r": Phase="Running", Reason="", readiness=true. Elapsed: 12.051099633s
Dec 23 13:55:06.822: INFO: Pod "pod-subpath-test-configmap-4c9r": Phase="Running", Reason="", readiness=true. Elapsed: 14.059349021s
Dec 23 13:55:08.834: INFO: Pod "pod-subpath-test-configmap-4c9r": Phase="Running", Reason="", readiness=true. Elapsed: 16.071170447s
Dec 23 13:55:10.841: INFO: Pod "pod-subpath-test-configmap-4c9r": Phase="Running", Reason="", readiness=true. Elapsed: 18.07850609s
Dec 23 13:55:12.847: INFO: Pod "pod-subpath-test-configmap-4c9r": Phase="Running", Reason="", readiness=true. Elapsed: 20.084085229s
Dec 23 13:55:14.854: INFO: Pod "pod-subpath-test-configmap-4c9r": Phase="Running", Reason="", readiness=true. Elapsed: 22.091146998s
Dec 23 13:55:16.862: INFO: Pod "pod-subpath-test-configmap-4c9r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.099190923s
STEP: Saw pod success
Dec 23 13:55:16.862: INFO: Pod "pod-subpath-test-configmap-4c9r" satisfied condition "Succeeded or Failed"
Dec 23 13:55:16.865: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-subpath-test-configmap-4c9r container test-container-subpath-configmap-4c9r: <nil>
STEP: delete the pod
Dec 23 13:55:16.888: INFO: Waiting for pod pod-subpath-test-configmap-4c9r to disappear
Dec 23 13:55:16.893: INFO: Pod pod-subpath-test-configmap-4c9r no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4c9r
Dec 23 13:55:16.893: INFO: Deleting pod "pod-subpath-test-configmap-4c9r" in namespace "subpath-8984"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:55:16.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8984" for this suite.

• [SLOW TEST:24.199 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":68,"skipped":1477,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:55:16.905: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Dec 23 13:55:16.950: INFO: Waiting up to 5m0s for pod "var-expansion-db62b90b-01b1-4109-af19-a14df32f0b13" in namespace "var-expansion-3628" to be "Succeeded or Failed"
Dec 23 13:55:16.959: INFO: Pod "var-expansion-db62b90b-01b1-4109-af19-a14df32f0b13": Phase="Pending", Reason="", readiness=false. Elapsed: 9.00968ms
Dec 23 13:55:18.966: INFO: Pod "var-expansion-db62b90b-01b1-4109-af19-a14df32f0b13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016553551s
Dec 23 13:55:20.975: INFO: Pod "var-expansion-db62b90b-01b1-4109-af19-a14df32f0b13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025072781s
STEP: Saw pod success
Dec 23 13:55:20.975: INFO: Pod "var-expansion-db62b90b-01b1-4109-af19-a14df32f0b13" satisfied condition "Succeeded or Failed"
Dec 23 13:55:20.978: INFO: Trying to get logs from node wt-k8s-3.novalocal pod var-expansion-db62b90b-01b1-4109-af19-a14df32f0b13 container dapi-container: <nil>
STEP: delete the pod
Dec 23 13:55:20.998: INFO: Waiting for pod var-expansion-db62b90b-01b1-4109-af19-a14df32f0b13 to disappear
Dec 23 13:55:21.001: INFO: Pod var-expansion-db62b90b-01b1-4109-af19-a14df32f0b13 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:55:21.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3628" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":69,"skipped":1481,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:55:21.009: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Dec 23 13:55:21.049: INFO: Waiting up to 5m0s for pod "downward-api-df793e14-f788-4c4a-af37-638f41dac512" in namespace "downward-api-74" to be "Succeeded or Failed"
Dec 23 13:55:21.052: INFO: Pod "downward-api-df793e14-f788-4c4a-af37-638f41dac512": Phase="Pending", Reason="", readiness=false. Elapsed: 2.451412ms
Dec 23 13:55:23.061: INFO: Pod "downward-api-df793e14-f788-4c4a-af37-638f41dac512": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011535914s
Dec 23 13:55:25.070: INFO: Pod "downward-api-df793e14-f788-4c4a-af37-638f41dac512": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020356604s
STEP: Saw pod success
Dec 23 13:55:25.070: INFO: Pod "downward-api-df793e14-f788-4c4a-af37-638f41dac512" satisfied condition "Succeeded or Failed"
Dec 23 13:55:25.073: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downward-api-df793e14-f788-4c4a-af37-638f41dac512 container dapi-container: <nil>
STEP: delete the pod
Dec 23 13:55:25.098: INFO: Waiting for pod downward-api-df793e14-f788-4c4a-af37-638f41dac512 to disappear
Dec 23 13:55:25.112: INFO: Pod downward-api-df793e14-f788-4c4a-af37-638f41dac512 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:55:25.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-74" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":70,"skipped":1516,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:55:25.122: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1000
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1000
STEP: Creating statefulset with conflicting port in namespace statefulset-1000
STEP: Waiting until pod test-pod will start running in namespace statefulset-1000
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1000
Dec 23 13:55:29.210: INFO: Observed stateful pod in namespace: statefulset-1000, name: ss-0, uid: 3d645678-bdcc-4ce5-9523-e2d7b49f016c, status phase: Pending. Waiting for statefulset controller to delete.
Dec 23 13:55:29.296: INFO: Observed stateful pod in namespace: statefulset-1000, name: ss-0, uid: 3d645678-bdcc-4ce5-9523-e2d7b49f016c, status phase: Failed. Waiting for statefulset controller to delete.
Dec 23 13:55:29.304: INFO: Observed stateful pod in namespace: statefulset-1000, name: ss-0, uid: 3d645678-bdcc-4ce5-9523-e2d7b49f016c, status phase: Failed. Waiting for statefulset controller to delete.
Dec 23 13:55:29.313: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1000
STEP: Removing pod with conflicting port in namespace statefulset-1000
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1000 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 23 13:55:33.383: INFO: Deleting all statefulset in ns statefulset-1000
Dec 23 13:55:33.386: INFO: Scaling statefulset ss to 0
Dec 23 13:55:43.402: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 13:55:43.404: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:55:43.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1000" for this suite.

• [SLOW TEST:18.528 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":71,"skipped":1517,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:55:43.650: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec 23 13:55:43.745: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 23 13:56:43.768: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:56:43.770: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Dec 23 13:56:47.845: INFO: found a healthy node: wt-k8s-3.novalocal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 13:57:03.936: INFO: pods created so far: [1 1 1]
Dec 23 13:57:03.936: INFO: length of pods created so far: 3
Dec 23 13:57:15.948: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:57:22.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4234" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:57:22.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-607" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:99.371 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":72,"skipped":1524,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:57:23.022: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2303
STEP: creating service affinity-nodeport-transition in namespace services-2303
STEP: creating replication controller affinity-nodeport-transition in namespace services-2303
I1223 13:57:23.091411      20 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-2303, replica count: 3
I1223 13:57:26.141875      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 23 13:57:26.158: INFO: Creating new exec pod
Dec 23 13:57:29.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2303 exec execpod-affinitychcl8 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Dec 23 13:57:29.713: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Dec 23 13:57:29.713: INFO: stdout: ""
Dec 23 13:57:29.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2303 exec execpod-affinitychcl8 -- /bin/sh -x -c nc -zv -t -w 2 10.104.225.189 80'
Dec 23 13:57:29.908: INFO: stderr: "+ nc -zv -t -w 2 10.104.225.189 80\nConnection to 10.104.225.189 80 port [tcp/http] succeeded!\n"
Dec 23 13:57:29.908: INFO: stdout: ""
Dec 23 13:57:29.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2303 exec execpod-affinitychcl8 -- /bin/sh -x -c nc -zv -t -w 2 10.22.19.17 30946'
Dec 23 13:57:30.115: INFO: stderr: "+ nc -zv -t -w 2 10.22.19.17 30946\nConnection to 10.22.19.17 30946 port [tcp/30946] succeeded!\n"
Dec 23 13:57:30.115: INFO: stdout: ""
Dec 23 13:57:30.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2303 exec execpod-affinitychcl8 -- /bin/sh -x -c nc -zv -t -w 2 10.22.19.25 30946'
Dec 23 13:57:30.321: INFO: stderr: "+ nc -zv -t -w 2 10.22.19.25 30946\nConnection to 10.22.19.25 30946 port [tcp/30946] succeeded!\n"
Dec 23 13:57:30.321: INFO: stdout: ""
Dec 23 13:57:30.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2303 exec execpod-affinitychcl8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.22.19.17:30946/ ; done'
Dec 23 13:57:30.657: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n"
Dec 23 13:57:30.657: INFO: stdout: "\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-qvgjj\naffinity-nodeport-transition-dqf4x\naffinity-nodeport-transition-qvgjj\naffinity-nodeport-transition-qvgjj\naffinity-nodeport-transition-dqf4x\naffinity-nodeport-transition-qvgjj\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-dqf4x\naffinity-nodeport-transition-qvgjj\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-qvgjj\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-qvgjj"
Dec 23 13:57:30.657: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.657: INFO: Received response from host: affinity-nodeport-transition-qvgjj
Dec 23 13:57:30.657: INFO: Received response from host: affinity-nodeport-transition-dqf4x
Dec 23 13:57:30.657: INFO: Received response from host: affinity-nodeport-transition-qvgjj
Dec 23 13:57:30.657: INFO: Received response from host: affinity-nodeport-transition-qvgjj
Dec 23 13:57:30.657: INFO: Received response from host: affinity-nodeport-transition-dqf4x
Dec 23 13:57:30.657: INFO: Received response from host: affinity-nodeport-transition-qvgjj
Dec 23 13:57:30.657: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.657: INFO: Received response from host: affinity-nodeport-transition-dqf4x
Dec 23 13:57:30.657: INFO: Received response from host: affinity-nodeport-transition-qvgjj
Dec 23 13:57:30.657: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.657: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.657: INFO: Received response from host: affinity-nodeport-transition-qvgjj
Dec 23 13:57:30.657: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.657: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.657: INFO: Received response from host: affinity-nodeport-transition-qvgjj
Dec 23 13:57:30.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2303 exec execpod-affinitychcl8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.22.19.17:30946/ ; done'
Dec 23 13:57:30.987: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30946/\n"
Dec 23 13:57:30.987: INFO: stdout: "\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-tj4wv\naffinity-nodeport-transition-tj4wv"
Dec 23 13:57:30.987: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.987: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.987: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.987: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.987: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.987: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.987: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.987: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.987: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.987: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.987: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.987: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.987: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.987: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.987: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.987: INFO: Received response from host: affinity-nodeport-transition-tj4wv
Dec 23 13:57:30.987: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2303, will wait for the garbage collector to delete the pods
Dec 23 13:57:31.064: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.20655ms
Dec 23 13:57:31.164: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.229817ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:58:08.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2303" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:45.101 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":73,"skipped":1577,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:58:08.124: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:58:12.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4619" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":74,"skipped":1589,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:58:12.884: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Dec 23 13:58:12.963: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Dec 23 13:58:13.475: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Dec 23 13:58:15.682: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744328679, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744328679, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744328679, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744328679, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-84dcb7b7db\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 13:58:27.124: INFO: Waited 9.428872037s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:58:27.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5459" for this suite.

• [SLOW TEST:15.019 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":75,"skipped":1595,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:58:27.903: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 13:58:27.959: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-5966fb30-f60e-4dd7-a339-067730631ccb" in namespace "security-context-test-7971" to be "Succeeded or Failed"
Dec 23 13:58:27.966: INFO: Pod "busybox-privileged-false-5966fb30-f60e-4dd7-a339-067730631ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.526679ms
Dec 23 13:58:29.974: INFO: Pod "busybox-privileged-false-5966fb30-f60e-4dd7-a339-067730631ccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015449191s
Dec 23 13:58:29.974: INFO: Pod "busybox-privileged-false-5966fb30-f60e-4dd7-a339-067730631ccb" satisfied condition "Succeeded or Failed"
Dec 23 13:58:29.989: INFO: Got logs for pod "busybox-privileged-false-5966fb30-f60e-4dd7-a339-067730631ccb": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:58:29.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7971" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":76,"skipped":1604,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:58:29.997: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6157.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6157.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6157.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6157.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6157.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6157.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6157.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6157.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6157.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6157.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6157.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6157.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6157.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 228.97.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.97.228_udp@PTR;check="$$(dig +tcp +noall +answer +search 228.97.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.97.228_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6157.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6157.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6157.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6157.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6157.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6157.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6157.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6157.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6157.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6157.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6157.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6157.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6157.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 228.97.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.97.228_udp@PTR;check="$$(dig +tcp +noall +answer +search 228.97.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.97.228_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 23 13:58:34.112: INFO: Unable to read wheezy_udp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:34.115: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:34.117: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:34.137: INFO: Unable to read jessie_udp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:34.139: INFO: Unable to read jessie_tcp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:34.142: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:34.144: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:34.159: INFO: Lookups using dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4 failed for: [wheezy_udp@dns-test-service.dns-6157.svc.cluster.local wheezy_tcp@dns-test-service.dns-6157.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6157.svc.cluster.local jessie_udp@dns-test-service.dns-6157.svc.cluster.local jessie_tcp@dns-test-service.dns-6157.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6157.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6157.svc.cluster.local]

Dec 23 13:58:39.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:39.165: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:39.189: INFO: Unable to read jessie_udp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:39.191: INFO: Unable to read jessie_tcp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:39.210: INFO: Lookups using dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4 failed for: [wheezy_udp@dns-test-service.dns-6157.svc.cluster.local wheezy_tcp@dns-test-service.dns-6157.svc.cluster.local jessie_udp@dns-test-service.dns-6157.svc.cluster.local jessie_tcp@dns-test-service.dns-6157.svc.cluster.local]

Dec 23 13:58:44.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:44.165: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:44.185: INFO: Unable to read jessie_udp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:44.188: INFO: Unable to read jessie_tcp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:44.207: INFO: Lookups using dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4 failed for: [wheezy_udp@dns-test-service.dns-6157.svc.cluster.local wheezy_tcp@dns-test-service.dns-6157.svc.cluster.local jessie_udp@dns-test-service.dns-6157.svc.cluster.local jessie_tcp@dns-test-service.dns-6157.svc.cluster.local]

Dec 23 13:58:49.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:49.166: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:49.195: INFO: Unable to read jessie_udp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:49.199: INFO: Unable to read jessie_tcp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:49.226: INFO: Lookups using dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4 failed for: [wheezy_udp@dns-test-service.dns-6157.svc.cluster.local wheezy_tcp@dns-test-service.dns-6157.svc.cluster.local jessie_udp@dns-test-service.dns-6157.svc.cluster.local jessie_tcp@dns-test-service.dns-6157.svc.cluster.local]

Dec 23 13:58:54.164: INFO: Unable to read wheezy_udp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:54.167: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:54.193: INFO: Unable to read jessie_udp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:54.196: INFO: Unable to read jessie_tcp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:54.220: INFO: Lookups using dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4 failed for: [wheezy_udp@dns-test-service.dns-6157.svc.cluster.local wheezy_tcp@dns-test-service.dns-6157.svc.cluster.local jessie_udp@dns-test-service.dns-6157.svc.cluster.local jessie_tcp@dns-test-service.dns-6157.svc.cluster.local]

Dec 23 13:58:59.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:59.165: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:59.188: INFO: Unable to read jessie_udp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:59.190: INFO: Unable to read jessie_tcp@dns-test-service.dns-6157.svc.cluster.local from pod dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4: the server could not find the requested resource (get pods dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4)
Dec 23 13:58:59.223: INFO: Lookups using dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4 failed for: [wheezy_udp@dns-test-service.dns-6157.svc.cluster.local wheezy_tcp@dns-test-service.dns-6157.svc.cluster.local jessie_udp@dns-test-service.dns-6157.svc.cluster.local jessie_tcp@dns-test-service.dns-6157.svc.cluster.local]

Dec 23 13:59:04.219: INFO: DNS probes using dns-6157/dns-test-1092e871-c8c0-4e50-a76b-9b15a89367c4 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 13:59:04.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6157" for this suite.

• [SLOW TEST:34.390 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":77,"skipped":1608,"failed":0}
SS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 13:59:04.387: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7010
STEP: creating service affinity-clusterip in namespace services-7010
STEP: creating replication controller affinity-clusterip in namespace services-7010
I1223 13:59:04.516159      20 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-7010, replica count: 3
I1223 13:59:07.566491      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1223 13:59:10.566761      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 23 13:59:10.583: INFO: Creating new exec pod
Dec 23 13:59:15.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-7010 exec execpod-affinity675dz -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Dec 23 13:59:15.885: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Dec 23 13:59:15.885: INFO: stdout: ""
Dec 23 13:59:15.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-7010 exec execpod-affinity675dz -- /bin/sh -x -c nc -zv -t -w 2 10.106.216.200 80'
Dec 23 13:59:16.138: INFO: stderr: "+ nc -zv -t -w 2 10.106.216.200 80\nConnection to 10.106.216.200 80 port [tcp/http] succeeded!\n"
Dec 23 13:59:16.138: INFO: stdout: ""
Dec 23 13:59:16.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-7010 exec execpod-affinity675dz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.216.200:80/ ; done'
Dec 23 13:59:16.416: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.216.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.216.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.216.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.216.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.216.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.216.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.216.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.216.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.216.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.216.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.216.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.216.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.216.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.216.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.216.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.216.200:80/\n"
Dec 23 13:59:16.416: INFO: stdout: "\naffinity-clusterip-dsbgs\naffinity-clusterip-dsbgs\naffinity-clusterip-dsbgs\naffinity-clusterip-dsbgs\naffinity-clusterip-dsbgs\naffinity-clusterip-dsbgs\naffinity-clusterip-dsbgs\naffinity-clusterip-dsbgs\naffinity-clusterip-dsbgs\naffinity-clusterip-dsbgs\naffinity-clusterip-dsbgs\naffinity-clusterip-dsbgs\naffinity-clusterip-dsbgs\naffinity-clusterip-dsbgs\naffinity-clusterip-dsbgs\naffinity-clusterip-dsbgs"
Dec 23 13:59:16.416: INFO: Received response from host: affinity-clusterip-dsbgs
Dec 23 13:59:16.416: INFO: Received response from host: affinity-clusterip-dsbgs
Dec 23 13:59:16.416: INFO: Received response from host: affinity-clusterip-dsbgs
Dec 23 13:59:16.416: INFO: Received response from host: affinity-clusterip-dsbgs
Dec 23 13:59:16.416: INFO: Received response from host: affinity-clusterip-dsbgs
Dec 23 13:59:16.416: INFO: Received response from host: affinity-clusterip-dsbgs
Dec 23 13:59:16.416: INFO: Received response from host: affinity-clusterip-dsbgs
Dec 23 13:59:16.416: INFO: Received response from host: affinity-clusterip-dsbgs
Dec 23 13:59:16.416: INFO: Received response from host: affinity-clusterip-dsbgs
Dec 23 13:59:16.416: INFO: Received response from host: affinity-clusterip-dsbgs
Dec 23 13:59:16.416: INFO: Received response from host: affinity-clusterip-dsbgs
Dec 23 13:59:16.416: INFO: Received response from host: affinity-clusterip-dsbgs
Dec 23 13:59:16.416: INFO: Received response from host: affinity-clusterip-dsbgs
Dec 23 13:59:16.416: INFO: Received response from host: affinity-clusterip-dsbgs
Dec 23 13:59:16.416: INFO: Received response from host: affinity-clusterip-dsbgs
Dec 23 13:59:16.416: INFO: Received response from host: affinity-clusterip-dsbgs
Dec 23 13:59:16.416: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7010, will wait for the garbage collector to delete the pods
Dec 23 13:59:16.491: INFO: Deleting ReplicationController affinity-clusterip took: 6.155686ms
Dec 23 13:59:16.592: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.780459ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:00:19.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7010" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:75.005 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":78,"skipped":1610,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:00:19.394: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-vxzgt in namespace proxy-2629
I1223 14:00:19.614891      20 runners.go:190] Created replication controller with name: proxy-service-vxzgt, namespace: proxy-2629, replica count: 1
I1223 14:00:20.665883      20 runners.go:190] proxy-service-vxzgt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1223 14:00:21.668788      20 runners.go:190] proxy-service-vxzgt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1223 14:00:22.669018      20 runners.go:190] proxy-service-vxzgt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1223 14:00:23.669224      20 runners.go:190] proxy-service-vxzgt Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 23 14:00:23.673: INFO: setup took 4.239365917s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 23 14:00:23.687: INFO: (0) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 13.579851ms)
Dec 23 14:00:23.688: INFO: (0) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 14.122614ms)
Dec 23 14:00:23.691: INFO: (0) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 16.525882ms)
Dec 23 14:00:23.691: INFO: (0) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 17.360615ms)
Dec 23 14:00:23.691: INFO: (0) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 15.910751ms)
Dec 23 14:00:23.692: INFO: (0) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 17.112913ms)
Dec 23 14:00:23.692: INFO: (0) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 16.960126ms)
Dec 23 14:00:23.692: INFO: (0) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 17.909022ms)
Dec 23 14:00:23.692: INFO: (0) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 17.182734ms)
Dec 23 14:00:23.692: INFO: (0) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 17.75098ms)
Dec 23 14:00:23.701: INFO: (0) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 27.787255ms)
Dec 23 14:00:23.701: INFO: (0) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 26.598643ms)
Dec 23 14:00:23.701: INFO: (0) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 26.747417ms)
Dec 23 14:00:23.702: INFO: (0) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 28.178284ms)
Dec 23 14:00:23.702: INFO: (0) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 27.64086ms)
Dec 23 14:00:23.702: INFO: (0) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 27.540606ms)
Dec 23 14:00:23.716: INFO: (1) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 12.803958ms)
Dec 23 14:00:23.716: INFO: (1) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 13.693007ms)
Dec 23 14:00:23.716: INFO: (1) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 13.525989ms)
Dec 23 14:00:23.716: INFO: (1) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 14.162042ms)
Dec 23 14:00:23.716: INFO: (1) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 14.430813ms)
Dec 23 14:00:23.716: INFO: (1) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 13.487452ms)
Dec 23 14:00:23.716: INFO: (1) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 13.358902ms)
Dec 23 14:00:23.717: INFO: (1) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 12.95109ms)
Dec 23 14:00:23.717: INFO: (1) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 12.861976ms)
Dec 23 14:00:23.717: INFO: (1) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 12.760792ms)
Dec 23 14:00:23.720: INFO: (1) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 17.953639ms)
Dec 23 14:00:23.720: INFO: (1) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 18.376239ms)
Dec 23 14:00:23.720: INFO: (1) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 17.150454ms)
Dec 23 14:00:23.721: INFO: (1) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 19.088766ms)
Dec 23 14:00:23.721: INFO: (1) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 17.68505ms)
Dec 23 14:00:23.721: INFO: (1) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 18.572465ms)
Dec 23 14:00:23.748: INFO: (2) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 26.427355ms)
Dec 23 14:00:23.749: INFO: (2) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 27.580109ms)
Dec 23 14:00:23.749: INFO: (2) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 28.087916ms)
Dec 23 14:00:23.749: INFO: (2) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 26.771053ms)
Dec 23 14:00:23.750: INFO: (2) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 28.070441ms)
Dec 23 14:00:23.750: INFO: (2) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 28.616847ms)
Dec 23 14:00:23.750: INFO: (2) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 29.050888ms)
Dec 23 14:00:23.750: INFO: (2) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 27.912091ms)
Dec 23 14:00:23.750: INFO: (2) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 28.036934ms)
Dec 23 14:00:23.750: INFO: (2) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 28.484931ms)
Dec 23 14:00:23.750: INFO: (2) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 28.903116ms)
Dec 23 14:00:23.750: INFO: (2) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 28.372549ms)
Dec 23 14:00:23.753: INFO: (2) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 30.556336ms)
Dec 23 14:00:23.753: INFO: (2) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 30.855786ms)
Dec 23 14:00:23.753: INFO: (2) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 31.428263ms)
Dec 23 14:00:23.753: INFO: (2) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 31.56341ms)
Dec 23 14:00:23.767: INFO: (3) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 13.637781ms)
Dec 23 14:00:23.767: INFO: (3) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 13.321972ms)
Dec 23 14:00:23.767: INFO: (3) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 12.571698ms)
Dec 23 14:00:23.767: INFO: (3) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 13.219937ms)
Dec 23 14:00:23.767: INFO: (3) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 12.517581ms)
Dec 23 14:00:23.767: INFO: (3) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 12.447063ms)
Dec 23 14:00:23.767: INFO: (3) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 12.782798ms)
Dec 23 14:00:23.767: INFO: (3) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 13.235209ms)
Dec 23 14:00:23.768: INFO: (3) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 13.613997ms)
Dec 23 14:00:23.768: INFO: (3) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 14.163381ms)
Dec 23 14:00:23.768: INFO: (3) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 13.903877ms)
Dec 23 14:00:23.772: INFO: (3) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 18.217455ms)
Dec 23 14:00:23.772: INFO: (3) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 17.260149ms)
Dec 23 14:00:23.772: INFO: (3) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 18.866485ms)
Dec 23 14:00:23.772: INFO: (3) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 18.122234ms)
Dec 23 14:00:23.773: INFO: (3) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 18.415248ms)
Dec 23 14:00:23.787: INFO: (4) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 13.924122ms)
Dec 23 14:00:23.787: INFO: (4) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 14.007143ms)
Dec 23 14:00:23.787: INFO: (4) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 13.61714ms)
Dec 23 14:00:23.787: INFO: (4) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 13.949101ms)
Dec 23 14:00:23.787: INFO: (4) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 13.73555ms)
Dec 23 14:00:23.787: INFO: (4) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 14.160122ms)
Dec 23 14:00:23.787: INFO: (4) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 14.452597ms)
Dec 23 14:00:23.787: INFO: (4) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 13.995518ms)
Dec 23 14:00:23.787: INFO: (4) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 14.159711ms)
Dec 23 14:00:23.787: INFO: (4) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 13.855019ms)
Dec 23 14:00:23.790: INFO: (4) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 17.448641ms)
Dec 23 14:00:23.791: INFO: (4) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 18.181022ms)
Dec 23 14:00:23.791: INFO: (4) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 17.722514ms)
Dec 23 14:00:23.791: INFO: (4) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 18.439039ms)
Dec 23 14:00:23.791: INFO: (4) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 17.94618ms)
Dec 23 14:00:23.791: INFO: (4) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 18.299035ms)
Dec 23 14:00:23.795: INFO: (5) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 3.824487ms)
Dec 23 14:00:23.802: INFO: (5) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 8.434669ms)
Dec 23 14:00:23.802: INFO: (5) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 10.105428ms)
Dec 23 14:00:23.802: INFO: (5) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 9.052448ms)
Dec 23 14:00:23.802: INFO: (5) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 10.039531ms)
Dec 23 14:00:23.802: INFO: (5) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 10.520832ms)
Dec 23 14:00:23.803: INFO: (5) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 10.550813ms)
Dec 23 14:00:23.803: INFO: (5) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 10.824196ms)
Dec 23 14:00:23.803: INFO: (5) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 11.645113ms)
Dec 23 14:00:23.803: INFO: (5) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 10.681238ms)
Dec 23 14:00:23.808: INFO: (5) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 16.245312ms)
Dec 23 14:00:23.808: INFO: (5) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 16.48953ms)
Dec 23 14:00:23.808: INFO: (5) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 15.086438ms)
Dec 23 14:00:23.809: INFO: (5) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 15.434978ms)
Dec 23 14:00:23.809: INFO: (5) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 15.725616ms)
Dec 23 14:00:23.809: INFO: (5) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 15.64918ms)
Dec 23 14:00:23.822: INFO: (6) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 13.004198ms)
Dec 23 14:00:23.822: INFO: (6) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 13.233674ms)
Dec 23 14:00:23.822: INFO: (6) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 13.341425ms)
Dec 23 14:00:23.822: INFO: (6) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 13.052124ms)
Dec 23 14:00:23.822: INFO: (6) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 13.418652ms)
Dec 23 14:00:23.822: INFO: (6) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 13.626211ms)
Dec 23 14:00:23.823: INFO: (6) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 14.229133ms)
Dec 23 14:00:23.823: INFO: (6) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 14.569663ms)
Dec 23 14:00:23.823: INFO: (6) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 14.212173ms)
Dec 23 14:00:23.823: INFO: (6) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 14.344518ms)
Dec 23 14:00:23.828: INFO: (6) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 18.914444ms)
Dec 23 14:00:23.828: INFO: (6) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 19.025861ms)
Dec 23 14:00:23.828: INFO: (6) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 18.75423ms)
Dec 23 14:00:23.829: INFO: (6) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 19.288998ms)
Dec 23 14:00:23.829: INFO: (6) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 19.871904ms)
Dec 23 14:00:23.831: INFO: (6) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 21.298272ms)
Dec 23 14:00:23.838: INFO: (7) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 7.395042ms)
Dec 23 14:00:23.838: INFO: (7) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 7.409442ms)
Dec 23 14:00:23.839: INFO: (7) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 7.839224ms)
Dec 23 14:00:23.839: INFO: (7) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 7.683996ms)
Dec 23 14:00:23.850: INFO: (7) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 18.139397ms)
Dec 23 14:00:23.850: INFO: (7) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 18.235635ms)
Dec 23 14:00:23.850: INFO: (7) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 18.563147ms)
Dec 23 14:00:23.850: INFO: (7) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 18.268291ms)
Dec 23 14:00:23.850: INFO: (7) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 18.878551ms)
Dec 23 14:00:23.850: INFO: (7) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 18.344242ms)
Dec 23 14:00:23.852: INFO: (7) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 20.139658ms)
Dec 23 14:00:23.853: INFO: (7) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 20.933508ms)
Dec 23 14:00:23.853: INFO: (7) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 21.638441ms)
Dec 23 14:00:23.853: INFO: (7) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 21.176423ms)
Dec 23 14:00:23.853: INFO: (7) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 21.553315ms)
Dec 23 14:00:23.855: INFO: (7) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 22.642051ms)
Dec 23 14:00:23.868: INFO: (8) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 12.513903ms)
Dec 23 14:00:23.869: INFO: (8) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 13.724667ms)
Dec 23 14:00:23.869: INFO: (8) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 13.90123ms)
Dec 23 14:00:23.869: INFO: (8) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 14.14966ms)
Dec 23 14:00:23.869: INFO: (8) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 14.554519ms)
Dec 23 14:00:23.869: INFO: (8) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 14.370588ms)
Dec 23 14:00:23.869: INFO: (8) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 14.53322ms)
Dec 23 14:00:23.869: INFO: (8) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 14.218088ms)
Dec 23 14:00:23.870: INFO: (8) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 14.778288ms)
Dec 23 14:00:23.870: INFO: (8) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 14.928417ms)
Dec 23 14:00:23.870: INFO: (8) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 15.609806ms)
Dec 23 14:00:23.870: INFO: (8) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 15.207232ms)
Dec 23 14:00:23.870: INFO: (8) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 15.609087ms)
Dec 23 14:00:23.870: INFO: (8) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 15.315561ms)
Dec 23 14:00:23.874: INFO: (8) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 19.113985ms)
Dec 23 14:00:23.874: INFO: (8) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 19.131633ms)
Dec 23 14:00:23.886: INFO: (9) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 10.377282ms)
Dec 23 14:00:23.886: INFO: (9) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 11.300342ms)
Dec 23 14:00:23.896: INFO: (9) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 21.769912ms)
Dec 23 14:00:23.896: INFO: (9) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 21.706396ms)
Dec 23 14:00:23.896: INFO: (9) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 22.188493ms)
Dec 23 14:00:23.896: INFO: (9) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 21.548602ms)
Dec 23 14:00:23.896: INFO: (9) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 20.93581ms)
Dec 23 14:00:23.897: INFO: (9) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 20.72187ms)
Dec 23 14:00:23.897: INFO: (9) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 22.266007ms)
Dec 23 14:00:23.897: INFO: (9) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 21.709864ms)
Dec 23 14:00:23.897: INFO: (9) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 20.633701ms)
Dec 23 14:00:23.897: INFO: (9) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 21.090241ms)
Dec 23 14:00:23.897: INFO: (9) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 21.38438ms)
Dec 23 14:00:23.897: INFO: (9) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 21.535799ms)
Dec 23 14:00:23.897: INFO: (9) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 21.066304ms)
Dec 23 14:00:23.897: INFO: (9) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 22.436559ms)
Dec 23 14:00:23.906: INFO: (10) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 8.961262ms)
Dec 23 14:00:23.907: INFO: (10) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 10.194031ms)
Dec 23 14:00:23.909: INFO: (10) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 10.726468ms)
Dec 23 14:00:23.910: INFO: (10) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 12.776281ms)
Dec 23 14:00:23.910: INFO: (10) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 12.479331ms)
Dec 23 14:00:23.910: INFO: (10) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 11.102845ms)
Dec 23 14:00:23.910: INFO: (10) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 11.028251ms)
Dec 23 14:00:23.910: INFO: (10) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 12.990358ms)
Dec 23 14:00:23.910: INFO: (10) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 12.094834ms)
Dec 23 14:00:23.911: INFO: (10) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 13.466578ms)
Dec 23 14:00:23.911: INFO: (10) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 13.169001ms)
Dec 23 14:00:23.914: INFO: (10) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 16.086332ms)
Dec 23 14:00:23.914: INFO: (10) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 16.01899ms)
Dec 23 14:00:23.915: INFO: (10) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 15.822848ms)
Dec 23 14:00:23.915: INFO: (10) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 16.013519ms)
Dec 23 14:00:23.915: INFO: (10) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 16.352381ms)
Dec 23 14:00:23.930: INFO: (11) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 15.189898ms)
Dec 23 14:00:23.932: INFO: (11) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 16.220718ms)
Dec 23 14:00:23.933: INFO: (11) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 17.551428ms)
Dec 23 14:00:23.933: INFO: (11) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 15.917265ms)
Dec 23 14:00:23.933: INFO: (11) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 16.363939ms)
Dec 23 14:00:23.933: INFO: (11) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 17.631902ms)
Dec 23 14:00:23.933: INFO: (11) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 16.211336ms)
Dec 23 14:00:23.933: INFO: (11) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 17.815347ms)
Dec 23 14:00:23.933: INFO: (11) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 16.894244ms)
Dec 23 14:00:23.933: INFO: (11) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 17.262176ms)
Dec 23 14:00:23.934: INFO: (11) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 18.727406ms)
Dec 23 14:00:23.934: INFO: (11) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 18.486805ms)
Dec 23 14:00:23.934: INFO: (11) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 18.975782ms)
Dec 23 14:00:23.935: INFO: (11) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 18.998666ms)
Dec 23 14:00:23.935: INFO: (11) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 19.923916ms)
Dec 23 14:00:23.936: INFO: (11) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 20.592281ms)
Dec 23 14:00:23.945: INFO: (12) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 8.85898ms)
Dec 23 14:00:23.948: INFO: (12) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 10.903153ms)
Dec 23 14:00:23.948: INFO: (12) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 10.839673ms)
Dec 23 14:00:23.948: INFO: (12) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 10.661723ms)
Dec 23 14:00:23.948: INFO: (12) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 11.004972ms)
Dec 23 14:00:23.948: INFO: (12) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 11.040191ms)
Dec 23 14:00:23.949: INFO: (12) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 11.379445ms)
Dec 23 14:00:23.949: INFO: (12) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 11.19545ms)
Dec 23 14:00:23.950: INFO: (12) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 11.990018ms)
Dec 23 14:00:23.951: INFO: (12) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 12.686049ms)
Dec 23 14:00:23.951: INFO: (12) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 13.186555ms)
Dec 23 14:00:23.951: INFO: (12) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 14.633164ms)
Dec 23 14:00:23.958: INFO: (12) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 20.454329ms)
Dec 23 14:00:23.958: INFO: (12) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 20.435462ms)
Dec 23 14:00:23.958: INFO: (12) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 20.528359ms)
Dec 23 14:00:23.963: INFO: (12) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 24.982016ms)
Dec 23 14:00:23.980: INFO: (13) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 16.362931ms)
Dec 23 14:00:23.983: INFO: (13) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 19.501082ms)
Dec 23 14:00:23.983: INFO: (13) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 19.667499ms)
Dec 23 14:00:23.983: INFO: (13) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 19.053589ms)
Dec 23 14:00:23.983: INFO: (13) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 18.904057ms)
Dec 23 14:00:23.983: INFO: (13) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 19.166537ms)
Dec 23 14:00:23.983: INFO: (13) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 19.506853ms)
Dec 23 14:00:23.984: INFO: (13) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 20.732327ms)
Dec 23 14:00:23.985: INFO: (13) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 21.149691ms)
Dec 23 14:00:23.985: INFO: (13) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 21.24221ms)
Dec 23 14:00:23.986: INFO: (13) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 22.377542ms)
Dec 23 14:00:23.986: INFO: (13) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 22.838843ms)
Dec 23 14:00:23.986: INFO: (13) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 23.108023ms)
Dec 23 14:00:23.986: INFO: (13) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 22.392862ms)
Dec 23 14:00:23.986: INFO: (13) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 23.183339ms)
Dec 23 14:00:23.987: INFO: (13) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 22.95303ms)
Dec 23 14:00:23.997: INFO: (14) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 10.411096ms)
Dec 23 14:00:23.998: INFO: (14) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 10.652536ms)
Dec 23 14:00:23.998: INFO: (14) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 10.842143ms)
Dec 23 14:00:24.022: INFO: (14) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 34.36187ms)
Dec 23 14:00:24.023: INFO: (14) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 34.999645ms)
Dec 23 14:00:24.023: INFO: (14) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 36.142116ms)
Dec 23 14:00:24.024: INFO: (14) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 36.695673ms)
Dec 23 14:00:24.025: INFO: (14) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 36.710038ms)
Dec 23 14:00:24.025: INFO: (14) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 37.567428ms)
Dec 23 14:00:24.025: INFO: (14) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 37.508178ms)
Dec 23 14:00:24.028: INFO: (14) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 40.719374ms)
Dec 23 14:00:24.029: INFO: (14) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 41.290062ms)
Dec 23 14:00:24.029: INFO: (14) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 40.581839ms)
Dec 23 14:00:24.029: INFO: (14) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 40.885347ms)
Dec 23 14:00:24.030: INFO: (14) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 42.12854ms)
Dec 23 14:00:24.030: INFO: (14) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 41.642256ms)
Dec 23 14:00:24.041: INFO: (15) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 11.142971ms)
Dec 23 14:00:24.041: INFO: (15) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 11.217427ms)
Dec 23 14:00:24.041: INFO: (15) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 11.468858ms)
Dec 23 14:00:24.042: INFO: (15) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 10.948666ms)
Dec 23 14:00:24.044: INFO: (15) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 13.885999ms)
Dec 23 14:00:24.044: INFO: (15) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 13.435576ms)
Dec 23 14:00:24.045: INFO: (15) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 14.837821ms)
Dec 23 14:00:24.045: INFO: (15) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 14.549586ms)
Dec 23 14:00:24.045: INFO: (15) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 14.645203ms)
Dec 23 14:00:24.045: INFO: (15) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 14.840645ms)
Dec 23 14:00:24.047: INFO: (15) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 17.887529ms)
Dec 23 14:00:24.048: INFO: (15) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 17.604359ms)
Dec 23 14:00:24.048: INFO: (15) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 16.735854ms)
Dec 23 14:00:24.048: INFO: (15) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 17.837633ms)
Dec 23 14:00:24.048: INFO: (15) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 17.017196ms)
Dec 23 14:00:24.048: INFO: (15) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 16.999264ms)
Dec 23 14:00:24.059: INFO: (16) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 11.369491ms)
Dec 23 14:00:24.059: INFO: (16) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 10.543824ms)
Dec 23 14:00:24.059: INFO: (16) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 10.934745ms)
Dec 23 14:00:24.061: INFO: (16) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 12.682094ms)
Dec 23 14:00:24.062: INFO: (16) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 12.270157ms)
Dec 23 14:00:24.062: INFO: (16) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 12.381022ms)
Dec 23 14:00:24.062: INFO: (16) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 12.257454ms)
Dec 23 14:00:24.062: INFO: (16) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 13.715654ms)
Dec 23 14:00:24.063: INFO: (16) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 13.695099ms)
Dec 23 14:00:24.063: INFO: (16) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 13.803045ms)
Dec 23 14:00:24.063: INFO: (16) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 14.695067ms)
Dec 23 14:00:24.063: INFO: (16) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 15.019034ms)
Dec 23 14:00:24.065: INFO: (16) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 15.882052ms)
Dec 23 14:00:24.065: INFO: (16) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 16.573641ms)
Dec 23 14:00:24.065: INFO: (16) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 16.507943ms)
Dec 23 14:00:24.065: INFO: (16) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 16.304718ms)
Dec 23 14:00:24.077: INFO: (17) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 11.669484ms)
Dec 23 14:00:24.077: INFO: (17) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 11.420289ms)
Dec 23 14:00:24.078: INFO: (17) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 12.130526ms)
Dec 23 14:00:24.078: INFO: (17) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 12.573326ms)
Dec 23 14:00:24.079: INFO: (17) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 13.343175ms)
Dec 23 14:00:24.079: INFO: (17) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 12.872553ms)
Dec 23 14:00:24.079: INFO: (17) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 13.687895ms)
Dec 23 14:00:24.079: INFO: (17) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 13.290867ms)
Dec 23 14:00:24.079: INFO: (17) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 13.085423ms)
Dec 23 14:00:24.083: INFO: (17) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 17.110215ms)
Dec 23 14:00:24.083: INFO: (17) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 17.827894ms)
Dec 23 14:00:24.083: INFO: (17) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 17.213468ms)
Dec 23 14:00:24.084: INFO: (17) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 17.461223ms)
Dec 23 14:00:24.084: INFO: (17) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 18.276587ms)
Dec 23 14:00:24.084: INFO: (17) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 17.763325ms)
Dec 23 14:00:24.084: INFO: (17) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 18.634924ms)
Dec 23 14:00:24.097: INFO: (18) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 11.4164ms)
Dec 23 14:00:24.097: INFO: (18) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 12.082026ms)
Dec 23 14:00:24.097: INFO: (18) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 11.885628ms)
Dec 23 14:00:24.097: INFO: (18) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 11.846981ms)
Dec 23 14:00:24.097: INFO: (18) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 11.470323ms)
Dec 23 14:00:24.098: INFO: (18) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 11.927555ms)
Dec 23 14:00:24.098: INFO: (18) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 12.861169ms)
Dec 23 14:00:24.098: INFO: (18) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 12.247591ms)
Dec 23 14:00:24.098: INFO: (18) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 13.458153ms)
Dec 23 14:00:24.098: INFO: (18) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 12.776829ms)
Dec 23 14:00:24.098: INFO: (18) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 12.247327ms)
Dec 23 14:00:24.099: INFO: (18) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 13.14709ms)
Dec 23 14:00:24.099: INFO: (18) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 14.163292ms)
Dec 23 14:00:24.099: INFO: (18) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 13.672012ms)
Dec 23 14:00:24.099: INFO: (18) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 12.983486ms)
Dec 23 14:00:24.099: INFO: (18) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 13.790374ms)
Dec 23 14:00:24.111: INFO: (19) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 11.99482ms)
Dec 23 14:00:24.114: INFO: (19) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:460/proxy/: tls baz (200; 14.566308ms)
Dec 23 14:00:24.114: INFO: (19) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 15.1602ms)
Dec 23 14:00:24.115: INFO: (19) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:162/proxy/: bar (200; 15.0598ms)
Dec 23 14:00:24.115: INFO: (19) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:160/proxy/: foo (200; 15.544436ms)
Dec 23 14:00:24.115: INFO: (19) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:462/proxy/: tls qux (200; 15.929518ms)
Dec 23 14:00:24.115: INFO: (19) /api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/http:proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">... (200; 15.876004ms)
Dec 23 14:00:24.115: INFO: (19) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l:1080/proxy/rewriteme">test<... (200; 15.918593ms)
Dec 23 14:00:24.115: INFO: (19) /api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/proxy-service-vxzgt-nvn2l/proxy/rewriteme">test</a> (200; 15.515535ms)
Dec 23 14:00:24.117: INFO: (19) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname1/proxy/: foo (200; 17.692522ms)
Dec 23 14:00:24.117: INFO: (19) /api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/: <a href="/api/v1/namespaces/proxy-2629/pods/https:proxy-service-vxzgt-nvn2l:443/proxy/tlsrewritem... (200; 17.435689ms)
Dec 23 14:00:24.119: INFO: (19) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname2/proxy/: bar (200; 19.564538ms)
Dec 23 14:00:24.119: INFO: (19) /api/v1/namespaces/proxy-2629/services/proxy-service-vxzgt:portname1/proxy/: foo (200; 19.94353ms)
Dec 23 14:00:24.119: INFO: (19) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname1/proxy/: tls baz (200; 20.245909ms)
Dec 23 14:00:24.120: INFO: (19) /api/v1/namespaces/proxy-2629/services/http:proxy-service-vxzgt:portname2/proxy/: bar (200; 20.45431ms)
Dec 23 14:00:24.120: INFO: (19) /api/v1/namespaces/proxy-2629/services/https:proxy-service-vxzgt:tlsportname2/proxy/: tls qux (200; 20.477845ms)
STEP: deleting ReplicationController proxy-service-vxzgt in namespace proxy-2629, will wait for the garbage collector to delete the pods
Dec 23 14:00:24.179: INFO: Deleting ReplicationController proxy-service-vxzgt took: 6.694002ms
Dec 23 14:00:24.280: INFO: Terminating ReplicationController proxy-service-vxzgt pods took: 100.215417ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:01:19.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2629" for this suite.

• [SLOW TEST:60.012 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":79,"skipped":1621,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:01:19.406: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:01:26.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7201" for this suite.

• [SLOW TEST:7.087 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":80,"skipped":1638,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:01:26.494: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec 23 14:01:26.554: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 23 14:02:26.581: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Dec 23 14:02:26.617: INFO: Created pod: pod0-sched-preemption-low-priority
Dec 23 14:02:26.646: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:02:50.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6418" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:84.269 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":81,"skipped":1663,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:02:50.764: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Dec 23 14:02:50.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-6356 create -f -'
Dec 23 14:02:51.481: INFO: stderr: ""
Dec 23 14:02:51.481: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Dec 23 14:02:51.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-6356 diff -f -'
Dec 23 14:02:52.273: INFO: rc: 1
Dec 23 14:02:52.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-6356 delete -f -'
Dec 23 14:02:52.381: INFO: stderr: ""
Dec 23 14:02:52.381: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:02:52.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6356" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":82,"skipped":1669,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:02:52.417: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 14:02:52.577: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2aa2160-aa0b-4b21-bea6-02119ec809bc" in namespace "projected-7073" to be "Succeeded or Failed"
Dec 23 14:02:52.591: INFO: Pod "downwardapi-volume-c2aa2160-aa0b-4b21-bea6-02119ec809bc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.455071ms
Dec 23 14:02:54.596: INFO: Pod "downwardapi-volume-c2aa2160-aa0b-4b21-bea6-02119ec809bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018900286s
Dec 23 14:02:56.603: INFO: Pod "downwardapi-volume-c2aa2160-aa0b-4b21-bea6-02119ec809bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026786446s
STEP: Saw pod success
Dec 23 14:02:56.604: INFO: Pod "downwardapi-volume-c2aa2160-aa0b-4b21-bea6-02119ec809bc" satisfied condition "Succeeded or Failed"
Dec 23 14:02:56.606: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-c2aa2160-aa0b-4b21-bea6-02119ec809bc container client-container: <nil>
STEP: delete the pod
Dec 23 14:02:56.645: INFO: Waiting for pod downwardapi-volume-c2aa2160-aa0b-4b21-bea6-02119ec809bc to disappear
Dec 23 14:02:56.650: INFO: Pod downwardapi-volume-c2aa2160-aa0b-4b21-bea6-02119ec809bc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:02:56.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7073" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":83,"skipped":1704,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:02:56.661: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image abcsys.cn:5000/library/httpd:2.4.38-alpine
Dec 23 14:02:56.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-3980 run e2e-test-httpd-pod --image=abcsys.cn:5000/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Dec 23 14:02:56.815: INFO: stderr: ""
Dec 23 14:02:56.815: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Dec 23 14:03:01.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-3980 get pod e2e-test-httpd-pod -o json'
Dec 23 14:03:01.980: INFO: stderr: ""
Dec 23 14:03:01.980: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.244.201.248/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.201.248/32\"\n        },\n        \"creationTimestamp\": \"2020-12-23T14:02:42Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-23T14:02:42Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-23T14:02:43Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.244.201.248\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-23T14:02:44Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3980\",\n        \"resourceVersion\": \"86934\",\n        \"uid\": \"56316d3c-a681-4adc-8e4a-dfa25040e1f0\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"abcsys.cn:5000/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-jnfj2\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"wt-k8s-3.novalocal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-jnfj2\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-jnfj2\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-23T14:02:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-23T14:02:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-23T14:02:44Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-23T14:02:42Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://e1d0025bfdd0731e01585782b7bf3b89adcbd5c1657ce90e9aa293cbd1adb4e9\",\n                \"image\": \"abcsys.cn:5000/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://abcsys.cn:5000/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-12-23T14:02:44Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.22.19.25\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.201.248\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.201.248\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-12-23T14:02:43Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 23 14:03:01.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-3980 replace -f -'
Dec 23 14:03:02.406: INFO: stderr: ""
Dec 23 14:03:02.406: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image abcsys.cn:5000/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Dec 23 14:03:02.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-3980 delete pods e2e-test-httpd-pod'
Dec 23 14:03:09.261: INFO: stderr: ""
Dec 23 14:03:09.261: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:03:09.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3980" for this suite.

• [SLOW TEST:12.620 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":84,"skipped":1722,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:03:09.281: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-7d2b878d-f73b-4934-99d6-990edecf93bc
STEP: Creating a pod to test consume configMaps
Dec 23 14:03:09.323: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bed03459-beb3-4d2c-8b28-b96fa44decbf" in namespace "projected-9694" to be "Succeeded or Failed"
Dec 23 14:03:09.327: INFO: Pod "pod-projected-configmaps-bed03459-beb3-4d2c-8b28-b96fa44decbf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.204185ms
Dec 23 14:03:11.335: INFO: Pod "pod-projected-configmaps-bed03459-beb3-4d2c-8b28-b96fa44decbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012452408s
STEP: Saw pod success
Dec 23 14:03:11.335: INFO: Pod "pod-projected-configmaps-bed03459-beb3-4d2c-8b28-b96fa44decbf" satisfied condition "Succeeded or Failed"
Dec 23 14:03:11.338: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-projected-configmaps-bed03459-beb3-4d2c-8b28-b96fa44decbf container agnhost-container: <nil>
STEP: delete the pod
Dec 23 14:03:11.372: INFO: Waiting for pod pod-projected-configmaps-bed03459-beb3-4d2c-8b28-b96fa44decbf to disappear
Dec 23 14:03:11.379: INFO: Pod pod-projected-configmaps-bed03459-beb3-4d2c-8b28-b96fa44decbf no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:03:11.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9694" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":85,"skipped":1727,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:03:11.389: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 23 14:03:11.761: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 14:03:14.826: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
Dec 23 14:03:14.920: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:15.038: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:15.138: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:15.233: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:15.335: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:15.437: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:15.534: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:15.637: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:15.734: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:15.835: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:15.936: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:16.034: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:16.134: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:16.233: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:16.338: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:16.451: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:16.533: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:16.634: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:16.735: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:16.833: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:16.950: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:17.230: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:17.345: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:17.438: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:17.535: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:17.636: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:17.737: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:17.834: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:17.934: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:18.033: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:18.136: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:18.239: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:18.336: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:18.436: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:18.534: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:18.637: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:18.749: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:18.835: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:18.933: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:19.034: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:19.135: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:19.244: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:19.335: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:19.434: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:19.540: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:19.635: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:19.734: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:19.836: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:19.936: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:20.034: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:20.134: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:20.235: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:20.334: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:20.434: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:20.534: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:20.636: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:20.743: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:20.847: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:20.933: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:21.035: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:21.135: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:21.233: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:21.338: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:21.436: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:21.537: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:21.636: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:21.734: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:21.836: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:21.934: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:22.035: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:22.134: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:22.233: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:22.339: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:22.433: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:22.536: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:22.638: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:22.736: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:22.835: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:22.935: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:23.037: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:23.136: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:23.238: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:23.335: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:23.439: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:23.534: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:23.634: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:23.734: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:23.833: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:23.935: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:24.036: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:24.162: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:24.242: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:24.337: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:24.439: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:24.540: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:24.634: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:24.735: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:24.833: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:03:24.933: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource definition that should be denied by the webhook
Dec 23 14:03:25.032: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:03:25.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-304" for this suite.
STEP: Destroying namespace "webhook-304-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:13.775 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":86,"skipped":1767,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:03:25.164: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:03:25.430: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 23 14:03:25.453: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:25.462: INFO: Number of nodes with available pods: 0
Dec 23 14:03:25.462: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 14:03:26.469: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:26.472: INFO: Number of nodes with available pods: 0
Dec 23 14:03:26.472: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 14:03:27.471: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:27.475: INFO: Number of nodes with available pods: 1
Dec 23 14:03:27.475: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 14:03:28.470: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:28.473: INFO: Number of nodes with available pods: 2
Dec 23 14:03:28.473: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 23 14:03:28.507: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:28.507: INFO: Wrong image for pod: daemon-set-4thvg. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:28.522: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:29.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:29.528: INFO: Wrong image for pod: daemon-set-4thvg. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:29.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:30.527: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:30.527: INFO: Wrong image for pod: daemon-set-4thvg. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:30.527: INFO: Pod daemon-set-4thvg is not available
Dec 23 14:03:30.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:31.527: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:31.527: INFO: Wrong image for pod: daemon-set-4thvg. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:31.527: INFO: Pod daemon-set-4thvg is not available
Dec 23 14:03:31.530: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:32.529: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:32.529: INFO: Wrong image for pod: daemon-set-4thvg. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:32.529: INFO: Pod daemon-set-4thvg is not available
Dec 23 14:03:32.533: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:33.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:33.528: INFO: Wrong image for pod: daemon-set-4thvg. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:33.528: INFO: Pod daemon-set-4thvg is not available
Dec 23 14:03:33.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:34.526: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:34.526: INFO: Wrong image for pod: daemon-set-4thvg. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:34.526: INFO: Pod daemon-set-4thvg is not available
Dec 23 14:03:34.530: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:35.529: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:35.529: INFO: Wrong image for pod: daemon-set-4thvg. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:35.529: INFO: Pod daemon-set-4thvg is not available
Dec 23 14:03:35.533: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:36.529: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:36.529: INFO: Wrong image for pod: daemon-set-4thvg. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:36.529: INFO: Pod daemon-set-4thvg is not available
Dec 23 14:03:36.532: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:37.532: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:37.532: INFO: Wrong image for pod: daemon-set-4thvg. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:37.532: INFO: Pod daemon-set-4thvg is not available
Dec 23 14:03:37.536: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:38.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:38.528: INFO: Wrong image for pod: daemon-set-4thvg. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:38.528: INFO: Pod daemon-set-4thvg is not available
Dec 23 14:03:38.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:39.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:39.528: INFO: Pod daemon-set-lt7jf is not available
Dec 23 14:03:39.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:40.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:40.528: INFO: Pod daemon-set-lt7jf is not available
Dec 23 14:03:40.532: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:41.527: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:41.527: INFO: Pod daemon-set-lt7jf is not available
Dec 23 14:03:41.530: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:42.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:42.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:43.527: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:43.527: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:03:43.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:44.526: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:44.526: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:03:44.529: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:45.527: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:45.527: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:03:45.530: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:46.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:46.528: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:03:46.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:47.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:47.528: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:03:47.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:48.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:48.528: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:03:48.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:49.529: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:49.529: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:03:49.534: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:50.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:50.529: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:03:50.532: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:51.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:51.529: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:03:51.532: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:52.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:52.528: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:03:52.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:53.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:53.528: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:03:53.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:54.526: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:54.526: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:03:54.529: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:55.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:55.528: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:03:55.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:56.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:56.528: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:03:56.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:57.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:57.528: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:03:57.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:58.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:58.528: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:03:58.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:03:59.529: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:03:59.529: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:03:59.532: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:04:00.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:04:00.528: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:04:00.533: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:04:01.527: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:04:01.527: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:04:01.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:04:02.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:04:02.528: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:04:02.532: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:04:03.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:04:03.528: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:04:03.532: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:04:04.526: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:04:04.526: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:04:04.528: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:04:05.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:04:05.528: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:04:05.532: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:04:06.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:04:06.528: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:04:06.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:04:07.528: INFO: Wrong image for pod: daemon-set-4cc4x. Expected: abcsys.cn:5000/e2e-test-images/agnhost:2.21, got: abcsys.cn:5000/library/httpd:2.4.38-alpine.
Dec 23 14:04:07.528: INFO: Pod daemon-set-4cc4x is not available
Dec 23 14:04:07.531: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:04:08.531: INFO: Pod daemon-set-q47p6 is not available
Dec 23 14:04:08.535: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 23 14:04:08.538: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:04:08.542: INFO: Number of nodes with available pods: 1
Dec 23 14:04:08.542: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 14:04:09.549: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:04:09.551: INFO: Number of nodes with available pods: 1
Dec 23 14:04:09.551: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 14:04:10.549: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 14:04:10.551: INFO: Number of nodes with available pods: 2
Dec 23 14:04:10.551: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3204, will wait for the garbage collector to delete the pods
Dec 23 14:04:10.622: INFO: Deleting DaemonSet.extensions daemon-set took: 5.704805ms
Dec 23 14:04:11.222: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.206828ms
Dec 23 14:04:19.334: INFO: Number of nodes with available pods: 0
Dec 23 14:04:19.335: INFO: Number of running nodes: 0, number of available pods: 0
Dec 23 14:04:19.342: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"87511"},"items":null}

Dec 23 14:04:19.345: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"87511"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:04:19.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3204" for this suite.

• [SLOW TEST:54.212 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":87,"skipped":1779,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:04:19.377: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 23 14:04:19.430: INFO: Waiting up to 5m0s for pod "pod-4a4c3ed4-cbc4-4ffa-8362-fbb1d613c711" in namespace "emptydir-3131" to be "Succeeded or Failed"
Dec 23 14:04:19.434: INFO: Pod "pod-4a4c3ed4-cbc4-4ffa-8362-fbb1d613c711": Phase="Pending", Reason="", readiness=false. Elapsed: 4.369363ms
Dec 23 14:04:21.442: INFO: Pod "pod-4a4c3ed4-cbc4-4ffa-8362-fbb1d613c711": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012229536s
STEP: Saw pod success
Dec 23 14:04:21.442: INFO: Pod "pod-4a4c3ed4-cbc4-4ffa-8362-fbb1d613c711" satisfied condition "Succeeded or Failed"
Dec 23 14:04:21.451: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-4a4c3ed4-cbc4-4ffa-8362-fbb1d613c711 container test-container: <nil>
STEP: delete the pod
Dec 23 14:04:21.482: INFO: Waiting for pod pod-4a4c3ed4-cbc4-4ffa-8362-fbb1d613c711 to disappear
Dec 23 14:04:21.490: INFO: Pod pod-4a4c3ed4-cbc4-4ffa-8362-fbb1d613c711 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:04:21.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3131" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":88,"skipped":1806,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:04:21.501: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:04:21.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3475" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":89,"skipped":1838,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:04:21.550: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-40faf70e-48d0-4101-a088-9d966d910989
STEP: Creating a pod to test consume secrets
Dec 23 14:04:21.602: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d5b5ba5c-5a77-4859-b98a-9a37549c46c9" in namespace "projected-7917" to be "Succeeded or Failed"
Dec 23 14:04:21.607: INFO: Pod "pod-projected-secrets-d5b5ba5c-5a77-4859-b98a-9a37549c46c9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.710354ms
Dec 23 14:04:23.615: INFO: Pod "pod-projected-secrets-d5b5ba5c-5a77-4859-b98a-9a37549c46c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013084284s
STEP: Saw pod success
Dec 23 14:04:23.615: INFO: Pod "pod-projected-secrets-d5b5ba5c-5a77-4859-b98a-9a37549c46c9" satisfied condition "Succeeded or Failed"
Dec 23 14:04:23.617: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-projected-secrets-d5b5ba5c-5a77-4859-b98a-9a37549c46c9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 23 14:04:23.636: INFO: Waiting for pod pod-projected-secrets-d5b5ba5c-5a77-4859-b98a-9a37549c46c9 to disappear
Dec 23 14:04:23.639: INFO: Pod pod-projected-secrets-d5b5ba5c-5a77-4859-b98a-9a37549c46c9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:04:23.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7917" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":90,"skipped":1858,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:04:23.663: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:04:34.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8230" for this suite.

• [SLOW TEST:11.107 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":91,"skipped":1866,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:04:34.770: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:04:34.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9570" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":92,"skipped":1885,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:04:34.829: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6114
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-6114
I1223 14:04:34.906358      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6114, replica count: 2
I1223 14:04:37.957248      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 23 14:04:37.957: INFO: Creating new exec pod
Dec 23 14:04:41.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-6114 exec execpodc954m -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 23 14:04:41.420: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 23 14:04:41.420: INFO: stdout: ""
Dec 23 14:04:41.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-6114 exec execpodc954m -- /bin/sh -x -c nc -zv -t -w 2 10.108.237.62 80'
Dec 23 14:04:41.652: INFO: stderr: "+ nc -zv -t -w 2 10.108.237.62 80\nConnection to 10.108.237.62 80 port [tcp/http] succeeded!\n"
Dec 23 14:04:41.652: INFO: stdout: ""
Dec 23 14:04:41.652: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:04:41.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6114" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.882 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":93,"skipped":1895,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:04:41.713: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Dec 23 14:04:41.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-158 cluster-info'
Dec 23 14:04:41.921: INFO: stderr: ""
Dec 23 14:04:41.921: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:04:41.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-158" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":94,"skipped":1910,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:04:41.931: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-6325
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 23 14:04:41.965: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 23 14:04:41.999: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 23 14:04:44.006: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 23 14:04:46.006: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:04:48.007: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:04:50.007: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:04:52.009: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:04:54.008: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:04:56.008: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:04:58.008: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:05:00.005: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:05:02.008: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 23 14:05:02.013: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Dec 23 14:05:04.053: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Dec 23 14:05:04.053: INFO: Going to poll 10.244.1.42 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Dec 23 14:05:04.055: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.42:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6325 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:05:04.055: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 14:05:04.159: INFO: Found all 1 expected endpoints: [netserver-0]
Dec 23 14:05:04.159: INFO: Going to poll 10.244.201.254 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Dec 23 14:05:04.163: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.201.254:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6325 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:05:04.163: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 14:05:04.287: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:05:04.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6325" for this suite.

• [SLOW TEST:22.366 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":95,"skipped":1943,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:05:04.297: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Dec 23 14:05:04.369: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Dec 23 14:05:20.722: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 14:05:24.646: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:05:40.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-293" for this suite.

• [SLOW TEST:35.909 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":96,"skipped":1948,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:05:40.207: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:05:42.287: INFO: Deleting pod "var-expansion-4eba0b37-27b9-4906-863a-fb4179995f27" in namespace "var-expansion-5158"
Dec 23 14:05:42.292: INFO: Wait up to 5m0s for pod "var-expansion-4eba0b37-27b9-4906-863a-fb4179995f27" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:06:20.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5158" for this suite.

• [SLOW TEST:40.114 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":97,"skipped":1950,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:06:20.321: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-8aab6356-e96a-44f9-8fcb-c9085b340906
STEP: Creating a pod to test consume secrets
Dec 23 14:06:20.388: INFO: Waiting up to 5m0s for pod "pod-secrets-ba2253fa-d8b4-47c6-8468-fc447f24667e" in namespace "secrets-6281" to be "Succeeded or Failed"
Dec 23 14:06:20.402: INFO: Pod "pod-secrets-ba2253fa-d8b4-47c6-8468-fc447f24667e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.887316ms
Dec 23 14:06:22.405: INFO: Pod "pod-secrets-ba2253fa-d8b4-47c6-8468-fc447f24667e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017122168s
STEP: Saw pod success
Dec 23 14:06:22.405: INFO: Pod "pod-secrets-ba2253fa-d8b4-47c6-8468-fc447f24667e" satisfied condition "Succeeded or Failed"
Dec 23 14:06:22.407: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-secrets-ba2253fa-d8b4-47c6-8468-fc447f24667e container secret-volume-test: <nil>
STEP: delete the pod
Dec 23 14:06:22.448: INFO: Waiting for pod pod-secrets-ba2253fa-d8b4-47c6-8468-fc447f24667e to disappear
Dec 23 14:06:22.451: INFO: Pod pod-secrets-ba2253fa-d8b4-47c6-8468-fc447f24667e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:06:22.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6281" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":98,"skipped":1960,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:06:22.460: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1223 14:06:32.533983      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 23 14:07:34.552: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:07:34.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1082" for this suite.

• [SLOW TEST:72.122 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":99,"skipped":1973,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:07:34.583: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:07:34.628: INFO: Creating deployment "webserver-deployment"
Dec 23 14:07:34.633: INFO: Waiting for observed generation 1
Dec 23 14:07:36.644: INFO: Waiting for all required pods to come up
Dec 23 14:07:36.655: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 23 14:07:40.679: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 23 14:07:40.684: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 23 14:07:40.691: INFO: Updating deployment webserver-deployment
Dec 23 14:07:40.691: INFO: Waiting for observed generation 2
Dec 23 14:07:42.712: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 23 14:07:42.715: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 23 14:07:42.717: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 23 14:07:42.724: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 23 14:07:42.724: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 23 14:07:42.726: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 23 14:07:42.729: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 23 14:07:42.729: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 23 14:07:42.739: INFO: Updating deployment webserver-deployment
Dec 23 14:07:42.739: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 23 14:07:42.755: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 23 14:07:42.766: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Dec 23 14:07:42.844: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3996  252f98a7-05b1-475e-b21a-57a91f55ea5b 88575 3 2020-12-23 14:07:20 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-12-23 14:07:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-23 14:07:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c87418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2020-12-23 14:07:26 +0000 UTC,LastTransitionTime:2020-12-23 14:07:20 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-12-23 14:07:28 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec 23 14:07:42.888: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-3996  885d1736-9f1d-44ca-b5ce-16df5790e3a1 88559 3 2020-12-23 14:07:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 252f98a7-05b1-475e-b21a-57a91f55ea5b 0xc004c878d7 0xc004c878d8}] []  [{kube-controller-manager Update apps/v1 2020-12-23 14:07:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"252f98a7-05b1-475e-b21a-57a91f55ea5b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c87958 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 23 14:07:42.888: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 23 14:07:42.889: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7674d4c7bd  deployment-3996  3c85bc91-e443-456d-918f-3e6fdeb85523 88556 3 2020-12-23 14:07:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 252f98a7-05b1-475e-b21a-57a91f55ea5b 0xc004c87807 0xc004c87808}] []  [{kube-controller-manager Update apps/v1 2020-12-23 14:07:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"252f98a7-05b1-475e-b21a-57a91f55ea5b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7674d4c7bd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[] [] []  []} {[] [] [{httpd abcsys.cn:5000/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c87878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec 23 14:07:42.943: INFO: Pod "webserver-deployment-7674d4c7bd-5h25t" is not available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-5h25t webserver-deployment-7674d4c7bd- deployment-3996  3589bd76-9803-4d39-ab02-f9feab802358 88605 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc004c87da7 0xc004c87da8}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.25,PodIP:,StartTime:2020-12-23 14:07:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.944: INFO: Pod "webserver-deployment-7674d4c7bd-6jkhc" is not available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-6jkhc webserver-deployment-7674d4c7bd- deployment-3996  129bea59-b929-460e-809b-b2650ed6b985 88606 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc004c87f37 0xc004c87f38}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.944: INFO: Pod "webserver-deployment-7674d4c7bd-7pfc5" is available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-7pfc5 webserver-deployment-7674d4c7bd- deployment-3996  a0e80af5-836a-4a47-a311-82293189d0de 88439 0 2020-12-23 14:07:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[cni.projectcalico.org/podIP:10.244.201.230/32 cni.projectcalico.org/podIPs:10.244.201.230/32] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc006636060 0xc006636061}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-23 14:07:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-23 14:07:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.201.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.25,PodIP:10.244.201.230,StartTime:2020-12-23 14:07:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-23 14:07:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,ImageID:docker-pullable://abcsys.cn:5000/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://aaa22cb105b19d9e1dc98d16e0482c5303b883e4f9abaf4dd62b5c1e045db72b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.201.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.945: INFO: Pod "webserver-deployment-7674d4c7bd-9xfjx" is available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-9xfjx webserver-deployment-7674d4c7bd- deployment-3996  26c83102-0f12-498a-a9e4-5cc9e30cf8ba 88424 0 2020-12-23 14:07:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[cni.projectcalico.org/podIP:10.244.1.48/32 cni.projectcalico.org/podIPs:10.244.1.48/32] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc006636237 0xc006636238}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-23 14:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-23 14:07:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.17,PodIP:10.244.1.48,StartTime:2020-12-23 14:07:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-23 14:07:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,ImageID:docker-pullable://abcsys.cn:5000/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://3a600bd88ecb2dc64194d1010abeae11f810dd63ba8f9699fb823455bf2e8340,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.946: INFO: Pod "webserver-deployment-7674d4c7bd-cvwbq" is not available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-cvwbq webserver-deployment-7674d4c7bd- deployment-3996  7261892f-576f-4798-bf09-8beb3fc4bdc2 88598 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc0066363f0 0xc0066363f1}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.946: INFO: Pod "webserver-deployment-7674d4c7bd-d4h9s" is available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-d4h9s webserver-deployment-7674d4c7bd- deployment-3996  10754d29-1928-456d-847f-11945f3032ca 88427 0 2020-12-23 14:07:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[cni.projectcalico.org/podIP:10.244.1.58/32 cni.projectcalico.org/podIPs:10.244.1.58/32] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc006636530 0xc006636531}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-23 14:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-23 14:07:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.17,PodIP:10.244.1.58,StartTime:2020-12-23 14:07:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-23 14:07:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,ImageID:docker-pullable://abcsys.cn:5000/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://282e389d27312d4166f1fb85597b81fecdb1c1e054a95a4e1a57636dd06ffc48,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.946: INFO: Pod "webserver-deployment-7674d4c7bd-ffr2h" is available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-ffr2h webserver-deployment-7674d4c7bd- deployment-3996  411401a6-23e4-4297-91e5-937a3fb8d8d4 88416 0 2020-12-23 14:07:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[cni.projectcalico.org/podIP:10.244.1.50/32 cni.projectcalico.org/podIPs:10.244.1.50/32] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc006636710 0xc006636711}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-23 14:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-23 14:07:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.17,PodIP:10.244.1.50,StartTime:2020-12-23 14:07:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-23 14:07:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,ImageID:docker-pullable://abcsys.cn:5000/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://cca73c861df977ed7facc0d799bf50c5cef1a49cb2a552dd605b8449000eca50,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.946: INFO: Pod "webserver-deployment-7674d4c7bd-fkkh8" is not available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-fkkh8 webserver-deployment-7674d4c7bd- deployment-3996  3e4d6ff1-ff1c-4bd3-a40b-a3022f1897ce 88591 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc0066368d0 0xc0066368d1}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.17,PodIP:,StartTime:2020-12-23 14:07:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.947: INFO: Pod "webserver-deployment-7674d4c7bd-fz7gg" is available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-fz7gg webserver-deployment-7674d4c7bd- deployment-3996  4d52f0d8-e217-482b-aeb1-2d1f5d070cf9 88419 0 2020-12-23 14:07:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[cni.projectcalico.org/podIP:10.244.1.44/32 cni.projectcalico.org/podIPs:10.244.1.44/32] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc006636a77 0xc006636a78}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-23 14:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-23 14:07:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.17,PodIP:10.244.1.44,StartTime:2020-12-23 14:07:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-23 14:07:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,ImageID:docker-pullable://abcsys.cn:5000/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://74566fd52c0d3fd73d6ad0e3cf46d2415946d8fc74255a6c85d3940021f91e85,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.947: INFO: Pod "webserver-deployment-7674d4c7bd-ggg7d" is not available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-ggg7d webserver-deployment-7674d4c7bd- deployment-3996  dcc75a22-3ac8-4b86-847f-6aa45762fb50 88592 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc006636c30 0xc006636c31}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.947: INFO: Pod "webserver-deployment-7674d4c7bd-jmbp7" is available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-jmbp7 webserver-deployment-7674d4c7bd- deployment-3996  bc6c9e81-7094-451f-91e7-23d70329b3a2 88449 0 2020-12-23 14:07:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[cni.projectcalico.org/podIP:10.244.201.235/32 cni.projectcalico.org/podIPs:10.244.201.235/32] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc006636d50 0xc006636d51}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-23 14:07:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-23 14:07:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.201.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.25,PodIP:10.244.201.235,StartTime:2020-12-23 14:07:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-23 14:07:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,ImageID:docker-pullable://abcsys.cn:5000/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://6184b4a641d9c8cafec18cb575d149a4dce181962606dabc229414eed18ffe26,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.201.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.948: INFO: Pod "webserver-deployment-7674d4c7bd-k855k" is not available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-k855k webserver-deployment-7674d4c7bd- deployment-3996  6df6a610-7743-43e3-bf9a-e019767fac64 88580 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc006636f07 0xc006636f08}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.948: INFO: Pod "webserver-deployment-7674d4c7bd-lgzp2" is available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-lgzp2 webserver-deployment-7674d4c7bd- deployment-3996  fdda358d-3930-462f-b09b-d2203b689c27 88443 0 2020-12-23 14:07:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[cni.projectcalico.org/podIP:10.244.201.240/32 cni.projectcalico.org/podIPs:10.244.201.240/32] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc006637030 0xc006637031}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-23 14:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-23 14:07:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.201.240\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.25,PodIP:10.244.201.240,StartTime:2020-12-23 14:07:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-23 14:07:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,ImageID:docker-pullable://abcsys.cn:5000/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://e51478e9d7e8cc47d6515628a96d2b1eb5b031fa5227dc98f8dd44b7ddc83ca7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.201.240,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.948: INFO: Pod "webserver-deployment-7674d4c7bd-ljn7b" is available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-ljn7b webserver-deployment-7674d4c7bd- deployment-3996  c94afb0c-8dd6-4ccb-8060-968e07ed7815 88446 0 2020-12-23 14:07:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[cni.projectcalico.org/podIP:10.244.201.241/32 cni.projectcalico.org/podIPs:10.244.201.241/32] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc0066371e7 0xc0066371e8}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-23 14:07:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-23 14:07:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.201.241\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.25,PodIP:10.244.201.241,StartTime:2020-12-23 14:07:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-23 14:07:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,ImageID:docker-pullable://abcsys.cn:5000/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://70d20a4eff0da61d944201dc2ce4024430a3b0067decd36ed1ac30d815f224eb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.201.241,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.948: INFO: Pod "webserver-deployment-7674d4c7bd-lnnql" is not available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-lnnql webserver-deployment-7674d4c7bd- deployment-3996  72ebf41b-4f6f-4d9f-80c5-c79c192c26cc 88596 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc006637507 0xc006637508}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.949: INFO: Pod "webserver-deployment-7674d4c7bd-qcrnb" is not available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-qcrnb webserver-deployment-7674d4c7bd- deployment-3996  7b93cdc4-7510-4f76-b186-50a7b9363000 88585 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc006637630 0xc006637631}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.949: INFO: Pod "webserver-deployment-7674d4c7bd-rwvpq" is not available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-rwvpq webserver-deployment-7674d4c7bd- deployment-3996  f71d2936-d2de-4c2f-9d31-09a6179c5454 88579 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc006637750 0xc006637751}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.949: INFO: Pod "webserver-deployment-7674d4c7bd-v8gb8" is not available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-v8gb8 webserver-deployment-7674d4c7bd- deployment-3996  85bca407-6729-4b21-b88d-5c3dbd6e6b65 88588 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc006637880 0xc006637881}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.25,PodIP:,StartTime:2020-12-23 14:07:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.950: INFO: Pod "webserver-deployment-7674d4c7bd-z57bb" is not available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-z57bb webserver-deployment-7674d4c7bd- deployment-3996  9fde3e59-6075-494e-85ac-ebd8d1d94211 88599 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc0066379f7 0xc0066379f8}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.950: INFO: Pod "webserver-deployment-7674d4c7bd-zd8xt" is not available:
&Pod{ObjectMeta:{webserver-deployment-7674d4c7bd-zd8xt webserver-deployment-7674d4c7bd- deployment-3996  f643dfe8-bef4-494b-99e0-8b5063123a3d 88586 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7674d4c7bd] map[] [{apps/v1 ReplicaSet webserver-deployment-7674d4c7bd 3c85bc91-e443-456d-918f-3e6fdeb85523 0xc006637b20 0xc006637b21}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c85bc91-e443-456d-918f-3e6fdeb85523\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.950: INFO: Pod "webserver-deployment-795d758f88-2hmvp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2hmvp webserver-deployment-795d758f88- deployment-3996  e875a408-fa52-4c8a-b5df-718f5e6a7208 88553 0 2020-12-23 14:07:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.201.194/32 cni.projectcalico.org/podIPs:10.244.201.194/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 885d1736-9f1d-44ca-b5ce-16df5790e3a1 0xc006637c50 0xc006637c51}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"885d1736-9f1d-44ca-b5ce-16df5790e3a1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-23 14:07:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.25,PodIP:,StartTime:2020-12-23 14:07:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.950: INFO: Pod "webserver-deployment-795d758f88-4d9hk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4d9hk webserver-deployment-795d758f88- deployment-3996  1263bd40-2140-470d-bfc8-5d28f2174166 88574 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 885d1736-9f1d-44ca-b5ce-16df5790e3a1 0xc006637e00 0xc006637e01}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"885d1736-9f1d-44ca-b5ce-16df5790e3a1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.950: INFO: Pod "webserver-deployment-795d758f88-5vxv8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5vxv8 webserver-deployment-795d758f88- deployment-3996  10c57981-2c7d-47cf-b1c0-71996a5e73ac 88545 0 2020-12-23 14:07:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.201.196/32 cni.projectcalico.org/podIPs:10.244.201.196/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 885d1736-9f1d-44ca-b5ce-16df5790e3a1 0xc006637f30 0xc006637f31}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"885d1736-9f1d-44ca-b5ce-16df5790e3a1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-23 14:07:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.25,PodIP:,StartTime:2020-12-23 14:07:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.951: INFO: Pod "webserver-deployment-795d758f88-6576r" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6576r webserver-deployment-795d758f88- deployment-3996  257d10dd-48e5-4c14-914a-b83bf7ec0fbb 88611 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 885d1736-9f1d-44ca-b5ce-16df5790e3a1 0xc004a82330 0xc004a82331}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"885d1736-9f1d-44ca-b5ce-16df5790e3a1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.951: INFO: Pod "webserver-deployment-795d758f88-8h8hk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8h8hk webserver-deployment-795d758f88- deployment-3996  436f0304-de2f-4a07-9124-11606962fb75 88539 0 2020-12-23 14:07:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.1.55/32 cni.projectcalico.org/podIPs:10.244.1.55/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 885d1736-9f1d-44ca-b5ce-16df5790e3a1 0xc004a82467 0xc004a82468}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"885d1736-9f1d-44ca-b5ce-16df5790e3a1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-23 14:07:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.17,PodIP:,StartTime:2020-12-23 14:07:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.951: INFO: Pod "webserver-deployment-795d758f88-bvcl9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bvcl9 webserver-deployment-795d758f88- deployment-3996  13f867c3-9dd5-43f3-992e-57214f9a2c23 88613 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 885d1736-9f1d-44ca-b5ce-16df5790e3a1 0xc004a82620 0xc004a82621}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"885d1736-9f1d-44ca-b5ce-16df5790e3a1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.951: INFO: Pod "webserver-deployment-795d758f88-f4xzp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-f4xzp webserver-deployment-795d758f88- deployment-3996  44479a33-6efd-4693-94b6-2fdf20644c62 88600 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 885d1736-9f1d-44ca-b5ce-16df5790e3a1 0xc004a82760 0xc004a82761}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"885d1736-9f1d-44ca-b5ce-16df5790e3a1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.952: INFO: Pod "webserver-deployment-795d758f88-qt6wr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qt6wr webserver-deployment-795d758f88- deployment-3996  576065b1-a6c0-439c-a07b-1488b4f1844c 88538 0 2020-12-23 14:07:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.1.53/32 cni.projectcalico.org/podIPs:10.244.1.53/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 885d1736-9f1d-44ca-b5ce-16df5790e3a1 0xc004a828b0 0xc004a828b1}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"885d1736-9f1d-44ca-b5ce-16df5790e3a1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-23 14:07:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.17,PodIP:,StartTime:2020-12-23 14:07:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.952: INFO: Pod "webserver-deployment-795d758f88-r5kx2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-r5kx2 webserver-deployment-795d758f88- deployment-3996  74638061-78ec-49fa-b277-dc979eb64386 88597 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 885d1736-9f1d-44ca-b5ce-16df5790e3a1 0xc004a82a70 0xc004a82a71}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"885d1736-9f1d-44ca-b5ce-16df5790e3a1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.952: INFO: Pod "webserver-deployment-795d758f88-rzgwq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rzgwq webserver-deployment-795d758f88- deployment-3996  3d1c7449-1269-435c-a60c-fa90dd1ae49f 88609 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 885d1736-9f1d-44ca-b5ce-16df5790e3a1 0xc004a82bb0 0xc004a82bb1}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"885d1736-9f1d-44ca-b5ce-16df5790e3a1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.952: INFO: Pod "webserver-deployment-795d758f88-wptht" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wptht webserver-deployment-795d758f88- deployment-3996  0ae792fd-f0c3-4e6a-b9bc-6cc5ce6600ec 88549 0 2020-12-23 14:07:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.201.239/32 cni.projectcalico.org/podIPs:10.244.201.239/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 885d1736-9f1d-44ca-b5ce-16df5790e3a1 0xc004a82ce0 0xc004a82ce1}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"885d1736-9f1d-44ca-b5ce-16df5790e3a1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-23 14:07:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.25,PodIP:,StartTime:2020-12-23 14:07:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.953: INFO: Pod "webserver-deployment-795d758f88-x92xj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-x92xj webserver-deployment-795d758f88- deployment-3996  f8485735-10a3-4be2-8194-b761e0c74445 88593 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 885d1736-9f1d-44ca-b5ce-16df5790e3a1 0xc004a82e90 0xc004a82e91}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"885d1736-9f1d-44ca-b5ce-16df5790e3a1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 14:07:42.953: INFO: Pod "webserver-deployment-795d758f88-xxkp5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xxkp5 webserver-deployment-795d758f88- deployment-3996  1abdbb01-8aeb-4ea7-851d-517aead96f5d 88612 0 2020-12-23 14:07:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 885d1736-9f1d-44ca-b5ce-16df5790e3a1 0xc004a82fc0 0xc004a82fc1}] []  [{kube-controller-manager Update v1 2020-12-23 14:07:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"885d1736-9f1d-44ca-b5ce-16df5790e3a1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4tpwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4tpwt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4tpwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:07:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:07:42.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3996" for this suite.

• [SLOW TEST:8.466 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":100,"skipped":1986,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:07:43.050: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-e5f915d9-2d37-4fbc-9039-d2768288bd76
STEP: Creating a pod to test consume secrets
Dec 23 14:07:43.272: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0233cb1a-ba37-40be-8c74-137ca759d858" in namespace "projected-792" to be "Succeeded or Failed"
Dec 23 14:07:43.284: INFO: Pod "pod-projected-secrets-0233cb1a-ba37-40be-8c74-137ca759d858": Phase="Pending", Reason="", readiness=false. Elapsed: 11.649131ms
Dec 23 14:07:46.473: INFO: Pod "pod-projected-secrets-0233cb1a-ba37-40be-8c74-137ca759d858": Phase="Pending", Reason="", readiness=false. Elapsed: 3.200879787s
Dec 23 14:07:48.558: INFO: Pod "pod-projected-secrets-0233cb1a-ba37-40be-8c74-137ca759d858": Phase="Pending", Reason="", readiness=false. Elapsed: 5.285738385s
Dec 23 14:07:50.572: INFO: Pod "pod-projected-secrets-0233cb1a-ba37-40be-8c74-137ca759d858": Phase="Pending", Reason="", readiness=false. Elapsed: 7.299950391s
Dec 23 14:07:52.582: INFO: Pod "pod-projected-secrets-0233cb1a-ba37-40be-8c74-137ca759d858": Phase="Pending", Reason="", readiness=false. Elapsed: 9.309717405s
Dec 23 14:07:54.593: INFO: Pod "pod-projected-secrets-0233cb1a-ba37-40be-8c74-137ca759d858": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.320886508s
STEP: Saw pod success
Dec 23 14:07:54.593: INFO: Pod "pod-projected-secrets-0233cb1a-ba37-40be-8c74-137ca759d858" satisfied condition "Succeeded or Failed"
Dec 23 14:07:54.596: INFO: Trying to get logs from node wt-k8s-2 pod pod-projected-secrets-0233cb1a-ba37-40be-8c74-137ca759d858 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 23 14:07:54.642: INFO: Waiting for pod pod-projected-secrets-0233cb1a-ba37-40be-8c74-137ca759d858 to disappear
Dec 23 14:07:54.647: INFO: Pod pod-projected-secrets-0233cb1a-ba37-40be-8c74-137ca759d858 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:07:54.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-792" for this suite.

• [SLOW TEST:11.609 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":101,"skipped":1994,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:07:54.660: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Dec 23 14:07:54.698: INFO: PodSpec: initContainers in spec.initContainers
Dec 23 14:08:39.504: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-82230b63-77e1-4600-bb09-36ad56524fea", GenerateName:"", Namespace:"init-container-5296", SelfLink:"", UID:"b6f0df3b-9ae2-4860-b2da-b64626d5c691", ResourceVersion:"89130", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63744329260, loc:(*time.Location)(0x7962e20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"698426510"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.244.201.199/32", "cni.projectcalico.org/podIPs":"10.244.201.199/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0046011c0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0046011e0)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004601200), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004601220)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004601240), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004601260)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-ktn86", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001446b80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"abcsys.cn:5000/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ktn86", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"abcsys.cn:5000/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ktn86", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"abcsys.cn:5000/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ktn86", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0034e06f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"wt-k8s-3.novalocal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003544af0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0034e0770)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0034e0790)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0034e0798), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0034e079c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc003415590), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329260, loc:(*time.Location)(0x7962e20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329260, loc:(*time.Location)(0x7962e20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329260, loc:(*time.Location)(0x7962e20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329260, loc:(*time.Location)(0x7962e20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.22.19.25", PodIP:"10.244.201.199", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.201.199"}}, StartTime:(*v1.Time)(0xc004601280), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003544bd0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003544c40)}, Ready:false, RestartCount:3, Image:"abcsys.cn:5000/library/busybox:1.29", ImageID:"docker-pullable://abcsys.cn:5000/library/busybox@sha256:e004c2cc521c95383aebb1fb5893719aa7a8eae2e7a71f316a4410784edb00a9", ContainerID:"docker://c086cfe955a3411a2bbc6d3593c45523dbfdc040eb3cbd51a0d15143c213b7ec", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0046012c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"abcsys.cn:5000/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0046012a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"abcsys.cn:5000/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc0034e081f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:08:39.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5296" for this suite.

• [SLOW TEST:44.859 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":102,"skipped":2008,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:08:39.519: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Dec 23 14:08:39.568: INFO: Waiting up to 5m0s for pod "client-containers-64b1b849-7192-44ed-b310-281ca672d30b" in namespace "containers-7625" to be "Succeeded or Failed"
Dec 23 14:08:39.573: INFO: Pod "client-containers-64b1b849-7192-44ed-b310-281ca672d30b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.452315ms
Dec 23 14:08:41.581: INFO: Pod "client-containers-64b1b849-7192-44ed-b310-281ca672d30b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012687928s
STEP: Saw pod success
Dec 23 14:08:41.581: INFO: Pod "client-containers-64b1b849-7192-44ed-b310-281ca672d30b" satisfied condition "Succeeded or Failed"
Dec 23 14:08:41.583: INFO: Trying to get logs from node wt-k8s-3.novalocal pod client-containers-64b1b849-7192-44ed-b310-281ca672d30b container agnhost-container: <nil>
STEP: delete the pod
Dec 23 14:08:41.616: INFO: Waiting for pod client-containers-64b1b849-7192-44ed-b310-281ca672d30b to disappear
Dec 23 14:08:41.618: INFO: Pod client-containers-64b1b849-7192-44ed-b310-281ca672d30b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:08:41.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7625" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":103,"skipped":2015,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:08:41.628: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7814
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-7814
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7814
Dec 23 14:08:41.707: INFO: Found 0 stateful pods, waiting for 1
Dec 23 14:08:51.717: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 23 14:08:51.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 23 14:08:52.342: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 23 14:08:52.342: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 23 14:08:52.342: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 23 14:08:52.346: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 23 14:09:02.364: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 23 14:09:02.364: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 14:09:02.383: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Dec 23 14:09:02.383: INFO: ss-0  wt-k8s-3.novalocal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:27 +0000 UTC  }]
Dec 23 14:09:02.383: INFO: 
Dec 23 14:09:02.383: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 23 14:09:03.390: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990184982s
Dec 23 14:09:04.395: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983244544s
Dec 23 14:09:05.401: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978325207s
Dec 23 14:09:06.407: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.971974542s
Dec 23 14:09:07.413: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.965753582s
Dec 23 14:09:08.419: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.960116121s
Dec 23 14:09:09.425: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.954123731s
Dec 23 14:09:10.432: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.947674548s
Dec 23 14:09:11.440: INFO: Verifying statefulset ss doesn't scale past 3 for another 940.930089ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7814
Dec 23 14:09:12.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:09:12.665: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 23 14:09:12.665: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 23 14:09:12.665: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 23 14:09:12.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:09:12.864: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 23 14:09:12.865: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 23 14:09:12.865: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 23 14:09:12.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:09:13.078: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 23 14:09:13.078: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 23 14:09:13.078: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 23 14:09:13.082: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec 23 14:09:23.088: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 14:09:23.088: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 14:09:23.088: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 23 14:09:23.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 23 14:09:23.307: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 23 14:09:23.307: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 23 14:09:23.307: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 23 14:09:23.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 23 14:09:23.515: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 23 14:09:23.515: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 23 14:09:23.515: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 23 14:09:23.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 23 14:09:23.740: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 23 14:09:23.740: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 23 14:09:23.740: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 23 14:09:23.740: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 14:09:23.744: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec 23 14:09:33.761: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 23 14:09:33.761: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 23 14:09:33.761: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 23 14:09:33.774: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Dec 23 14:09:33.774: INFO: ss-0  wt-k8s-3.novalocal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:27 +0000 UTC  }]
Dec 23 14:09:33.774: INFO: ss-1  wt-k8s-3.novalocal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:33.774: INFO: ss-2  wt-k8s-3.novalocal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:33.774: INFO: 
Dec 23 14:09:33.774: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 23 14:09:34.781: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Dec 23 14:09:34.781: INFO: ss-0  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:27 +0000 UTC  }]
Dec 23 14:09:34.782: INFO: ss-1  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:34.782: INFO: ss-2  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:34.782: INFO: 
Dec 23 14:09:34.782: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 23 14:09:35.789: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Dec 23 14:09:35.789: INFO: ss-0  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:27 +0000 UTC  }]
Dec 23 14:09:35.789: INFO: ss-1  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:35.789: INFO: ss-2  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:35.789: INFO: 
Dec 23 14:09:35.789: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 23 14:09:36.793: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Dec 23 14:09:36.793: INFO: ss-0  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:27 +0000 UTC  }]
Dec 23 14:09:36.793: INFO: ss-1  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:36.793: INFO: ss-2  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:36.793: INFO: 
Dec 23 14:09:36.793: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 23 14:09:37.800: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Dec 23 14:09:37.801: INFO: ss-0  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:27 +0000 UTC  }]
Dec 23 14:09:37.801: INFO: ss-1  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:37.801: INFO: ss-2  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:37.801: INFO: 
Dec 23 14:09:37.801: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 23 14:09:38.808: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Dec 23 14:09:38.808: INFO: ss-0  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:27 +0000 UTC  }]
Dec 23 14:09:38.808: INFO: ss-1  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:38.808: INFO: ss-2  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:38.808: INFO: 
Dec 23 14:09:38.808: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 23 14:09:39.815: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Dec 23 14:09:39.815: INFO: ss-0  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:27 +0000 UTC  }]
Dec 23 14:09:39.815: INFO: ss-1  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:39.815: INFO: ss-2  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:39.815: INFO: 
Dec 23 14:09:39.815: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 23 14:09:40.828: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Dec 23 14:09:40.828: INFO: ss-0  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:27 +0000 UTC  }]
Dec 23 14:09:40.828: INFO: ss-1  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:40.828: INFO: ss-2  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:40.828: INFO: 
Dec 23 14:09:40.828: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 23 14:09:41.835: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Dec 23 14:09:41.835: INFO: ss-0  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:27 +0000 UTC  }]
Dec 23 14:09:41.835: INFO: ss-1  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:41.835: INFO: ss-2  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:41.835: INFO: 
Dec 23 14:09:41.835: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 23 14:09:42.843: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Dec 23 14:09:42.843: INFO: ss-0  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:27 +0000 UTC  }]
Dec 23 14:09:42.843: INFO: ss-1  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:42.843: INFO: ss-2  wt-k8s-3.novalocal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:09:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:08:48 +0000 UTC  }]
Dec 23 14:09:42.843: INFO: 
Dec 23 14:09:42.843: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7814
Dec 23 14:09:43.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:09:44.003: INFO: rc: 1
Dec 23 14:09:44.003: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Dec 23 14:09:54.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:09:54.161: INFO: rc: 1
Dec 23 14:09:54.161: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Dec 23 14:10:04.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:10:04.327: INFO: rc: 1
Dec 23 14:10:04.327: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Dec 23 14:10:14.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:10:14.495: INFO: rc: 1
Dec 23 14:10:14.495: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Dec 23 14:10:24.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:10:24.614: INFO: rc: 1
Dec 23 14:10:24.614: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:10:34.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:10:34.732: INFO: rc: 1
Dec 23 14:10:34.732: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:10:44.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:10:44.848: INFO: rc: 1
Dec 23 14:10:44.848: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:10:54.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:10:54.983: INFO: rc: 1
Dec 23 14:10:54.983: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:11:04.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:11:05.112: INFO: rc: 1
Dec 23 14:11:05.112: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:11:15.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:11:15.228: INFO: rc: 1
Dec 23 14:11:15.228: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:11:25.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:11:25.340: INFO: rc: 1
Dec 23 14:11:25.340: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:11:35.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:11:35.468: INFO: rc: 1
Dec 23 14:11:35.468: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:11:45.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:11:45.590: INFO: rc: 1
Dec 23 14:11:45.590: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:11:55.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:11:55.743: INFO: rc: 1
Dec 23 14:11:55.744: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:12:05.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:12:05.870: INFO: rc: 1
Dec 23 14:12:05.870: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:12:15.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:12:15.997: INFO: rc: 1
Dec 23 14:12:15.997: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:12:25.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:12:26.111: INFO: rc: 1
Dec 23 14:12:26.111: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:12:36.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:12:36.225: INFO: rc: 1
Dec 23 14:12:36.225: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:12:46.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:12:46.335: INFO: rc: 1
Dec 23 14:12:46.335: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:12:56.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:12:56.444: INFO: rc: 1
Dec 23 14:12:56.444: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:13:06.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:13:06.557: INFO: rc: 1
Dec 23 14:13:06.557: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:13:16.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:13:16.663: INFO: rc: 1
Dec 23 14:13:16.663: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:13:26.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:13:26.776: INFO: rc: 1
Dec 23 14:13:26.776: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:13:36.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:13:36.886: INFO: rc: 1
Dec 23 14:13:36.886: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:13:46.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:13:46.998: INFO: rc: 1
Dec 23 14:13:46.998: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:13:56.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:13:57.114: INFO: rc: 1
Dec 23 14:13:57.114: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:14:07.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:14:07.223: INFO: rc: 1
Dec 23 14:14:07.223: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:14:17.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:14:17.332: INFO: rc: 1
Dec 23 14:14:17.332: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:14:27.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:14:27.443: INFO: rc: 1
Dec 23 14:14:27.443: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:14:37.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:14:37.557: INFO: rc: 1
Dec 23 14:14:37.557: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 14:14:47.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-7814 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:14:47.673: INFO: rc: 1
Dec 23 14:14:47.673: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Dec 23 14:14:47.673: INFO: Scaling statefulset ss to 0
Dec 23 14:14:47.687: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 23 14:14:47.690: INFO: Deleting all statefulset in ns statefulset-7814
Dec 23 14:14:47.692: INFO: Scaling statefulset ss to 0
Dec 23 14:14:47.700: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 14:14:47.702: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:14:47.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7814" for this suite.

• [SLOW TEST:366.111 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":104,"skipped":2052,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:14:47.740: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:14:49.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6984" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":105,"skipped":2063,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:14:49.831: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Dec 23 14:14:49.884: INFO: Waiting up to 5m0s for pod "downward-api-cee30728-14e9-4fbe-917c-806b33ab6dbd" in namespace "downward-api-3619" to be "Succeeded or Failed"
Dec 23 14:14:49.893: INFO: Pod "downward-api-cee30728-14e9-4fbe-917c-806b33ab6dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.268161ms
Dec 23 14:14:51.901: INFO: Pod "downward-api-cee30728-14e9-4fbe-917c-806b33ab6dbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017162633s
STEP: Saw pod success
Dec 23 14:14:51.901: INFO: Pod "downward-api-cee30728-14e9-4fbe-917c-806b33ab6dbd" satisfied condition "Succeeded or Failed"
Dec 23 14:14:51.904: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downward-api-cee30728-14e9-4fbe-917c-806b33ab6dbd container dapi-container: <nil>
STEP: delete the pod
Dec 23 14:14:51.937: INFO: Waiting for pod downward-api-cee30728-14e9-4fbe-917c-806b33ab6dbd to disappear
Dec 23 14:14:51.941: INFO: Pod downward-api-cee30728-14e9-4fbe-917c-806b33ab6dbd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:14:51.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3619" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":106,"skipped":2092,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:14:51.954: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 23 14:14:56.058: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 23 14:14:56.062: INFO: Pod pod-with-poststart-http-hook still exists
Dec 23 14:14:58.062: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 23 14:14:58.069: INFO: Pod pod-with-poststart-http-hook still exists
Dec 23 14:15:00.062: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 23 14:15:00.067: INFO: Pod pod-with-poststart-http-hook still exists
Dec 23 14:15:02.063: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 23 14:15:02.074: INFO: Pod pod-with-poststart-http-hook still exists
Dec 23 14:15:04.062: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 23 14:15:04.072: INFO: Pod pod-with-poststart-http-hook still exists
Dec 23 14:15:06.062: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 23 14:15:06.067: INFO: Pod pod-with-poststart-http-hook still exists
Dec 23 14:15:08.062: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 23 14:15:08.069: INFO: Pod pod-with-poststart-http-hook still exists
Dec 23 14:15:10.062: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 23 14:15:10.071: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:15:10.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9841" for this suite.

• [SLOW TEST:18.129 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":107,"skipped":2100,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:15:10.083: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 23 14:15:10.968: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 23 14:15:12.983: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329697, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329697, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329697, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329697, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-76bbb7fdd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 14:15:16.033: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:15:16.038: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1887-crds.webhook.example.com via the AdmissionRegistration API
Dec 23 14:15:16.861: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:16.976: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:17.085: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:17.178: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:17.299: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:17.380: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:17.480: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:17.580: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:17.685: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:17.778: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:17.877: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:17.976: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:18.082: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:18.175: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:18.277: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:18.375: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:18.475: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:18.575: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:18.681: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:18.788: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:18.877: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:18.976: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:19.076: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:19.176: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:19.279: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:19.376: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:19.481: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:19.578: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:19.706: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:19.796: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:19.878: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:19.976: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:20.077: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:20.177: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:20.279: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:20.378: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:20.475: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:20.581: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:20.686: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:20.778: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:20.877: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:20.978: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:21.075: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:21.177: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:21.277: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:21.390: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:21.476: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:21.577: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:21.676: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:21.789: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:21.878: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:21.977: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:22.079: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:22.182: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:22.276: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:22.376: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:22.479: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:22.583: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:22.682: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:22.777: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:22.879: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:22.978: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:23.078: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:23.191: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:23.277: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:23.377: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:23.475: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:23.576: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:15:23.901: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:15:24.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5049" for this suite.
STEP: Destroying namespace "webhook-5049-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:14.572 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":108,"skipped":2104,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:15:24.655: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 23 14:15:30.790: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 23 14:15:30.793: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 23 14:15:32.793: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 23 14:15:32.798: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 23 14:15:34.793: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 23 14:15:34.802: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 23 14:15:36.793: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 23 14:15:36.839: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 23 14:15:38.793: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 23 14:15:38.804: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:15:38.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9102" for this suite.

• [SLOW TEST:14.160 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":109,"skipped":2105,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:15:38.815: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 14:15:38.869: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b08f8a0b-280a-40f9-a354-4cecdadd8962" in namespace "downward-api-2719" to be "Succeeded or Failed"
Dec 23 14:15:38.876: INFO: Pod "downwardapi-volume-b08f8a0b-280a-40f9-a354-4cecdadd8962": Phase="Pending", Reason="", readiness=false. Elapsed: 6.948887ms
Dec 23 14:15:40.886: INFO: Pod "downwardapi-volume-b08f8a0b-280a-40f9-a354-4cecdadd8962": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016668223s
STEP: Saw pod success
Dec 23 14:15:40.886: INFO: Pod "downwardapi-volume-b08f8a0b-280a-40f9-a354-4cecdadd8962" satisfied condition "Succeeded or Failed"
Dec 23 14:15:40.889: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-b08f8a0b-280a-40f9-a354-4cecdadd8962 container client-container: <nil>
STEP: delete the pod
Dec 23 14:15:40.939: INFO: Waiting for pod downwardapi-volume-b08f8a0b-280a-40f9-a354-4cecdadd8962 to disappear
Dec 23 14:15:40.941: INFO: Pod downwardapi-volume-b08f8a0b-280a-40f9-a354-4cecdadd8962 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:15:40.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2719" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":110,"skipped":2108,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:15:40.954: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:16:41.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8349" for this suite.

• [SLOW TEST:60.060 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":111,"skipped":2123,"failed":0}
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:16:41.015: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Dec 23 14:16:41.052: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:16:45.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8072" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":112,"skipped":2129,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:16:45.395: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 23 14:16:45.444: INFO: Waiting up to 5m0s for pod "pod-c2f336c6-26de-4092-b56f-a5b6d7a74c16" in namespace "emptydir-3618" to be "Succeeded or Failed"
Dec 23 14:16:45.450: INFO: Pod "pod-c2f336c6-26de-4092-b56f-a5b6d7a74c16": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068307ms
Dec 23 14:16:47.454: INFO: Pod "pod-c2f336c6-26de-4092-b56f-a5b6d7a74c16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010195554s
Dec 23 14:16:49.462: INFO: Pod "pod-c2f336c6-26de-4092-b56f-a5b6d7a74c16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017721419s
STEP: Saw pod success
Dec 23 14:16:49.462: INFO: Pod "pod-c2f336c6-26de-4092-b56f-a5b6d7a74c16" satisfied condition "Succeeded or Failed"
Dec 23 14:16:49.464: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-c2f336c6-26de-4092-b56f-a5b6d7a74c16 container test-container: <nil>
STEP: delete the pod
Dec 23 14:16:49.489: INFO: Waiting for pod pod-c2f336c6-26de-4092-b56f-a5b6d7a74c16 to disappear
Dec 23 14:16:49.492: INFO: Pod pod-c2f336c6-26de-4092-b56f-a5b6d7a74c16 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:16:49.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3618" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":113,"skipped":2130,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:16:49.504: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Dec 23 14:16:49.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-9678 create -f -'
Dec 23 14:16:50.093: INFO: stderr: ""
Dec 23 14:16:50.093: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 23 14:16:51.099: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 23 14:16:51.099: INFO: Found 0 / 1
Dec 23 14:16:52.099: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 23 14:16:52.099: INFO: Found 1 / 1
Dec 23 14:16:52.099: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 23 14:16:52.102: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 23 14:16:52.102: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 23 14:16:52.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-9678 patch pod agnhost-primary-jbsp9 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 23 14:16:52.218: INFO: stderr: ""
Dec 23 14:16:52.218: INFO: stdout: "pod/agnhost-primary-jbsp9 patched\n"
STEP: checking annotations
Dec 23 14:16:52.221: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 23 14:16:52.221: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:16:52.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9678" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":114,"skipped":2132,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:16:52.229: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:16:52.269: INFO: Creating deployment "test-recreate-deployment"
Dec 23 14:16:52.273: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 23 14:16:52.290: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 23 14:16:54.302: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 23 14:16:54.304: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 23 14:16:54.311: INFO: Updating deployment test-recreate-deployment
Dec 23 14:16:54.311: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Dec 23 14:16:54.475: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7930  6b48a300-5e7c-4b9f-abb3-79d31f118998 90736 2 2020-12-23 14:16:38 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-12-23 14:16:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-23 14:16:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd abcsys.cn:5000/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032fbe48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-12-23 14:16:40 +0000 UTC,LastTransitionTime:2020-12-23 14:16:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-978dbf94d" is progressing.,LastUpdateTime:2020-12-23 14:16:40 +0000 UTC,LastTransitionTime:2020-12-23 14:16:38 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 23 14:16:54.481: INFO: New ReplicaSet "test-recreate-deployment-978dbf94d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-978dbf94d  deployment-7930  ac780b64-2708-4385-b4ed-369f69beee8e 90734 1 2020-12-23 14:16:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:978dbf94d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 6b48a300-5e7c-4b9f-abb3-79d31f118998 0xc0023d8390 0xc0023d8391}] []  [{kube-controller-manager Update apps/v1 2020-12-23 14:16:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b48a300-5e7c-4b9f-abb3-79d31f118998\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 978dbf94d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:978dbf94d] map[] [] []  []} {[] [] [{httpd abcsys.cn:5000/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0023d8428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 23 14:16:54.481: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 23 14:16:54.481: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-77fdc55d67  deployment-7930  920c1960-ac70-4911-af85-e311a71ed578 90724 2 2020-12-23 14:16:38 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:77fdc55d67] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 6b48a300-5e7c-4b9f-abb3-79d31f118998 0xc0023d8297 0xc0023d8298}] []  [{kube-controller-manager Update apps/v1 2020-12-23 14:16:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b48a300-5e7c-4b9f-abb3-79d31f118998\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 77fdc55d67,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:77fdc55d67] map[] [] []  []} {[] [] [{agnhost abcsys.cn:5000/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0023d8328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 23 14:16:54.485: INFO: Pod "test-recreate-deployment-978dbf94d-pdjt5" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-978dbf94d-pdjt5 test-recreate-deployment-978dbf94d- deployment-7930  ec31ca2c-d88a-40d6-be52-3da64015cd8d 90735 0 2020-12-23 14:16:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:978dbf94d] map[] [{apps/v1 ReplicaSet test-recreate-deployment-978dbf94d ac780b64-2708-4385-b4ed-369f69beee8e 0xc0023d8b80 0xc0023d8b81}] []  [{kube-controller-manager Update v1 2020-12-23 14:16:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac780b64-2708-4385-b4ed-369f69beee8e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-12-23 14:16:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bfdh7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bfdh7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bfdh7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:16:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:16:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:16:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:16:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.17,PodIP:,StartTime:2020-12-23 14:16:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:abcsys.cn:5000/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:16:54.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7930" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":115,"skipped":2166,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:16:54.493: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 14:16:54.579: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d229c179-9798-41f8-b007-b15f74080756" in namespace "projected-7630" to be "Succeeded or Failed"
Dec 23 14:16:54.600: INFO: Pod "downwardapi-volume-d229c179-9798-41f8-b007-b15f74080756": Phase="Pending", Reason="", readiness=false. Elapsed: 20.35578ms
Dec 23 14:16:56.606: INFO: Pod "downwardapi-volume-d229c179-9798-41f8-b007-b15f74080756": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026268973s
STEP: Saw pod success
Dec 23 14:16:56.606: INFO: Pod "downwardapi-volume-d229c179-9798-41f8-b007-b15f74080756" satisfied condition "Succeeded or Failed"
Dec 23 14:16:56.609: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-d229c179-9798-41f8-b007-b15f74080756 container client-container: <nil>
STEP: delete the pod
Dec 23 14:16:56.634: INFO: Waiting for pod downwardapi-volume-d229c179-9798-41f8-b007-b15f74080756 to disappear
Dec 23 14:16:56.638: INFO: Pod downwardapi-volume-d229c179-9798-41f8-b007-b15f74080756 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:16:56.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7630" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":116,"skipped":2169,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:16:56.648: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:17:00.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4915" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":117,"skipped":2191,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:17:00.744: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 14:17:00.791: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15e07534-b4da-4730-a2bf-395e3d4ee1d7" in namespace "downward-api-3612" to be "Succeeded or Failed"
Dec 23 14:17:00.799: INFO: Pod "downwardapi-volume-15e07534-b4da-4730-a2bf-395e3d4ee1d7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021364ms
Dec 23 14:17:02.812: INFO: Pod "downwardapi-volume-15e07534-b4da-4730-a2bf-395e3d4ee1d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02094753s
STEP: Saw pod success
Dec 23 14:17:02.812: INFO: Pod "downwardapi-volume-15e07534-b4da-4730-a2bf-395e3d4ee1d7" satisfied condition "Succeeded or Failed"
Dec 23 14:17:02.815: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-15e07534-b4da-4730-a2bf-395e3d4ee1d7 container client-container: <nil>
STEP: delete the pod
Dec 23 14:17:02.847: INFO: Waiting for pod downwardapi-volume-15e07534-b4da-4730-a2bf-395e3d4ee1d7 to disappear
Dec 23 14:17:02.851: INFO: Pod downwardapi-volume-15e07534-b4da-4730-a2bf-395e3d4ee1d7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:17:02.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3612" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":118,"skipped":2194,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:17:02.865: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 23 14:17:02.927: INFO: Waiting up to 5m0s for pod "pod-d2b0fa5a-7479-4628-bd18-396c050c6ac7" in namespace "emptydir-2373" to be "Succeeded or Failed"
Dec 23 14:17:02.939: INFO: Pod "pod-d2b0fa5a-7479-4628-bd18-396c050c6ac7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.160547ms
Dec 23 14:17:04.947: INFO: Pod "pod-d2b0fa5a-7479-4628-bd18-396c050c6ac7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020023886s
STEP: Saw pod success
Dec 23 14:17:04.947: INFO: Pod "pod-d2b0fa5a-7479-4628-bd18-396c050c6ac7" satisfied condition "Succeeded or Failed"
Dec 23 14:17:04.950: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-d2b0fa5a-7479-4628-bd18-396c050c6ac7 container test-container: <nil>
STEP: delete the pod
Dec 23 14:17:04.967: INFO: Waiting for pod pod-d2b0fa5a-7479-4628-bd18-396c050c6ac7 to disappear
Dec 23 14:17:04.971: INFO: Pod pod-d2b0fa5a-7479-4628-bd18-396c050c6ac7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:17:04.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2373" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":119,"skipped":2227,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:17:04.987: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Dec 23 14:17:05.041: INFO: observed Pod pod-test in namespace pods-8565 in phase Pending conditions []
Dec 23 14:17:05.048: INFO: observed Pod pod-test in namespace pods-8565 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:16:51 +0000 UTC  }]
Dec 23 14:17:05.068: INFO: observed Pod pod-test in namespace pods-8565 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:16:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:16:51 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:16:51 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:16:51 +0000 UTC  }]
Dec 23 14:17:06.030: INFO: observed Pod pod-test in namespace pods-8565 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:16:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:16:51 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:16:51 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-23 14:16:51 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Dec 23 14:17:06.875: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Dec 23 14:17:06.908: INFO: observed event type ADDED
Dec 23 14:17:06.908: INFO: observed event type MODIFIED
Dec 23 14:17:06.908: INFO: observed event type MODIFIED
Dec 23 14:17:06.908: INFO: observed event type MODIFIED
Dec 23 14:17:06.909: INFO: observed event type MODIFIED
Dec 23 14:17:06.909: INFO: observed event type MODIFIED
Dec 23 14:17:06.909: INFO: observed event type MODIFIED
Dec 23 14:17:06.909: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:17:06.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8565" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":120,"skipped":2239,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:17:06.919: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:17:06.966: INFO: The status of Pod test-webserver-006b7ce0-b8e1-4c1b-a8e4-708527f88036 is Pending, waiting for it to be Running (with Ready = true)
Dec 23 14:17:08.974: INFO: The status of Pod test-webserver-006b7ce0-b8e1-4c1b-a8e4-708527f88036 is Running (Ready = false)
Dec 23 14:17:10.974: INFO: The status of Pod test-webserver-006b7ce0-b8e1-4c1b-a8e4-708527f88036 is Running (Ready = false)
Dec 23 14:17:12.975: INFO: The status of Pod test-webserver-006b7ce0-b8e1-4c1b-a8e4-708527f88036 is Running (Ready = false)
Dec 23 14:17:14.974: INFO: The status of Pod test-webserver-006b7ce0-b8e1-4c1b-a8e4-708527f88036 is Running (Ready = false)
Dec 23 14:17:16.976: INFO: The status of Pod test-webserver-006b7ce0-b8e1-4c1b-a8e4-708527f88036 is Running (Ready = false)
Dec 23 14:17:18.979: INFO: The status of Pod test-webserver-006b7ce0-b8e1-4c1b-a8e4-708527f88036 is Running (Ready = false)
Dec 23 14:17:20.974: INFO: The status of Pod test-webserver-006b7ce0-b8e1-4c1b-a8e4-708527f88036 is Running (Ready = false)
Dec 23 14:17:22.975: INFO: The status of Pod test-webserver-006b7ce0-b8e1-4c1b-a8e4-708527f88036 is Running (Ready = false)
Dec 23 14:17:24.973: INFO: The status of Pod test-webserver-006b7ce0-b8e1-4c1b-a8e4-708527f88036 is Running (Ready = false)
Dec 23 14:17:26.975: INFO: The status of Pod test-webserver-006b7ce0-b8e1-4c1b-a8e4-708527f88036 is Running (Ready = false)
Dec 23 14:17:28.974: INFO: The status of Pod test-webserver-006b7ce0-b8e1-4c1b-a8e4-708527f88036 is Running (Ready = true)
Dec 23 14:17:28.976: INFO: Container started at 2020-12-23 14:16:54 +0000 UTC, pod became ready at 2020-12-23 14:17:13 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:17:28.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8586" for this suite.

• [SLOW TEST:22.067 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":121,"skipped":2244,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:17:28.986: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-1adabe8c-5d64-41a7-9e91-bbc24470cbc1
STEP: Creating configMap with name cm-test-opt-upd-edd4989e-1fc1-4268-a9ef-bbef4c3881d2
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1adabe8c-5d64-41a7-9e91-bbc24470cbc1
STEP: Updating configmap cm-test-opt-upd-edd4989e-1fc1-4268-a9ef-bbef4c3881d2
STEP: Creating configMap with name cm-test-opt-create-451d5cb3-d27c-43a0-92c6-50989f4ed6a6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:18:55.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2755" for this suite.

• [SLOW TEST:86.574 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":122,"skipped":2250,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:18:55.560: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 23 14:18:55.615: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Dec 23 14:18:55.619: INFO: starting watch
STEP: patching
STEP: updating
Dec 23 14:18:55.630: INFO: waiting for watch events with expected annotations
Dec 23 14:18:55.630: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:18:55.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9752" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":123,"skipped":2289,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:18:55.697: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Dec 23 14:18:55.746: INFO: Waiting up to 5m0s for pod "var-expansion-cbc093a2-e01b-48aa-86e1-cfa1e772ff0b" in namespace "var-expansion-590" to be "Succeeded or Failed"
Dec 23 14:18:55.752: INFO: Pod "var-expansion-cbc093a2-e01b-48aa-86e1-cfa1e772ff0b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.523082ms
Dec 23 14:18:57.759: INFO: Pod "var-expansion-cbc093a2-e01b-48aa-86e1-cfa1e772ff0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013217603s
STEP: Saw pod success
Dec 23 14:18:57.760: INFO: Pod "var-expansion-cbc093a2-e01b-48aa-86e1-cfa1e772ff0b" satisfied condition "Succeeded or Failed"
Dec 23 14:18:57.764: INFO: Trying to get logs from node wt-k8s-2 pod var-expansion-cbc093a2-e01b-48aa-86e1-cfa1e772ff0b container dapi-container: <nil>
STEP: delete the pod
Dec 23 14:18:57.803: INFO: Waiting for pod var-expansion-cbc093a2-e01b-48aa-86e1-cfa1e772ff0b to disappear
Dec 23 14:18:57.810: INFO: Pod var-expansion-cbc093a2-e01b-48aa-86e1-cfa1e772ff0b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:18:57.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-590" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":124,"skipped":2330,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:18:57.886: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:18:57.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-913" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":125,"skipped":2348,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:18:57.977: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:18:58.025: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 23 14:19:02.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-4580 --namespace=crd-publish-openapi-4580 create -f -'
Dec 23 14:19:02.836: INFO: stderr: ""
Dec 23 14:19:02.836: INFO: stdout: "e2e-test-crd-publish-openapi-8126-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 23 14:19:02.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-4580 --namespace=crd-publish-openapi-4580 delete e2e-test-crd-publish-openapi-8126-crds test-cr'
Dec 23 14:19:02.946: INFO: stderr: ""
Dec 23 14:19:02.946: INFO: stdout: "e2e-test-crd-publish-openapi-8126-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 23 14:19:02.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-4580 --namespace=crd-publish-openapi-4580 apply -f -'
Dec 23 14:19:03.375: INFO: stderr: ""
Dec 23 14:19:03.375: INFO: stdout: "e2e-test-crd-publish-openapi-8126-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 23 14:19:03.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-4580 --namespace=crd-publish-openapi-4580 delete e2e-test-crd-publish-openapi-8126-crds test-cr'
Dec 23 14:19:03.498: INFO: stderr: ""
Dec 23 14:19:03.498: INFO: stdout: "e2e-test-crd-publish-openapi-8126-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Dec 23 14:19:03.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-4580 explain e2e-test-crd-publish-openapi-8126-crds'
Dec 23 14:19:03.837: INFO: stderr: ""
Dec 23 14:19:03.837: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8126-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:19:07.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4580" for this suite.

• [SLOW TEST:9.832 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":126,"skipped":2364,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:19:07.810: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 14:19:07.848: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8718bbda-8a55-4dbe-8b88-60940ce41f4b" in namespace "downward-api-6659" to be "Succeeded or Failed"
Dec 23 14:19:07.854: INFO: Pod "downwardapi-volume-8718bbda-8a55-4dbe-8b88-60940ce41f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.361311ms
Dec 23 14:19:09.857: INFO: Pod "downwardapi-volume-8718bbda-8a55-4dbe-8b88-60940ce41f4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008543896s
STEP: Saw pod success
Dec 23 14:19:09.857: INFO: Pod "downwardapi-volume-8718bbda-8a55-4dbe-8b88-60940ce41f4b" satisfied condition "Succeeded or Failed"
Dec 23 14:19:09.859: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-8718bbda-8a55-4dbe-8b88-60940ce41f4b container client-container: <nil>
STEP: delete the pod
Dec 23 14:19:09.886: INFO: Waiting for pod downwardapi-volume-8718bbda-8a55-4dbe-8b88-60940ce41f4b to disappear
Dec 23 14:19:09.894: INFO: Pod downwardapi-volume-8718bbda-8a55-4dbe-8b88-60940ce41f4b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:19:09.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6659" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":127,"skipped":2376,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:19:09.910: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 14:19:09.955: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d18d3597-8b38-4947-9a45-83141a97975e" in namespace "downward-api-5264" to be "Succeeded or Failed"
Dec 23 14:19:09.963: INFO: Pod "downwardapi-volume-d18d3597-8b38-4947-9a45-83141a97975e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.425905ms
Dec 23 14:19:11.972: INFO: Pod "downwardapi-volume-d18d3597-8b38-4947-9a45-83141a97975e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016839842s
Dec 23 14:19:13.980: INFO: Pod "downwardapi-volume-d18d3597-8b38-4947-9a45-83141a97975e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024877476s
STEP: Saw pod success
Dec 23 14:19:13.980: INFO: Pod "downwardapi-volume-d18d3597-8b38-4947-9a45-83141a97975e" satisfied condition "Succeeded or Failed"
Dec 23 14:19:13.982: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-d18d3597-8b38-4947-9a45-83141a97975e container client-container: <nil>
STEP: delete the pod
Dec 23 14:19:14.001: INFO: Waiting for pod downwardapi-volume-d18d3597-8b38-4947-9a45-83141a97975e to disappear
Dec 23 14:19:14.004: INFO: Pod downwardapi-volume-d18d3597-8b38-4947-9a45-83141a97975e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:19:14.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5264" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":128,"skipped":2378,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:19:14.012: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Dec 23 14:19:14.057: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 23 14:19:14.064: INFO: Waiting for terminating namespaces to be deleted...
Dec 23 14:19:14.066: INFO: 
Logging pods the apiserver thinks is on node wt-k8s-2 before test
Dec 23 14:19:14.071: INFO: calico-node-5czbz from kube-system started at 2020-12-23 08:19:38 +0000 UTC (1 container statuses recorded)
Dec 23 14:19:14.071: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 14:19:14.071: INFO: kube-proxy-55qvq from kube-system started at 2020-12-23 08:19:38 +0000 UTC (1 container statuses recorded)
Dec 23 14:19:14.071: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 23 14:19:14.071: INFO: sonobuoy from sonobuoy started at 2020-12-23 13:24:44 +0000 UTC (1 container statuses recorded)
Dec 23 14:19:14.071: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 23 14:19:14.071: INFO: sonobuoy-e2e-job-381779db02e94c8b from sonobuoy started at 2020-12-23 13:24:46 +0000 UTC (2 container statuses recorded)
Dec 23 14:19:14.071: INFO: 	Container e2e ready: true, restart count 0
Dec 23 14:19:14.071: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 14:19:14.071: INFO: sonobuoy-systemd-logs-daemon-set-a3aba41d66fb4680-2x864 from sonobuoy started at 2020-12-23 13:24:46 +0000 UTC (2 container statuses recorded)
Dec 23 14:19:14.071: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 14:19:14.071: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 14:19:14.071: INFO: 
Logging pods the apiserver thinks is on node wt-k8s-3.novalocal before test
Dec 23 14:19:14.074: INFO: calico-kube-controllers-744cfdf676-wbsnz from kube-system started at 2020-12-23 13:43:17 +0000 UTC (1 container statuses recorded)
Dec 23 14:19:14.074: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 23 14:19:14.074: INFO: calico-node-lz86h from kube-system started at 2020-12-23 08:18:23 +0000 UTC (1 container statuses recorded)
Dec 23 14:19:14.074: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 14:19:14.074: INFO: kube-proxy-pnq9z from kube-system started at 2020-12-23 08:14:45 +0000 UTC (1 container statuses recorded)
Dec 23 14:19:14.074: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 23 14:19:14.074: INFO: sonobuoy-systemd-logs-daemon-set-a3aba41d66fb4680-n5vgr from sonobuoy started at 2020-12-23 13:24:32 +0000 UTC (2 container statuses recorded)
Dec 23 14:19:14.074: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Dec 23 14:19:14.074: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-8138e7d4-1269-480a-9f80-e6330de480e3 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-8138e7d4-1269-480a-9f80-e6330de480e3 off the node wt-k8s-3.novalocal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8138e7d4-1269-480a-9f80-e6330de480e3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:19:22.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-139" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:8.162 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":129,"skipped":2406,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:19:22.174: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:19:22.229: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 23 14:19:27.237: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 23 14:19:27.237: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 23 14:19:29.246: INFO: Creating deployment "test-rollover-deployment"
Dec 23 14:19:29.255: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 23 14:19:31.271: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 23 14:19:31.277: INFO: Ensure that both replica sets have 1 created replica
Dec 23 14:19:31.282: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 23 14:19:31.293: INFO: Updating deployment test-rollover-deployment
Dec 23 14:19:31.293: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 23 14:19:33.310: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 23 14:19:33.315: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 23 14:19:33.319: INFO: all replica sets need to contain the pod-template-hash label
Dec 23 14:19:33.319: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329957, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-68db7cdb6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 14:19:35.338: INFO: all replica sets need to contain the pod-template-hash label
Dec 23 14:19:35.338: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329959, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-68db7cdb6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 14:19:37.329: INFO: all replica sets need to contain the pod-template-hash label
Dec 23 14:19:37.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329959, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-68db7cdb6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 14:19:39.333: INFO: all replica sets need to contain the pod-template-hash label
Dec 23 14:19:39.333: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329959, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-68db7cdb6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 14:19:41.334: INFO: all replica sets need to contain the pod-template-hash label
Dec 23 14:19:41.334: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329959, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-68db7cdb6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 14:19:43.327: INFO: all replica sets need to contain the pod-template-hash label
Dec 23 14:19:43.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329959, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329955, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-68db7cdb6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 14:19:45.331: INFO: 
Dec 23 14:19:45.331: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Dec 23 14:19:45.337: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4686  e257280c-6860-4829-87b8-b5b923d45e63 91618 2 2020-12-23 14:19:15 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-12-23 14:19:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-23 14:19:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost abcsys.cn:5000/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051a2c68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-12-23 14:19:15 +0000 UTC,LastTransitionTime:2020-12-23 14:19:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-68db7cdb6" has successfully progressed.,LastUpdateTime:2020-12-23 14:19:29 +0000 UTC,LastTransitionTime:2020-12-23 14:19:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 23 14:19:45.340: INFO: New ReplicaSet "test-rollover-deployment-68db7cdb6" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-68db7cdb6  deployment-4686  e248256e-8391-4a53-85ce-33922be24e8b 91605 2 2020-12-23 14:19:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:68db7cdb6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment e257280c-6860-4829-87b8-b5b923d45e63 0xc0051a32f0 0xc0051a32f1}] []  [{kube-controller-manager Update apps/v1 2020-12-23 14:19:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e257280c-6860-4829-87b8-b5b923d45e63\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 68db7cdb6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:68db7cdb6] map[] [] []  []} {[] [] [{agnhost abcsys.cn:5000/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051a3388 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 23 14:19:45.340: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 23 14:19:45.340: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4686  c426170e-59ea-4edd-8d1c-d5f37aea95f6 91617 2 2020-12-23 14:19:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment e257280c-6860-4829-87b8-b5b923d45e63 0xc0051a31a7 0xc0051a31a8}] []  [{e2e.test Update apps/v1 2020-12-23 14:19:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-23 14:19:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e257280c-6860-4829-87b8-b5b923d45e63\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd abcsys.cn:5000/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0051a3268 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 23 14:19:45.340: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-4686  30de784a-e84f-4320-baba-40b5709460da 91567 2 2020-12-23 14:19:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment e257280c-6860-4829-87b8-b5b923d45e63 0xc0051a3407 0xc0051a3408}] []  [{kube-controller-manager Update apps/v1 2020-12-23 14:19:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e257280c-6860-4829-87b8-b5b923d45e63\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051a34b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 23 14:19:45.343: INFO: Pod "test-rollover-deployment-68db7cdb6-84qxx" is available:
&Pod{ObjectMeta:{test-rollover-deployment-68db7cdb6-84qxx test-rollover-deployment-68db7cdb6- deployment-4686  1ea164a8-d1f4-45c6-8674-4fc39d041588 91586 0 2020-12-23 14:19:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:68db7cdb6] map[cni.projectcalico.org/podIP:10.244.201.220/32 cni.projectcalico.org/podIPs:10.244.201.220/32] [{apps/v1 ReplicaSet test-rollover-deployment-68db7cdb6 e248256e-8391-4a53-85ce-33922be24e8b 0xc0051a3ae0 0xc0051a3ae1}] []  [{kube-controller-manager Update v1 2020-12-23 14:19:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e248256e-8391-4a53-85ce-33922be24e8b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-23 14:19:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-23 14:19:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.201.220\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-slpfp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-slpfp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:abcsys.cn:5000/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-slpfp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:19:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:19:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:19:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:19:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.25,PodIP:10.244.201.220,StartTime:2020-12-23 14:19:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-23 14:19:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:abcsys.cn:5000/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://abcsys.cn:5000/e2e-test-images/agnhost@sha256:e23f72f469927ead04b7bb2a5cedf8569906c38ced37a64767542e7d22831d45,ContainerID:docker://50010c634abd6872f3a9c388faf2ac974d3f3b3ec71e4124004ed285a4fdd050,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.201.220,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:19:45.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4686" for this suite.

• [SLOW TEST:23.187 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":130,"skipped":2408,"failed":0}
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:19:45.362: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 23 14:19:48.460: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:19:48.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1361" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":131,"skipped":2410,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:19:48.486: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 23 14:19:49.289: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 23 14:19:51.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329975, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329975, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329975, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744329975, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-76bbb7fdd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 14:19:54.325: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
Dec 23 14:19:54.388: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:54.505: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:54.603: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:54.708: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:54.803: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:54.966: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:55.006: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:55.159: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:55.202: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:55.305: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:55.407: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:55.523: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:55.603: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:55.716: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:55.811: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:55.908: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:56.004: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:56.102: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:56.203: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:56.304: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:56.406: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:56.503: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:56.603: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:56.703: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:56.805: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:56.902: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:57.005: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:57.106: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:57.211: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:57.304: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:57.404: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:57.505: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:57.638: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:57.707: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:57.805: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:57.907: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:58.075: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:58.105: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:58.215: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:58.305: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:58.404: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:58.505: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:58.604: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:58.708: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:58.942: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:59.004: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:59.102: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:59.204: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:59.303: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:59.404: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:59.504: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:59.603: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:59.705: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:59.802: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:19:59.903: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:00.008: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:00.105: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:00.205: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:00.321: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:00.402: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:00.504: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:00.738: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:00.804: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:00.903: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:01.004: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:01.105: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:01.202: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:01.302: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:01.406: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:01.505: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:01.603: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:01.703: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:01.804: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:01.904: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:02.005: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:02.104: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:02.208: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:02.309: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:02.403: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:02.504: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:02.605: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:02.705: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:02.803: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:20:02.901: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:20:03.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4981" for this suite.
STEP: Destroying namespace "webhook-4981-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:14.719 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":132,"skipped":2438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:20:03.206: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Dec 23 14:20:03.245: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:20:24.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8625" for this suite.

• [SLOW TEST:21.431 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":133,"skipped":2482,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:20:24.637: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 14:20:24.692: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44e77d56-1137-4123-93df-019546f29881" in namespace "projected-2641" to be "Succeeded or Failed"
Dec 23 14:20:24.699: INFO: Pod "downwardapi-volume-44e77d56-1137-4123-93df-019546f29881": Phase="Pending", Reason="", readiness=false. Elapsed: 7.024452ms
Dec 23 14:20:26.707: INFO: Pod "downwardapi-volume-44e77d56-1137-4123-93df-019546f29881": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014837149s
STEP: Saw pod success
Dec 23 14:20:26.707: INFO: Pod "downwardapi-volume-44e77d56-1137-4123-93df-019546f29881" satisfied condition "Succeeded or Failed"
Dec 23 14:20:26.709: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-44e77d56-1137-4123-93df-019546f29881 container client-container: <nil>
STEP: delete the pod
Dec 23 14:20:26.733: INFO: Waiting for pod downwardapi-volume-44e77d56-1137-4123-93df-019546f29881 to disappear
Dec 23 14:20:26.735: INFO: Pod downwardapi-volume-44e77d56-1137-4123-93df-019546f29881 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:20:26.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2641" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":134,"skipped":2516,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:20:26.743: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 14:20:26.794: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f01d07df-ea5f-49b1-8cdf-d435fe53c353" in namespace "projected-8250" to be "Succeeded or Failed"
Dec 23 14:20:26.802: INFO: Pod "downwardapi-volume-f01d07df-ea5f-49b1-8cdf-d435fe53c353": Phase="Pending", Reason="", readiness=false. Elapsed: 7.787217ms
Dec 23 14:20:28.809: INFO: Pod "downwardapi-volume-f01d07df-ea5f-49b1-8cdf-d435fe53c353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014615042s
Dec 23 14:20:30.816: INFO: Pod "downwardapi-volume-f01d07df-ea5f-49b1-8cdf-d435fe53c353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021768136s
STEP: Saw pod success
Dec 23 14:20:30.816: INFO: Pod "downwardapi-volume-f01d07df-ea5f-49b1-8cdf-d435fe53c353" satisfied condition "Succeeded or Failed"
Dec 23 14:20:30.819: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-f01d07df-ea5f-49b1-8cdf-d435fe53c353 container client-container: <nil>
STEP: delete the pod
Dec 23 14:20:30.842: INFO: Waiting for pod downwardapi-volume-f01d07df-ea5f-49b1-8cdf-d435fe53c353 to disappear
Dec 23 14:20:30.845: INFO: Pod downwardapi-volume-f01d07df-ea5f-49b1-8cdf-d435fe53c353 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:20:30.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8250" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":135,"skipped":2538,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:20:30.854: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-2dccc1a6-0414-470e-8bf4-bc4fbb9587e8
STEP: Creating a pod to test consume configMaps
Dec 23 14:20:30.902: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-33b5c7dc-0f9c-48f2-9590-028417dfc220" in namespace "projected-6428" to be "Succeeded or Failed"
Dec 23 14:20:30.906: INFO: Pod "pod-projected-configmaps-33b5c7dc-0f9c-48f2-9590-028417dfc220": Phase="Pending", Reason="", readiness=false. Elapsed: 3.621139ms
Dec 23 14:20:32.914: INFO: Pod "pod-projected-configmaps-33b5c7dc-0f9c-48f2-9590-028417dfc220": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012052382s
Dec 23 14:20:34.922: INFO: Pod "pod-projected-configmaps-33b5c7dc-0f9c-48f2-9590-028417dfc220": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020361831s
STEP: Saw pod success
Dec 23 14:20:34.922: INFO: Pod "pod-projected-configmaps-33b5c7dc-0f9c-48f2-9590-028417dfc220" satisfied condition "Succeeded or Failed"
Dec 23 14:20:34.924: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-projected-configmaps-33b5c7dc-0f9c-48f2-9590-028417dfc220 container agnhost-container: <nil>
STEP: delete the pod
Dec 23 14:20:34.947: INFO: Waiting for pod pod-projected-configmaps-33b5c7dc-0f9c-48f2-9590-028417dfc220 to disappear
Dec 23 14:20:34.950: INFO: Pod pod-projected-configmaps-33b5c7dc-0f9c-48f2-9590-028417dfc220 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:20:34.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6428" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":136,"skipped":2539,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:20:34.958: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5849.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5849.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 23 14:20:39.036: INFO: DNS probes using dns-5849/dns-test-d59ba692-ffda-4162-962c-f6637cfe4a29 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:20:39.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5849" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":137,"skipped":2565,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:20:39.061: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:20:39.106: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Dec 23 14:20:43.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-8467 --namespace=crd-publish-openapi-8467 create -f -'
Dec 23 14:20:43.838: INFO: stderr: ""
Dec 23 14:20:43.838: INFO: stdout: "e2e-test-crd-publish-openapi-2567-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 23 14:20:43.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-8467 --namespace=crd-publish-openapi-8467 delete e2e-test-crd-publish-openapi-2567-crds test-foo'
Dec 23 14:20:43.944: INFO: stderr: ""
Dec 23 14:20:43.944: INFO: stdout: "e2e-test-crd-publish-openapi-2567-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 23 14:20:43.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-8467 --namespace=crd-publish-openapi-8467 apply -f -'
Dec 23 14:20:44.456: INFO: stderr: ""
Dec 23 14:20:44.456: INFO: stdout: "e2e-test-crd-publish-openapi-2567-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 23 14:20:44.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-8467 --namespace=crd-publish-openapi-8467 delete e2e-test-crd-publish-openapi-2567-crds test-foo'
Dec 23 14:20:44.570: INFO: stderr: ""
Dec 23 14:20:44.570: INFO: stdout: "e2e-test-crd-publish-openapi-2567-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Dec 23 14:20:44.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-8467 --namespace=crd-publish-openapi-8467 create -f -'
Dec 23 14:20:44.971: INFO: rc: 1
Dec 23 14:20:44.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-8467 --namespace=crd-publish-openapi-8467 apply -f -'
Dec 23 14:20:45.331: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Dec 23 14:20:45.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-8467 --namespace=crd-publish-openapi-8467 create -f -'
Dec 23 14:20:45.759: INFO: rc: 1
Dec 23 14:20:45.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-8467 --namespace=crd-publish-openapi-8467 apply -f -'
Dec 23 14:20:46.239: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Dec 23 14:20:46.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-8467 explain e2e-test-crd-publish-openapi-2567-crds'
Dec 23 14:20:46.680: INFO: stderr: ""
Dec 23 14:20:46.680: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2567-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Dec 23 14:20:46.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-8467 explain e2e-test-crd-publish-openapi-2567-crds.metadata'
Dec 23 14:20:47.087: INFO: stderr: ""
Dec 23 14:20:47.087: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2567-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 23 14:20:47.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-8467 explain e2e-test-crd-publish-openapi-2567-crds.spec'
Dec 23 14:20:47.930: INFO: stderr: ""
Dec 23 14:20:47.930: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2567-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 23 14:20:47.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-8467 explain e2e-test-crd-publish-openapi-2567-crds.spec.bars'
Dec 23 14:20:48.614: INFO: stderr: ""
Dec 23 14:20:48.614: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2567-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Dec 23 14:20:48.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-8467 explain e2e-test-crd-publish-openapi-2567-crds.spec.bars2'
Dec 23 14:20:49.217: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:20:53.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8467" for this suite.

• [SLOW TEST:14.104 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":138,"skipped":2591,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:20:53.165: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:21:21.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9654" for this suite.

• [SLOW TEST:28.103 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":139,"skipped":2591,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:21:21.270: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 23 14:21:21.328: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4834  69435156-539b-48ae-b97b-482bf3637b75 92296 0 2020-12-23 14:21:07 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-12-23 14:21:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 23 14:21:21.329: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4834  69435156-539b-48ae-b97b-482bf3637b75 92297 0 2020-12-23 14:21:07 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-12-23 14:21:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:21:21.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4834" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":140,"skipped":2616,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:21:21.337: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 23 14:21:22.231: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 23 14:21:24.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744330068, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744330068, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744330068, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744330068, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-76bbb7fdd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 14:21:27.299: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:21:27.305: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Registering the custom resource webhook via the AdmissionRegistration API
Dec 23 14:21:29.268: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:29.384: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:29.484: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:29.582: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:29.683: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:29.782: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:29.881: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:29.981: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:30.083: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:30.185: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:30.315: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:30.381: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:30.482: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:30.583: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:30.684: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:30.784: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:30.881: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:30.983: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:31.082: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:31.189: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:31.283: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:31.383: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:31.482: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:31.582: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:31.681: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:31.781: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:31.885: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:31.986: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:32.083: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:32.182: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:32.283: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:32.382: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:32.482: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:32.584: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:32.686: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:32.782: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:32.883: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:32.992: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:33.085: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:33.187: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:33.294: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:33.382: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:33.483: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:33.584: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:33.684: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:33.782: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:33.891: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:33.982: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:34.082: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:34.182: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:34.295: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:34.409: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:34.484: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:34.583: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:34.681: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:34.781: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:21:34.882: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:21:35.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1924" for this suite.
STEP: Destroying namespace "webhook-1924-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:14.333 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":141,"skipped":2636,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:21:35.671: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:21:35.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1701" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":142,"skipped":2673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:21:35.791: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:21:35.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-67" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":143,"skipped":2707,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:21:35.867: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2554
STEP: creating service affinity-clusterip-transition in namespace services-2554
STEP: creating replication controller affinity-clusterip-transition in namespace services-2554
I1223 14:21:35.996238      20 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-2554, replica count: 3
I1223 14:21:39.046884      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 23 14:21:39.055: INFO: Creating new exec pod
Dec 23 14:21:44.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2554 exec execpod-affinity8m624 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Dec 23 14:21:44.314: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Dec 23 14:21:44.314: INFO: stdout: ""
Dec 23 14:21:44.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2554 exec execpod-affinity8m624 -- /bin/sh -x -c nc -zv -t -w 2 10.104.137.39 80'
Dec 23 14:21:44.524: INFO: stderr: "+ nc -zv -t -w 2 10.104.137.39 80\nConnection to 10.104.137.39 80 port [tcp/http] succeeded!\n"
Dec 23 14:21:44.524: INFO: stdout: ""
Dec 23 14:21:44.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2554 exec execpod-affinity8m624 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.104.137.39:80/ ; done'
Dec 23 14:21:44.870: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n"
Dec 23 14:21:44.870: INFO: stdout: "\naffinity-clusterip-transition-mpb9x\naffinity-clusterip-transition-gb6tc\naffinity-clusterip-transition-mpb9x\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-gb6tc\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-mpb9x\naffinity-clusterip-transition-mpb9x\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-mpb9x\naffinity-clusterip-transition-gb6tc\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-mpb9x\naffinity-clusterip-transition-mpb9x\naffinity-clusterip-transition-gb6tc"
Dec 23 14:21:44.870: INFO: Received response from host: affinity-clusterip-transition-mpb9x
Dec 23 14:21:44.870: INFO: Received response from host: affinity-clusterip-transition-gb6tc
Dec 23 14:21:44.870: INFO: Received response from host: affinity-clusterip-transition-mpb9x
Dec 23 14:21:44.870: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:44.870: INFO: Received response from host: affinity-clusterip-transition-gb6tc
Dec 23 14:21:44.870: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:44.870: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:44.870: INFO: Received response from host: affinity-clusterip-transition-mpb9x
Dec 23 14:21:44.870: INFO: Received response from host: affinity-clusterip-transition-mpb9x
Dec 23 14:21:44.870: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:44.870: INFO: Received response from host: affinity-clusterip-transition-mpb9x
Dec 23 14:21:44.870: INFO: Received response from host: affinity-clusterip-transition-gb6tc
Dec 23 14:21:44.870: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:44.870: INFO: Received response from host: affinity-clusterip-transition-mpb9x
Dec 23 14:21:44.870: INFO: Received response from host: affinity-clusterip-transition-mpb9x
Dec 23 14:21:44.870: INFO: Received response from host: affinity-clusterip-transition-gb6tc
Dec 23 14:21:44.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2554 exec execpod-affinity8m624 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.104.137.39:80/ ; done'
Dec 23 14:21:45.186: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.137.39:80/\n"
Dec 23 14:21:45.186: INFO: stdout: "\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-x47jd\naffinity-clusterip-transition-x47jd"
Dec 23 14:21:45.186: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:45.187: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:45.187: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:45.187: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:45.187: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:45.187: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:45.187: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:45.187: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:45.187: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:45.187: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:45.187: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:45.187: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:45.187: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:45.187: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:45.187: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:45.187: INFO: Received response from host: affinity-clusterip-transition-x47jd
Dec 23 14:21:45.187: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2554, will wait for the garbage collector to delete the pods
Dec 23 14:21:45.269: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.351655ms
Dec 23 14:21:45.369: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.178294ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:21:59.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2554" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:23.608 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":144,"skipped":2741,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:21:59.475: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-fd592525-5a97-4677-a5c7-7f98cc626ea8
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-fd592525-5a97-4677-a5c7-7f98cc626ea8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:22:03.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2177" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":145,"skipped":2751,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:22:03.576: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-f5503932-9dc4-438f-b422-f404c51f5656
STEP: Creating a pod to test consume secrets
Dec 23 14:22:03.620: INFO: Waiting up to 5m0s for pod "pod-secrets-6dd8daf2-2fd1-478e-a2ca-bac0586acfb6" in namespace "secrets-9640" to be "Succeeded or Failed"
Dec 23 14:22:03.640: INFO: Pod "pod-secrets-6dd8daf2-2fd1-478e-a2ca-bac0586acfb6": Phase="Pending", Reason="", readiness=false. Elapsed: 20.017656ms
Dec 23 14:22:05.648: INFO: Pod "pod-secrets-6dd8daf2-2fd1-478e-a2ca-bac0586acfb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027844673s
STEP: Saw pod success
Dec 23 14:22:05.648: INFO: Pod "pod-secrets-6dd8daf2-2fd1-478e-a2ca-bac0586acfb6" satisfied condition "Succeeded or Failed"
Dec 23 14:22:05.652: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-secrets-6dd8daf2-2fd1-478e-a2ca-bac0586acfb6 container secret-volume-test: <nil>
STEP: delete the pod
Dec 23 14:22:05.676: INFO: Waiting for pod pod-secrets-6dd8daf2-2fd1-478e-a2ca-bac0586acfb6 to disappear
Dec 23 14:22:05.680: INFO: Pod pod-secrets-6dd8daf2-2fd1-478e-a2ca-bac0586acfb6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:22:05.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9640" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":146,"skipped":2752,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:22:05.696: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2424
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Dec 23 14:22:05.781: INFO: Found 0 stateful pods, waiting for 3
Dec 23 14:22:15.795: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 14:22:15.795: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 14:22:15.795: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 14:22:15.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-2424 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 23 14:22:16.093: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 23 14:22:16.093: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 23 14:22:16.093: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from abcsys.cn:5000/library/httpd:2.4.38-alpine to abcsys.cn:5000/library/httpd:2.4.39-alpine
Dec 23 14:22:26.139: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 23 14:22:36.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-2424 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:22:36.388: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 23 14:22:36.389: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 23 14:22:36.389: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 23 14:22:46.431: INFO: Waiting for StatefulSet statefulset-2424/ss2 to complete update
Dec 23 14:22:46.431: INFO: Waiting for Pod statefulset-2424/ss2-0 to have revision ss2-54677df74d update revision ss2-866bd64d95
Dec 23 14:22:46.431: INFO: Waiting for Pod statefulset-2424/ss2-1 to have revision ss2-54677df74d update revision ss2-866bd64d95
Dec 23 14:22:46.431: INFO: Waiting for Pod statefulset-2424/ss2-2 to have revision ss2-54677df74d update revision ss2-866bd64d95
Dec 23 14:22:56.444: INFO: Waiting for StatefulSet statefulset-2424/ss2 to complete update
Dec 23 14:22:56.444: INFO: Waiting for Pod statefulset-2424/ss2-0 to have revision ss2-54677df74d update revision ss2-866bd64d95
Dec 23 14:22:56.444: INFO: Waiting for Pod statefulset-2424/ss2-1 to have revision ss2-54677df74d update revision ss2-866bd64d95
Dec 23 14:23:06.442: INFO: Waiting for StatefulSet statefulset-2424/ss2 to complete update
Dec 23 14:23:06.442: INFO: Waiting for Pod statefulset-2424/ss2-0 to have revision ss2-54677df74d update revision ss2-866bd64d95
Dec 23 14:23:16.443: INFO: Waiting for StatefulSet statefulset-2424/ss2 to complete update
Dec 23 14:23:16.443: INFO: Waiting for Pod statefulset-2424/ss2-0 to have revision ss2-54677df74d update revision ss2-866bd64d95
STEP: Rolling back to a previous revision
Dec 23 14:23:26.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-2424 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 23 14:23:26.680: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 23 14:23:26.680: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 23 14:23:26.680: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 23 14:23:36.730: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 23 14:23:46.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-2424 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 14:23:46.976: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 23 14:23:46.976: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 23 14:23:46.976: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 23 14:23:56.998: INFO: Waiting for StatefulSet statefulset-2424/ss2 to complete update
Dec 23 14:23:56.998: INFO: Waiting for Pod statefulset-2424/ss2-0 to have revision ss2-866bd64d95 update revision ss2-54677df74d
Dec 23 14:23:56.998: INFO: Waiting for Pod statefulset-2424/ss2-1 to have revision ss2-866bd64d95 update revision ss2-54677df74d
Dec 23 14:23:56.998: INFO: Waiting for Pod statefulset-2424/ss2-2 to have revision ss2-866bd64d95 update revision ss2-54677df74d
Dec 23 14:24:07.014: INFO: Waiting for StatefulSet statefulset-2424/ss2 to complete update
Dec 23 14:24:07.014: INFO: Waiting for Pod statefulset-2424/ss2-0 to have revision ss2-866bd64d95 update revision ss2-54677df74d
Dec 23 14:24:07.014: INFO: Waiting for Pod statefulset-2424/ss2-1 to have revision ss2-866bd64d95 update revision ss2-54677df74d
Dec 23 14:24:07.014: INFO: Waiting for Pod statefulset-2424/ss2-2 to have revision ss2-866bd64d95 update revision ss2-54677df74d
Dec 23 14:24:17.016: INFO: Waiting for StatefulSet statefulset-2424/ss2 to complete update
Dec 23 14:24:17.016: INFO: Waiting for Pod statefulset-2424/ss2-0 to have revision ss2-866bd64d95 update revision ss2-54677df74d
Dec 23 14:24:17.016: INFO: Waiting for Pod statefulset-2424/ss2-1 to have revision ss2-866bd64d95 update revision ss2-54677df74d
Dec 23 14:24:17.016: INFO: Waiting for Pod statefulset-2424/ss2-2 to have revision ss2-866bd64d95 update revision ss2-54677df74d
Dec 23 14:24:27.015: INFO: Waiting for StatefulSet statefulset-2424/ss2 to complete update
Dec 23 14:24:27.016: INFO: Waiting for Pod statefulset-2424/ss2-0 to have revision ss2-866bd64d95 update revision ss2-54677df74d
Dec 23 14:24:27.016: INFO: Waiting for Pod statefulset-2424/ss2-1 to have revision ss2-866bd64d95 update revision ss2-54677df74d
Dec 23 14:24:37.008: INFO: Waiting for StatefulSet statefulset-2424/ss2 to complete update
Dec 23 14:24:37.008: INFO: Waiting for Pod statefulset-2424/ss2-0 to have revision ss2-866bd64d95 update revision ss2-54677df74d
Dec 23 14:24:47.009: INFO: Waiting for StatefulSet statefulset-2424/ss2 to complete update
Dec 23 14:24:47.009: INFO: Waiting for Pod statefulset-2424/ss2-0 to have revision ss2-866bd64d95 update revision ss2-54677df74d
Dec 23 14:24:57.016: INFO: Waiting for StatefulSet statefulset-2424/ss2 to complete update
Dec 23 14:24:57.016: INFO: Waiting for Pod statefulset-2424/ss2-0 to have revision ss2-866bd64d95 update revision ss2-54677df74d
Dec 23 14:25:07.006: INFO: Waiting for StatefulSet statefulset-2424/ss2 to complete update
Dec 23 14:25:07.006: INFO: Waiting for Pod statefulset-2424/ss2-0 to have revision ss2-866bd64d95 update revision ss2-54677df74d
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 23 14:25:17.016: INFO: Deleting all statefulset in ns statefulset-2424
Dec 23 14:25:17.019: INFO: Scaling statefulset ss2 to 0
Dec 23 14:26:17.046: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 14:26:17.049: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:26:17.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2424" for this suite.

• [SLOW TEST:251.392 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":147,"skipped":2775,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:26:17.090: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:26:17.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-1880 create -f -'
Dec 23 14:26:17.699: INFO: stderr: ""
Dec 23 14:26:17.699: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Dec 23 14:26:17.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-1880 create -f -'
Dec 23 14:26:18.106: INFO: stderr: ""
Dec 23 14:26:18.106: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 23 14:26:19.111: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 23 14:26:19.111: INFO: Found 0 / 1
Dec 23 14:26:20.114: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 23 14:26:20.114: INFO: Found 1 / 1
Dec 23 14:26:20.114: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 23 14:26:20.117: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 23 14:26:20.117: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 23 14:26:20.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-1880 describe pod agnhost-primary-4n9p6'
Dec 23 14:26:20.256: INFO: stderr: ""
Dec 23 14:26:20.256: INFO: stdout: "Name:         agnhost-primary-4n9p6\nNamespace:    kubectl-1880\nPriority:     0\nNode:         wt-k8s-3.novalocal/10.22.19.25\nStart Time:   Wed, 23 Dec 2020 14:26:03 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 10.244.201.195/32\n              cni.projectcalico.org/podIPs: 10.244.201.195/32\nStatus:       Running\nIP:           10.244.201.195\nIPs:\n  IP:           10.244.201.195\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://e7605b8d2fde66ccae7900d4ca30f45e4e0472b28f539eebae2d26c443a27935\n    Image:          abcsys.cn:5000/e2e-test-images/agnhost:2.21\n    Image ID:       docker-pullable://abcsys.cn:5000/e2e-test-images/agnhost@sha256:e23f72f469927ead04b7bb2a5cedf8569906c38ced37a64767542e7d22831d45\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 23 Dec 2020 14:26:05 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zjgkc (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-zjgkc:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-zjgkc\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  17s   default-scheduler  Successfully assigned kubectl-1880/agnhost-primary-4n9p6 to wt-k8s-3.novalocal\n  Normal  Pulled     15s   kubelet            Container image \"abcsys.cn:5000/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    15s   kubelet            Created container agnhost-primary\n  Normal  Started    15s   kubelet            Started container agnhost-primary\n"
Dec 23 14:26:20.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-1880 describe rc agnhost-primary'
Dec 23 14:26:20.387: INFO: stderr: ""
Dec 23 14:26:20.387: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1880\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        abcsys.cn:5000/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  17s   replication-controller  Created pod: agnhost-primary-4n9p6\n"
Dec 23 14:26:20.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-1880 describe service agnhost-primary'
Dec 23 14:26:20.495: INFO: stderr: ""
Dec 23 14:26:20.495: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1880\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.100.210.141\nIPs:               10.100.210.141\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.201.195:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 23 14:26:20.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-1880 describe node wt-k8s-1.novalocal'
Dec 23 14:26:20.650: INFO: stderr: ""
Dec 23 14:26:20.650: INFO: stdout: "Name:               wt-k8s-1.novalocal\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=wt-k8s-1.novalocal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.22.19.9/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.214.194\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 23 Dec 2020 08:13:05 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  wt-k8s-1.novalocal\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 23 Dec 2020 14:26:05 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 23 Dec 2020 08:21:02 +0000   Wed, 23 Dec 2020 08:21:02 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 23 Dec 2020 14:26:04 +0000   Wed, 23 Dec 2020 08:13:04 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 23 Dec 2020 14:26:04 +0000   Wed, 23 Dec 2020 08:13:04 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 23 Dec 2020 14:26:04 +0000   Wed, 23 Dec 2020 08:13:04 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 23 Dec 2020 14:26:04 +0000   Wed, 23 Dec 2020 08:13:21 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.22.19.9\n  Hostname:    wt-k8s-1.novalocal\nCapacity:\n  cpu:                2\n  ephemeral-storage:  40883180Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3880176Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  37677938626\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3777776Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 878cf4a0cab1472088a0006fe5ccfd0e\n  System UUID:                878CF4A0-CAB1-4720-88A0-006FE5CCFD0E\n  Boot ID:                    6fd83f4f-27fe-49ba-86bf-c0393c4cb932\n  Kernel Version:             3.10.0-1160.11.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://18.6.1\n  Kubelet Version:            v1.20.1\n  Kube-Proxy Version:         v1.20.1\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-q8c4z                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         6h7m\n  kube-system                 coredns-7f89b7bc75-jx8v5                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     6h12m\n  kube-system                 coredns-7f89b7bc75-q2x48                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     6h12m\n  kube-system                 etcd-wt-k8s-1.novalocal                                    100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         6h13m\n  kube-system                 kube-apiserver-wt-k8s-1.novalocal                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         6h13m\n  kube-system                 kube-controller-manager-wt-k8s-1.novalocal                 200m (10%)    0 (0%)      0 (0%)           0 (0%)         6h13m\n  kube-system                 kube-proxy-57sg2                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h12m\n  kube-system                 kube-scheduler-wt-k8s-1.novalocal                          100m (5%)     0 (0%)      0 (0%)           0 (0%)         6h13m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-a3aba41d66fb4680-2jr72    0 (0%)        0 (0%)      0 (0%)           0 (0%)         61m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1100m (55%)  0 (0%)\n  memory             240Mi (6%)   340Mi (9%)\n  ephemeral-storage  100Mi (0%)   0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
Dec 23 14:26:20.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-1880 describe namespace kubectl-1880'
Dec 23 14:26:20.763: INFO: stderr: ""
Dec 23 14:26:20.763: INFO: stdout: "Name:         kubectl-1880\nLabels:       e2e-framework=kubectl\n              e2e-run=aad65d35-1944-4c91-b392-f02573ae4d26\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:26:20.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1880" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":148,"skipped":2807,"failed":0}

------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:26:20.772: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 23 14:26:20.837: INFO: starting watch
STEP: patching
STEP: updating
Dec 23 14:26:20.845: INFO: waiting for watch events with expected annotations
Dec 23 14:26:20.845: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:26:20.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-3495" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":149,"skipped":2807,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:26:20.879: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-6af7c16b-3cb4-4b82-8313-edd021a81858
STEP: Creating a pod to test consume configMaps
Dec 23 14:26:20.923: INFO: Waiting up to 5m0s for pod "pod-configmaps-bdcd2e67-423d-44c2-8807-e43773fb7959" in namespace "configmap-9085" to be "Succeeded or Failed"
Dec 23 14:26:20.926: INFO: Pod "pod-configmaps-bdcd2e67-423d-44c2-8807-e43773fb7959": Phase="Pending", Reason="", readiness=false. Elapsed: 3.029288ms
Dec 23 14:26:22.933: INFO: Pod "pod-configmaps-bdcd2e67-423d-44c2-8807-e43773fb7959": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009418704s
STEP: Saw pod success
Dec 23 14:26:22.933: INFO: Pod "pod-configmaps-bdcd2e67-423d-44c2-8807-e43773fb7959" satisfied condition "Succeeded or Failed"
Dec 23 14:26:22.935: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-configmaps-bdcd2e67-423d-44c2-8807-e43773fb7959 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 23 14:26:22.967: INFO: Waiting for pod pod-configmaps-bdcd2e67-423d-44c2-8807-e43773fb7959 to disappear
Dec 23 14:26:22.973: INFO: Pod pod-configmaps-bdcd2e67-423d-44c2-8807-e43773fb7959 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:26:22.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9085" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":150,"skipped":2815,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:26:22.989: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:26:23.025: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:26:25.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1747" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":151,"skipped":2819,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:26:25.148: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 23 14:26:26.128: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 23 14:26:28.155: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744330372, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744330372, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744330372, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744330372, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-76bbb7fdd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 14:26:31.180: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
Dec 23 14:26:31.209: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:31.327: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:31.428: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:31.524: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:31.622: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:31.723: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:31.822: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:31.923: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:32.024: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:32.126: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:32.223: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:32.323: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:32.423: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:32.525: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:32.625: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:32.726: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:32.824: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:32.924: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:33.022: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:33.128: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:33.224: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:33.329: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:33.423: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:33.525: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:33.621: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:33.724: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:33.824: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:33.922: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:34.022: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:34.124: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:34.227: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:34.347: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:34.426: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:34.522: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:34.622: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:34.722: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:34.823: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:34.923: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:35.025: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:35.123: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:35.223: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:35.323: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:35.431: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:35.524: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:35.644: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:35.759: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:35.825: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:35.928: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:36.025: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:36.125: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:36.223: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:36.337: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:36.424: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:36.523: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:36.623: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:36.721: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:36.824: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:36.922: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:37.023: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:37.124: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:37.226: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:37.322: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:37.423: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:37.523: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:37.626: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:37.728: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:37.823: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:37.925: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:38.030: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:38.129: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:38.226: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:38.325: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:38.423: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:38.534: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:38.627: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:38.731: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:38.824: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:26:38.926: INFO: Waiting for webhook configuration to be ready...
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:26:39.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4666" for this suite.
STEP: Destroying namespace "webhook-4666-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:14.011 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":152,"skipped":2833,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:26:39.164: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-a65d3f9f-9413-4118-a211-2585f471ffa5
STEP: Creating a pod to test consume configMaps
Dec 23 14:26:39.205: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ccda9a69-4679-475d-9f55-92f4587f7afd" in namespace "projected-1204" to be "Succeeded or Failed"
Dec 23 14:26:39.209: INFO: Pod "pod-projected-configmaps-ccda9a69-4679-475d-9f55-92f4587f7afd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059699ms
Dec 23 14:26:41.216: INFO: Pod "pod-projected-configmaps-ccda9a69-4679-475d-9f55-92f4587f7afd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011014039s
STEP: Saw pod success
Dec 23 14:26:41.216: INFO: Pod "pod-projected-configmaps-ccda9a69-4679-475d-9f55-92f4587f7afd" satisfied condition "Succeeded or Failed"
Dec 23 14:26:41.218: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-projected-configmaps-ccda9a69-4679-475d-9f55-92f4587f7afd container agnhost-container: <nil>
STEP: delete the pod
Dec 23 14:26:41.240: INFO: Waiting for pod pod-projected-configmaps-ccda9a69-4679-475d-9f55-92f4587f7afd to disappear
Dec 23 14:26:41.245: INFO: Pod pod-projected-configmaps-ccda9a69-4679-475d-9f55-92f4587f7afd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:26:41.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1204" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":153,"skipped":2937,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:26:41.263: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:26:41.311: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Creating first CR 
Dec 23 14:26:41.883: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-23T14:26:27Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-23T14:26:27Z]] name:name1 resourceVersion:94239 uid:829a86a6-a233-44af-9f33-124efe9432cf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Dec 23 14:26:51.895: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-23T14:26:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-23T14:26:37Z]] name:name2 resourceVersion:94292 uid:505c7935-9d72-4c9b-9e38-1a23af3a9656] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Dec 23 14:27:01.904: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-23T14:26:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-23T14:26:47Z]] name:name1 resourceVersion:94320 uid:829a86a6-a233-44af-9f33-124efe9432cf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Dec 23 14:27:11.915: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-23T14:26:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-23T14:26:57Z]] name:name2 resourceVersion:94344 uid:505c7935-9d72-4c9b-9e38-1a23af3a9656] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Dec 23 14:27:21.927: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-23T14:26:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-23T14:26:47Z]] name:name1 resourceVersion:94360 uid:829a86a6-a233-44af-9f33-124efe9432cf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Dec 23 14:27:31.974: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-23T14:26:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-23T14:26:57Z]] name:name2 resourceVersion:94377 uid:505c7935-9d72-4c9b-9e38-1a23af3a9656] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:27:42.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6378" for this suite.

• [SLOW TEST:61.238 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":154,"skipped":2963,"failed":0}
SSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:27:42.501: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Dec 23 14:27:42.553: INFO: created test-podtemplate-1
Dec 23 14:27:42.557: INFO: created test-podtemplate-2
Dec 23 14:27:42.560: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Dec 23 14:27:42.562: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Dec 23 14:27:42.592: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:27:42.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-170" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":155,"skipped":2966,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:27:42.607: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:27:53.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1521" for this suite.

• [SLOW TEST:11.197 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":156,"skipped":2966,"failed":0}
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:27:53.804: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 23 14:27:56.396: INFO: Successfully updated pod "pod-update-activedeadlineseconds-862b80be-71fa-44ab-b584-f79f4eb71bd4"
Dec 23 14:27:56.396: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-862b80be-71fa-44ab-b584-f79f4eb71bd4" in namespace "pods-1021" to be "terminated due to deadline exceeded"
Dec 23 14:27:56.402: INFO: Pod "pod-update-activedeadlineseconds-862b80be-71fa-44ab-b584-f79f4eb71bd4": Phase="Running", Reason="", readiness=true. Elapsed: 6.312078ms
Dec 23 14:27:58.406: INFO: Pod "pod-update-activedeadlineseconds-862b80be-71fa-44ab-b584-f79f4eb71bd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010282895s
Dec 23 14:28:00.411: INFO: Pod "pod-update-activedeadlineseconds-862b80be-71fa-44ab-b584-f79f4eb71bd4": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.015265367s
Dec 23 14:28:00.411: INFO: Pod "pod-update-activedeadlineseconds-862b80be-71fa-44ab-b584-f79f4eb71bd4" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:28:00.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1021" for this suite.

• [SLOW TEST:6.617 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":157,"skipped":2966,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:28:00.422: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-65fc3906-314c-4b59-b048-b7d56974c4c9
STEP: Creating a pod to test consume configMaps
Dec 23 14:28:00.463: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c70cbb2c-9ab2-4870-b665-43788f4beada" in namespace "projected-208" to be "Succeeded or Failed"
Dec 23 14:28:00.472: INFO: Pod "pod-projected-configmaps-c70cbb2c-9ab2-4870-b665-43788f4beada": Phase="Pending", Reason="", readiness=false. Elapsed: 9.002763ms
Dec 23 14:28:02.481: INFO: Pod "pod-projected-configmaps-c70cbb2c-9ab2-4870-b665-43788f4beada": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018183319s
STEP: Saw pod success
Dec 23 14:28:02.481: INFO: Pod "pod-projected-configmaps-c70cbb2c-9ab2-4870-b665-43788f4beada" satisfied condition "Succeeded or Failed"
Dec 23 14:28:02.483: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-projected-configmaps-c70cbb2c-9ab2-4870-b665-43788f4beada container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 23 14:28:02.504: INFO: Waiting for pod pod-projected-configmaps-c70cbb2c-9ab2-4870-b665-43788f4beada to disappear
Dec 23 14:28:02.508: INFO: Pod pod-projected-configmaps-c70cbb2c-9ab2-4870-b665-43788f4beada no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:28:02.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-208" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":158,"skipped":2990,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:28:02.517: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Dec 23 14:28:07.080: INFO: Successfully updated pod "adopt-release-c4lmj"
STEP: Checking that the Job readopts the Pod
Dec 23 14:28:07.080: INFO: Waiting up to 15m0s for pod "adopt-release-c4lmj" in namespace "job-2755" to be "adopted"
Dec 23 14:28:07.086: INFO: Pod "adopt-release-c4lmj": Phase="Running", Reason="", readiness=true. Elapsed: 6.050083ms
Dec 23 14:28:09.094: INFO: Pod "adopt-release-c4lmj": Phase="Running", Reason="", readiness=true. Elapsed: 2.014517684s
Dec 23 14:28:09.095: INFO: Pod "adopt-release-c4lmj" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Dec 23 14:28:09.605: INFO: Successfully updated pod "adopt-release-c4lmj"
STEP: Checking that the Job releases the Pod
Dec 23 14:28:09.605: INFO: Waiting up to 15m0s for pod "adopt-release-c4lmj" in namespace "job-2755" to be "released"
Dec 23 14:28:09.612: INFO: Pod "adopt-release-c4lmj": Phase="Running", Reason="", readiness=true. Elapsed: 7.32341ms
Dec 23 14:28:11.620: INFO: Pod "adopt-release-c4lmj": Phase="Running", Reason="", readiness=true. Elapsed: 2.015093081s
Dec 23 14:28:11.620: INFO: Pod "adopt-release-c4lmj" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:28:11.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2755" for this suite.

• [SLOW TEST:9.111 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":159,"skipped":3016,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:28:11.630: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Dec 23 14:28:11.669: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 14:28:15.635: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:28:31.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-73" for this suite.

• [SLOW TEST:19.602 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":160,"skipped":3055,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:28:31.232: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-5929
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 23 14:28:31.285: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 23 14:28:31.329: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 23 14:28:33.336: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 23 14:28:35.338: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:28:37.338: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:28:39.332: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:28:41.341: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:28:43.337: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:28:45.335: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:28:47.345: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:28:49.332: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:28:51.337: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 23 14:28:51.342: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec 23 14:28:53.347: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Dec 23 14:28:55.378: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Dec 23 14:28:55.378: INFO: Breadth first check of 10.244.1.15 on host 10.22.19.17...
Dec 23 14:28:55.380: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.201.205:9080/dial?request=hostname&protocol=http&host=10.244.1.15&port=8080&tries=1'] Namespace:pod-network-test-5929 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:28:55.380: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 14:28:55.494: INFO: Waiting for responses: map[]
Dec 23 14:28:55.494: INFO: reached 10.244.1.15 after 0/1 tries
Dec 23 14:28:55.494: INFO: Breadth first check of 10.244.201.240 on host 10.22.19.25...
Dec 23 14:28:55.497: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.201.205:9080/dial?request=hostname&protocol=http&host=10.244.201.240&port=8080&tries=1'] Namespace:pod-network-test-5929 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:28:55.497: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 14:28:55.594: INFO: Waiting for responses: map[]
Dec 23 14:28:55.594: INFO: reached 10.244.201.240 after 0/1 tries
Dec 23 14:28:55.594: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:28:55.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5929" for this suite.

• [SLOW TEST:24.372 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":161,"skipped":3081,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:28:55.604: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-7f9da6d5-0213-45d0-b55a-19641b9bf41b
STEP: Creating a pod to test consume secrets
Dec 23 14:28:55.651: INFO: Waiting up to 5m0s for pod "pod-secrets-6c1367e8-e8b7-48c7-a5d8-7e113424abe0" in namespace "secrets-1918" to be "Succeeded or Failed"
Dec 23 14:28:55.657: INFO: Pod "pod-secrets-6c1367e8-e8b7-48c7-a5d8-7e113424abe0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.690361ms
Dec 23 14:28:57.666: INFO: Pod "pod-secrets-6c1367e8-e8b7-48c7-a5d8-7e113424abe0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015556775s
STEP: Saw pod success
Dec 23 14:28:57.666: INFO: Pod "pod-secrets-6c1367e8-e8b7-48c7-a5d8-7e113424abe0" satisfied condition "Succeeded or Failed"
Dec 23 14:28:57.669: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-secrets-6c1367e8-e8b7-48c7-a5d8-7e113424abe0 container secret-volume-test: <nil>
STEP: delete the pod
Dec 23 14:28:57.697: INFO: Waiting for pod pod-secrets-6c1367e8-e8b7-48c7-a5d8-7e113424abe0 to disappear
Dec 23 14:28:57.702: INFO: Pod pod-secrets-6c1367e8-e8b7-48c7-a5d8-7e113424abe0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:28:57.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1918" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":162,"skipped":3082,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:28:57.710: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-f1e16d39-c11d-44e8-ab19-d4a4cf404a01
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-f1e16d39-c11d-44e8-ab19-d4a4cf404a01
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:29:01.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7066" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":163,"skipped":3088,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:29:01.847: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 14:29:01.895: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e8e13c2-d418-4773-8ed3-f8f36d052f31" in namespace "projected-9799" to be "Succeeded or Failed"
Dec 23 14:29:01.900: INFO: Pod "downwardapi-volume-2e8e13c2-d418-4773-8ed3-f8f36d052f31": Phase="Pending", Reason="", readiness=false. Elapsed: 5.616532ms
Dec 23 14:29:03.909: INFO: Pod "downwardapi-volume-2e8e13c2-d418-4773-8ed3-f8f36d052f31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014345968s
STEP: Saw pod success
Dec 23 14:29:03.909: INFO: Pod "downwardapi-volume-2e8e13c2-d418-4773-8ed3-f8f36d052f31" satisfied condition "Succeeded or Failed"
Dec 23 14:29:03.911: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-2e8e13c2-d418-4773-8ed3-f8f36d052f31 container client-container: <nil>
STEP: delete the pod
Dec 23 14:29:03.941: INFO: Waiting for pod downwardapi-volume-2e8e13c2-d418-4773-8ed3-f8f36d052f31 to disappear
Dec 23 14:29:03.944: INFO: Pod downwardapi-volume-2e8e13c2-d418-4773-8ed3-f8f36d052f31 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:29:03.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9799" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":164,"skipped":3163,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:29:03.953: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Dec 23 14:29:04.002: INFO: Waiting up to 5m0s for pod "downward-api-1d92257f-30c4-44fd-94e1-0894da47be3c" in namespace "downward-api-254" to be "Succeeded or Failed"
Dec 23 14:29:04.006: INFO: Pod "downward-api-1d92257f-30c4-44fd-94e1-0894da47be3c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.656427ms
Dec 23 14:29:06.024: INFO: Pod "downward-api-1d92257f-30c4-44fd-94e1-0894da47be3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022161807s
STEP: Saw pod success
Dec 23 14:29:06.024: INFO: Pod "downward-api-1d92257f-30c4-44fd-94e1-0894da47be3c" satisfied condition "Succeeded or Failed"
Dec 23 14:29:06.028: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downward-api-1d92257f-30c4-44fd-94e1-0894da47be3c container dapi-container: <nil>
STEP: delete the pod
Dec 23 14:29:06.067: INFO: Waiting for pod downward-api-1d92257f-30c4-44fd-94e1-0894da47be3c to disappear
Dec 23 14:29:06.073: INFO: Pod downward-api-1d92257f-30c4-44fd-94e1-0894da47be3c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:29:06.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-254" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":165,"skipped":3178,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:29:06.084: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:29:06.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3260" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":166,"skipped":3180,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:29:06.190: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1420.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1420.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1420.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1420.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 23 14:29:10.266: INFO: DNS probes using dns-test-21dfd43d-a954-43b6-b41b-bf4729def018 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1420.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1420.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1420.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1420.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 23 14:29:12.355: INFO: File wheezy_udp@dns-test-service-3.dns-1420.svc.cluster.local from pod  dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 23 14:29:12.362: INFO: File jessie_udp@dns-test-service-3.dns-1420.svc.cluster.local from pod  dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 23 14:29:12.362: INFO: Lookups using dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 failed for: [wheezy_udp@dns-test-service-3.dns-1420.svc.cluster.local jessie_udp@dns-test-service-3.dns-1420.svc.cluster.local]

Dec 23 14:29:17.366: INFO: File wheezy_udp@dns-test-service-3.dns-1420.svc.cluster.local from pod  dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 23 14:29:17.369: INFO: File jessie_udp@dns-test-service-3.dns-1420.svc.cluster.local from pod  dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 23 14:29:17.369: INFO: Lookups using dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 failed for: [wheezy_udp@dns-test-service-3.dns-1420.svc.cluster.local jessie_udp@dns-test-service-3.dns-1420.svc.cluster.local]

Dec 23 14:29:22.366: INFO: File wheezy_udp@dns-test-service-3.dns-1420.svc.cluster.local from pod  dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 23 14:29:22.368: INFO: File jessie_udp@dns-test-service-3.dns-1420.svc.cluster.local from pod  dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 23 14:29:22.368: INFO: Lookups using dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 failed for: [wheezy_udp@dns-test-service-3.dns-1420.svc.cluster.local jessie_udp@dns-test-service-3.dns-1420.svc.cluster.local]

Dec 23 14:29:27.366: INFO: File wheezy_udp@dns-test-service-3.dns-1420.svc.cluster.local from pod  dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 23 14:29:27.369: INFO: File jessie_udp@dns-test-service-3.dns-1420.svc.cluster.local from pod  dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 23 14:29:27.369: INFO: Lookups using dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 failed for: [wheezy_udp@dns-test-service-3.dns-1420.svc.cluster.local jessie_udp@dns-test-service-3.dns-1420.svc.cluster.local]

Dec 23 14:29:32.366: INFO: File wheezy_udp@dns-test-service-3.dns-1420.svc.cluster.local from pod  dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 23 14:29:32.368: INFO: File jessie_udp@dns-test-service-3.dns-1420.svc.cluster.local from pod  dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 23 14:29:32.368: INFO: Lookups using dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 failed for: [wheezy_udp@dns-test-service-3.dns-1420.svc.cluster.local jessie_udp@dns-test-service-3.dns-1420.svc.cluster.local]

Dec 23 14:29:37.366: INFO: File wheezy_udp@dns-test-service-3.dns-1420.svc.cluster.local from pod  dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 23 14:29:37.369: INFO: File jessie_udp@dns-test-service-3.dns-1420.svc.cluster.local from pod  dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 23 14:29:37.369: INFO: Lookups using dns-1420/dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 failed for: [wheezy_udp@dns-test-service-3.dns-1420.svc.cluster.local jessie_udp@dns-test-service-3.dns-1420.svc.cluster.local]

Dec 23 14:29:42.368: INFO: DNS probes using dns-test-95ca568f-c9bf-439d-90de-cdd46f113393 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1420.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1420.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1420.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1420.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 23 14:29:46.493: INFO: DNS probes using dns-test-2a332062-87e1-4adc-8a6d-86eb3c74a306 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:29:46.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1420" for this suite.

• [SLOW TEST:40.369 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":167,"skipped":3192,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:29:46.559: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 14:29:46.606: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f119eef-1d00-42da-ad32-9601a4649217" in namespace "downward-api-513" to be "Succeeded or Failed"
Dec 23 14:29:46.617: INFO: Pod "downwardapi-volume-4f119eef-1d00-42da-ad32-9601a4649217": Phase="Pending", Reason="", readiness=false. Elapsed: 10.912237ms
Dec 23 14:29:48.626: INFO: Pod "downwardapi-volume-4f119eef-1d00-42da-ad32-9601a4649217": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020200064s
Dec 23 14:29:50.632: INFO: Pod "downwardapi-volume-4f119eef-1d00-42da-ad32-9601a4649217": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026134118s
STEP: Saw pod success
Dec 23 14:29:50.632: INFO: Pod "downwardapi-volume-4f119eef-1d00-42da-ad32-9601a4649217" satisfied condition "Succeeded or Failed"
Dec 23 14:29:50.634: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-4f119eef-1d00-42da-ad32-9601a4649217 container client-container: <nil>
STEP: delete the pod
Dec 23 14:29:50.658: INFO: Waiting for pod downwardapi-volume-4f119eef-1d00-42da-ad32-9601a4649217 to disappear
Dec 23 14:29:50.661: INFO: Pod downwardapi-volume-4f119eef-1d00-42da-ad32-9601a4649217 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:29:50.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-513" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":168,"skipped":3197,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:29:50.671: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Dec 23 14:29:52.742: INFO: Pod pod-hostip-5d70e015-0500-4294-9988-ed6d794ed9b0 has hostIP: 10.22.19.25
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:29:52.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8554" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":169,"skipped":3205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:29:52.751: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Dec 23 14:29:55.345: INFO: Successfully updated pod "annotationupdate4cbeb738-c80b-47e0-8faa-7971df74fb14"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:29:57.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5271" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":170,"skipped":3236,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:29:57.387: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 23 14:29:57.433: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2158  366702c3-319b-4043-95fa-833b3ea84da6 95308 0 2020-12-23 14:29:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-23 14:29:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 23 14:29:57.434: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2158  366702c3-319b-4043-95fa-833b3ea84da6 95308 0 2020-12-23 14:29:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-23 14:29:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 23 14:30:07.450: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2158  366702c3-319b-4043-95fa-833b3ea84da6 95355 0 2020-12-23 14:29:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-23 14:29:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 23 14:30:07.450: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2158  366702c3-319b-4043-95fa-833b3ea84da6 95355 0 2020-12-23 14:29:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-23 14:29:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 23 14:30:17.473: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2158  366702c3-319b-4043-95fa-833b3ea84da6 95373 0 2020-12-23 14:29:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-23 14:29:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 23 14:30:17.473: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2158  366702c3-319b-4043-95fa-833b3ea84da6 95373 0 2020-12-23 14:29:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-23 14:29:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 23 14:30:27.483: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2158  366702c3-319b-4043-95fa-833b3ea84da6 95394 0 2020-12-23 14:29:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-23 14:29:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 23 14:30:27.483: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2158  366702c3-319b-4043-95fa-833b3ea84da6 95394 0 2020-12-23 14:29:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-23 14:29:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 23 14:30:37.502: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2158  e8a2c0dd-a5ff-4412-af98-a6d51b35e1f6 95409 0 2020-12-23 14:30:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-23 14:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 23 14:30:37.502: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2158  e8a2c0dd-a5ff-4412-af98-a6d51b35e1f6 95409 0 2020-12-23 14:30:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-23 14:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 23 14:30:47.519: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2158  e8a2c0dd-a5ff-4412-af98-a6d51b35e1f6 95428 0 2020-12-23 14:30:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-23 14:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 23 14:30:47.519: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2158  e8a2c0dd-a5ff-4412-af98-a6d51b35e1f6 95428 0 2020-12-23 14:30:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-23 14:30:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:30:57.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2158" for this suite.

• [SLOW TEST:60.153 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":171,"skipped":3250,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:30:57.540: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 23 14:31:01.617: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 14:31:01.624: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 23 14:31:03.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 14:31:03.631: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 23 14:31:05.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 14:31:05.632: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 23 14:31:07.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 14:31:07.633: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 23 14:31:09.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 14:31:09.631: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:31:09.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-303" for this suite.

• [SLOW TEST:12.109 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":172,"skipped":3260,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:31:09.650: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:31:09.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7658" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":173,"skipped":3287,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:31:09.775: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-325412c5-be8e-4394-b0de-9c128fb1ab77
STEP: Creating a pod to test consume configMaps
Dec 23 14:31:09.825: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ab1c501-f3fb-4614-b6e3-c88c905d8576" in namespace "configmap-7439" to be "Succeeded or Failed"
Dec 23 14:31:09.827: INFO: Pod "pod-configmaps-1ab1c501-f3fb-4614-b6e3-c88c905d8576": Phase="Pending", Reason="", readiness=false. Elapsed: 2.882132ms
Dec 23 14:31:11.835: INFO: Pod "pod-configmaps-1ab1c501-f3fb-4614-b6e3-c88c905d8576": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010382769s
STEP: Saw pod success
Dec 23 14:31:11.835: INFO: Pod "pod-configmaps-1ab1c501-f3fb-4614-b6e3-c88c905d8576" satisfied condition "Succeeded or Failed"
Dec 23 14:31:11.838: INFO: Trying to get logs from node wt-k8s-2 pod pod-configmaps-1ab1c501-f3fb-4614-b6e3-c88c905d8576 container agnhost-container: <nil>
STEP: delete the pod
Dec 23 14:31:11.893: INFO: Waiting for pod pod-configmaps-1ab1c501-f3fb-4614-b6e3-c88c905d8576 to disappear
Dec 23 14:31:11.896: INFO: Pod pod-configmaps-1ab1c501-f3fb-4614-b6e3-c88c905d8576 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:31:11.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7439" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":174,"skipped":3298,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:31:11.913: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:31:11.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1961" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":175,"skipped":3314,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:31:11.976: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-095b53cb-9b85-4b12-9b44-3b3f16fbabf1 in namespace container-probe-3146
Dec 23 14:31:14.068: INFO: Started pod busybox-095b53cb-9b85-4b12-9b44-3b3f16fbabf1 in namespace container-probe-3146
STEP: checking the pod's current state and verifying that restartCount is present
Dec 23 14:31:14.071: INFO: Initial restart count of pod busybox-095b53cb-9b85-4b12-9b44-3b3f16fbabf1 is 0
Dec 23 14:32:06.400: INFO: Restart count of pod container-probe-3146/busybox-095b53cb-9b85-4b12-9b44-3b3f16fbabf1 is now 1 (52.329223622s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:32:06.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3146" for this suite.

• [SLOW TEST:54.456 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":176,"skipped":3323,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:32:06.434: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 23 14:32:08.525: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:32:08.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6143" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":177,"skipped":3346,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:32:08.552: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-3c033143-84e6-4be1-b40e-10fee9ece2ee
STEP: Creating secret with name s-test-opt-upd-718af08f-7c53-468c-85dc-d781a5bff014
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-3c033143-84e6-4be1-b40e-10fee9ece2ee
STEP: Updating secret s-test-opt-upd-718af08f-7c53-468c-85dc-d781a5bff014
STEP: Creating secret with name s-test-opt-create-a646aad9-50e8-46d6-8d6b-1389b8da34cf
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:33:27.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8818" for this suite.

• [SLOW TEST:78.941 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":178,"skipped":3354,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:33:27.493: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Dec 23 14:33:27.538: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Dec 23 14:33:27.543: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 23 14:33:27.543: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Dec 23 14:33:27.551: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 23 14:33:27.551: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Dec 23 14:33:27.571: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Dec 23 14:33:27.571: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Dec 23 14:33:34.631: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:33:34.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-6839" for this suite.

• [SLOW TEST:7.174 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":179,"skipped":3377,"failed":0}
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:33:34.668: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 23 14:33:37.248: INFO: Successfully updated pod "pod-update-3018c60f-6acc-45bb-8cf0-0efc9989ba9a"
STEP: verifying the updated pod is in kubernetes
Dec 23 14:33:37.255: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:33:37.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-692" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":180,"skipped":3377,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:33:37.265: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 23 14:33:37.617: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 23 14:33:39.630: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744330803, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744330803, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744330803, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744330803, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-76bbb7fdd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 14:33:42.684: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:33:42.691: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4255-crds.webhook.example.com via the AdmissionRegistration API
Dec 23 14:33:43.224: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:43.403: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:43.518: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:43.540: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:43.643: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:43.746: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:43.847: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:43.952: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:44.045: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:44.138: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:44.254: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:44.395: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:44.443: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:44.543: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:44.643: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:44.741: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:44.852: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:44.998: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:45.071: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:45.138: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:45.238: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:45.339: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:45.438: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:45.539: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:45.640: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:45.744: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:45.840: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:45.940: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:46.042: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:46.139: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:46.239: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:46.340: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:46.437: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:46.540: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:46.638: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:46.739: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:46.839: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:46.939: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:47.039: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:47.143: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:47.241: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:47.337: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:47.438: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:47.543: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:47.639: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:47.739: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:47.848: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:47.940: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:48.054: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:48.138: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:48.239: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:48.350: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:48.439: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:48.539: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:48.638: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:48.742: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:48.841: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:48.938: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:49.039: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:49.140: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:49.240: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:49.341: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:49.516: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:49.538: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:49.638: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:49.737: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:49.840: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:49.939: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:50.039: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:50.143: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:50.245: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:50.340: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:50.439: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:50.539: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:50.638: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:50.740: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:33:50.838: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:33:51.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1633" for this suite.
STEP: Destroying namespace "webhook-1633-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:14.369 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":181,"skipped":3392,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:33:51.634: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 14:33:51.684: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db7eb9bf-915b-4229-acc8-f4fce53cdb3a" in namespace "downward-api-7282" to be "Succeeded or Failed"
Dec 23 14:33:51.689: INFO: Pod "downwardapi-volume-db7eb9bf-915b-4229-acc8-f4fce53cdb3a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.160461ms
Dec 23 14:33:53.700: INFO: Pod "downwardapi-volume-db7eb9bf-915b-4229-acc8-f4fce53cdb3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016047222s
STEP: Saw pod success
Dec 23 14:33:53.700: INFO: Pod "downwardapi-volume-db7eb9bf-915b-4229-acc8-f4fce53cdb3a" satisfied condition "Succeeded or Failed"
Dec 23 14:33:53.703: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-db7eb9bf-915b-4229-acc8-f4fce53cdb3a container client-container: <nil>
STEP: delete the pod
Dec 23 14:33:53.724: INFO: Waiting for pod downwardapi-volume-db7eb9bf-915b-4229-acc8-f4fce53cdb3a to disappear
Dec 23 14:33:53.726: INFO: Pod downwardapi-volume-db7eb9bf-915b-4229-acc8-f4fce53cdb3a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:33:53.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7282" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":182,"skipped":3396,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:33:53.735: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:33:53.777: INFO: Waiting up to 5m0s for pod "busybox-user-65534-99305baf-61a5-4bf2-9e81-e84c497bb472" in namespace "security-context-test-5827" to be "Succeeded or Failed"
Dec 23 14:33:53.781: INFO: Pod "busybox-user-65534-99305baf-61a5-4bf2-9e81-e84c497bb472": Phase="Pending", Reason="", readiness=false. Elapsed: 4.316683ms
Dec 23 14:33:55.789: INFO: Pod "busybox-user-65534-99305baf-61a5-4bf2-9e81-e84c497bb472": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012533818s
Dec 23 14:33:55.789: INFO: Pod "busybox-user-65534-99305baf-61a5-4bf2-9e81-e84c497bb472" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:33:55.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5827" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":183,"skipped":3415,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:33:55.798: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Dec 23 14:33:55.836: INFO: namespace kubectl-9602
Dec 23 14:33:55.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-9602 create -f -'
Dec 23 14:33:56.687: INFO: stderr: ""
Dec 23 14:33:56.687: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 23 14:33:57.704: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 23 14:33:57.704: INFO: Found 0 / 1
Dec 23 14:33:58.694: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 23 14:33:58.694: INFO: Found 1 / 1
Dec 23 14:33:58.694: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 23 14:33:58.699: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 23 14:33:58.699: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 23 14:33:58.699: INFO: wait on agnhost-primary startup in kubectl-9602 
Dec 23 14:33:58.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-9602 logs agnhost-primary-mt9sh agnhost-primary'
Dec 23 14:33:58.819: INFO: stderr: ""
Dec 23 14:33:58.819: INFO: stdout: "Paused\n"
STEP: exposing RC
Dec 23 14:33:58.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-9602 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Dec 23 14:33:58.990: INFO: stderr: ""
Dec 23 14:33:58.990: INFO: stdout: "service/rm2 exposed\n"
Dec 23 14:33:59.001: INFO: Service rm2 in namespace kubectl-9602 found.
STEP: exposing service
Dec 23 14:34:01.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-9602 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Dec 23 14:34:01.209: INFO: stderr: ""
Dec 23 14:34:01.209: INFO: stdout: "service/rm3 exposed\n"
Dec 23 14:34:01.216: INFO: Service rm3 in namespace kubectl-9602 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:34:03.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9602" for this suite.

• [SLOW TEST:7.435 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":184,"skipped":3426,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:34:03.233: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-7955/configmap-test-38df9bf0-0a78-4e1d-a517-6f65c7de61d7
STEP: Creating a pod to test consume configMaps
Dec 23 14:34:03.278: INFO: Waiting up to 5m0s for pod "pod-configmaps-62de2d0f-c935-4cc2-a424-ef3f633d3e44" in namespace "configmap-7955" to be "Succeeded or Failed"
Dec 23 14:34:03.281: INFO: Pod "pod-configmaps-62de2d0f-c935-4cc2-a424-ef3f633d3e44": Phase="Pending", Reason="", readiness=false. Elapsed: 3.248433ms
Dec 23 14:34:05.290: INFO: Pod "pod-configmaps-62de2d0f-c935-4cc2-a424-ef3f633d3e44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012386764s
Dec 23 14:34:07.298: INFO: Pod "pod-configmaps-62de2d0f-c935-4cc2-a424-ef3f633d3e44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019940787s
STEP: Saw pod success
Dec 23 14:34:07.298: INFO: Pod "pod-configmaps-62de2d0f-c935-4cc2-a424-ef3f633d3e44" satisfied condition "Succeeded or Failed"
Dec 23 14:34:07.300: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-configmaps-62de2d0f-c935-4cc2-a424-ef3f633d3e44 container env-test: <nil>
STEP: delete the pod
Dec 23 14:34:07.324: INFO: Waiting for pod pod-configmaps-62de2d0f-c935-4cc2-a424-ef3f633d3e44 to disappear
Dec 23 14:34:07.329: INFO: Pod pod-configmaps-62de2d0f-c935-4cc2-a424-ef3f633d3e44 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:34:07.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7955" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":185,"skipped":3428,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:34:07.344: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:34:11.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4551" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":186,"skipped":3461,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:34:11.459: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3902
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-3902
I1223 14:34:11.537464      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-3902, replica count: 2
Dec 23 14:34:14.588: INFO: Creating new exec pod
I1223 14:34:14.588641      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 23 14:34:17.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-3902 exec execpodjzt8g -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 23 14:34:17.870: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 23 14:34:17.870: INFO: stdout: ""
Dec 23 14:34:17.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-3902 exec execpodjzt8g -- /bin/sh -x -c nc -zv -t -w 2 10.108.165.187 80'
Dec 23 14:34:18.085: INFO: stderr: "+ nc -zv -t -w 2 10.108.165.187 80\nConnection to 10.108.165.187 80 port [tcp/http] succeeded!\n"
Dec 23 14:34:18.085: INFO: stdout: ""
Dec 23 14:34:18.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-3902 exec execpodjzt8g -- /bin/sh -x -c nc -zv -t -w 2 10.22.19.17 30249'
Dec 23 14:34:18.285: INFO: stderr: "+ nc -zv -t -w 2 10.22.19.17 30249\nConnection to 10.22.19.17 30249 port [tcp/30249] succeeded!\n"
Dec 23 14:34:18.285: INFO: stdout: ""
Dec 23 14:34:18.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-3902 exec execpodjzt8g -- /bin/sh -x -c nc -zv -t -w 2 10.22.19.25 30249'
Dec 23 14:34:18.485: INFO: stderr: "+ nc -zv -t -w 2 10.22.19.25 30249\nConnection to 10.22.19.25 30249 port [tcp/30249] succeeded!\n"
Dec 23 14:34:18.485: INFO: stdout: ""
Dec 23 14:34:18.485: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:34:18.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3902" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:7.109 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":187,"skipped":3469,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:34:18.568: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1223 14:34:19.661527      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 23 14:35:21.685: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:35:21.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6528" for this suite.

• [SLOW TEST:63.134 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":188,"skipped":3472,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:35:21.703: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 23 14:35:23.342: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec 23 14:35:25.362: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744330909, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744330909, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744330909, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744330909, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-55cf5fff84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 14:35:28.383: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:35:28.390: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 14:35:28.980: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:15Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:29.090: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:15Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:29.190: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:15Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:29.289: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:15Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:29.390: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:15Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:29.490: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:15Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:29.603: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:15Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:29.727: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:15Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:29.789: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:15Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:29.890: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:15Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:29.990: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:16Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:30.090: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:16Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:30.189: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:16Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:30.288: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:16Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:30.389: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:16Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:30.488: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:16Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:30.588: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:16Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:30.690: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:16Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:30.790: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:16Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:30.917: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:16Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:31.117: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:17Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:31.317: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:17Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:31.517: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:17Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:31.716: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:17Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:31.916: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:17Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:32.117: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:18Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:32.317: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:18Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:32.517: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:18Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:32.716: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:18Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:32.918: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:18Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:33.116: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:19Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:33.317: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:19Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:33.517: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:19Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:33.718: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:19Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:33.917: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:19Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:34.116: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:20Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:34.322: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:20Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:34.518: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:20Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:34.716: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:20Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:34.916: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:20Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:35.121: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:21Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:35.316: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:21Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:35.518: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:21Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:35.717: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:21Z is before 2020-12-23T14:35:22Z
Dec 23 14:35:35.917: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-1101-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-9981.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T14:35:21Z is before 2020-12-23T14:35:22Z
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:35:37.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9981" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:15.670 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":189,"skipped":3486,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:35:37.374: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-121
STEP: creating replication controller nodeport-test in namespace services-121
I1223 14:35:37.458159      20 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-121, replica count: 2
Dec 23 14:35:40.508: INFO: Creating new exec pod
I1223 14:35:40.508637      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 23 14:35:43.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-121 exec execpodwvs9v -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Dec 23 14:35:43.767: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 23 14:35:43.767: INFO: stdout: ""
Dec 23 14:35:43.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-121 exec execpodwvs9v -- /bin/sh -x -c nc -zv -t -w 2 10.111.55.1 80'
Dec 23 14:35:43.967: INFO: stderr: "+ nc -zv -t -w 2 10.111.55.1 80\nConnection to 10.111.55.1 80 port [tcp/http] succeeded!\n"
Dec 23 14:35:43.967: INFO: stdout: ""
Dec 23 14:35:43.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-121 exec execpodwvs9v -- /bin/sh -x -c nc -zv -t -w 2 10.22.19.17 31130'
Dec 23 14:35:44.168: INFO: stderr: "+ nc -zv -t -w 2 10.22.19.17 31130\nConnection to 10.22.19.17 31130 port [tcp/31130] succeeded!\n"
Dec 23 14:35:44.168: INFO: stdout: ""
Dec 23 14:35:44.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-121 exec execpodwvs9v -- /bin/sh -x -c nc -zv -t -w 2 10.22.19.25 31130'
Dec 23 14:35:44.394: INFO: stderr: "+ nc -zv -t -w 2 10.22.19.25 31130\nConnection to 10.22.19.25 31130 port [tcp/31130] succeeded!\n"
Dec 23 14:35:44.395: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:35:44.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-121" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:7.035 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":190,"skipped":3516,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:35:44.410: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 23 14:35:44.463: INFO: Waiting up to 5m0s for pod "pod-4ef0afb1-5e16-49d7-92c6-946c036692b8" in namespace "emptydir-545" to be "Succeeded or Failed"
Dec 23 14:35:44.469: INFO: Pod "pod-4ef0afb1-5e16-49d7-92c6-946c036692b8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.479301ms
Dec 23 14:35:46.478: INFO: Pod "pod-4ef0afb1-5e16-49d7-92c6-946c036692b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015127273s
Dec 23 14:35:48.486: INFO: Pod "pod-4ef0afb1-5e16-49d7-92c6-946c036692b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02288888s
STEP: Saw pod success
Dec 23 14:35:48.486: INFO: Pod "pod-4ef0afb1-5e16-49d7-92c6-946c036692b8" satisfied condition "Succeeded or Failed"
Dec 23 14:35:48.488: INFO: Trying to get logs from node wt-k8s-2 pod pod-4ef0afb1-5e16-49d7-92c6-946c036692b8 container test-container: <nil>
STEP: delete the pod
Dec 23 14:35:48.525: INFO: Waiting for pod pod-4ef0afb1-5e16-49d7-92c6-946c036692b8 to disappear
Dec 23 14:35:48.529: INFO: Pod pod-4ef0afb1-5e16-49d7-92c6-946c036692b8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:35:48.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-545" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":191,"skipped":3524,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:35:48.538: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 23 14:35:52.614: INFO: &Pod{ObjectMeta:{send-events-b54733b2-09f8-4f99-9381-576a7d111514  events-781  69b394ed-217e-46ec-9402-7a0b84da9841 97035 0 2020-12-23 14:35:34 +0000 UTC <nil> <nil> map[name:foo time:584732417] map[cni.projectcalico.org/podIP:10.244.1.14/32 cni.projectcalico.org/podIPs:10.244.1.14/32] [] []  [{e2e.test Update v1 2020-12-23 14:35:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-23 14:35:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-23 14:35:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wz48m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wz48m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:abcsys.cn:5000/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wz48m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:35:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:35:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:35:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:35:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.17,PodIP:10.244.1.14,StartTime:2020-12-23 14:35:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-23 14:35:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:abcsys.cn:5000/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://abcsys.cn:5000/e2e-test-images/agnhost@sha256:e23f72f469927ead04b7bb2a5cedf8569906c38ced37a64767542e7d22831d45,ContainerID:docker://8ff7696d16a5be5b93c6314d3f3442590950b33854a908baca8e739a48819ea5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Dec 23 14:35:54.623: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 23 14:35:56.631: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:35:56.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-781" for this suite.

• [SLOW TEST:8.115 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":192,"skipped":3528,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:35:56.654: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:35:58.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7915" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":193,"skipped":3530,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:35:58.742: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-4ae05ddb-f4f2-47f9-bc02-b3580956a93b in namespace container-probe-3164
Dec 23 14:36:00.867: INFO: Started pod liveness-4ae05ddb-f4f2-47f9-bc02-b3580956a93b in namespace container-probe-3164
STEP: checking the pod's current state and verifying that restartCount is present
Dec 23 14:36:00.871: INFO: Initial restart count of pod liveness-4ae05ddb-f4f2-47f9-bc02-b3580956a93b is 0
Dec 23 14:36:16.930: INFO: Restart count of pod container-probe-3164/liveness-4ae05ddb-f4f2-47f9-bc02-b3580956a93b is now 1 (16.058476445s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:36:16.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3164" for this suite.

• [SLOW TEST:18.219 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":194,"skipped":3555,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:36:16.963: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec 23 14:36:17.024: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 23 14:37:17.056: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:37:17.058: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:37:17.117: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Dec 23 14:37:17.120: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:37:17.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5245" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:37:17.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1853" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.249 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":195,"skipped":3604,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:37:17.213: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 23 14:37:17.261: INFO: Waiting up to 5m0s for pod "pod-8e3d2216-ab44-4240-b66a-52e4d04d2c8b" in namespace "emptydir-335" to be "Succeeded or Failed"
Dec 23 14:37:17.264: INFO: Pod "pod-8e3d2216-ab44-4240-b66a-52e4d04d2c8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.831997ms
Dec 23 14:37:19.271: INFO: Pod "pod-8e3d2216-ab44-4240-b66a-52e4d04d2c8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010100489s
Dec 23 14:37:21.277: INFO: Pod "pod-8e3d2216-ab44-4240-b66a-52e4d04d2c8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015964262s
STEP: Saw pod success
Dec 23 14:37:21.277: INFO: Pod "pod-8e3d2216-ab44-4240-b66a-52e4d04d2c8b" satisfied condition "Succeeded or Failed"
Dec 23 14:37:21.279: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-8e3d2216-ab44-4240-b66a-52e4d04d2c8b container test-container: <nil>
STEP: delete the pod
Dec 23 14:37:21.301: INFO: Waiting for pod pod-8e3d2216-ab44-4240-b66a-52e4d04d2c8b to disappear
Dec 23 14:37:21.307: INFO: Pod pod-8e3d2216-ab44-4240-b66a-52e4d04d2c8b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:37:21.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-335" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":196,"skipped":3607,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:37:21.314: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-53452010-8c8a-4fbf-9206-ee8fabfc0ed1
STEP: Creating a pod to test consume configMaps
Dec 23 14:37:21.406: INFO: Waiting up to 5m0s for pod "pod-configmaps-c7dbc8f9-1431-4227-99fe-25700575c6e4" in namespace "configmap-4099" to be "Succeeded or Failed"
Dec 23 14:37:21.426: INFO: Pod "pod-configmaps-c7dbc8f9-1431-4227-99fe-25700575c6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.229548ms
Dec 23 14:37:23.438: INFO: Pod "pod-configmaps-c7dbc8f9-1431-4227-99fe-25700575c6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031620319s
Dec 23 14:37:25.446: INFO: Pod "pod-configmaps-c7dbc8f9-1431-4227-99fe-25700575c6e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03924045s
STEP: Saw pod success
Dec 23 14:37:25.446: INFO: Pod "pod-configmaps-c7dbc8f9-1431-4227-99fe-25700575c6e4" satisfied condition "Succeeded or Failed"
Dec 23 14:37:25.448: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-configmaps-c7dbc8f9-1431-4227-99fe-25700575c6e4 container agnhost-container: <nil>
STEP: delete the pod
Dec 23 14:37:25.480: INFO: Waiting for pod pod-configmaps-c7dbc8f9-1431-4227-99fe-25700575c6e4 to disappear
Dec 23 14:37:25.482: INFO: Pod pod-configmaps-c7dbc8f9-1431-4227-99fe-25700575c6e4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:37:25.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4099" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":197,"skipped":3624,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:37:25.489: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1687
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Dec 23 14:37:25.560: INFO: Found 0 stateful pods, waiting for 3
Dec 23 14:37:35.571: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 14:37:35.571: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 14:37:35.571: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from abcsys.cn:5000/library/httpd:2.4.38-alpine to abcsys.cn:5000/library/httpd:2.4.39-alpine
Dec 23 14:37:35.595: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 23 14:37:45.644: INFO: Updating stateful set ss2
Dec 23 14:37:45.671: INFO: Waiting for Pod statefulset-1687/ss2-2 to have revision ss2-54677df74d update revision ss2-866bd64d95
STEP: Restoring Pods to the correct revision when they are deleted
Dec 23 14:37:55.750: INFO: Found 1 stateful pods, waiting for 3
Dec 23 14:38:05.763: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 14:38:05.763: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 14:38:05.763: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 23 14:38:05.792: INFO: Updating stateful set ss2
Dec 23 14:38:05.833: INFO: Waiting for Pod statefulset-1687/ss2-1 to have revision ss2-54677df74d update revision ss2-866bd64d95
Dec 23 14:38:15.853: INFO: Waiting for Pod statefulset-1687/ss2-1 to have revision ss2-54677df74d update revision ss2-866bd64d95
Dec 23 14:38:25.870: INFO: Updating stateful set ss2
Dec 23 14:38:25.885: INFO: Waiting for StatefulSet statefulset-1687/ss2 to complete update
Dec 23 14:38:25.885: INFO: Waiting for Pod statefulset-1687/ss2-0 to have revision ss2-54677df74d update revision ss2-866bd64d95
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 23 14:38:35.899: INFO: Deleting all statefulset in ns statefulset-1687
Dec 23 14:38:35.901: INFO: Scaling statefulset ss2 to 0
Dec 23 14:39:35.936: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 14:39:35.938: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:39:35.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1687" for this suite.

• [SLOW TEST:130.479 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":198,"skipped":3627,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:39:35.969: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-3451/configmap-test-fffb2050-d86a-4635-a0f8-7b4e929e17bb
STEP: Creating a pod to test consume configMaps
Dec 23 14:39:36.014: INFO: Waiting up to 5m0s for pod "pod-configmaps-5f3e0a13-48fe-44f3-87ea-7396ecd4cd69" in namespace "configmap-3451" to be "Succeeded or Failed"
Dec 23 14:39:36.019: INFO: Pod "pod-configmaps-5f3e0a13-48fe-44f3-87ea-7396ecd4cd69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.493376ms
Dec 23 14:39:38.027: INFO: Pod "pod-configmaps-5f3e0a13-48fe-44f3-87ea-7396ecd4cd69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012455815s
Dec 23 14:39:40.036: INFO: Pod "pod-configmaps-5f3e0a13-48fe-44f3-87ea-7396ecd4cd69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021938113s
STEP: Saw pod success
Dec 23 14:39:40.036: INFO: Pod "pod-configmaps-5f3e0a13-48fe-44f3-87ea-7396ecd4cd69" satisfied condition "Succeeded or Failed"
Dec 23 14:39:40.038: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-configmaps-5f3e0a13-48fe-44f3-87ea-7396ecd4cd69 container env-test: <nil>
STEP: delete the pod
Dec 23 14:39:40.068: INFO: Waiting for pod pod-configmaps-5f3e0a13-48fe-44f3-87ea-7396ecd4cd69 to disappear
Dec 23 14:39:40.072: INFO: Pod pod-configmaps-5f3e0a13-48fe-44f3-87ea-7396ecd4cd69 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:39:40.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3451" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":199,"skipped":3637,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:39:40.080: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 23 14:39:40.683: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 14:39:43.742: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
Dec 23 14:39:43.827: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:43.959: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:44.055: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:44.171: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:44.259: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:44.378: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:44.459: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:44.563: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:44.656: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:44.757: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:44.861: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:44.957: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:45.097: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:45.165: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:45.268: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:45.366: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:45.458: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:45.557: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:45.671: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:45.758: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:45.875: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:45.975: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:46.058: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:46.156: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:46.255: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:46.364: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:46.460: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:46.562: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:46.665: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:46.762: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:46.856: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:46.960: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:47.057: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:47.168: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:47.278: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:47.377: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:47.463: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:47.557: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:47.658: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:47.758: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:47.862: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:47.961: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:48.059: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:48.161: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:48.267: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:48.361: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:48.458: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:48.579: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:48.666: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:48.770: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:48.866: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:48.962: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:49.063: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:49.171: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:49.268: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:49.370: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:49.473: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:49.559: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:49.660: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:49.762: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:49.865: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:49.964: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:50.062: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:50.159: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:50.262: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:50.374: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:50.723: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:50.759: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:50.858: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:50.958: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:51.064: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:51.156: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:51.267: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:51.372: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:51.460: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:51.556: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:51.661: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:51.768: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:51.862: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:51.963: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:52.067: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:52.174: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:52.281: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:52.362: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:52.460: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:52.556: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:52.675: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:52.775: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:52.867: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:52.958: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:53.069: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:53.166: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:53.265: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:53.364: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:53.457: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:53.564: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:53.660: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:53.759: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:39:53.867: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:39:54.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1014" for this suite.
STEP: Destroying namespace "webhook-1014-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:14.125 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":200,"skipped":3654,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:39:54.206: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 23 14:39:54.997: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 23 14:39:57.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744331181, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744331181, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744331181, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744331181, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-76bbb7fdd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 14:40:00.060: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:40:00.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8732" for this suite.
STEP: Destroying namespace "webhook-8732-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.960 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":201,"skipped":3707,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:40:00.167: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1124
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1124
STEP: creating replication controller externalsvc in namespace services-1124
I1223 14:40:00.333941      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1124, replica count: 2
I1223 14:40:03.384446      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Dec 23 14:40:03.424: INFO: Creating new exec pod
Dec 23 14:40:07.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-1124 exec execpodnr4dr -- /bin/sh -x -c nslookup nodeport-service.services-1124.svc.cluster.local'
Dec 23 14:40:07.892: INFO: stderr: "+ nslookup nodeport-service.services-1124.svc.cluster.local\n"
Dec 23 14:40:07.892: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-1124.svc.cluster.local\tcanonical name = externalsvc.services-1124.svc.cluster.local.\nName:\texternalsvc.services-1124.svc.cluster.local\nAddress: 10.96.11.254\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1124, will wait for the garbage collector to delete the pods
Dec 23 14:40:07.953: INFO: Deleting ReplicationController externalsvc took: 5.661894ms
Dec 23 14:40:08.053: INFO: Terminating ReplicationController externalsvc pods took: 100.28652ms
Dec 23 14:40:19.758: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:40:19.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1124" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:19.642 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":202,"skipped":3715,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:40:19.810: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Dec 23 14:40:19.890: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 23 14:41:19.911: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:41:19.921: INFO: Starting informer...
STEP: Starting pods...
Dec 23 14:41:20.138: INFO: Pod1 is running on wt-k8s-3.novalocal. Tainting Node
Dec 23 14:41:22.361: INFO: Pod2 is running on wt-k8s-3.novalocal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Dec 23 14:41:39.258: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec 23 14:41:59.267: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:41:59.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5460" for this suite.

• [SLOW TEST:99.507 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":203,"skipped":3763,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:41:59.318: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:42:05.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5475" for this suite.
STEP: Destroying namespace "nsdeletetest-6424" for this suite.
Dec 23 14:42:05.527: INFO: Namespace nsdeletetest-6424 was already deleted
STEP: Destroying namespace "nsdeletetest-7301" for this suite.

• [SLOW TEST:6.213 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":204,"skipped":3772,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:42:05.532: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 23 14:42:06.219: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 23 14:42:08.228: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744331312, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744331312, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744331312, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744331312, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-76bbb7fdd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 14:42:11.278: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:42:11.287: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-903-crds.webhook.example.com via the AdmissionRegistration API
Dec 23 14:42:11.856: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:11.970: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:12.070: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:12.172: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:12.270: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:12.369: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:12.492: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:12.617: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:12.678: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:12.777: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:12.869: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:12.971: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:13.076: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:13.178: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:13.270: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:13.373: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:13.471: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:13.573: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:13.675: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:13.770: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:13.870: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:13.970: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:14.071: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:14.171: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:14.273: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:14.377: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:14.473: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:14.572: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:14.669: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:14.770: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:14.869: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:14.970: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:15.070: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:15.175: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:15.275: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:15.376: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:15.471: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:15.571: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:15.675: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:15.773: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:15.869: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:15.969: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:16.070: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:16.172: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:16.273: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:16.369: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:16.475: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:16.571: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:16.670: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:16.772: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:16.870: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:16.970: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:17.071: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:17.171: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:17.269: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:17.371: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:17.474: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:17.578: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:17.671: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:17.772: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:17.871: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:17.970: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:18.074: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:18.171: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:18.271: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:18.370: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:18.471: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:18.581: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:18.672: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:18.777: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:42:18.872: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:42:19.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6748" for this suite.
STEP: Destroying namespace "webhook-6748-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:14.199 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":205,"skipped":3817,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:42:19.731: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-6a1ddd78-9783-42a5-a48e-63b7575778a6 in namespace container-probe-3049
Dec 23 14:42:23.819: INFO: Started pod busybox-6a1ddd78-9783-42a5-a48e-63b7575778a6 in namespace container-probe-3049
STEP: checking the pod's current state and verifying that restartCount is present
Dec 23 14:42:23.821: INFO: Initial restart count of pod busybox-6a1ddd78-9783-42a5-a48e-63b7575778a6 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:46:24.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3049" for this suite.

• [SLOW TEST:245.250 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":206,"skipped":3825,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:46:24.983: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Dec 23 14:46:25.034: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:46:47.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3141" for this suite.

• [SLOW TEST:22.807 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":207,"skipped":3835,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:46:47.791: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-2ce5361e-1707-4919-a388-77116dac7190
STEP: Creating a pod to test consume configMaps
Dec 23 14:46:47.838: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9addc5e1-3e65-42a1-adc4-6713ee401293" in namespace "projected-8811" to be "Succeeded or Failed"
Dec 23 14:46:47.851: INFO: Pod "pod-projected-configmaps-9addc5e1-3e65-42a1-adc4-6713ee401293": Phase="Pending", Reason="", readiness=false. Elapsed: 12.723599ms
Dec 23 14:46:49.858: INFO: Pod "pod-projected-configmaps-9addc5e1-3e65-42a1-adc4-6713ee401293": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019648421s
STEP: Saw pod success
Dec 23 14:46:49.858: INFO: Pod "pod-projected-configmaps-9addc5e1-3e65-42a1-adc4-6713ee401293" satisfied condition "Succeeded or Failed"
Dec 23 14:46:49.865: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-projected-configmaps-9addc5e1-3e65-42a1-adc4-6713ee401293 container agnhost-container: <nil>
STEP: delete the pod
Dec 23 14:46:49.946: INFO: Waiting for pod pod-projected-configmaps-9addc5e1-3e65-42a1-adc4-6713ee401293 to disappear
Dec 23 14:46:49.954: INFO: Pod pod-projected-configmaps-9addc5e1-3e65-42a1-adc4-6713ee401293 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:46:49.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8811" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":208,"skipped":3857,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:46:49.971: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-174
Dec 23 14:46:52.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-174 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec 23 14:46:52.763: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec 23 14:46:52.763: INFO: stdout: "iptables"
Dec 23 14:46:52.763: INFO: proxyMode: iptables
Dec 23 14:46:52.775: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 23 14:46:52.778: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-174
STEP: creating replication controller affinity-clusterip-timeout in namespace services-174
I1223 14:46:52.824495      20 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-174, replica count: 3
I1223 14:46:55.875100      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1223 14:46:58.875297      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 23 14:46:58.885: INFO: Creating new exec pod
Dec 23 14:47:01.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-174 exec execpod-affinityp8glv -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Dec 23 14:47:02.128: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Dec 23 14:47:02.128: INFO: stdout: ""
Dec 23 14:47:02.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-174 exec execpod-affinityp8glv -- /bin/sh -x -c nc -zv -t -w 2 10.104.8.52 80'
Dec 23 14:47:02.357: INFO: stderr: "+ nc -zv -t -w 2 10.104.8.52 80\nConnection to 10.104.8.52 80 port [tcp/http] succeeded!\n"
Dec 23 14:47:02.357: INFO: stdout: ""
Dec 23 14:47:02.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-174 exec execpod-affinityp8glv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.104.8.52:80/ ; done'
Dec 23 14:47:02.694: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n"
Dec 23 14:47:02.694: INFO: stdout: "\naffinity-clusterip-timeout-v5t54\naffinity-clusterip-timeout-v5t54\naffinity-clusterip-timeout-v5t54\naffinity-clusterip-timeout-v5t54\naffinity-clusterip-timeout-v5t54\naffinity-clusterip-timeout-v5t54\naffinity-clusterip-timeout-v5t54\naffinity-clusterip-timeout-v5t54\naffinity-clusterip-timeout-v5t54\naffinity-clusterip-timeout-v5t54\naffinity-clusterip-timeout-v5t54\naffinity-clusterip-timeout-v5t54\naffinity-clusterip-timeout-v5t54\naffinity-clusterip-timeout-v5t54\naffinity-clusterip-timeout-v5t54\naffinity-clusterip-timeout-v5t54"
Dec 23 14:47:02.694: INFO: Received response from host: affinity-clusterip-timeout-v5t54
Dec 23 14:47:02.694: INFO: Received response from host: affinity-clusterip-timeout-v5t54
Dec 23 14:47:02.694: INFO: Received response from host: affinity-clusterip-timeout-v5t54
Dec 23 14:47:02.694: INFO: Received response from host: affinity-clusterip-timeout-v5t54
Dec 23 14:47:02.694: INFO: Received response from host: affinity-clusterip-timeout-v5t54
Dec 23 14:47:02.694: INFO: Received response from host: affinity-clusterip-timeout-v5t54
Dec 23 14:47:02.694: INFO: Received response from host: affinity-clusterip-timeout-v5t54
Dec 23 14:47:02.694: INFO: Received response from host: affinity-clusterip-timeout-v5t54
Dec 23 14:47:02.694: INFO: Received response from host: affinity-clusterip-timeout-v5t54
Dec 23 14:47:02.694: INFO: Received response from host: affinity-clusterip-timeout-v5t54
Dec 23 14:47:02.694: INFO: Received response from host: affinity-clusterip-timeout-v5t54
Dec 23 14:47:02.694: INFO: Received response from host: affinity-clusterip-timeout-v5t54
Dec 23 14:47:02.694: INFO: Received response from host: affinity-clusterip-timeout-v5t54
Dec 23 14:47:02.694: INFO: Received response from host: affinity-clusterip-timeout-v5t54
Dec 23 14:47:02.694: INFO: Received response from host: affinity-clusterip-timeout-v5t54
Dec 23 14:47:02.694: INFO: Received response from host: affinity-clusterip-timeout-v5t54
Dec 23 14:47:02.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-174 exec execpod-affinityp8glv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.104.8.52:80/'
Dec 23 14:47:02.908: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n"
Dec 23 14:47:02.908: INFO: stdout: "affinity-clusterip-timeout-v5t54"
Dec 23 14:47:22.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-174 exec execpod-affinityp8glv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.104.8.52:80/'
Dec 23 14:47:23.134: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n"
Dec 23 14:47:23.134: INFO: stdout: "affinity-clusterip-timeout-v5t54"
Dec 23 14:47:43.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-174 exec execpod-affinityp8glv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.104.8.52:80/'
Dec 23 14:47:43.968: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n"
Dec 23 14:47:43.968: INFO: stdout: "affinity-clusterip-timeout-v5t54"
Dec 23 14:48:03.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-174 exec execpod-affinityp8glv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.104.8.52:80/'
Dec 23 14:48:04.177: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.104.8.52:80/\n"
Dec 23 14:48:04.177: INFO: stdout: "affinity-clusterip-timeout-xnmz2"
Dec 23 14:48:04.177: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-174, will wait for the garbage collector to delete the pods
Dec 23 14:48:04.255: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.895192ms
Dec 23 14:48:04.856: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 600.679042ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:48:19.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-174" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:89.473 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":209,"skipped":3867,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:48:19.444: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Dec 23 14:48:19.482: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 23 14:48:19.489: INFO: Waiting for terminating namespaces to be deleted...
Dec 23 14:48:19.492: INFO: 
Logging pods the apiserver thinks is on node wt-k8s-2 before test
Dec 23 14:48:19.499: INFO: calico-kube-controllers-744cfdf676-fbnzz from kube-system started at 2020-12-23 14:41:22 +0000 UTC (1 container statuses recorded)
Dec 23 14:48:19.499: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 23 14:48:19.499: INFO: calico-node-5czbz from kube-system started at 2020-12-23 08:19:38 +0000 UTC (1 container statuses recorded)
Dec 23 14:48:19.499: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 14:48:19.499: INFO: kube-proxy-55qvq from kube-system started at 2020-12-23 08:19:38 +0000 UTC (1 container statuses recorded)
Dec 23 14:48:19.499: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 23 14:48:19.499: INFO: sonobuoy from sonobuoy started at 2020-12-23 13:24:44 +0000 UTC (1 container statuses recorded)
Dec 23 14:48:19.499: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 23 14:48:19.499: INFO: sonobuoy-e2e-job-381779db02e94c8b from sonobuoy started at 2020-12-23 13:24:46 +0000 UTC (2 container statuses recorded)
Dec 23 14:48:19.499: INFO: 	Container e2e ready: true, restart count 0
Dec 23 14:48:19.499: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 14:48:19.499: INFO: sonobuoy-systemd-logs-daemon-set-a3aba41d66fb4680-2x864 from sonobuoy started at 2020-12-23 13:24:46 +0000 UTC (2 container statuses recorded)
Dec 23 14:48:19.499: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Dec 23 14:48:19.499: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 14:48:19.499: INFO: 
Logging pods the apiserver thinks is on node wt-k8s-3.novalocal before test
Dec 23 14:48:19.504: INFO: calico-node-lz86h from kube-system started at 2020-12-23 08:18:23 +0000 UTC (1 container statuses recorded)
Dec 23 14:48:19.505: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 14:48:19.505: INFO: kube-proxy-pnq9z from kube-system started at 2020-12-23 08:14:45 +0000 UTC (1 container statuses recorded)
Dec 23 14:48:19.505: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 23 14:48:19.505: INFO: sonobuoy-systemd-logs-daemon-set-a3aba41d66fb4680-n5vgr from sonobuoy started at 2020-12-23 13:24:32 +0000 UTC (2 container statuses recorded)
Dec 23 14:48:19.505: INFO: 	Container sonobuoy-worker ready: false, restart count 11
Dec 23 14:48:19.505: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-8921d227-f923-4d67-b927-ebb0efe3b184 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.22.19.25 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.22.19.25 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Dec 23 14:48:29.662: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.22.19.25 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6590 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:48:29.662: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.22.19.25, port: 54321
Dec 23 14:48:29.778: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.22.19.25:54321/hostname] Namespace:sched-pred-6590 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:48:29.778: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.22.19.25, port: 54321 UDP
Dec 23 14:48:29.874: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.22.19.25 54321] Namespace:sched-pred-6590 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:48:29.874: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Dec 23 14:48:34.965: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.22.19.25 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6590 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:48:34.965: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.22.19.25, port: 54321
Dec 23 14:48:35.083: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.22.19.25:54321/hostname] Namespace:sched-pred-6590 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:48:35.083: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.22.19.25, port: 54321 UDP
Dec 23 14:48:35.171: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.22.19.25 54321] Namespace:sched-pred-6590 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:48:35.171: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Dec 23 14:48:40.266: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.22.19.25 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6590 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:48:40.266: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.22.19.25, port: 54321
Dec 23 14:48:40.379: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.22.19.25:54321/hostname] Namespace:sched-pred-6590 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:48:40.379: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.22.19.25, port: 54321 UDP
Dec 23 14:48:40.471: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.22.19.25 54321] Namespace:sched-pred-6590 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:48:40.471: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Dec 23 14:48:45.559: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.22.19.25 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6590 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:48:45.559: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.22.19.25, port: 54321
Dec 23 14:48:45.655: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.22.19.25:54321/hostname] Namespace:sched-pred-6590 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:48:45.655: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.22.19.25, port: 54321 UDP
Dec 23 14:48:45.762: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.22.19.25 54321] Namespace:sched-pred-6590 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:48:45.762: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Dec 23 14:48:50.861: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.22.19.25 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6590 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:48:50.862: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.22.19.25, port: 54321
Dec 23 14:48:50.977: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.22.19.25:54321/hostname] Namespace:sched-pred-6590 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:48:50.977: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.22.19.25, port: 54321 UDP
Dec 23 14:48:51.077: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.22.19.25 54321] Namespace:sched-pred-6590 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:48:51.077: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: removing the label kubernetes.io/e2e-8921d227-f923-4d67-b927-ebb0efe3b184 off the node wt-k8s-3.novalocal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8921d227-f923-4d67-b927-ebb0efe3b184
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:48:56.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6590" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:36.768 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":210,"skipped":3875,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:48:56.212: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:48:56.250: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 23 14:49:00.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-9340 --namespace=crd-publish-openapi-9340 create -f -'
Dec 23 14:49:01.121: INFO: stderr: ""
Dec 23 14:49:01.121: INFO: stdout: "e2e-test-crd-publish-openapi-2267-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 23 14:49:01.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-9340 --namespace=crd-publish-openapi-9340 delete e2e-test-crd-publish-openapi-2267-crds test-cr'
Dec 23 14:49:01.244: INFO: stderr: ""
Dec 23 14:49:01.244: INFO: stdout: "e2e-test-crd-publish-openapi-2267-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 23 14:49:01.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-9340 --namespace=crd-publish-openapi-9340 apply -f -'
Dec 23 14:49:01.845: INFO: stderr: ""
Dec 23 14:49:01.845: INFO: stdout: "e2e-test-crd-publish-openapi-2267-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 23 14:49:01.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-9340 --namespace=crd-publish-openapi-9340 delete e2e-test-crd-publish-openapi-2267-crds test-cr'
Dec 23 14:49:01.971: INFO: stderr: ""
Dec 23 14:49:01.971: INFO: stdout: "e2e-test-crd-publish-openapi-2267-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 23 14:49:01.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-9340 explain e2e-test-crd-publish-openapi-2267-crds'
Dec 23 14:49:02.435: INFO: stderr: ""
Dec 23 14:49:02.435: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2267-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:49:06.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9340" for this suite.

• [SLOW TEST:10.130 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":211,"skipped":3877,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:49:06.343: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8963.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8963.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8963.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8963.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8963.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8963.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 23 14:49:10.442: INFO: DNS probes using dns-8963/dns-test-34fba460-e091-48e6-83f9-3c05cbc29070 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:49:10.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8963" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":212,"skipped":3912,"failed":0}
S
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:49:10.481: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:49:10.521: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6674
I1223 14:49:10.535586      20 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6674, replica count: 1
I1223 14:49:11.585949      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1223 14:49:12.586199      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 23 14:49:12.732: INFO: Created: latency-svc-6plgk
Dec 23 14:49:12.749: INFO: Got endpoints: latency-svc-6plgk [63.045895ms]
Dec 23 14:49:12.802: INFO: Created: latency-svc-k7ppw
Dec 23 14:49:12.834: INFO: Created: latency-svc-rxgmw
Dec 23 14:49:12.834: INFO: Got endpoints: latency-svc-k7ppw [84.89825ms]
Dec 23 14:49:12.842: INFO: Got endpoints: latency-svc-rxgmw [92.48089ms]
Dec 23 14:49:12.886: INFO: Created: latency-svc-8642n
Dec 23 14:49:12.897: INFO: Got endpoints: latency-svc-8642n [147.336438ms]
Dec 23 14:49:12.926: INFO: Created: latency-svc-gd96l
Dec 23 14:49:12.950: INFO: Got endpoints: latency-svc-gd96l [199.54524ms]
Dec 23 14:49:12.950: INFO: Created: latency-svc-sdnxt
Dec 23 14:49:12.956: INFO: Got endpoints: latency-svc-sdnxt [204.815936ms]
Dec 23 14:49:12.992: INFO: Created: latency-svc-dph9f
Dec 23 14:49:12.994: INFO: Got endpoints: latency-svc-dph9f [243.753526ms]
Dec 23 14:49:13.087: INFO: Created: latency-svc-vqx27
Dec 23 14:49:13.100: INFO: Got endpoints: latency-svc-vqx27 [348.945056ms]
Dec 23 14:49:13.133: INFO: Created: latency-svc-vm5wj
Dec 23 14:49:13.150: INFO: Got endpoints: latency-svc-vm5wj [399.742883ms]
Dec 23 14:49:13.156: INFO: Created: latency-svc-95k9d
Dec 23 14:49:13.170: INFO: Got endpoints: latency-svc-95k9d [419.70241ms]
Dec 23 14:49:13.186: INFO: Created: latency-svc-sjzk2
Dec 23 14:49:13.255: INFO: Got endpoints: latency-svc-sjzk2 [504.699961ms]
Dec 23 14:49:13.267: INFO: Created: latency-svc-bwh7s
Dec 23 14:49:13.273: INFO: Got endpoints: latency-svc-bwh7s [522.646901ms]
Dec 23 14:49:13.361: INFO: Created: latency-svc-2pnmr
Dec 23 14:49:13.463: INFO: Got endpoints: latency-svc-2pnmr [712.526996ms]
Dec 23 14:49:13.472: INFO: Created: latency-svc-22vqp
Dec 23 14:49:13.502: INFO: Got endpoints: latency-svc-22vqp [750.746295ms]
Dec 23 14:49:13.508: INFO: Created: latency-svc-9nklm
Dec 23 14:49:13.517: INFO: Got endpoints: latency-svc-9nklm [765.95448ms]
Dec 23 14:49:13.553: INFO: Created: latency-svc-2xns4
Dec 23 14:49:13.566: INFO: Got endpoints: latency-svc-2xns4 [815.130494ms]
Dec 23 14:49:13.604: INFO: Created: latency-svc-hgc9w
Dec 23 14:49:13.611: INFO: Got endpoints: latency-svc-hgc9w [776.85954ms]
Dec 23 14:49:13.634: INFO: Created: latency-svc-jr4r7
Dec 23 14:49:13.672: INFO: Created: latency-svc-p9x2l
Dec 23 14:49:13.673: INFO: Got endpoints: latency-svc-jr4r7 [830.379442ms]
Dec 23 14:49:13.704: INFO: Created: latency-svc-qsltn
Dec 23 14:49:13.746: INFO: Got endpoints: latency-svc-p9x2l [849.269571ms]
Dec 23 14:49:13.760: INFO: Got endpoints: latency-svc-qsltn [810.430675ms]
Dec 23 14:49:13.767: INFO: Created: latency-svc-b2wbb
Dec 23 14:49:13.769: INFO: Got endpoints: latency-svc-b2wbb [813.350623ms]
Dec 23 14:49:13.792: INFO: Created: latency-svc-8wzct
Dec 23 14:49:13.801: INFO: Got endpoints: latency-svc-8wzct [807.151177ms]
Dec 23 14:49:13.811: INFO: Created: latency-svc-j67cb
Dec 23 14:49:13.825: INFO: Got endpoints: latency-svc-j67cb [725.139489ms]
Dec 23 14:49:13.841: INFO: Created: latency-svc-2mzp9
Dec 23 14:49:13.887: INFO: Got endpoints: latency-svc-2mzp9 [736.585625ms]
Dec 23 14:49:13.896: INFO: Created: latency-svc-v8lfh
Dec 23 14:49:13.909: INFO: Got endpoints: latency-svc-v8lfh [738.702555ms]
Dec 23 14:49:13.925: INFO: Created: latency-svc-npslh
Dec 23 14:49:13.956: INFO: Got endpoints: latency-svc-npslh [701.012985ms]
Dec 23 14:49:13.979: INFO: Created: latency-svc-sqjtf
Dec 23 14:49:14.019: INFO: Got endpoints: latency-svc-sqjtf [745.832258ms]
Dec 23 14:49:14.069: INFO: Created: latency-svc-p52t2
Dec 23 14:49:14.079: INFO: Got endpoints: latency-svc-p52t2 [615.569405ms]
Dec 23 14:49:14.156: INFO: Created: latency-svc-9cd2q
Dec 23 14:49:14.177: INFO: Got endpoints: latency-svc-9cd2q [674.812143ms]
Dec 23 14:49:14.180: INFO: Created: latency-svc-hr8sd
Dec 23 14:49:14.292: INFO: Created: latency-svc-wf2wl
Dec 23 14:49:14.295: INFO: Got endpoints: latency-svc-hr8sd [778.17445ms]
Dec 23 14:49:14.308: INFO: Got endpoints: latency-svc-wf2wl [742.1751ms]
Dec 23 14:49:14.322: INFO: Created: latency-svc-gkh28
Dec 23 14:49:14.342: INFO: Created: latency-svc-smx8g
Dec 23 14:49:14.342: INFO: Got endpoints: latency-svc-gkh28 [731.072849ms]
Dec 23 14:49:14.372: INFO: Got endpoints: latency-svc-smx8g [699.010162ms]
Dec 23 14:49:14.379: INFO: Created: latency-svc-tqggw
Dec 23 14:49:14.395: INFO: Got endpoints: latency-svc-tqggw [648.876041ms]
Dec 23 14:49:14.449: INFO: Created: latency-svc-8pmhk
Dec 23 14:49:14.457: INFO: Got endpoints: latency-svc-8pmhk [696.852036ms]
Dec 23 14:49:14.485: INFO: Created: latency-svc-xq7fh
Dec 23 14:49:14.504: INFO: Got endpoints: latency-svc-xq7fh [735.088602ms]
Dec 23 14:49:14.530: INFO: Created: latency-svc-69wzx
Dec 23 14:49:14.534: INFO: Got endpoints: latency-svc-69wzx [732.788101ms]
Dec 23 14:49:14.575: INFO: Created: latency-svc-c8xnx
Dec 23 14:49:14.580: INFO: Got endpoints: latency-svc-c8xnx [755.349911ms]
Dec 23 14:49:14.619: INFO: Created: latency-svc-9tbmq
Dec 23 14:49:14.627: INFO: Got endpoints: latency-svc-9tbmq [739.987872ms]
Dec 23 14:49:14.693: INFO: Created: latency-svc-sw655
Dec 23 14:49:14.746: INFO: Created: latency-svc-77s7r
Dec 23 14:49:14.746: INFO: Got endpoints: latency-svc-sw655 [836.934076ms]
Dec 23 14:49:14.755: INFO: Got endpoints: latency-svc-77s7r [798.714819ms]
Dec 23 14:49:14.797: INFO: Created: latency-svc-f6lrh
Dec 23 14:49:14.867: INFO: Got endpoints: latency-svc-f6lrh [848.138128ms]
Dec 23 14:49:14.878: INFO: Created: latency-svc-x4q44
Dec 23 14:49:14.888: INFO: Got endpoints: latency-svc-x4q44 [808.820668ms]
Dec 23 14:49:14.898: INFO: Created: latency-svc-dqvgb
Dec 23 14:49:14.908: INFO: Got endpoints: latency-svc-dqvgb [731.089025ms]
Dec 23 14:49:14.921: INFO: Created: latency-svc-nvkhf
Dec 23 14:49:14.935: INFO: Got endpoints: latency-svc-nvkhf [639.203015ms]
Dec 23 14:49:14.935: INFO: Created: latency-svc-5f7nj
Dec 23 14:49:14.942: INFO: Got endpoints: latency-svc-5f7nj [33.902069ms]
Dec 23 14:49:14.961: INFO: Created: latency-svc-sm544
Dec 23 14:49:15.011: INFO: Got endpoints: latency-svc-sm544 [703.631928ms]
Dec 23 14:49:15.015: INFO: Created: latency-svc-zdxsf
Dec 23 14:49:15.028: INFO: Got endpoints: latency-svc-zdxsf [685.510479ms]
Dec 23 14:49:15.038: INFO: Created: latency-svc-9tvl8
Dec 23 14:49:15.044: INFO: Got endpoints: latency-svc-9tvl8 [672.561518ms]
Dec 23 14:49:15.071: INFO: Created: latency-svc-jmh6x
Dec 23 14:49:15.076: INFO: Got endpoints: latency-svc-jmh6x [680.289568ms]
Dec 23 14:49:15.143: INFO: Created: latency-svc-xg5tv
Dec 23 14:49:15.157: INFO: Got endpoints: latency-svc-xg5tv [700.274226ms]
Dec 23 14:49:15.236: INFO: Created: latency-svc-2plc6
Dec 23 14:49:15.273: INFO: Created: latency-svc-688fv
Dec 23 14:49:15.274: INFO: Got endpoints: latency-svc-2plc6 [769.816754ms]
Dec 23 14:49:15.281: INFO: Got endpoints: latency-svc-688fv [747.329087ms]
Dec 23 14:49:15.297: INFO: Created: latency-svc-4m7fg
Dec 23 14:49:15.307: INFO: Got endpoints: latency-svc-4m7fg [726.182944ms]
Dec 23 14:49:15.318: INFO: Created: latency-svc-tw6zm
Dec 23 14:49:15.324: INFO: Got endpoints: latency-svc-tw6zm [696.691864ms]
Dec 23 14:49:15.362: INFO: Created: latency-svc-6wcnq
Dec 23 14:49:15.399: INFO: Created: latency-svc-dw5x4
Dec 23 14:49:15.399: INFO: Got endpoints: latency-svc-6wcnq [653.117368ms]
Dec 23 14:49:15.417: INFO: Got endpoints: latency-svc-dw5x4 [661.527715ms]
Dec 23 14:49:15.417: INFO: Created: latency-svc-ddt76
Dec 23 14:49:15.426: INFO: Got endpoints: latency-svc-ddt76 [558.785209ms]
Dec 23 14:49:15.449: INFO: Created: latency-svc-c9gsj
Dec 23 14:49:15.460: INFO: Got endpoints: latency-svc-c9gsj [572.405608ms]
Dec 23 14:49:15.525: INFO: Created: latency-svc-9qxr6
Dec 23 14:49:15.540: INFO: Got endpoints: latency-svc-9qxr6 [605.848003ms]
Dec 23 14:49:15.566: INFO: Created: latency-svc-qqfth
Dec 23 14:49:15.589: INFO: Got endpoints: latency-svc-qqfth [647.314347ms]
Dec 23 14:49:15.594: INFO: Created: latency-svc-85mzm
Dec 23 14:49:15.601: INFO: Got endpoints: latency-svc-85mzm [589.746662ms]
Dec 23 14:49:15.618: INFO: Created: latency-svc-j8m2z
Dec 23 14:49:15.660: INFO: Got endpoints: latency-svc-j8m2z [632.154593ms]
Dec 23 14:49:15.663: INFO: Created: latency-svc-rntgx
Dec 23 14:49:15.671: INFO: Got endpoints: latency-svc-rntgx [626.453516ms]
Dec 23 14:49:15.704: INFO: Created: latency-svc-zkvjs
Dec 23 14:49:15.704: INFO: Got endpoints: latency-svc-zkvjs [628.421503ms]
Dec 23 14:49:15.822: INFO: Created: latency-svc-ls4dj
Dec 23 14:49:15.831: INFO: Got endpoints: latency-svc-ls4dj [673.904509ms]
Dec 23 14:49:15.886: INFO: Created: latency-svc-p44zl
Dec 23 14:49:15.898: INFO: Got endpoints: latency-svc-p44zl [623.538794ms]
Dec 23 14:49:15.945: INFO: Created: latency-svc-ff7qw
Dec 23 14:49:15.959: INFO: Got endpoints: latency-svc-ff7qw [677.221339ms]
Dec 23 14:49:15.979: INFO: Created: latency-svc-7rnsk
Dec 23 14:49:15.988: INFO: Got endpoints: latency-svc-7rnsk [681.457841ms]
Dec 23 14:49:16.099: INFO: Created: latency-svc-tx77s
Dec 23 14:49:16.115: INFO: Got endpoints: latency-svc-tx77s [791.354745ms]
Dec 23 14:49:16.132: INFO: Created: latency-svc-xl8pb
Dec 23 14:49:16.149: INFO: Got endpoints: latency-svc-xl8pb [749.627791ms]
Dec 23 14:49:16.159: INFO: Created: latency-svc-2ff2n
Dec 23 14:49:16.164: INFO: Got endpoints: latency-svc-2ff2n [747.120551ms]
Dec 23 14:49:16.180: INFO: Created: latency-svc-khswv
Dec 23 14:49:16.392: INFO: Got endpoints: latency-svc-khswv [965.477312ms]
Dec 23 14:49:16.402: INFO: Created: latency-svc-hc4vv
Dec 23 14:49:16.433: INFO: Got endpoints: latency-svc-hc4vv [972.941203ms]
Dec 23 14:49:16.435: INFO: Created: latency-svc-jbqpd
Dec 23 14:49:16.445: INFO: Got endpoints: latency-svc-jbqpd [904.463315ms]
Dec 23 14:49:16.478: INFO: Created: latency-svc-sxqxq
Dec 23 14:49:16.481: INFO: Got endpoints: latency-svc-sxqxq [891.899369ms]
Dec 23 14:49:16.567: INFO: Created: latency-svc-rqzzb
Dec 23 14:49:16.571: INFO: Got endpoints: latency-svc-rqzzb [969.271744ms]
Dec 23 14:49:16.611: INFO: Created: latency-svc-62kcg
Dec 23 14:49:16.641: INFO: Created: latency-svc-qh65c
Dec 23 14:49:16.644: INFO: Got endpoints: latency-svc-62kcg [983.855463ms]
Dec 23 14:49:16.655: INFO: Got endpoints: latency-svc-qh65c [983.759948ms]
Dec 23 14:49:16.723: INFO: Created: latency-svc-wftbj
Dec 23 14:49:16.810: INFO: Got endpoints: latency-svc-wftbj [1.106351997s]
Dec 23 14:49:16.820: INFO: Created: latency-svc-rc9jn
Dec 23 14:49:16.878: INFO: Created: latency-svc-jwb8g
Dec 23 14:49:16.884: INFO: Got endpoints: latency-svc-rc9jn [1.052506946s]
Dec 23 14:49:16.896: INFO: Got endpoints: latency-svc-jwb8g [998.519282ms]
Dec 23 14:49:16.897: INFO: Created: latency-svc-jvkj7
Dec 23 14:49:16.907: INFO: Got endpoints: latency-svc-jvkj7 [947.867982ms]
Dec 23 14:49:16.940: INFO: Created: latency-svc-bvn8s
Dec 23 14:49:16.940: INFO: Got endpoints: latency-svc-bvn8s [951.236602ms]
Dec 23 14:49:16.947: INFO: Created: latency-svc-hfzxd
Dec 23 14:49:16.957: INFO: Got endpoints: latency-svc-hfzxd [842.104136ms]
Dec 23 14:49:17.004: INFO: Created: latency-svc-6q89f
Dec 23 14:49:17.023: INFO: Created: latency-svc-hqxmh
Dec 23 14:49:17.024: INFO: Got endpoints: latency-svc-6q89f [874.976736ms]
Dec 23 14:49:17.037: INFO: Got endpoints: latency-svc-hqxmh [873.186574ms]
Dec 23 14:49:17.082: INFO: Created: latency-svc-5s4sw
Dec 23 14:49:17.095: INFO: Got endpoints: latency-svc-5s4sw [702.806544ms]
Dec 23 14:49:17.137: INFO: Created: latency-svc-68f6n
Dec 23 14:49:17.143: INFO: Got endpoints: latency-svc-68f6n [710.005479ms]
Dec 23 14:49:17.165: INFO: Created: latency-svc-rk5vv
Dec 23 14:49:17.174: INFO: Got endpoints: latency-svc-rk5vv [728.90664ms]
Dec 23 14:49:17.223: INFO: Created: latency-svc-zpbdc
Dec 23 14:49:17.270: INFO: Created: latency-svc-zwvwp
Dec 23 14:49:17.270: INFO: Got endpoints: latency-svc-zpbdc [789.333537ms]
Dec 23 14:49:17.278: INFO: Got endpoints: latency-svc-zwvwp [707.566842ms]
Dec 23 14:49:17.322: INFO: Created: latency-svc-ms6gw
Dec 23 14:49:17.330: INFO: Got endpoints: latency-svc-ms6gw [686.425013ms]
Dec 23 14:49:17.354: INFO: Created: latency-svc-l7qg5
Dec 23 14:49:17.513: INFO: Created: latency-svc-59f47
Dec 23 14:49:17.514: INFO: Got endpoints: latency-svc-l7qg5 [858.791307ms]
Dec 23 14:49:17.520: INFO: Got endpoints: latency-svc-59f47 [709.372994ms]
Dec 23 14:49:17.548: INFO: Created: latency-svc-hrc98
Dec 23 14:49:17.555: INFO: Got endpoints: latency-svc-hrc98 [671.556451ms]
Dec 23 14:49:17.579: INFO: Created: latency-svc-sw8sc
Dec 23 14:49:17.579: INFO: Got endpoints: latency-svc-sw8sc [683.017253ms]
Dec 23 14:49:17.596: INFO: Created: latency-svc-srh5n
Dec 23 14:49:17.601: INFO: Got endpoints: latency-svc-srh5n [694.34491ms]
Dec 23 14:49:17.651: INFO: Created: latency-svc-72q4q
Dec 23 14:49:17.678: INFO: Created: latency-svc-6jkj2
Dec 23 14:49:17.678: INFO: Got endpoints: latency-svc-72q4q [738.253408ms]
Dec 23 14:49:17.687: INFO: Got endpoints: latency-svc-6jkj2 [730.02053ms]
Dec 23 14:49:17.700: INFO: Created: latency-svc-xthw7
Dec 23 14:49:17.723: INFO: Got endpoints: latency-svc-xthw7 [698.872945ms]
Dec 23 14:49:17.729: INFO: Created: latency-svc-lk8sx
Dec 23 14:49:17.731: INFO: Got endpoints: latency-svc-lk8sx [693.852738ms]
Dec 23 14:49:17.804: INFO: Created: latency-svc-55xv9
Dec 23 14:49:17.814: INFO: Got endpoints: latency-svc-55xv9 [719.650411ms]
Dec 23 14:49:17.850: INFO: Created: latency-svc-tj6nt
Dec 23 14:49:17.854: INFO: Got endpoints: latency-svc-tj6nt [711.03756ms]
Dec 23 14:49:17.882: INFO: Created: latency-svc-tplmh
Dec 23 14:49:17.890: INFO: Got endpoints: latency-svc-tplmh [715.758548ms]
Dec 23 14:49:17.943: INFO: Created: latency-svc-fhsgm
Dec 23 14:49:17.970: INFO: Got endpoints: latency-svc-fhsgm [699.612857ms]
Dec 23 14:49:17.971: INFO: Created: latency-svc-ctcrv
Dec 23 14:49:17.988: INFO: Got endpoints: latency-svc-ctcrv [709.468944ms]
Dec 23 14:49:17.993: INFO: Created: latency-svc-cctwz
Dec 23 14:49:18.008: INFO: Got endpoints: latency-svc-cctwz [677.298983ms]
Dec 23 14:49:18.009: INFO: Created: latency-svc-cs6sj
Dec 23 14:49:18.020: INFO: Got endpoints: latency-svc-cs6sj [506.176095ms]
Dec 23 14:49:18.038: INFO: Created: latency-svc-dzh9s
Dec 23 14:49:18.079: INFO: Got endpoints: latency-svc-dzh9s [559.184158ms]
Dec 23 14:49:18.086: INFO: Created: latency-svc-fdh47
Dec 23 14:49:18.103: INFO: Got endpoints: latency-svc-fdh47 [547.972436ms]
Dec 23 14:49:18.105: INFO: Created: latency-svc-drsh7
Dec 23 14:49:18.115: INFO: Got endpoints: latency-svc-drsh7 [535.035562ms]
Dec 23 14:49:18.133: INFO: Created: latency-svc-6sbf8
Dec 23 14:49:18.142: INFO: Got endpoints: latency-svc-6sbf8 [541.046453ms]
Dec 23 14:49:18.202: INFO: Created: latency-svc-r5z4g
Dec 23 14:49:18.236: INFO: Got endpoints: latency-svc-r5z4g [558.443815ms]
Dec 23 14:49:18.237: INFO: Created: latency-svc-gdnmf
Dec 23 14:49:18.248: INFO: Got endpoints: latency-svc-gdnmf [560.687544ms]
Dec 23 14:49:18.281: INFO: Created: latency-svc-j5pph
Dec 23 14:49:18.290: INFO: Got endpoints: latency-svc-j5pph [566.586228ms]
Dec 23 14:49:18.332: INFO: Created: latency-svc-wc2z4
Dec 23 14:49:18.354: INFO: Got endpoints: latency-svc-wc2z4 [622.633279ms]
Dec 23 14:49:18.361: INFO: Created: latency-svc-zgk5f
Dec 23 14:49:18.386: INFO: Got endpoints: latency-svc-zgk5f [571.422877ms]
Dec 23 14:49:18.391: INFO: Created: latency-svc-gdrlh
Dec 23 14:49:18.399: INFO: Got endpoints: latency-svc-gdrlh [544.249761ms]
Dec 23 14:49:18.436: INFO: Created: latency-svc-5xtlr
Dec 23 14:49:18.513: INFO: Got endpoints: latency-svc-5xtlr [622.945465ms]
Dec 23 14:49:18.526: INFO: Created: latency-svc-9zmdr
Dec 23 14:49:18.530: INFO: Got endpoints: latency-svc-9zmdr [559.61879ms]
Dec 23 14:49:18.570: INFO: Created: latency-svc-nwjzt
Dec 23 14:49:18.652: INFO: Got endpoints: latency-svc-nwjzt [664.533268ms]
Dec 23 14:49:18.689: INFO: Created: latency-svc-bvwbg
Dec 23 14:49:18.692: INFO: Got endpoints: latency-svc-bvwbg [684.645407ms]
Dec 23 14:49:18.790: INFO: Created: latency-svc-t9z6f
Dec 23 14:49:18.858: INFO: Created: latency-svc-kqbsv
Dec 23 14:49:18.860: INFO: Got endpoints: latency-svc-t9z6f [840.132034ms]
Dec 23 14:49:18.888: INFO: Got endpoints: latency-svc-kqbsv [809.027561ms]
Dec 23 14:49:18.890: INFO: Created: latency-svc-gz2pj
Dec 23 14:49:18.932: INFO: Got endpoints: latency-svc-gz2pj [828.21255ms]
Dec 23 14:49:18.942: INFO: Created: latency-svc-6lvxd
Dec 23 14:49:18.980: INFO: Created: latency-svc-wz96x
Dec 23 14:49:18.983: INFO: Got endpoints: latency-svc-6lvxd [868.091987ms]
Dec 23 14:49:18.987: INFO: Got endpoints: latency-svc-wz96x [845.056374ms]
Dec 23 14:49:19.032: INFO: Created: latency-svc-gtjzg
Dec 23 14:49:19.124: INFO: Got endpoints: latency-svc-gtjzg [887.467971ms]
Dec 23 14:49:19.129: INFO: Created: latency-svc-shvnh
Dec 23 14:49:19.158: INFO: Got endpoints: latency-svc-shvnh [909.814507ms]
Dec 23 14:49:19.166: INFO: Created: latency-svc-r2skh
Dec 23 14:49:19.166: INFO: Got endpoints: latency-svc-r2skh [876.585958ms]
Dec 23 14:49:19.193: INFO: Created: latency-svc-t6gg9
Dec 23 14:49:19.212: INFO: Got endpoints: latency-svc-t6gg9 [857.767543ms]
Dec 23 14:49:19.215: INFO: Created: latency-svc-cdqxj
Dec 23 14:49:19.264: INFO: Got endpoints: latency-svc-cdqxj [878.018573ms]
Dec 23 14:49:19.267: INFO: Created: latency-svc-6r6gw
Dec 23 14:49:19.274: INFO: Got endpoints: latency-svc-6r6gw [875.5683ms]
Dec 23 14:49:19.301: INFO: Created: latency-svc-f7qnn
Dec 23 14:49:19.316: INFO: Got endpoints: latency-svc-f7qnn [803.333176ms]
Dec 23 14:49:19.316: INFO: Created: latency-svc-d4w2v
Dec 23 14:49:19.333: INFO: Got endpoints: latency-svc-d4w2v [803.297158ms]
Dec 23 14:49:19.402: INFO: Created: latency-svc-l6662
Dec 23 14:49:19.413: INFO: Got endpoints: latency-svc-l6662 [760.272402ms]
Dec 23 14:49:19.439: INFO: Created: latency-svc-glj2v
Dec 23 14:49:19.446: INFO: Got endpoints: latency-svc-glj2v [753.222881ms]
Dec 23 14:49:19.471: INFO: Created: latency-svc-jprhs
Dec 23 14:49:19.481: INFO: Got endpoints: latency-svc-jprhs [621.181233ms]
Dec 23 14:49:19.493: INFO: Created: latency-svc-thdcs
Dec 23 14:49:19.547: INFO: Got endpoints: latency-svc-thdcs [658.99595ms]
Dec 23 14:49:19.552: INFO: Created: latency-svc-wkxql
Dec 23 14:49:19.559: INFO: Got endpoints: latency-svc-wkxql [627.542345ms]
Dec 23 14:49:19.582: INFO: Created: latency-svc-mxqqn
Dec 23 14:49:19.599: INFO: Got endpoints: latency-svc-mxqqn [616.678923ms]
Dec 23 14:49:19.609: INFO: Created: latency-svc-cdqrv
Dec 23 14:49:19.612: INFO: Got endpoints: latency-svc-cdqrv [624.269595ms]
Dec 23 14:49:19.624: INFO: Created: latency-svc-f46wm
Dec 23 14:49:19.735: INFO: Created: latency-svc-wwnbn
Dec 23 14:49:19.737: INFO: Got endpoints: latency-svc-f46wm [612.630221ms]
Dec 23 14:49:19.754: INFO: Got endpoints: latency-svc-wwnbn [596.009349ms]
Dec 23 14:49:19.763: INFO: Created: latency-svc-kfbv9
Dec 23 14:49:19.882: INFO: Got endpoints: latency-svc-kfbv9 [716.114937ms]
Dec 23 14:49:19.883: INFO: Created: latency-svc-gr977
Dec 23 14:49:19.891: INFO: Got endpoints: latency-svc-gr977 [679.355436ms]
Dec 23 14:49:19.938: INFO: Created: latency-svc-q6kp5
Dec 23 14:49:19.945: INFO: Got endpoints: latency-svc-q6kp5 [681.170659ms]
Dec 23 14:49:19.958: INFO: Created: latency-svc-74bjv
Dec 23 14:49:19.972: INFO: Created: latency-svc-h4cgv
Dec 23 14:49:19.972: INFO: Got endpoints: latency-svc-74bjv [697.936673ms]
Dec 23 14:49:20.022: INFO: Got endpoints: latency-svc-h4cgv [705.758753ms]
Dec 23 14:49:20.026: INFO: Created: latency-svc-zq4g6
Dec 23 14:49:20.034: INFO: Got endpoints: latency-svc-zq4g6 [701.197105ms]
Dec 23 14:49:20.053: INFO: Created: latency-svc-xbx6r
Dec 23 14:49:20.066: INFO: Got endpoints: latency-svc-xbx6r [653.71306ms]
Dec 23 14:49:20.069: INFO: Created: latency-svc-xq2jw
Dec 23 14:49:20.092: INFO: Got endpoints: latency-svc-xq2jw [646.492399ms]
Dec 23 14:49:20.093: INFO: Created: latency-svc-f2b8n
Dec 23 14:49:20.097: INFO: Got endpoints: latency-svc-f2b8n [615.640121ms]
Dec 23 14:49:20.109: INFO: Created: latency-svc-xwxmj
Dec 23 14:49:20.162: INFO: Created: latency-svc-b9skb
Dec 23 14:49:20.163: INFO: Got endpoints: latency-svc-xwxmj [615.972684ms]
Dec 23 14:49:20.189: INFO: Created: latency-svc-xzcnt
Dec 23 14:49:20.191: INFO: Got endpoints: latency-svc-b9skb [631.523829ms]
Dec 23 14:49:20.199: INFO: Got endpoints: latency-svc-xzcnt [599.309231ms]
Dec 23 14:49:20.254: INFO: Created: latency-svc-nkkrz
Dec 23 14:49:20.299: INFO: Created: latency-svc-fhtm6
Dec 23 14:49:20.301: INFO: Got endpoints: latency-svc-nkkrz [688.928808ms]
Dec 23 14:49:20.306: INFO: Got endpoints: latency-svc-fhtm6 [569.839617ms]
Dec 23 14:49:20.321: INFO: Created: latency-svc-z4gsp
Dec 23 14:49:20.329: INFO: Got endpoints: latency-svc-z4gsp [574.865228ms]
Dec 23 14:49:20.349: INFO: Created: latency-svc-9542p
Dec 23 14:49:20.358: INFO: Got endpoints: latency-svc-9542p [475.69683ms]
Dec 23 14:49:20.368: INFO: Created: latency-svc-f9b4t
Dec 23 14:49:20.374: INFO: Got endpoints: latency-svc-f9b4t [483.403508ms]
Dec 23 14:49:20.390: INFO: Created: latency-svc-qxlr5
Dec 23 14:49:20.485: INFO: Got endpoints: latency-svc-qxlr5 [539.964471ms]
Dec 23 14:49:20.489: INFO: Created: latency-svc-zmx22
Dec 23 14:49:20.520: INFO: Created: latency-svc-2wnzz
Dec 23 14:49:20.520: INFO: Got endpoints: latency-svc-zmx22 [547.931767ms]
Dec 23 14:49:20.532: INFO: Got endpoints: latency-svc-2wnzz [509.733987ms]
Dec 23 14:49:20.544: INFO: Created: latency-svc-ch8rt
Dec 23 14:49:20.567: INFO: Created: latency-svc-7jqs2
Dec 23 14:49:20.571: INFO: Got endpoints: latency-svc-ch8rt [536.387651ms]
Dec 23 14:49:20.618: INFO: Got endpoints: latency-svc-7jqs2 [551.210485ms]
Dec 23 14:49:20.618: INFO: Created: latency-svc-r5f2w
Dec 23 14:49:20.636: INFO: Created: latency-svc-qtzvg
Dec 23 14:49:20.637: INFO: Got endpoints: latency-svc-r5f2w [544.652774ms]
Dec 23 14:49:20.647: INFO: Got endpoints: latency-svc-qtzvg [550.43897ms]
Dec 23 14:49:20.658: INFO: Created: latency-svc-wkkfn
Dec 23 14:49:20.666: INFO: Got endpoints: latency-svc-wkkfn [502.378913ms]
Dec 23 14:49:20.688: INFO: Created: latency-svc-tklcd
Dec 23 14:49:20.690: INFO: Got endpoints: latency-svc-tklcd [499.121781ms]
Dec 23 14:49:20.705: INFO: Created: latency-svc-cnzb8
Dec 23 14:49:20.763: INFO: Got endpoints: latency-svc-cnzb8 [564.336805ms]
Dec 23 14:49:20.764: INFO: Created: latency-svc-8nlx9
Dec 23 14:49:20.814: INFO: Got endpoints: latency-svc-8nlx9 [513.300063ms]
Dec 23 14:49:20.814: INFO: Created: latency-svc-rfrrw
Dec 23 14:49:20.835: INFO: Got endpoints: latency-svc-rfrrw [528.506112ms]
Dec 23 14:49:20.836: INFO: Created: latency-svc-66mqm
Dec 23 14:49:20.891: INFO: Got endpoints: latency-svc-66mqm [562.401441ms]
Dec 23 14:49:20.895: INFO: Created: latency-svc-xp7gt
Dec 23 14:49:20.895: INFO: Got endpoints: latency-svc-xp7gt [536.991346ms]
Dec 23 14:49:20.930: INFO: Created: latency-svc-4rj4m
Dec 23 14:49:20.977: INFO: Created: latency-svc-pnprg
Dec 23 14:49:20.983: INFO: Got endpoints: latency-svc-4rj4m [608.364219ms]
Dec 23 14:49:21.044: INFO: Created: latency-svc-45td6
Dec 23 14:49:21.044: INFO: Got endpoints: latency-svc-pnprg [559.093179ms]
Dec 23 14:49:21.074: INFO: Created: latency-svc-f46wg
Dec 23 14:49:21.074: INFO: Got endpoints: latency-svc-45td6 [553.912033ms]
Dec 23 14:49:21.088: INFO: Created: latency-svc-pt5tf
Dec 23 14:49:21.090: INFO: Got endpoints: latency-svc-f46wg [558.598484ms]
Dec 23 14:49:21.100: INFO: Got endpoints: latency-svc-pt5tf [529.009697ms]
Dec 23 14:49:21.109: INFO: Created: latency-svc-bn6lw
Dec 23 14:49:21.116: INFO: Got endpoints: latency-svc-bn6lw [498.482102ms]
Dec 23 14:49:21.129: INFO: Created: latency-svc-42vts
Dec 23 14:49:21.180: INFO: Created: latency-svc-cvj47
Dec 23 14:49:21.184: INFO: Got endpoints: latency-svc-42vts [547.258948ms]
Dec 23 14:49:21.188: INFO: Got endpoints: latency-svc-cvj47 [540.47828ms]
Dec 23 14:49:21.261: INFO: Created: latency-svc-76c57
Dec 23 14:49:21.267: INFO: Got endpoints: latency-svc-76c57 [601.154958ms]
Dec 23 14:49:21.301: INFO: Created: latency-svc-l5m6p
Dec 23 14:49:21.312: INFO: Got endpoints: latency-svc-l5m6p [622.345407ms]
Dec 23 14:49:21.378: INFO: Created: latency-svc-9bqs7
Dec 23 14:49:21.418: INFO: Got endpoints: latency-svc-9bqs7 [655.161815ms]
Dec 23 14:49:21.423: INFO: Created: latency-svc-h2q84
Dec 23 14:49:21.426: INFO: Got endpoints: latency-svc-h2q84 [612.23865ms]
Dec 23 14:49:21.444: INFO: Created: latency-svc-xbl4m
Dec 23 14:49:21.470: INFO: Got endpoints: latency-svc-xbl4m [635.328569ms]
Dec 23 14:49:21.476: INFO: Created: latency-svc-ff5jc
Dec 23 14:49:21.487: INFO: Got endpoints: latency-svc-ff5jc [595.498306ms]
Dec 23 14:49:21.492: INFO: Created: latency-svc-f88xt
Dec 23 14:49:21.497: INFO: Got endpoints: latency-svc-f88xt [601.766435ms]
Dec 23 14:49:21.507: INFO: Created: latency-svc-lfsl7
Dec 23 14:49:21.515: INFO: Got endpoints: latency-svc-lfsl7 [532.199253ms]
Dec 23 14:49:21.545: INFO: Created: latency-svc-fvzjq
Dec 23 14:49:21.565: INFO: Got endpoints: latency-svc-fvzjq [520.570201ms]
Dec 23 14:49:21.565: INFO: Created: latency-svc-p9cv4
Dec 23 14:49:21.583: INFO: Got endpoints: latency-svc-p9cv4 [508.399573ms]
Dec 23 14:49:21.585: INFO: Created: latency-svc-w8gl6
Dec 23 14:49:21.593: INFO: Got endpoints: latency-svc-w8gl6 [502.468452ms]
Dec 23 14:49:21.610: INFO: Created: latency-svc-zvr57
Dec 23 14:49:21.628: INFO: Got endpoints: latency-svc-zvr57 [528.260492ms]
Dec 23 14:49:21.631: INFO: Created: latency-svc-dr4jg
Dec 23 14:49:21.638: INFO: Got endpoints: latency-svc-dr4jg [522.044872ms]
Dec 23 14:49:21.748: INFO: Created: latency-svc-4zf2s
Dec 23 14:49:21.763: INFO: Got endpoints: latency-svc-4zf2s [579.31578ms]
Dec 23 14:49:21.764: INFO: Created: latency-svc-4bfc5
Dec 23 14:49:21.773: INFO: Got endpoints: latency-svc-4bfc5 [584.830065ms]
Dec 23 14:49:21.789: INFO: Created: latency-svc-m4xdp
Dec 23 14:49:21.867: INFO: Created: latency-svc-gvwrm
Dec 23 14:49:21.867: INFO: Got endpoints: latency-svc-m4xdp [600.389839ms]
Dec 23 14:49:21.882: INFO: Got endpoints: latency-svc-gvwrm [569.595382ms]
Dec 23 14:49:21.882: INFO: Latencies: [33.902069ms 84.89825ms 92.48089ms 147.336438ms 199.54524ms 204.815936ms 243.753526ms 348.945056ms 399.742883ms 419.70241ms 475.69683ms 483.403508ms 498.482102ms 499.121781ms 502.378913ms 502.468452ms 504.699961ms 506.176095ms 508.399573ms 509.733987ms 513.300063ms 520.570201ms 522.044872ms 522.646901ms 528.260492ms 528.506112ms 529.009697ms 532.199253ms 535.035562ms 536.387651ms 536.991346ms 539.964471ms 540.47828ms 541.046453ms 544.249761ms 544.652774ms 547.258948ms 547.931767ms 547.972436ms 550.43897ms 551.210485ms 553.912033ms 558.443815ms 558.598484ms 558.785209ms 559.093179ms 559.184158ms 559.61879ms 560.687544ms 562.401441ms 564.336805ms 566.586228ms 569.595382ms 569.839617ms 571.422877ms 572.405608ms 574.865228ms 579.31578ms 584.830065ms 589.746662ms 595.498306ms 596.009349ms 599.309231ms 600.389839ms 601.154958ms 601.766435ms 605.848003ms 608.364219ms 612.23865ms 612.630221ms 615.569405ms 615.640121ms 615.972684ms 616.678923ms 621.181233ms 622.345407ms 622.633279ms 622.945465ms 623.538794ms 624.269595ms 626.453516ms 627.542345ms 628.421503ms 631.523829ms 632.154593ms 635.328569ms 639.203015ms 646.492399ms 647.314347ms 648.876041ms 653.117368ms 653.71306ms 655.161815ms 658.99595ms 661.527715ms 664.533268ms 671.556451ms 672.561518ms 673.904509ms 674.812143ms 677.221339ms 677.298983ms 679.355436ms 680.289568ms 681.170659ms 681.457841ms 683.017253ms 684.645407ms 685.510479ms 686.425013ms 688.928808ms 693.852738ms 694.34491ms 696.691864ms 696.852036ms 697.936673ms 698.872945ms 699.010162ms 699.612857ms 700.274226ms 701.012985ms 701.197105ms 702.806544ms 703.631928ms 705.758753ms 707.566842ms 709.372994ms 709.468944ms 710.005479ms 711.03756ms 712.526996ms 715.758548ms 716.114937ms 719.650411ms 725.139489ms 726.182944ms 728.90664ms 730.02053ms 731.072849ms 731.089025ms 732.788101ms 735.088602ms 736.585625ms 738.253408ms 738.702555ms 739.987872ms 742.1751ms 745.832258ms 747.120551ms 747.329087ms 749.627791ms 750.746295ms 753.222881ms 755.349911ms 760.272402ms 765.95448ms 769.816754ms 776.85954ms 778.17445ms 789.333537ms 791.354745ms 798.714819ms 803.297158ms 803.333176ms 807.151177ms 808.820668ms 809.027561ms 810.430675ms 813.350623ms 815.130494ms 828.21255ms 830.379442ms 836.934076ms 840.132034ms 842.104136ms 845.056374ms 848.138128ms 849.269571ms 857.767543ms 858.791307ms 868.091987ms 873.186574ms 874.976736ms 875.5683ms 876.585958ms 878.018573ms 887.467971ms 891.899369ms 904.463315ms 909.814507ms 947.867982ms 951.236602ms 965.477312ms 969.271744ms 972.941203ms 983.759948ms 983.855463ms 998.519282ms 1.052506946s 1.106351997s]
Dec 23 14:49:21.882: INFO: 50 %ile: 677.221339ms
Dec 23 14:49:21.882: INFO: 90 %ile: 868.091987ms
Dec 23 14:49:21.882: INFO: 99 %ile: 1.052506946s
Dec 23 14:49:21.882: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:49:21.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6674" for this suite.

• [SLOW TEST:11.417 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":213,"skipped":3913,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:49:21.898: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Dec 23 14:49:22.248: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 23 14:49:22.255: INFO: Waiting for terminating namespaces to be deleted...
Dec 23 14:49:22.258: INFO: 
Logging pods the apiserver thinks is on node wt-k8s-2 before test
Dec 23 14:49:22.263: INFO: calico-kube-controllers-744cfdf676-fbnzz from kube-system started at 2020-12-23 14:41:22 +0000 UTC (1 container statuses recorded)
Dec 23 14:49:22.263: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 23 14:49:22.263: INFO: calico-node-5czbz from kube-system started at 2020-12-23 08:19:38 +0000 UTC (1 container statuses recorded)
Dec 23 14:49:22.263: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 14:49:22.263: INFO: kube-proxy-55qvq from kube-system started at 2020-12-23 08:19:38 +0000 UTC (1 container statuses recorded)
Dec 23 14:49:22.263: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 23 14:49:22.263: INFO: sonobuoy from sonobuoy started at 2020-12-23 13:24:44 +0000 UTC (1 container statuses recorded)
Dec 23 14:49:22.263: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 23 14:49:22.263: INFO: sonobuoy-e2e-job-381779db02e94c8b from sonobuoy started at 2020-12-23 13:24:46 +0000 UTC (2 container statuses recorded)
Dec 23 14:49:22.263: INFO: 	Container e2e ready: true, restart count 0
Dec 23 14:49:22.263: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 14:49:22.263: INFO: sonobuoy-systemd-logs-daemon-set-a3aba41d66fb4680-2x864 from sonobuoy started at 2020-12-23 13:24:46 +0000 UTC (2 container statuses recorded)
Dec 23 14:49:22.263: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Dec 23 14:49:22.263: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 14:49:22.263: INFO: 
Logging pods the apiserver thinks is on node wt-k8s-3.novalocal before test
Dec 23 14:49:22.267: INFO: calico-node-lz86h from kube-system started at 2020-12-23 08:18:23 +0000 UTC (1 container statuses recorded)
Dec 23 14:49:22.267: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 14:49:22.267: INFO: kube-proxy-pnq9z from kube-system started at 2020-12-23 08:14:45 +0000 UTC (1 container statuses recorded)
Dec 23 14:49:22.267: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 23 14:49:22.267: INFO: sonobuoy-systemd-logs-daemon-set-a3aba41d66fb4680-n5vgr from sonobuoy started at 2020-12-23 13:24:32 +0000 UTC (2 container statuses recorded)
Dec 23 14:49:22.267: INFO: 	Container sonobuoy-worker ready: false, restart count 11
Dec 23 14:49:22.267: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 14:49:22.267: INFO: svc-latency-rc-tmhdn from svc-latency-6674 started at 2020-12-23 14:48:56 +0000 UTC (1 container statuses recorded)
Dec 23 14:49:22.267: INFO: 	Container svc-latency-rc ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node wt-k8s-2
STEP: verifying the node has the label node wt-k8s-3.novalocal
Dec 23 14:49:22.310: INFO: Pod calico-kube-controllers-744cfdf676-fbnzz requesting resource cpu=0m on Node wt-k8s-2
Dec 23 14:49:22.310: INFO: Pod calico-node-5czbz requesting resource cpu=250m on Node wt-k8s-2
Dec 23 14:49:22.310: INFO: Pod calico-node-lz86h requesting resource cpu=250m on Node wt-k8s-3.novalocal
Dec 23 14:49:22.310: INFO: Pod kube-proxy-55qvq requesting resource cpu=0m on Node wt-k8s-2
Dec 23 14:49:22.310: INFO: Pod kube-proxy-pnq9z requesting resource cpu=0m on Node wt-k8s-3.novalocal
Dec 23 14:49:22.310: INFO: Pod sonobuoy requesting resource cpu=0m on Node wt-k8s-2
Dec 23 14:49:22.310: INFO: Pod sonobuoy-e2e-job-381779db02e94c8b requesting resource cpu=0m on Node wt-k8s-2
Dec 23 14:49:22.310: INFO: Pod sonobuoy-systemd-logs-daemon-set-a3aba41d66fb4680-2x864 requesting resource cpu=0m on Node wt-k8s-2
Dec 23 14:49:22.310: INFO: Pod sonobuoy-systemd-logs-daemon-set-a3aba41d66fb4680-n5vgr requesting resource cpu=0m on Node wt-k8s-3.novalocal
Dec 23 14:49:22.310: INFO: Pod svc-latency-rc-tmhdn requesting resource cpu=0m on Node wt-k8s-3.novalocal
STEP: Starting Pods to consume most of the cluster CPU.
Dec 23 14:49:22.310: INFO: Creating a pod which consumes cpu=1225m on Node wt-k8s-3.novalocal
Dec 23 14:49:22.315: INFO: Creating a pod which consumes cpu=1225m on Node wt-k8s-2
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c9368ba-4872-4292-ba08-8eed69ff1220.16535fe980c8629c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5462/filler-pod-8c9368ba-4872-4292-ba08-8eed69ff1220 to wt-k8s-3.novalocal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c9368ba-4872-4292-ba08-8eed69ff1220.16535fe9cb7847cd], Reason = [Pulled], Message = [Container image "abcsys.cn:5000/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c9368ba-4872-4292-ba08-8eed69ff1220.16535fe9cf4a015a], Reason = [Created], Message = [Created container filler-pod-8c9368ba-4872-4292-ba08-8eed69ff1220]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c9368ba-4872-4292-ba08-8eed69ff1220.16535fe9da1505c5], Reason = [Started], Message = [Started container filler-pod-8c9368ba-4872-4292-ba08-8eed69ff1220]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d536100a-1ec8-4c69-9d52-53af47ad3e01.16535fe980dc32d7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5462/filler-pod-d536100a-1ec8-4c69-9d52-53af47ad3e01 to wt-k8s-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d536100a-1ec8-4c69-9d52-53af47ad3e01.16535fed058f702d], Reason = [Pulled], Message = [Container image "abcsys.cn:5000/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d536100a-1ec8-4c69-9d52-53af47ad3e01.16535fed09421e08], Reason = [Created], Message = [Created container filler-pod-d536100a-1ec8-4c69-9d52-53af47ad3e01]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d536100a-1ec8-4c69-9d52-53af47ad3e01.16535fed12777c73], Reason = [Started], Message = [Started container filler-pod-d536100a-1ec8-4c69-9d52-53af47ad3e01]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16535fea72ba0ab0], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node wt-k8s-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node wt-k8s-3.novalocal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:49:27.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5462" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:5.653 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":214,"skipped":3925,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:49:27.553: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:49:31.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2473" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":215,"skipped":3943,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:49:31.740: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 23 14:49:36.991: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:49:38.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8797" for this suite.

• [SLOW TEST:6.348 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":216,"skipped":3945,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:49:38.087: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-bjs4
STEP: Creating a pod to test atomic-volume-subpath
Dec 23 14:49:38.326: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-bjs4" in namespace "subpath-7591" to be "Succeeded or Failed"
Dec 23 14:49:38.337: INFO: Pod "pod-subpath-test-projected-bjs4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.116412ms
Dec 23 14:49:40.349: INFO: Pod "pod-subpath-test-projected-bjs4": Phase="Running", Reason="", readiness=true. Elapsed: 2.022715233s
Dec 23 14:49:42.361: INFO: Pod "pod-subpath-test-projected-bjs4": Phase="Running", Reason="", readiness=true. Elapsed: 4.034980411s
Dec 23 14:49:44.369: INFO: Pod "pod-subpath-test-projected-bjs4": Phase="Running", Reason="", readiness=true. Elapsed: 6.043265769s
Dec 23 14:49:46.377: INFO: Pod "pod-subpath-test-projected-bjs4": Phase="Running", Reason="", readiness=true. Elapsed: 8.050764274s
Dec 23 14:49:48.384: INFO: Pod "pod-subpath-test-projected-bjs4": Phase="Running", Reason="", readiness=true. Elapsed: 10.057893179s
Dec 23 14:49:50.388: INFO: Pod "pod-subpath-test-projected-bjs4": Phase="Running", Reason="", readiness=true. Elapsed: 12.062550538s
Dec 23 14:49:52.398: INFO: Pod "pod-subpath-test-projected-bjs4": Phase="Running", Reason="", readiness=true. Elapsed: 14.072540555s
Dec 23 14:49:54.403: INFO: Pod "pod-subpath-test-projected-bjs4": Phase="Running", Reason="", readiness=true. Elapsed: 16.077444787s
Dec 23 14:49:56.411: INFO: Pod "pod-subpath-test-projected-bjs4": Phase="Running", Reason="", readiness=true. Elapsed: 18.085029047s
Dec 23 14:49:58.419: INFO: Pod "pod-subpath-test-projected-bjs4": Phase="Running", Reason="", readiness=true. Elapsed: 20.093410607s
Dec 23 14:50:00.435: INFO: Pod "pod-subpath-test-projected-bjs4": Phase="Running", Reason="", readiness=true. Elapsed: 22.109663813s
Dec 23 14:50:02.444: INFO: Pod "pod-subpath-test-projected-bjs4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.118146961s
STEP: Saw pod success
Dec 23 14:50:02.444: INFO: Pod "pod-subpath-test-projected-bjs4" satisfied condition "Succeeded or Failed"
Dec 23 14:50:02.446: INFO: Trying to get logs from node wt-k8s-2 pod pod-subpath-test-projected-bjs4 container test-container-subpath-projected-bjs4: <nil>
STEP: delete the pod
Dec 23 14:50:02.485: INFO: Waiting for pod pod-subpath-test-projected-bjs4 to disappear
Dec 23 14:50:02.488: INFO: Pod pod-subpath-test-projected-bjs4 no longer exists
STEP: Deleting pod pod-subpath-test-projected-bjs4
Dec 23 14:50:02.488: INFO: Deleting pod "pod-subpath-test-projected-bjs4" in namespace "subpath-7591"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:50:02.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7591" for this suite.

• [SLOW TEST:24.412 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":217,"skipped":3948,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:50:02.499: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1223 14:50:04.095901      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 23 14:51:06.131: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:51:06.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7985" for this suite.

• [SLOW TEST:63.653 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":218,"skipped":3949,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:51:06.152: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 23 14:51:06.212: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1368  4b5e13c3-24e2-48ef-89ed-81690bcf23dc 102284 0 2020-12-23 14:50:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-23 14:50:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 23 14:51:06.213: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1368  4b5e13c3-24e2-48ef-89ed-81690bcf23dc 102285 0 2020-12-23 14:50:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-23 14:50:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 23 14:51:06.213: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1368  4b5e13c3-24e2-48ef-89ed-81690bcf23dc 102286 0 2020-12-23 14:50:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-23 14:50:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 23 14:51:16.237: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1368  4b5e13c3-24e2-48ef-89ed-81690bcf23dc 102335 0 2020-12-23 14:50:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-23 14:50:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 23 14:51:16.237: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1368  4b5e13c3-24e2-48ef-89ed-81690bcf23dc 102336 0 2020-12-23 14:50:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-23 14:50:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 23 14:51:16.238: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1368  4b5e13c3-24e2-48ef-89ed-81690bcf23dc 102337 0 2020-12-23 14:50:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-23 14:50:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:51:16.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1368" for this suite.

• [SLOW TEST:10.094 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":219,"skipped":3958,"failed":0}
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:51:16.246: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Dec 23 14:51:16.284: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Dec 23 14:51:16.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-620 create -f -'
Dec 23 14:51:16.861: INFO: stderr: ""
Dec 23 14:51:16.861: INFO: stdout: "service/agnhost-replica created\n"
Dec 23 14:51:16.861: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Dec 23 14:51:16.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-620 create -f -'
Dec 23 14:51:17.302: INFO: stderr: ""
Dec 23 14:51:17.302: INFO: stdout: "service/agnhost-primary created\n"
Dec 23 14:51:17.303: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 23 14:51:17.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-620 create -f -'
Dec 23 14:51:17.695: INFO: stderr: ""
Dec 23 14:51:17.695: INFO: stdout: "service/frontend created\n"
Dec 23 14:51:17.696: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: abcsys.cn:5000/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Dec 23 14:51:17.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-620 create -f -'
Dec 23 14:51:18.131: INFO: stderr: ""
Dec 23 14:51:18.131: INFO: stdout: "deployment.apps/frontend created\n"
Dec 23 14:51:18.131: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: abcsys.cn:5000/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 23 14:51:18.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-620 create -f -'
Dec 23 14:51:18.633: INFO: stderr: ""
Dec 23 14:51:18.633: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Dec 23 14:51:18.633: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: abcsys.cn:5000/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 23 14:51:18.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-620 create -f -'
Dec 23 14:51:19.097: INFO: stderr: ""
Dec 23 14:51:19.097: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Dec 23 14:51:19.097: INFO: Waiting for all frontend pods to be Running.
Dec 23 14:51:24.147: INFO: Waiting for frontend to serve content.
Dec 23 14:51:24.160: INFO: Trying to add a new entry to the guestbook.
Dec 23 14:51:24.176: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec 23 14:51:24.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-620 delete --grace-period=0 --force -f -'
Dec 23 14:51:24.332: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 14:51:24.332: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Dec 23 14:51:24.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-620 delete --grace-period=0 --force -f -'
Dec 23 14:51:24.533: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 14:51:24.533: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Dec 23 14:51:24.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-620 delete --grace-period=0 --force -f -'
Dec 23 14:51:24.705: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 14:51:24.705: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 23 14:51:24.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-620 delete --grace-period=0 --force -f -'
Dec 23 14:51:24.838: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 14:51:24.839: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 23 14:51:24.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-620 delete --grace-period=0 --force -f -'
Dec 23 14:51:24.959: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 14:51:24.959: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Dec 23 14:51:24.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-620 delete --grace-period=0 --force -f -'
Dec 23 14:51:25.070: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 14:51:25.070: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:51:25.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-620" for this suite.

• [SLOW TEST:8.850 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":220,"skipped":3958,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:51:25.109: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9852
STEP: creating service affinity-nodeport in namespace services-9852
STEP: creating replication controller affinity-nodeport in namespace services-9852
I1223 14:51:25.197303      20 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-9852, replica count: 3
I1223 14:51:28.247860      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 23 14:51:28.259: INFO: Creating new exec pod
Dec 23 14:51:33.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-9852 exec execpod-affinity289rq -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Dec 23 14:51:33.493: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Dec 23 14:51:33.493: INFO: stdout: ""
Dec 23 14:51:33.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-9852 exec execpod-affinity289rq -- /bin/sh -x -c nc -zv -t -w 2 10.102.59.149 80'
Dec 23 14:51:33.695: INFO: stderr: "+ nc -zv -t -w 2 10.102.59.149 80\nConnection to 10.102.59.149 80 port [tcp/http] succeeded!\n"
Dec 23 14:51:33.695: INFO: stdout: ""
Dec 23 14:51:33.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-9852 exec execpod-affinity289rq -- /bin/sh -x -c nc -zv -t -w 2 10.22.19.17 30605'
Dec 23 14:51:33.884: INFO: stderr: "+ nc -zv -t -w 2 10.22.19.17 30605\nConnection to 10.22.19.17 30605 port [tcp/30605] succeeded!\n"
Dec 23 14:51:33.884: INFO: stdout: ""
Dec 23 14:51:33.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-9852 exec execpod-affinity289rq -- /bin/sh -x -c nc -zv -t -w 2 10.22.19.25 30605'
Dec 23 14:51:34.100: INFO: stderr: "+ nc -zv -t -w 2 10.22.19.25 30605\nConnection to 10.22.19.25 30605 port [tcp/30605] succeeded!\n"
Dec 23 14:51:34.100: INFO: stdout: ""
Dec 23 14:51:34.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-9852 exec execpod-affinity289rq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.22.19.17:30605/ ; done'
Dec 23 14:51:34.410: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:30605/\n"
Dec 23 14:51:34.410: INFO: stdout: "\naffinity-nodeport-m52zl\naffinity-nodeport-m52zl\naffinity-nodeport-m52zl\naffinity-nodeport-m52zl\naffinity-nodeport-m52zl\naffinity-nodeport-m52zl\naffinity-nodeport-m52zl\naffinity-nodeport-m52zl\naffinity-nodeport-m52zl\naffinity-nodeport-m52zl\naffinity-nodeport-m52zl\naffinity-nodeport-m52zl\naffinity-nodeport-m52zl\naffinity-nodeport-m52zl\naffinity-nodeport-m52zl\naffinity-nodeport-m52zl"
Dec 23 14:51:34.410: INFO: Received response from host: affinity-nodeport-m52zl
Dec 23 14:51:34.410: INFO: Received response from host: affinity-nodeport-m52zl
Dec 23 14:51:34.410: INFO: Received response from host: affinity-nodeport-m52zl
Dec 23 14:51:34.410: INFO: Received response from host: affinity-nodeport-m52zl
Dec 23 14:51:34.410: INFO: Received response from host: affinity-nodeport-m52zl
Dec 23 14:51:34.410: INFO: Received response from host: affinity-nodeport-m52zl
Dec 23 14:51:34.410: INFO: Received response from host: affinity-nodeport-m52zl
Dec 23 14:51:34.410: INFO: Received response from host: affinity-nodeport-m52zl
Dec 23 14:51:34.410: INFO: Received response from host: affinity-nodeport-m52zl
Dec 23 14:51:34.410: INFO: Received response from host: affinity-nodeport-m52zl
Dec 23 14:51:34.410: INFO: Received response from host: affinity-nodeport-m52zl
Dec 23 14:51:34.410: INFO: Received response from host: affinity-nodeport-m52zl
Dec 23 14:51:34.410: INFO: Received response from host: affinity-nodeport-m52zl
Dec 23 14:51:34.410: INFO: Received response from host: affinity-nodeport-m52zl
Dec 23 14:51:34.410: INFO: Received response from host: affinity-nodeport-m52zl
Dec 23 14:51:34.410: INFO: Received response from host: affinity-nodeport-m52zl
Dec 23 14:51:34.410: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9852, will wait for the garbage collector to delete the pods
Dec 23 14:51:34.500: INFO: Deleting ReplicationController affinity-nodeport took: 6.774227ms
Dec 23 14:51:35.101: INFO: Terminating ReplicationController affinity-nodeport pods took: 600.199972ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:52:08.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9852" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:43.021 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":221,"skipped":3996,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:52:08.131: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:52:08.169: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 23 14:52:08.177: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 23 14:52:13.193: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 23 14:52:13.194: INFO: Creating deployment "test-rolling-update-deployment"
Dec 23 14:52:13.198: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 23 14:52:13.227: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Dec 23 14:52:15.235: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 23 14:52:15.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744331919, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744331919, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744331919, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744331919, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-594f55c77f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 14:52:17.245: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Dec 23 14:52:17.253: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3900  d228a40f-d409-4146-8f74-b6f24bfc8f40 102907 1 2020-12-23 14:51:59 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-12-23 14:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-23 14:52:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost abcsys.cn:5000/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00073f258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-12-23 14:51:59 +0000 UTC,LastTransitionTime:2020-12-23 14:51:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-594f55c77f" has successfully progressed.,LastUpdateTime:2020-12-23 14:52:01 +0000 UTC,LastTransitionTime:2020-12-23 14:51:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 23 14:52:17.256: INFO: New ReplicaSet "test-rolling-update-deployment-594f55c77f" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-594f55c77f  deployment-3900  94d96cc0-a4a8-45f2-a3f4-d2da66569034 102896 1 2020-12-23 14:51:59 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:594f55c77f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d228a40f-d409-4146-8f74-b6f24bfc8f40 0xc005f638d7 0xc005f638d8}] []  [{kube-controller-manager Update apps/v1 2020-12-23 14:52:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d228a40f-d409-4146-8f74-b6f24bfc8f40\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 594f55c77f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:594f55c77f] map[] [] []  []} {[] [] [{agnhost abcsys.cn:5000/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f63968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 23 14:52:17.256: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 23 14:52:17.256: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3900  1dda6bd5-e05d-4660-8b87-cbf22ac6510f 102905 2 2020-12-23 14:51:54 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d228a40f-d409-4146-8f74-b6f24bfc8f40 0xc005f637c7 0xc005f637c8}] []  [{e2e.test Update apps/v1 2020-12-23 14:51:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-12-23 14:52:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d228a40f-d409-4146-8f74-b6f24bfc8f40\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd abcsys.cn:5000/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005f63868 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 23 14:52:17.259: INFO: Pod "test-rolling-update-deployment-594f55c77f-vlfr6" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-594f55c77f-vlfr6 test-rolling-update-deployment-594f55c77f- deployment-3900  e36b4671-afc5-4821-bf12-448f3c4d11a8 102895 0 2020-12-23 14:51:59 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:594f55c77f] map[cni.projectcalico.org/podIP:10.244.201.203/32 cni.projectcalico.org/podIPs:10.244.201.203/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-594f55c77f 94d96cc0-a4a8-45f2-a3f4-d2da66569034 0xc005f63d87 0xc005f63d88}] []  [{kube-controller-manager Update v1 2020-12-23 14:51:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94d96cc0-a4a8-45f2-a3f4-d2da66569034\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-12-23 14:52:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-12-23 14:52:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.201.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kcvtr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kcvtr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:abcsys.cn:5000/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kcvtr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wt-k8s-3.novalocal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:51:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:52:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:52:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-23 14:51:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.22.19.25,PodIP:10.244.201.203,StartTime:2020-12-23 14:51:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-23 14:52:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:abcsys.cn:5000/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://abcsys.cn:5000/e2e-test-images/agnhost@sha256:e23f72f469927ead04b7bb2a5cedf8569906c38ced37a64767542e7d22831d45,ContainerID:docker://315cd6ac429dc80deaff01eb3d2ecb955d1a357350542ed1ba5bf824fbfd99a1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.201.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:52:17.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3900" for this suite.

• [SLOW TEST:9.138 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":222,"skipped":4000,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:52:17.269: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-de29ac1b-beed-4cc1-be79-c6989b1929c9
STEP: Creating secret with name s-test-opt-upd-a126265d-ffa1-4812-93b0-a7540486fc9d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-de29ac1b-beed-4cc1-be79-c6989b1929c9
STEP: Updating secret s-test-opt-upd-a126265d-ffa1-4812-93b0-a7540486fc9d
STEP: Creating secret with name s-test-opt-create-0d27582d-0333-46d1-8430-3947159cfae4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:52:21.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-842" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":223,"skipped":4003,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:52:21.449: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Dec 23 14:52:21.498: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 23 14:53:21.527: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Dec 23 14:53:21.556: INFO: Created pod: pod0-sched-preemption-low-priority
Dec 23 14:53:21.593: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:54:09.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-403" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:108.274 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":224,"skipped":4036,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:54:09.724: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:54:09.777: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-5bf7aefe-01d3-462e-a6f1-b2dced76234b" in namespace "security-context-test-352" to be "Succeeded or Failed"
Dec 23 14:54:09.787: INFO: Pod "alpine-nnp-false-5bf7aefe-01d3-462e-a6f1-b2dced76234b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.204184ms
Dec 23 14:54:11.793: INFO: Pod "alpine-nnp-false-5bf7aefe-01d3-462e-a6f1-b2dced76234b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015685697s
Dec 23 14:54:13.801: INFO: Pod "alpine-nnp-false-5bf7aefe-01d3-462e-a6f1-b2dced76234b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023699569s
Dec 23 14:54:13.801: INFO: Pod "alpine-nnp-false-5bf7aefe-01d3-462e-a6f1-b2dced76234b" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:54:13.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-352" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":225,"skipped":4056,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:54:13.827: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-5b06a374-8398-4713-88e2-e458e0e86f0c
STEP: Creating a pod to test consume secrets
Dec 23 14:54:13.876: INFO: Waiting up to 5m0s for pod "pod-secrets-cd24c0fa-427a-4f26-8088-0984eb5fc149" in namespace "secrets-7326" to be "Succeeded or Failed"
Dec 23 14:54:13.879: INFO: Pod "pod-secrets-cd24c0fa-427a-4f26-8088-0984eb5fc149": Phase="Pending", Reason="", readiness=false. Elapsed: 3.015968ms
Dec 23 14:54:15.886: INFO: Pod "pod-secrets-cd24c0fa-427a-4f26-8088-0984eb5fc149": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010184363s
Dec 23 14:54:17.894: INFO: Pod "pod-secrets-cd24c0fa-427a-4f26-8088-0984eb5fc149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018703192s
STEP: Saw pod success
Dec 23 14:54:17.894: INFO: Pod "pod-secrets-cd24c0fa-427a-4f26-8088-0984eb5fc149" satisfied condition "Succeeded or Failed"
Dec 23 14:54:17.897: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-secrets-cd24c0fa-427a-4f26-8088-0984eb5fc149 container secret-volume-test: <nil>
STEP: delete the pod
Dec 23 14:54:17.919: INFO: Waiting for pod pod-secrets-cd24c0fa-427a-4f26-8088-0984eb5fc149 to disappear
Dec 23 14:54:17.921: INFO: Pod pod-secrets-cd24c0fa-427a-4f26-8088-0984eb5fc149 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:54:17.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7326" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":226,"skipped":4072,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:54:17.928: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image abcsys.cn:5000/library/httpd:2.4.38-alpine
Dec 23 14:54:17.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-7240 run e2e-test-httpd-pod --restart=Never --image=abcsys.cn:5000/library/httpd:2.4.38-alpine'
Dec 23 14:54:18.093: INFO: stderr: ""
Dec 23 14:54:18.093: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Dec 23 14:54:18.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-7240 delete pods e2e-test-httpd-pod'
Dec 23 14:55:19.256: INFO: stderr: ""
Dec 23 14:55:19.256: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:55:19.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7240" for this suite.

• [SLOW TEST:61.337 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":227,"skipped":4091,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:55:19.268: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4051
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-4051
Dec 23 14:55:19.332: INFO: Found 0 stateful pods, waiting for 1
Dec 23 14:55:29.345: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 23 14:55:29.376: INFO: Deleting all statefulset in ns statefulset-4051
Dec 23 14:55:29.385: INFO: Scaling statefulset ss to 0
Dec 23 14:55:49.432: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 14:55:49.434: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:55:49.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4051" for this suite.

• [SLOW TEST:30.206 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":228,"skipped":4169,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:55:49.475: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-845
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-845 to expose endpoints map[]
Dec 23 14:55:49.579: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Dec 23 14:55:50.591: INFO: successfully validated that service multi-endpoint-test in namespace services-845 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-845
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-845 to expose endpoints map[pod1:[100]]
Dec 23 14:55:52.629: INFO: successfully validated that service multi-endpoint-test in namespace services-845 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-845
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-845 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 23 14:55:55.667: INFO: successfully validated that service multi-endpoint-test in namespace services-845 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-845
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-845 to expose endpoints map[pod2:[101]]
Dec 23 14:55:55.719: INFO: successfully validated that service multi-endpoint-test in namespace services-845 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-845
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-845 to expose endpoints map[]
Dec 23 14:55:55.761: INFO: successfully validated that service multi-endpoint-test in namespace services-845 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:55:55.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-845" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.345 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":229,"skipped":4172,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:55:55.819: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:55:55.881: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:55:56.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7308" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":230,"skipped":4175,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:55:56.950: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-ce4b8761-0ada-4fbf-b9c7-c2726ea86988
STEP: Creating a pod to test consume secrets
Dec 23 14:55:57.098: INFO: Waiting up to 5m0s for pod "pod-secrets-2e8d58b0-3d3d-442e-bc0a-1b1c44a41f27" in namespace "secrets-8657" to be "Succeeded or Failed"
Dec 23 14:55:57.106: INFO: Pod "pod-secrets-2e8d58b0-3d3d-442e-bc0a-1b1c44a41f27": Phase="Pending", Reason="", readiness=false. Elapsed: 7.779158ms
Dec 23 14:55:59.115: INFO: Pod "pod-secrets-2e8d58b0-3d3d-442e-bc0a-1b1c44a41f27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016620017s
STEP: Saw pod success
Dec 23 14:55:59.115: INFO: Pod "pod-secrets-2e8d58b0-3d3d-442e-bc0a-1b1c44a41f27" satisfied condition "Succeeded or Failed"
Dec 23 14:55:59.117: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-secrets-2e8d58b0-3d3d-442e-bc0a-1b1c44a41f27 container secret-volume-test: <nil>
STEP: delete the pod
Dec 23 14:55:59.149: INFO: Waiting for pod pod-secrets-2e8d58b0-3d3d-442e-bc0a-1b1c44a41f27 to disappear
Dec 23 14:55:59.152: INFO: Pod pod-secrets-2e8d58b0-3d3d-442e-bc0a-1b1c44a41f27 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:55:59.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8657" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":231,"skipped":4186,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:55:59.160: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-e1506638-c2c6-4271-813a-716a0281a743
STEP: Creating a pod to test consume configMaps
Dec 23 14:55:59.377: INFO: Waiting up to 5m0s for pod "pod-configmaps-fd216cf5-424b-44f2-ab2d-3c6cd4fd1372" in namespace "configmap-3686" to be "Succeeded or Failed"
Dec 23 14:55:59.405: INFO: Pod "pod-configmaps-fd216cf5-424b-44f2-ab2d-3c6cd4fd1372": Phase="Pending", Reason="", readiness=false. Elapsed: 27.698945ms
Dec 23 14:56:01.412: INFO: Pod "pod-configmaps-fd216cf5-424b-44f2-ab2d-3c6cd4fd1372": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035030135s
Dec 23 14:56:03.422: INFO: Pod "pod-configmaps-fd216cf5-424b-44f2-ab2d-3c6cd4fd1372": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044788673s
STEP: Saw pod success
Dec 23 14:56:03.422: INFO: Pod "pod-configmaps-fd216cf5-424b-44f2-ab2d-3c6cd4fd1372" satisfied condition "Succeeded or Failed"
Dec 23 14:56:03.424: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-configmaps-fd216cf5-424b-44f2-ab2d-3c6cd4fd1372 container agnhost-container: <nil>
STEP: delete the pod
Dec 23 14:56:03.445: INFO: Waiting for pod pod-configmaps-fd216cf5-424b-44f2-ab2d-3c6cd4fd1372 to disappear
Dec 23 14:56:03.447: INFO: Pod pod-configmaps-fd216cf5-424b-44f2-ab2d-3c6cd4fd1372 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:56:03.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3686" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":232,"skipped":4189,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:56:03.456: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:56:32.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1261" for this suite.
STEP: Destroying namespace "nsdeletetest-7030" for this suite.
Dec 23 14:56:32.605: INFO: Namespace nsdeletetest-7030 was already deleted
STEP: Destroying namespace "nsdeletetest-2073" for this suite.

• [SLOW TEST:29.155 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":233,"skipped":4200,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:56:32.611: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Dec 23 14:56:33.169: INFO: created pod pod-service-account-defaultsa
Dec 23 14:56:33.170: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 23 14:56:33.175: INFO: created pod pod-service-account-mountsa
Dec 23 14:56:33.175: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 23 14:56:33.179: INFO: created pod pod-service-account-nomountsa
Dec 23 14:56:33.180: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 23 14:56:33.195: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 23 14:56:33.195: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 23 14:56:33.207: INFO: created pod pod-service-account-mountsa-mountspec
Dec 23 14:56:33.207: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 23 14:56:33.218: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 23 14:56:33.218: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 23 14:56:33.233: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 23 14:56:33.233: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 23 14:56:33.250: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 23 14:56:33.250: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 23 14:56:33.286: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 23 14:56:33.286: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:56:33.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3505" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":234,"skipped":4201,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:56:33.311: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-dc0afd96-1a1d-483f-b15b-587edbf77a55
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:56:37.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7211" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":235,"skipped":4207,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:56:37.443: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-bce1e619-5a30-4d55-8c15-34f33ce51654
STEP: Creating a pod to test consume secrets
Dec 23 14:56:37.485: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dc673c66-4399-4409-92a9-21aad877a9bf" in namespace "projected-9511" to be "Succeeded or Failed"
Dec 23 14:56:37.494: INFO: Pod "pod-projected-secrets-dc673c66-4399-4409-92a9-21aad877a9bf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.945242ms
Dec 23 14:56:39.503: INFO: Pod "pod-projected-secrets-dc673c66-4399-4409-92a9-21aad877a9bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017654754s
Dec 23 14:56:41.511: INFO: Pod "pod-projected-secrets-dc673c66-4399-4409-92a9-21aad877a9bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026084853s
STEP: Saw pod success
Dec 23 14:56:41.511: INFO: Pod "pod-projected-secrets-dc673c66-4399-4409-92a9-21aad877a9bf" satisfied condition "Succeeded or Failed"
Dec 23 14:56:41.513: INFO: Trying to get logs from node wt-k8s-2 pod pod-projected-secrets-dc673c66-4399-4409-92a9-21aad877a9bf container secret-volume-test: <nil>
STEP: delete the pod
Dec 23 14:56:41.546: INFO: Waiting for pod pod-projected-secrets-dc673c66-4399-4409-92a9-21aad877a9bf to disappear
Dec 23 14:56:41.554: INFO: Pod pod-projected-secrets-dc673c66-4399-4409-92a9-21aad877a9bf no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:56:41.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9511" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":236,"skipped":4220,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:56:41.564: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Dec 23 14:56:41.608: INFO: Waiting up to 5m0s for pod "var-expansion-cde63fb0-dad4-4d33-9ed1-cde27f96b8d8" in namespace "var-expansion-4496" to be "Succeeded or Failed"
Dec 23 14:56:41.611: INFO: Pod "var-expansion-cde63fb0-dad4-4d33-9ed1-cde27f96b8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.289674ms
Dec 23 14:56:43.620: INFO: Pod "var-expansion-cde63fb0-dad4-4d33-9ed1-cde27f96b8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01189401s
Dec 23 14:56:45.625: INFO: Pod "var-expansion-cde63fb0-dad4-4d33-9ed1-cde27f96b8d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016852783s
STEP: Saw pod success
Dec 23 14:56:45.625: INFO: Pod "var-expansion-cde63fb0-dad4-4d33-9ed1-cde27f96b8d8" satisfied condition "Succeeded or Failed"
Dec 23 14:56:45.627: INFO: Trying to get logs from node wt-k8s-2 pod var-expansion-cde63fb0-dad4-4d33-9ed1-cde27f96b8d8 container dapi-container: <nil>
STEP: delete the pod
Dec 23 14:56:45.655: INFO: Waiting for pod var-expansion-cde63fb0-dad4-4d33-9ed1-cde27f96b8d8 to disappear
Dec 23 14:56:45.658: INFO: Pod var-expansion-cde63fb0-dad4-4d33-9ed1-cde27f96b8d8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:56:45.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4496" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":237,"skipped":4235,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:56:45.673: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 14:56:45.734: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b77f14a-bd8d-4caa-a8ad-f2d798a2e4f8" in namespace "projected-9963" to be "Succeeded or Failed"
Dec 23 14:56:45.738: INFO: Pod "downwardapi-volume-2b77f14a-bd8d-4caa-a8ad-f2d798a2e4f8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.912084ms
Dec 23 14:56:47.746: INFO: Pod "downwardapi-volume-2b77f14a-bd8d-4caa-a8ad-f2d798a2e4f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011544912s
Dec 23 14:56:49.754: INFO: Pod "downwardapi-volume-2b77f14a-bd8d-4caa-a8ad-f2d798a2e4f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020412778s
STEP: Saw pod success
Dec 23 14:56:49.755: INFO: Pod "downwardapi-volume-2b77f14a-bd8d-4caa-a8ad-f2d798a2e4f8" satisfied condition "Succeeded or Failed"
Dec 23 14:56:49.758: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-2b77f14a-bd8d-4caa-a8ad-f2d798a2e4f8 container client-container: <nil>
STEP: delete the pod
Dec 23 14:56:49.780: INFO: Waiting for pod downwardapi-volume-2b77f14a-bd8d-4caa-a8ad-f2d798a2e4f8 to disappear
Dec 23 14:56:49.782: INFO: Pod downwardapi-volume-2b77f14a-bd8d-4caa-a8ad-f2d798a2e4f8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:56:49.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9963" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":238,"skipped":4240,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:56:49.792: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Dec 23 14:56:49.856: INFO: Waiting up to 5m0s for pod "downward-api-da5d9767-63af-4ed6-856f-6104a5119637" in namespace "downward-api-725" to be "Succeeded or Failed"
Dec 23 14:56:49.869: INFO: Pod "downward-api-da5d9767-63af-4ed6-856f-6104a5119637": Phase="Pending", Reason="", readiness=false. Elapsed: 12.298613ms
Dec 23 14:56:51.877: INFO: Pod "downward-api-da5d9767-63af-4ed6-856f-6104a5119637": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020451831s
Dec 23 14:56:53.887: INFO: Pod "downward-api-da5d9767-63af-4ed6-856f-6104a5119637": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030274803s
STEP: Saw pod success
Dec 23 14:56:53.887: INFO: Pod "downward-api-da5d9767-63af-4ed6-856f-6104a5119637" satisfied condition "Succeeded or Failed"
Dec 23 14:56:53.890: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downward-api-da5d9767-63af-4ed6-856f-6104a5119637 container dapi-container: <nil>
STEP: delete the pod
Dec 23 14:56:53.912: INFO: Waiting for pod downward-api-da5d9767-63af-4ed6-856f-6104a5119637 to disappear
Dec 23 14:56:53.917: INFO: Pod downward-api-da5d9767-63af-4ed6-856f-6104a5119637 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:56:53.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-725" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":239,"skipped":4268,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:56:53.925: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-ff66ba1f-0996-4341-991a-534afaea2fa9
STEP: Creating a pod to test consume configMaps
Dec 23 14:56:53.975: INFO: Waiting up to 5m0s for pod "pod-configmaps-292f9c32-586b-45ac-b037-6c2546288ccd" in namespace "configmap-9418" to be "Succeeded or Failed"
Dec 23 14:56:53.977: INFO: Pod "pod-configmaps-292f9c32-586b-45ac-b037-6c2546288ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.335331ms
Dec 23 14:56:55.982: INFO: Pod "pod-configmaps-292f9c32-586b-45ac-b037-6c2546288ccd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00672052s
STEP: Saw pod success
Dec 23 14:56:55.982: INFO: Pod "pod-configmaps-292f9c32-586b-45ac-b037-6c2546288ccd" satisfied condition "Succeeded or Failed"
Dec 23 14:56:55.984: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-configmaps-292f9c32-586b-45ac-b037-6c2546288ccd container agnhost-container: <nil>
STEP: delete the pod
Dec 23 14:56:56.014: INFO: Waiting for pod pod-configmaps-292f9c32-586b-45ac-b037-6c2546288ccd to disappear
Dec 23 14:56:56.017: INFO: Pod pod-configmaps-292f9c32-586b-45ac-b037-6c2546288ccd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:56:56.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9418" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":240,"skipped":4273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:56:56.026: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:56:56.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7529" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":241,"skipped":4302,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:56:56.160: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 23 14:56:56.981: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 23 14:56:58.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744332203, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744332203, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744332203, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744332203, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-76bbb7fdd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 14:57:02.088: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
Dec 23 14:57:02.117: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:02.234: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:02.331: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:02.436: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:02.535: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:02.639: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:02.731: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:02.832: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:02.932: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:03.032: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:03.132: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:03.230: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:03.334: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:03.432: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:03.533: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:03.631: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:03.731: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:03.831: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:04.096: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:04.132: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:04.238: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:04.344: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:04.434: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:04.531: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:04.632: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:04.730: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:04.831: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:04.931: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:05.050: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:05.134: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:05.232: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:05.330: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:05.438: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:05.531: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:05.632: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:05.732: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:05.834: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:05.930: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:06.034: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:06.131: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:06.230: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:06.330: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:06.433: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:06.678: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:06.733: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:06.829: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:06.938: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:07.032: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:07.133: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:07.231: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:07.331: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:07.430: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:07.531: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:07.632: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:07.733: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:07.835: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:07.931: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:08.031: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:08.134: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:08.235: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:08.332: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:08.430: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:08.536: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:08.636: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:08.735: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:08.833: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:08.932: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:09.034: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:09.130: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:09.231: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:09.333: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:09.429: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:09.530: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:09.630: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:09.735: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:09.832: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:09.932: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:57:20.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6609" for this suite.
STEP: Destroying namespace "webhook-6609-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:24.073 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":242,"skipped":4306,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:57:20.234: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:57:20.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1984" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":243,"skipped":4318,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:57:20.319: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:57:23.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2309" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":244,"skipped":4328,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:57:23.637: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1676.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1676.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1676.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1676.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1676.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1676.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 23 14:57:25.743: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1676/dns-test-689de5a4-1a9b-4d0e-a964-6b0f7be9adfd: the server could not find the requested resource (get pods dns-test-689de5a4-1a9b-4d0e-a964-6b0f7be9adfd)
Dec 23 14:57:25.743: INFO: Lookups using dns-1676/dns-test-689de5a4-1a9b-4d0e-a964-6b0f7be9adfd failed for: [jessie_tcp@PodARecord]

Dec 23 14:57:30.771: INFO: DNS probes using dns-1676/dns-test-689de5a4-1a9b-4d0e-a964-6b0f7be9adfd succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:57:30.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1676" for this suite.

• [SLOW TEST:7.209 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":245,"skipped":4331,"failed":0}
SSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:57:30.846: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:57:34.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5685" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":246,"skipped":4334,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:57:34.903: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 23 14:57:35.685: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 23 14:57:37.698: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744332241, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744332241, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744332241, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744332241, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-76bbb7fdd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 14:57:40.747: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
Dec 23 14:57:40.785: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:40.903: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:41.001: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:41.106: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:41.203: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:41.300: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:41.407: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:41.501: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:41.601: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:41.705: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:41.807: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:41.913: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:42.000: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:42.102: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:42.201: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:42.300: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:42.399: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:42.500: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:42.602: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:42.705: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:42.800: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:42.901: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:42.999: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:43.099: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:43.200: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:43.301: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:43.402: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:43.499: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:43.600: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:43.700: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:43.801: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:43.905: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:44.007: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:44.101: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:44.206: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:44.301: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:44.402: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:44.501: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:44.601: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:44.702: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:44.799: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:44.900: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:45.000: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:45.100: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:45.203: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:45.301: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:45.401: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:45.501: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:45.599: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:45.700: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:45.800: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:45.903: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:46.017: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:46.103: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:46.205: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:46.300: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:46.399: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:46.498: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:46.599: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:46.701: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:46.799: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:46.901: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:46.999: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:47.101: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:47.200: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:47.300: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:47.400: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:47.499: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:47.606: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:47.702: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:47.800: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:47.901: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:48.005: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:48.103: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:48.205: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:48.299: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:48.398: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:48.499: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:48.600: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:48.706: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:48.799: INFO: Waiting for webhook configuration to be ready...
Dec 23 14:57:48.901: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Dec 23 14:57:51.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=webhook-8673 attach --namespace=webhook-8673 to-be-attached-pod -i -c=container1'
Dec 23 14:57:51.151: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:57:51.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8673" for this suite.
STEP: Destroying namespace "webhook-8673-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:16.316 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":247,"skipped":4360,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:57:51.220: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Dec 23 14:57:51.280: INFO: created test-pod-1
Dec 23 14:57:51.289: INFO: created test-pod-2
Dec 23 14:57:51.335: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:57:51.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1974" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":248,"skipped":4394,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:57:51.435: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-1177
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 23 14:57:51.478: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 23 14:57:51.518: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 23 14:57:53.528: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 23 14:57:55.528: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:57:57.526: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:57:59.526: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:58:01.526: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:58:03.523: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:58:05.526: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:58:07.526: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:58:09.526: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 23 14:58:11.527: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 23 14:58:11.532: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Dec 23 14:58:15.557: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Dec 23 14:58:15.557: INFO: Breadth first check of 10.244.1.30 on host 10.22.19.17...
Dec 23 14:58:15.559: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.201.246:9080/dial?request=hostname&protocol=udp&host=10.244.1.30&port=8081&tries=1'] Namespace:pod-network-test-1177 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:58:15.559: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 14:58:15.667: INFO: Waiting for responses: map[]
Dec 23 14:58:15.667: INFO: reached 10.244.1.30 after 0/1 tries
Dec 23 14:58:15.667: INFO: Breadth first check of 10.244.201.238 on host 10.22.19.25...
Dec 23 14:58:15.671: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.201.246:9080/dial?request=hostname&protocol=udp&host=10.244.201.238&port=8081&tries=1'] Namespace:pod-network-test-1177 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 14:58:15.671: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 14:58:15.772: INFO: Waiting for responses: map[]
Dec 23 14:58:15.772: INFO: reached 10.244.201.238 after 0/1 tries
Dec 23 14:58:15.772: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:58:15.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1177" for this suite.

• [SLOW TEST:24.347 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":249,"skipped":4413,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:58:15.783: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Dec 23 14:58:15.836: INFO: created test-event-1
Dec 23 14:58:15.838: INFO: created test-event-2
Dec 23 14:58:15.841: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Dec 23 14:58:15.845: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Dec 23 14:58:15.877: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:58:15.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-320" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":250,"skipped":4442,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:58:15.891: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 14:58:15.929: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b0111b60-4921-414a-b204-658a318a8271" in namespace "downward-api-9443" to be "Succeeded or Failed"
Dec 23 14:58:15.939: INFO: Pod "downwardapi-volume-b0111b60-4921-414a-b204-658a318a8271": Phase="Pending", Reason="", readiness=false. Elapsed: 10.002649ms
Dec 23 14:58:17.947: INFO: Pod "downwardapi-volume-b0111b60-4921-414a-b204-658a318a8271": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017689354s
STEP: Saw pod success
Dec 23 14:58:17.947: INFO: Pod "downwardapi-volume-b0111b60-4921-414a-b204-658a318a8271" satisfied condition "Succeeded or Failed"
Dec 23 14:58:17.949: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-b0111b60-4921-414a-b204-658a318a8271 container client-container: <nil>
STEP: delete the pod
Dec 23 14:58:17.980: INFO: Waiting for pod downwardapi-volume-b0111b60-4921-414a-b204-658a318a8271 to disappear
Dec 23 14:58:17.982: INFO: Pod downwardapi-volume-b0111b60-4921-414a-b204-658a318a8271 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:58:17.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9443" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":251,"skipped":4480,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:58:17.994: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 14:58:18.037: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96d8dd58-ee07-45de-9b6d-c8399622c239" in namespace "projected-4665" to be "Succeeded or Failed"
Dec 23 14:58:18.046: INFO: Pod "downwardapi-volume-96d8dd58-ee07-45de-9b6d-c8399622c239": Phase="Pending", Reason="", readiness=false. Elapsed: 9.06079ms
Dec 23 14:58:20.054: INFO: Pod "downwardapi-volume-96d8dd58-ee07-45de-9b6d-c8399622c239": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016909912s
STEP: Saw pod success
Dec 23 14:58:20.054: INFO: Pod "downwardapi-volume-96d8dd58-ee07-45de-9b6d-c8399622c239" satisfied condition "Succeeded or Failed"
Dec 23 14:58:20.056: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-96d8dd58-ee07-45de-9b6d-c8399622c239 container client-container: <nil>
STEP: delete the pod
Dec 23 14:58:20.086: INFO: Waiting for pod downwardapi-volume-96d8dd58-ee07-45de-9b6d-c8399622c239 to disappear
Dec 23 14:58:20.099: INFO: Pod downwardapi-volume-96d8dd58-ee07-45de-9b6d-c8399622c239 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:58:20.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4665" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":252,"skipped":4483,"failed":0}
SSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:58:20.114: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Dec 23 14:58:20.175: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Dec 23 14:58:20.194: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:58:20.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2151" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":253,"skipped":4488,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:58:20.236: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-r4qk
STEP: Creating a pod to test atomic-volume-subpath
Dec 23 14:58:20.301: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-r4qk" in namespace "subpath-7342" to be "Succeeded or Failed"
Dec 23 14:58:20.311: INFO: Pod "pod-subpath-test-secret-r4qk": Phase="Pending", Reason="", readiness=false. Elapsed: 9.544921ms
Dec 23 14:58:22.319: INFO: Pod "pod-subpath-test-secret-r4qk": Phase="Running", Reason="", readiness=true. Elapsed: 2.017559219s
Dec 23 14:58:24.328: INFO: Pod "pod-subpath-test-secret-r4qk": Phase="Running", Reason="", readiness=true. Elapsed: 4.026796775s
Dec 23 14:58:26.334: INFO: Pod "pod-subpath-test-secret-r4qk": Phase="Running", Reason="", readiness=true. Elapsed: 6.032756957s
Dec 23 14:58:28.341: INFO: Pod "pod-subpath-test-secret-r4qk": Phase="Running", Reason="", readiness=true. Elapsed: 8.040310473s
Dec 23 14:58:30.349: INFO: Pod "pod-subpath-test-secret-r4qk": Phase="Running", Reason="", readiness=true. Elapsed: 10.048174189s
Dec 23 14:58:32.360: INFO: Pod "pod-subpath-test-secret-r4qk": Phase="Running", Reason="", readiness=true. Elapsed: 12.059297707s
Dec 23 14:58:34.401: INFO: Pod "pod-subpath-test-secret-r4qk": Phase="Running", Reason="", readiness=true. Elapsed: 14.100352413s
Dec 23 14:58:36.406: INFO: Pod "pod-subpath-test-secret-r4qk": Phase="Running", Reason="", readiness=true. Elapsed: 16.105171165s
Dec 23 14:58:38.414: INFO: Pod "pod-subpath-test-secret-r4qk": Phase="Running", Reason="", readiness=true. Elapsed: 18.113130809s
Dec 23 14:58:40.422: INFO: Pod "pod-subpath-test-secret-r4qk": Phase="Running", Reason="", readiness=true. Elapsed: 20.120624349s
Dec 23 14:58:42.425: INFO: Pod "pod-subpath-test-secret-r4qk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.124099871s
STEP: Saw pod success
Dec 23 14:58:42.425: INFO: Pod "pod-subpath-test-secret-r4qk" satisfied condition "Succeeded or Failed"
Dec 23 14:58:42.429: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-subpath-test-secret-r4qk container test-container-subpath-secret-r4qk: <nil>
STEP: delete the pod
Dec 23 14:58:42.456: INFO: Waiting for pod pod-subpath-test-secret-r4qk to disappear
Dec 23 14:58:42.469: INFO: Pod pod-subpath-test-secret-r4qk no longer exists
STEP: Deleting pod pod-subpath-test-secret-r4qk
Dec 23 14:58:42.470: INFO: Deleting pod "pod-subpath-test-secret-r4qk" in namespace "subpath-7342"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:58:42.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7342" for this suite.

• [SLOW TEST:22.244 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":254,"skipped":4492,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:58:42.481: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 14:58:42.524: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76962f85-4f87-48a7-b06a-f1fd18e25857" in namespace "downward-api-3364" to be "Succeeded or Failed"
Dec 23 14:58:42.536: INFO: Pod "downwardapi-volume-76962f85-4f87-48a7-b06a-f1fd18e25857": Phase="Pending", Reason="", readiness=false. Elapsed: 11.924806ms
Dec 23 14:58:44.544: INFO: Pod "downwardapi-volume-76962f85-4f87-48a7-b06a-f1fd18e25857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019123845s
STEP: Saw pod success
Dec 23 14:58:44.544: INFO: Pod "downwardapi-volume-76962f85-4f87-48a7-b06a-f1fd18e25857" satisfied condition "Succeeded or Failed"
Dec 23 14:58:44.546: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-76962f85-4f87-48a7-b06a-f1fd18e25857 container client-container: <nil>
STEP: delete the pod
Dec 23 14:58:44.567: INFO: Waiting for pod downwardapi-volume-76962f85-4f87-48a7-b06a-f1fd18e25857 to disappear
Dec 23 14:58:44.569: INFO: Pod downwardapi-volume-76962f85-4f87-48a7-b06a-f1fd18e25857 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:58:44.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3364" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":255,"skipped":4500,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:58:44.577: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Dec 23 14:58:44.615: INFO: Waiting up to 5m0s for pod "downward-api-07072e4f-8af7-4ca1-be5c-06fd02b87c0f" in namespace "downward-api-1821" to be "Succeeded or Failed"
Dec 23 14:58:44.623: INFO: Pod "downward-api-07072e4f-8af7-4ca1-be5c-06fd02b87c0f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.37902ms
Dec 23 14:58:46.628: INFO: Pod "downward-api-07072e4f-8af7-4ca1-be5c-06fd02b87c0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012958806s
STEP: Saw pod success
Dec 23 14:58:46.628: INFO: Pod "downward-api-07072e4f-8af7-4ca1-be5c-06fd02b87c0f" satisfied condition "Succeeded or Failed"
Dec 23 14:58:46.632: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downward-api-07072e4f-8af7-4ca1-be5c-06fd02b87c0f container dapi-container: <nil>
STEP: delete the pod
Dec 23 14:58:46.660: INFO: Waiting for pod downward-api-07072e4f-8af7-4ca1-be5c-06fd02b87c0f to disappear
Dec 23 14:58:46.663: INFO: Pod downward-api-07072e4f-8af7-4ca1-be5c-06fd02b87c0f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:58:46.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1821" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":256,"skipped":4504,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:58:46.672: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:58:46.712: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:58:48.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3911" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":257,"skipped":4509,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:58:48.092: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:58:48.162: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 23 14:58:52.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-2314 --namespace=crd-publish-openapi-2314 create -f -'
Dec 23 14:58:54.116: INFO: stderr: ""
Dec 23 14:58:54.116: INFO: stdout: "e2e-test-crd-publish-openapi-7645-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 23 14:58:54.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-2314 --namespace=crd-publish-openapi-2314 delete e2e-test-crd-publish-openapi-7645-crds test-cr'
Dec 23 14:58:54.243: INFO: stderr: ""
Dec 23 14:58:54.243: INFO: stdout: "e2e-test-crd-publish-openapi-7645-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 23 14:58:54.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-2314 --namespace=crd-publish-openapi-2314 apply -f -'
Dec 23 14:58:54.707: INFO: stderr: ""
Dec 23 14:58:54.707: INFO: stdout: "e2e-test-crd-publish-openapi-7645-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 23 14:58:54.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-2314 --namespace=crd-publish-openapi-2314 delete e2e-test-crd-publish-openapi-7645-crds test-cr'
Dec 23 14:58:54.815: INFO: stderr: ""
Dec 23 14:58:54.815: INFO: stdout: "e2e-test-crd-publish-openapi-7645-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 23 14:58:54.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=crd-publish-openapi-2314 explain e2e-test-crd-publish-openapi-7645-crds'
Dec 23 14:58:55.509: INFO: stderr: ""
Dec 23 14:58:55.509: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7645-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:58:59.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2314" for this suite.

• [SLOW TEST:11.280 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":258,"skipped":4536,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:58:59.373: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 14:58:59.410: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 23 14:59:01.453: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:59:02.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5866" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":259,"skipped":4565,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:59:02.472: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 23 14:59:02.523: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 23 14:59:07.542: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:59:08.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1919" for this suite.

• [SLOW TEST:6.116 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":260,"skipped":4592,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:59:08.588: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-a9790b3a-5ce9-4134-82ca-943c7b9d0a05
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 14:59:08.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4388" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":261,"skipped":4598,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 14:59:08.639: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1223 14:59:14.729914      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Dec 23 15:00:16.746: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:00:16.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4451" for this suite.

• [SLOW TEST:68.117 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":262,"skipped":4626,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:00:16.758: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-255886f8-c3ed-460e-b04f-2691ee184fed
STEP: Creating configMap with name cm-test-opt-upd-c1ad58de-c34b-4857-895e-6bc9e9968a84
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-255886f8-c3ed-460e-b04f-2691ee184fed
STEP: Updating configmap cm-test-opt-upd-c1ad58de-c34b-4857-895e-6bc9e9968a84
STEP: Creating configMap with name cm-test-opt-create-c6cba876-4c0b-4ad9-8f10-3bd4c640cf52
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:01:33.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1593" for this suite.

• [SLOW TEST:76.532 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":263,"skipped":4645,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:01:33.294: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 23 15:01:34.215: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 23 15:01:36.227: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744332480, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744332480, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744332480, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744332480, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-76bbb7fdd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 15:01:39.310: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
Dec 23 15:01:39.338: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:39.452: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:39.558: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:39.651: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:39.754: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:39.852: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:39.951: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:40.058: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:40.159: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:40.291: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:40.357: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:40.454: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:40.554: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:40.654: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:41.080: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:41.150: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:41.439: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:41.554: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:41.654: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:41.751: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:41.997: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:42.059: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:42.161: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:42.314: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:42.509: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:42.551: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:42.653: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:42.889: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:42.953: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:43.165: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:43.254: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:43.505: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:43.727: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:44.068: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:44.151: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:44.867: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:44.957: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:45.102: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:45.155: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:45.710: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:45.757: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:45.856: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:45.960: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:46.053: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:46.517: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:46.553: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:46.650: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:46.776: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:01:46.855: INFO: Waiting for webhook configuration to be ready...
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:01:46.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7242" for this suite.
STEP: Destroying namespace "webhook-7242-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:13.894 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":264,"skipped":4678,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:01:47.188: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-2a1f956a-a6d1-42de-918b-17776daf29af
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:01:47.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5902" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":265,"skipped":4680,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:01:47.231: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-134 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-134;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-134 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-134;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-134.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-134.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-134.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-134.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-134.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-134.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-134.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-134.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-134.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-134.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-134.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-134.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-134.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 207.30.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.30.207_udp@PTR;check="$$(dig +tcp +noall +answer +search 207.30.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.30.207_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-134 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-134;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-134 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-134;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-134.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-134.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-134.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-134.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-134.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-134.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-134.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-134.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-134.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-134.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-134.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-134.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-134.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 207.30.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.30.207_udp@PTR;check="$$(dig +tcp +noall +answer +search 207.30.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.30.207_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 23 15:01:51.538: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:51.541: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:51.543: INFO: Unable to read wheezy_udp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:51.545: INFO: Unable to read wheezy_tcp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:51.548: INFO: Unable to read wheezy_udp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:51.550: INFO: Unable to read wheezy_tcp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:51.552: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:51.554: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:51.570: INFO: Unable to read jessie_udp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:51.572: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:51.574: INFO: Unable to read jessie_udp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:51.576: INFO: Unable to read jessie_tcp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:51.578: INFO: Unable to read jessie_udp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:51.580: INFO: Unable to read jessie_tcp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:51.582: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:51.585: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:51.603: INFO: Lookups using dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-134 wheezy_tcp@dns-test-service.dns-134 wheezy_udp@dns-test-service.dns-134.svc wheezy_tcp@dns-test-service.dns-134.svc wheezy_udp@_http._tcp.dns-test-service.dns-134.svc wheezy_tcp@_http._tcp.dns-test-service.dns-134.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-134 jessie_tcp@dns-test-service.dns-134 jessie_udp@dns-test-service.dns-134.svc jessie_tcp@dns-test-service.dns-134.svc jessie_udp@_http._tcp.dns-test-service.dns-134.svc jessie_tcp@_http._tcp.dns-test-service.dns-134.svc]

Dec 23 15:01:56.608: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:56.611: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:56.613: INFO: Unable to read wheezy_udp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:56.615: INFO: Unable to read wheezy_tcp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:56.617: INFO: Unable to read wheezy_udp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:56.619: INFO: Unable to read wheezy_tcp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:56.645: INFO: Unable to read jessie_udp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:56.647: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:56.649: INFO: Unable to read jessie_udp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:56.652: INFO: Unable to read jessie_tcp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:56.654: INFO: Unable to read jessie_udp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:56.656: INFO: Unable to read jessie_tcp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:01:56.677: INFO: Lookups using dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-134 wheezy_tcp@dns-test-service.dns-134 wheezy_udp@dns-test-service.dns-134.svc wheezy_tcp@dns-test-service.dns-134.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-134 jessie_tcp@dns-test-service.dns-134 jessie_udp@dns-test-service.dns-134.svc jessie_tcp@dns-test-service.dns-134.svc]

Dec 23 15:02:01.609: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:01.612: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:01.615: INFO: Unable to read wheezy_udp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:01.617: INFO: Unable to read wheezy_tcp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:01.620: INFO: Unable to read wheezy_udp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:01.622: INFO: Unable to read wheezy_tcp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:01.643: INFO: Unable to read jessie_udp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:01.645: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:01.648: INFO: Unable to read jessie_udp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:01.650: INFO: Unable to read jessie_tcp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:01.652: INFO: Unable to read jessie_udp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:01.655: INFO: Unable to read jessie_tcp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:01.675: INFO: Lookups using dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-134 wheezy_tcp@dns-test-service.dns-134 wheezy_udp@dns-test-service.dns-134.svc wheezy_tcp@dns-test-service.dns-134.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-134 jessie_tcp@dns-test-service.dns-134 jessie_udp@dns-test-service.dns-134.svc jessie_tcp@dns-test-service.dns-134.svc]

Dec 23 15:02:06.608: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:06.612: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:06.614: INFO: Unable to read wheezy_udp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:06.617: INFO: Unable to read wheezy_tcp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:06.619: INFO: Unable to read wheezy_udp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:06.622: INFO: Unable to read wheezy_tcp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:06.645: INFO: Unable to read jessie_udp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:06.647: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:06.650: INFO: Unable to read jessie_udp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:06.652: INFO: Unable to read jessie_tcp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:06.655: INFO: Unable to read jessie_udp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:06.657: INFO: Unable to read jessie_tcp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:06.677: INFO: Lookups using dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-134 wheezy_tcp@dns-test-service.dns-134 wheezy_udp@dns-test-service.dns-134.svc wheezy_tcp@dns-test-service.dns-134.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-134 jessie_tcp@dns-test-service.dns-134 jessie_udp@dns-test-service.dns-134.svc jessie_tcp@dns-test-service.dns-134.svc]

Dec 23 15:02:11.610: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:11.613: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:11.616: INFO: Unable to read wheezy_udp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:11.619: INFO: Unable to read wheezy_tcp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:11.623: INFO: Unable to read wheezy_udp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:11.626: INFO: Unable to read wheezy_tcp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:11.648: INFO: Unable to read jessie_udp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:11.651: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:11.653: INFO: Unable to read jessie_udp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:11.656: INFO: Unable to read jessie_tcp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:11.659: INFO: Unable to read jessie_udp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:11.662: INFO: Unable to read jessie_tcp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:11.682: INFO: Lookups using dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-134 wheezy_tcp@dns-test-service.dns-134 wheezy_udp@dns-test-service.dns-134.svc wheezy_tcp@dns-test-service.dns-134.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-134 jessie_tcp@dns-test-service.dns-134 jessie_udp@dns-test-service.dns-134.svc jessie_tcp@dns-test-service.dns-134.svc]

Dec 23 15:02:16.607: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:16.611: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:16.615: INFO: Unable to read wheezy_udp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:16.618: INFO: Unable to read wheezy_tcp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:16.621: INFO: Unable to read wheezy_udp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:16.625: INFO: Unable to read wheezy_tcp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:16.661: INFO: Unable to read jessie_udp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:16.664: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:16.669: INFO: Unable to read jessie_udp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:16.673: INFO: Unable to read jessie_tcp@dns-test-service.dns-134 from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:16.678: INFO: Unable to read jessie_udp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:16.682: INFO: Unable to read jessie_tcp@dns-test-service.dns-134.svc from pod dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9: the server could not find the requested resource (get pods dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9)
Dec 23 15:02:16.708: INFO: Lookups using dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-134 wheezy_tcp@dns-test-service.dns-134 wheezy_udp@dns-test-service.dns-134.svc wheezy_tcp@dns-test-service.dns-134.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-134 jessie_tcp@dns-test-service.dns-134 jessie_udp@dns-test-service.dns-134.svc jessie_tcp@dns-test-service.dns-134.svc]

Dec 23 15:02:21.681: INFO: DNS probes using dns-134/dns-test-dd4371f9-afcf-46ce-8dc5-c5e8b54313d9 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:02:21.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-134" for this suite.

• [SLOW TEST:34.580 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":266,"skipped":4715,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:02:21.812: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-29011f09-ce73-4c8d-a096-2e645797be18
STEP: Creating a pod to test consume configMaps
Dec 23 15:02:21.935: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5f51d0fb-c1eb-4270-bc15-5c74c4aa8d6d" in namespace "projected-2171" to be "Succeeded or Failed"
Dec 23 15:02:21.946: INFO: Pod "pod-projected-configmaps-5f51d0fb-c1eb-4270-bc15-5c74c4aa8d6d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.816301ms
Dec 23 15:02:23.953: INFO: Pod "pod-projected-configmaps-5f51d0fb-c1eb-4270-bc15-5c74c4aa8d6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017263709s
STEP: Saw pod success
Dec 23 15:02:23.953: INFO: Pod "pod-projected-configmaps-5f51d0fb-c1eb-4270-bc15-5c74c4aa8d6d" satisfied condition "Succeeded or Failed"
Dec 23 15:02:23.955: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-projected-configmaps-5f51d0fb-c1eb-4270-bc15-5c74c4aa8d6d container agnhost-container: <nil>
STEP: delete the pod
Dec 23 15:02:24.231: INFO: Waiting for pod pod-projected-configmaps-5f51d0fb-c1eb-4270-bc15-5c74c4aa8d6d to disappear
Dec 23 15:02:24.238: INFO: Pod pod-projected-configmaps-5f51d0fb-c1eb-4270-bc15-5c74c4aa8d6d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:02:24.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2171" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":267,"skipped":4716,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:02:24.247: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 23 15:02:24.314: INFO: Waiting up to 5m0s for pod "pod-2d0a6403-515b-420b-8fb9-50f5b7801dbe" in namespace "emptydir-4447" to be "Succeeded or Failed"
Dec 23 15:02:24.320: INFO: Pod "pod-2d0a6403-515b-420b-8fb9-50f5b7801dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.923428ms
Dec 23 15:02:26.327: INFO: Pod "pod-2d0a6403-515b-420b-8fb9-50f5b7801dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01378936s
Dec 23 15:02:28.337: INFO: Pod "pod-2d0a6403-515b-420b-8fb9-50f5b7801dbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023236392s
STEP: Saw pod success
Dec 23 15:02:28.337: INFO: Pod "pod-2d0a6403-515b-420b-8fb9-50f5b7801dbe" satisfied condition "Succeeded or Failed"
Dec 23 15:02:28.340: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-2d0a6403-515b-420b-8fb9-50f5b7801dbe container test-container: <nil>
STEP: delete the pod
Dec 23 15:02:28.390: INFO: Waiting for pod pod-2d0a6403-515b-420b-8fb9-50f5b7801dbe to disappear
Dec 23 15:02:28.392: INFO: Pod pod-2d0a6403-515b-420b-8fb9-50f5b7801dbe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:02:28.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4447" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":268,"skipped":4723,"failed":0}
SS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:02:28.401: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 23 15:02:29.720: INFO: starting watch
STEP: patching
STEP: updating
Dec 23 15:02:29.728: INFO: waiting for watch events with expected annotations
Dec 23 15:02:29.728: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:02:29.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-5631" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":269,"skipped":4725,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:02:29.940: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-rsfz
STEP: Creating a pod to test atomic-volume-subpath
Dec 23 15:02:29.988: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rsfz" in namespace "subpath-8544" to be "Succeeded or Failed"
Dec 23 15:02:29.996: INFO: Pod "pod-subpath-test-configmap-rsfz": Phase="Pending", Reason="", readiness=false. Elapsed: 8.518496ms
Dec 23 15:02:32.004: INFO: Pod "pod-subpath-test-configmap-rsfz": Phase="Running", Reason="", readiness=true. Elapsed: 2.016170137s
Dec 23 15:02:34.012: INFO: Pod "pod-subpath-test-configmap-rsfz": Phase="Running", Reason="", readiness=true. Elapsed: 4.02445145s
Dec 23 15:02:36.024: INFO: Pod "pod-subpath-test-configmap-rsfz": Phase="Running", Reason="", readiness=true. Elapsed: 6.036588412s
Dec 23 15:02:38.029: INFO: Pod "pod-subpath-test-configmap-rsfz": Phase="Running", Reason="", readiness=true. Elapsed: 8.041431312s
Dec 23 15:02:40.037: INFO: Pod "pod-subpath-test-configmap-rsfz": Phase="Running", Reason="", readiness=true. Elapsed: 10.049687647s
Dec 23 15:02:42.046: INFO: Pod "pod-subpath-test-configmap-rsfz": Phase="Running", Reason="", readiness=true. Elapsed: 12.058154799s
Dec 23 15:02:44.053: INFO: Pod "pod-subpath-test-configmap-rsfz": Phase="Running", Reason="", readiness=true. Elapsed: 14.065601883s
Dec 23 15:02:46.062: INFO: Pod "pod-subpath-test-configmap-rsfz": Phase="Running", Reason="", readiness=true. Elapsed: 16.074436505s
Dec 23 15:02:48.070: INFO: Pod "pod-subpath-test-configmap-rsfz": Phase="Running", Reason="", readiness=true. Elapsed: 18.082327593s
Dec 23 15:02:50.079: INFO: Pod "pod-subpath-test-configmap-rsfz": Phase="Running", Reason="", readiness=true. Elapsed: 20.091278769s
Dec 23 15:02:53.341: INFO: Pod "pod-subpath-test-configmap-rsfz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.353074278s
STEP: Saw pod success
Dec 23 15:02:53.341: INFO: Pod "pod-subpath-test-configmap-rsfz" satisfied condition "Succeeded or Failed"
Dec 23 15:02:53.380: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-subpath-test-configmap-rsfz container test-container-subpath-configmap-rsfz: <nil>
STEP: delete the pod
Dec 23 15:02:53.415: INFO: Waiting for pod pod-subpath-test-configmap-rsfz to disappear
Dec 23 15:02:53.418: INFO: Pod pod-subpath-test-configmap-rsfz no longer exists
STEP: Deleting pod pod-subpath-test-configmap-rsfz
Dec 23 15:02:53.418: INFO: Deleting pod "pod-subpath-test-configmap-rsfz" in namespace "subpath-8544"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:02:53.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8544" for this suite.

• [SLOW TEST:23.489 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":270,"skipped":4733,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:02:53.429: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-2792/secret-test-d834e62b-1b12-4ee8-add6-aba266e41bc2
STEP: Creating a pod to test consume secrets
Dec 23 15:02:53.481: INFO: Waiting up to 5m0s for pod "pod-configmaps-732c47a0-f906-4fce-a96b-bea1a33931ae" in namespace "secrets-2792" to be "Succeeded or Failed"
Dec 23 15:02:53.494: INFO: Pod "pod-configmaps-732c47a0-f906-4fce-a96b-bea1a33931ae": Phase="Pending", Reason="", readiness=false. Elapsed: 12.734835ms
Dec 23 15:02:55.499: INFO: Pod "pod-configmaps-732c47a0-f906-4fce-a96b-bea1a33931ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018190819s
STEP: Saw pod success
Dec 23 15:02:55.499: INFO: Pod "pod-configmaps-732c47a0-f906-4fce-a96b-bea1a33931ae" satisfied condition "Succeeded or Failed"
Dec 23 15:02:55.502: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-configmaps-732c47a0-f906-4fce-a96b-bea1a33931ae container env-test: <nil>
STEP: delete the pod
Dec 23 15:02:55.525: INFO: Waiting for pod pod-configmaps-732c47a0-f906-4fce-a96b-bea1a33931ae to disappear
Dec 23 15:02:55.528: INFO: Pod pod-configmaps-732c47a0-f906-4fce-a96b-bea1a33931ae no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:02:55.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2792" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":271,"skipped":4759,"failed":0}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:02:55.536: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 23 15:02:59.632: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 23 15:02:59.635: INFO: Pod pod-with-prestop-http-hook still exists
Dec 23 15:03:01.635: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 23 15:03:01.645: INFO: Pod pod-with-prestop-http-hook still exists
Dec 23 15:03:03.635: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 23 15:03:03.639: INFO: Pod pod-with-prestop-http-hook still exists
Dec 23 15:03:05.635: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 23 15:03:05.645: INFO: Pod pod-with-prestop-http-hook still exists
Dec 23 15:03:07.635: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 23 15:03:07.641: INFO: Pod pod-with-prestop-http-hook still exists
Dec 23 15:03:09.635: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 23 15:03:09.643: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:03:09.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4920" for this suite.

• [SLOW TEST:14.123 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":272,"skipped":4762,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:03:09.659: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 23 15:03:09.715: INFO: Waiting up to 5m0s for pod "pod-01f8e6f1-597e-4d19-82a0-da9f1bb0f375" in namespace "emptydir-8760" to be "Succeeded or Failed"
Dec 23 15:03:09.719: INFO: Pod "pod-01f8e6f1-597e-4d19-82a0-da9f1bb0f375": Phase="Pending", Reason="", readiness=false. Elapsed: 3.668831ms
Dec 23 15:03:11.727: INFO: Pod "pod-01f8e6f1-597e-4d19-82a0-da9f1bb0f375": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011869272s
STEP: Saw pod success
Dec 23 15:03:11.727: INFO: Pod "pod-01f8e6f1-597e-4d19-82a0-da9f1bb0f375" satisfied condition "Succeeded or Failed"
Dec 23 15:03:11.729: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-01f8e6f1-597e-4d19-82a0-da9f1bb0f375 container test-container: <nil>
STEP: delete the pod
Dec 23 15:03:11.761: INFO: Waiting for pod pod-01f8e6f1-597e-4d19-82a0-da9f1bb0f375 to disappear
Dec 23 15:03:11.764: INFO: Pod pod-01f8e6f1-597e-4d19-82a0-da9f1bb0f375 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:03:11.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8760" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":273,"skipped":4762,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:03:11.775: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-bf2b324a-5bd4-479a-8406-b33655185219 in namespace container-probe-1678
Dec 23 15:03:13.849: INFO: Started pod liveness-bf2b324a-5bd4-479a-8406-b33655185219 in namespace container-probe-1678
STEP: checking the pod's current state and verifying that restartCount is present
Dec 23 15:03:13.851: INFO: Initial restart count of pod liveness-bf2b324a-5bd4-479a-8406-b33655185219 is 0
Dec 23 15:03:27.912: INFO: Restart count of pod container-probe-1678/liveness-bf2b324a-5bd4-479a-8406-b33655185219 is now 1 (14.060441179s elapsed)
Dec 23 15:03:45.976: INFO: Restart count of pod container-probe-1678/liveness-bf2b324a-5bd4-479a-8406-b33655185219 is now 2 (32.124932344s elapsed)
Dec 23 15:04:06.054: INFO: Restart count of pod container-probe-1678/liveness-bf2b324a-5bd4-479a-8406-b33655185219 is now 3 (52.202068111s elapsed)
Dec 23 15:04:26.132: INFO: Restart count of pod container-probe-1678/liveness-bf2b324a-5bd4-479a-8406-b33655185219 is now 4 (1m12.280693883s elapsed)
Dec 23 15:05:36.425: INFO: Restart count of pod container-probe-1678/liveness-bf2b324a-5bd4-479a-8406-b33655185219 is now 5 (2m22.573575537s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:05:36.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1678" for this suite.

• [SLOW TEST:144.682 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":274,"skipped":4777,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:05:36.458: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-ab6235eb-faca-41b2-93eb-ee6ed6f40188
STEP: Creating a pod to test consume configMaps
Dec 23 15:05:36.505: INFO: Waiting up to 5m0s for pod "pod-configmaps-5f53a921-2b78-4ec2-a55d-3d0e7c1c2a24" in namespace "configmap-401" to be "Succeeded or Failed"
Dec 23 15:05:36.509: INFO: Pod "pod-configmaps-5f53a921-2b78-4ec2-a55d-3d0e7c1c2a24": Phase="Pending", Reason="", readiness=false. Elapsed: 3.816692ms
Dec 23 15:05:38.515: INFO: Pod "pod-configmaps-5f53a921-2b78-4ec2-a55d-3d0e7c1c2a24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010051098s
STEP: Saw pod success
Dec 23 15:05:38.515: INFO: Pod "pod-configmaps-5f53a921-2b78-4ec2-a55d-3d0e7c1c2a24" satisfied condition "Succeeded or Failed"
Dec 23 15:05:38.517: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-configmaps-5f53a921-2b78-4ec2-a55d-3d0e7c1c2a24 container agnhost-container: <nil>
STEP: delete the pod
Dec 23 15:05:38.548: INFO: Waiting for pod pod-configmaps-5f53a921-2b78-4ec2-a55d-3d0e7c1c2a24 to disappear
Dec 23 15:05:38.551: INFO: Pod pod-configmaps-5f53a921-2b78-4ec2-a55d-3d0e7c1c2a24 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:05:38.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-401" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":275,"skipped":4793,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:05:38.566: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 23 15:05:39.056: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec 23 15:05:41.068: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744332725, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744332725, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744332725, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744332725, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-55cf5fff84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 15:05:44.123: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 15:05:44.129: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 15:05:44.723: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:30Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:44.846: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:30Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:44.955: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:31Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:45.033: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:31Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:45.133: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:31Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:45.231: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:31Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:45.331: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:31Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:45.432: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:31Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:45.532: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:31Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:45.633: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:31Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:45.733: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:31Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:45.838: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:31Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:45.931: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:32Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:46.032: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:32Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:46.132: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:32Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:46.232: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:32Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:46.332: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:32Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:46.433: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:32Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:46.532: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:32Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:46.651: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:32Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:46.853: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:32Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:47.052: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:33Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:47.285: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:33Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:47.453: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:33Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:47.653: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:33Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:47.852: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:33Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:48.052: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:34Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:48.256: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:34Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:48.452: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:34Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:48.654: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:34Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:48.851: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:34Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:49.052: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:35Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:49.261: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:35Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:49.451: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:35Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:49.652: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:35Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:49.851: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:35Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:50.052: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:36Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:50.252: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:36Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:50.452: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:36Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:50.653: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:36Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:50.852: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:36Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:51.052: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:37Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:51.256: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:37Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:51.452: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:37Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:51.652: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:37Z is before 2020-12-23T15:05:38Z
Dec 23 15:05:51.851: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-3496-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-4184.svc:9443/crdconvert?timeout=30s": x509: certificate has expired or is not yet valid: current time 2020-12-23T15:05:37Z is before 2020-12-23T15:05:38Z
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:05:53.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4184" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:15.262 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":276,"skipped":4837,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:05:53.828: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5787
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5787
STEP: creating replication controller externalsvc in namespace services-5787
I1223 15:05:53.996770      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-5787, replica count: 2
I1223 15:05:57.047218      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1223 15:06:00.047466      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Dec 23 15:06:00.080: INFO: Creating new exec pod
Dec 23 15:06:02.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-5787 exec execpodh982q -- /bin/sh -x -c nslookup clusterip-service.services-5787.svc.cluster.local'
Dec 23 15:06:02.603: INFO: stderr: "+ nslookup clusterip-service.services-5787.svc.cluster.local\n"
Dec 23 15:06:02.603: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-5787.svc.cluster.local\tcanonical name = externalsvc.services-5787.svc.cluster.local.\nName:\texternalsvc.services-5787.svc.cluster.local\nAddress: 10.103.212.236\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5787, will wait for the garbage collector to delete the pods
Dec 23 15:06:02.665: INFO: Deleting ReplicationController externalsvc took: 6.173755ms
Dec 23 15:06:03.267: INFO: Terminating ReplicationController externalsvc pods took: 601.877727ms
Dec 23 15:06:29.411: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:06:29.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5787" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:35.631 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":277,"skipped":4838,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:06:29.459: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Dec 23 15:06:29.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-6088 create -f -'
Dec 23 15:06:30.167: INFO: stderr: ""
Dec 23 15:06:30.167: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 23 15:06:30.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-6088 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 23 15:06:30.282: INFO: stderr: ""
Dec 23 15:06:30.282: INFO: stdout: "update-demo-nautilus-smn99 update-demo-nautilus-v45s4 "
Dec 23 15:06:30.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-6088 get pods update-demo-nautilus-smn99 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 23 15:06:30.380: INFO: stderr: ""
Dec 23 15:06:30.380: INFO: stdout: ""
Dec 23 15:06:30.380: INFO: update-demo-nautilus-smn99 is created but not running
Dec 23 15:06:35.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-6088 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 23 15:06:35.494: INFO: stderr: ""
Dec 23 15:06:35.494: INFO: stdout: "update-demo-nautilus-smn99 update-demo-nautilus-v45s4 "
Dec 23 15:06:35.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-6088 get pods update-demo-nautilus-smn99 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 23 15:06:35.591: INFO: stderr: ""
Dec 23 15:06:35.591: INFO: stdout: "true"
Dec 23 15:06:35.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-6088 get pods update-demo-nautilus-smn99 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 23 15:06:35.743: INFO: stderr: ""
Dec 23 15:06:35.743: INFO: stdout: "abcsys.cn:5000/kubernetes-e2e-test-images/nautilus:1.0"
Dec 23 15:06:35.743: INFO: validating pod update-demo-nautilus-smn99
Dec 23 15:06:35.748: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 23 15:06:35.748: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 23 15:06:35.748: INFO: update-demo-nautilus-smn99 is verified up and running
Dec 23 15:06:35.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-6088 get pods update-demo-nautilus-v45s4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 23 15:06:35.866: INFO: stderr: ""
Dec 23 15:06:35.866: INFO: stdout: "true"
Dec 23 15:06:35.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-6088 get pods update-demo-nautilus-v45s4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 23 15:06:35.986: INFO: stderr: ""
Dec 23 15:06:35.986: INFO: stdout: "abcsys.cn:5000/kubernetes-e2e-test-images/nautilus:1.0"
Dec 23 15:06:35.986: INFO: validating pod update-demo-nautilus-v45s4
Dec 23 15:06:35.990: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 23 15:06:35.990: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 23 15:06:35.990: INFO: update-demo-nautilus-v45s4 is verified up and running
STEP: using delete to clean up resources
Dec 23 15:06:35.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-6088 delete --grace-period=0 --force -f -'
Dec 23 15:06:36.092: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 15:06:36.092: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 23 15:06:36.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-6088 get rc,svc -l name=update-demo --no-headers'
Dec 23 15:06:36.211: INFO: stderr: "No resources found in kubectl-6088 namespace.\n"
Dec 23 15:06:36.211: INFO: stdout: ""
Dec 23 15:06:36.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=kubectl-6088 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 23 15:06:36.326: INFO: stderr: ""
Dec 23 15:06:36.326: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:06:36.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6088" for this suite.

• [SLOW TEST:6.881 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":278,"skipped":4844,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:06:36.341: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 23 15:06:36.391: INFO: Waiting up to 5m0s for pod "pod-b0a9f39b-9069-4180-94af-3e60422dccfe" in namespace "emptydir-327" to be "Succeeded or Failed"
Dec 23 15:06:36.399: INFO: Pod "pod-b0a9f39b-9069-4180-94af-3e60422dccfe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.86998ms
Dec 23 15:06:38.404: INFO: Pod "pod-b0a9f39b-9069-4180-94af-3e60422dccfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013307712s
Dec 23 15:06:40.412: INFO: Pod "pod-b0a9f39b-9069-4180-94af-3e60422dccfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021179521s
STEP: Saw pod success
Dec 23 15:06:40.412: INFO: Pod "pod-b0a9f39b-9069-4180-94af-3e60422dccfe" satisfied condition "Succeeded or Failed"
Dec 23 15:06:40.414: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-b0a9f39b-9069-4180-94af-3e60422dccfe container test-container: <nil>
STEP: delete the pod
Dec 23 15:06:40.436: INFO: Waiting for pod pod-b0a9f39b-9069-4180-94af-3e60422dccfe to disappear
Dec 23 15:06:40.439: INFO: Pod pod-b0a9f39b-9069-4180-94af-3e60422dccfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:06:40.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-327" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":279,"skipped":4870,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:06:40.447: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Dec 23 15:06:44.510: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2526 PodName:var-expansion-81564f15-86dd-4284-a15b-5b051232a44d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 15:06:44.510: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: test for file in mounted path
Dec 23 15:06:44.597: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2526 PodName:var-expansion-81564f15-86dd-4284-a15b-5b051232a44d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 15:06:44.597: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: updating the annotation value
Dec 23 15:06:45.193: INFO: Successfully updated pod "var-expansion-81564f15-86dd-4284-a15b-5b051232a44d"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Dec 23 15:06:45.198: INFO: Deleting pod "var-expansion-81564f15-86dd-4284-a15b-5b051232a44d" in namespace "var-expansion-2526"
Dec 23 15:06:45.205: INFO: Wait up to 5m0s for pod "var-expansion-81564f15-86dd-4284-a15b-5b051232a44d" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:07:31.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2526" for this suite.

• [SLOW TEST:50.774 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":280,"skipped":4880,"failed":0}
SS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:07:31.223: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2663
Dec 23 15:07:33.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2663 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec 23 15:07:33.503: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec 23 15:07:33.503: INFO: stdout: "iptables"
Dec 23 15:07:33.503: INFO: proxyMode: iptables
Dec 23 15:07:33.515: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 23 15:07:33.521: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-2663
STEP: creating replication controller affinity-nodeport-timeout in namespace services-2663
I1223 15:07:33.554142      20 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2663, replica count: 3
I1223 15:07:36.606324      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 23 15:07:36.621: INFO: Creating new exec pod
Dec 23 15:07:41.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2663 exec execpod-affinityjl9zr -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Dec 23 15:07:41.886: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Dec 23 15:07:41.886: INFO: stdout: ""
Dec 23 15:07:41.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2663 exec execpod-affinityjl9zr -- /bin/sh -x -c nc -zv -t -w 2 10.102.41.239 80'
Dec 23 15:07:42.091: INFO: stderr: "+ nc -zv -t -w 2 10.102.41.239 80\nConnection to 10.102.41.239 80 port [tcp/http] succeeded!\n"
Dec 23 15:07:42.091: INFO: stdout: ""
Dec 23 15:07:42.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2663 exec execpod-affinityjl9zr -- /bin/sh -x -c nc -zv -t -w 2 10.22.19.17 32071'
Dec 23 15:07:42.298: INFO: stderr: "+ nc -zv -t -w 2 10.22.19.17 32071\nConnection to 10.22.19.17 32071 port [tcp/32071] succeeded!\n"
Dec 23 15:07:42.298: INFO: stdout: ""
Dec 23 15:07:42.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2663 exec execpod-affinityjl9zr -- /bin/sh -x -c nc -zv -t -w 2 10.22.19.25 32071'
Dec 23 15:07:42.493: INFO: stderr: "+ nc -zv -t -w 2 10.22.19.25 32071\nConnection to 10.22.19.25 32071 port [tcp/32071] succeeded!\n"
Dec 23 15:07:42.493: INFO: stdout: ""
Dec 23 15:07:42.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2663 exec execpod-affinityjl9zr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.22.19.17:32071/ ; done'
Dec 23 15:07:42.846: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n"
Dec 23 15:07:42.846: INFO: stdout: "\naffinity-nodeport-timeout-trdsr\naffinity-nodeport-timeout-trdsr\naffinity-nodeport-timeout-trdsr\naffinity-nodeport-timeout-trdsr\naffinity-nodeport-timeout-trdsr\naffinity-nodeport-timeout-trdsr\naffinity-nodeport-timeout-trdsr\naffinity-nodeport-timeout-trdsr\naffinity-nodeport-timeout-trdsr\naffinity-nodeport-timeout-trdsr\naffinity-nodeport-timeout-trdsr\naffinity-nodeport-timeout-trdsr\naffinity-nodeport-timeout-trdsr\naffinity-nodeport-timeout-trdsr\naffinity-nodeport-timeout-trdsr\naffinity-nodeport-timeout-trdsr"
Dec 23 15:07:42.846: INFO: Received response from host: affinity-nodeport-timeout-trdsr
Dec 23 15:07:42.846: INFO: Received response from host: affinity-nodeport-timeout-trdsr
Dec 23 15:07:42.846: INFO: Received response from host: affinity-nodeport-timeout-trdsr
Dec 23 15:07:42.846: INFO: Received response from host: affinity-nodeport-timeout-trdsr
Dec 23 15:07:42.846: INFO: Received response from host: affinity-nodeport-timeout-trdsr
Dec 23 15:07:42.846: INFO: Received response from host: affinity-nodeport-timeout-trdsr
Dec 23 15:07:42.846: INFO: Received response from host: affinity-nodeport-timeout-trdsr
Dec 23 15:07:42.846: INFO: Received response from host: affinity-nodeport-timeout-trdsr
Dec 23 15:07:42.846: INFO: Received response from host: affinity-nodeport-timeout-trdsr
Dec 23 15:07:42.846: INFO: Received response from host: affinity-nodeport-timeout-trdsr
Dec 23 15:07:42.846: INFO: Received response from host: affinity-nodeport-timeout-trdsr
Dec 23 15:07:42.846: INFO: Received response from host: affinity-nodeport-timeout-trdsr
Dec 23 15:07:42.846: INFO: Received response from host: affinity-nodeport-timeout-trdsr
Dec 23 15:07:42.846: INFO: Received response from host: affinity-nodeport-timeout-trdsr
Dec 23 15:07:42.846: INFO: Received response from host: affinity-nodeport-timeout-trdsr
Dec 23 15:07:42.846: INFO: Received response from host: affinity-nodeport-timeout-trdsr
Dec 23 15:07:42.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2663 exec execpod-affinityjl9zr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.22.19.17:32071/'
Dec 23 15:07:43.054: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n"
Dec 23 15:07:43.054: INFO: stdout: "affinity-nodeport-timeout-trdsr"
Dec 23 15:08:03.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=services-2663 exec execpod-affinityjl9zr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.22.19.17:32071/'
Dec 23 15:08:03.293: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.22.19.17:32071/\n"
Dec 23 15:08:03.293: INFO: stdout: "affinity-nodeport-timeout-7r54q"
Dec 23 15:08:03.293: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2663, will wait for the garbage collector to delete the pods
Dec 23 15:08:03.374: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 5.68134ms
Dec 23 15:08:03.974: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 600.225495ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:08:29.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2663" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:58.250 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":281,"skipped":4882,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:08:29.474: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 15:08:29.527: INFO: Creating ReplicaSet my-hostname-basic-8fef165a-18b3-4b31-aae5-11f77f970382
Dec 23 15:08:29.534: INFO: Pod name my-hostname-basic-8fef165a-18b3-4b31-aae5-11f77f970382: Found 0 pods out of 1
Dec 23 15:08:34.554: INFO: Pod name my-hostname-basic-8fef165a-18b3-4b31-aae5-11f77f970382: Found 1 pods out of 1
Dec 23 15:08:34.554: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8fef165a-18b3-4b31-aae5-11f77f970382" is running
Dec 23 15:08:34.558: INFO: Pod "my-hostname-basic-8fef165a-18b3-4b31-aae5-11f77f970382-c7dlv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-23 15:08:15 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-23 15:08:17 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-23 15:08:17 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-23 15:08:15 +0000 UTC Reason: Message:}])
Dec 23 15:08:34.559: INFO: Trying to dial the pod
Dec 23 15:08:39.574: INFO: Controller my-hostname-basic-8fef165a-18b3-4b31-aae5-11f77f970382: Got expected result from replica 1 [my-hostname-basic-8fef165a-18b3-4b31-aae5-11f77f970382-c7dlv]: "my-hostname-basic-8fef165a-18b3-4b31-aae5-11f77f970382-c7dlv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:08:39.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1279" for this suite.

• [SLOW TEST:10.111 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":282,"skipped":4890,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:08:39.586: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 15:08:41.724: INFO: Waiting up to 5m0s for pod "client-envvars-561c9d53-32dd-47be-86f9-5c9037e38888" in namespace "pods-2339" to be "Succeeded or Failed"
Dec 23 15:08:41.732: INFO: Pod "client-envvars-561c9d53-32dd-47be-86f9-5c9037e38888": Phase="Pending", Reason="", readiness=false. Elapsed: 7.566274ms
Dec 23 15:08:43.740: INFO: Pod "client-envvars-561c9d53-32dd-47be-86f9-5c9037e38888": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015759379s
Dec 23 15:08:45.746: INFO: Pod "client-envvars-561c9d53-32dd-47be-86f9-5c9037e38888": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021936437s
STEP: Saw pod success
Dec 23 15:08:45.746: INFO: Pod "client-envvars-561c9d53-32dd-47be-86f9-5c9037e38888" satisfied condition "Succeeded or Failed"
Dec 23 15:08:45.749: INFO: Trying to get logs from node wt-k8s-3.novalocal pod client-envvars-561c9d53-32dd-47be-86f9-5c9037e38888 container env3cont: <nil>
STEP: delete the pod
Dec 23 15:08:45.781: INFO: Waiting for pod client-envvars-561c9d53-32dd-47be-86f9-5c9037e38888 to disappear
Dec 23 15:08:45.785: INFO: Pod client-envvars-561c9d53-32dd-47be-86f9-5c9037e38888 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:08:45.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2339" for this suite.

• [SLOW TEST:6.213 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":283,"skipped":4893,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:08:45.800: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:08:53.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4531" for this suite.

• [SLOW TEST:8.062 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":284,"skipped":4924,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:08:53.863: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 15:08:53.921: INFO: Waiting up to 5m0s for pod "downwardapi-volume-190b9745-ff17-4065-b588-f60384181158" in namespace "projected-4847" to be "Succeeded or Failed"
Dec 23 15:08:53.927: INFO: Pod "downwardapi-volume-190b9745-ff17-4065-b588-f60384181158": Phase="Pending", Reason="", readiness=false. Elapsed: 5.749622ms
Dec 23 15:08:55.935: INFO: Pod "downwardapi-volume-190b9745-ff17-4065-b588-f60384181158": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013315208s
Dec 23 15:08:57.944: INFO: Pod "downwardapi-volume-190b9745-ff17-4065-b588-f60384181158": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022522437s
STEP: Saw pod success
Dec 23 15:08:57.944: INFO: Pod "downwardapi-volume-190b9745-ff17-4065-b588-f60384181158" satisfied condition "Succeeded or Failed"
Dec 23 15:08:57.948: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-190b9745-ff17-4065-b588-f60384181158 container client-container: <nil>
STEP: delete the pod
Dec 23 15:08:57.967: INFO: Waiting for pod downwardapi-volume-190b9745-ff17-4065-b588-f60384181158 to disappear
Dec 23 15:08:57.975: INFO: Pod downwardapi-volume-190b9745-ff17-4065-b588-f60384181158 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:08:57.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4847" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":285,"skipped":4954,"failed":0}
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:08:57.986: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:09:02.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4696" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":286,"skipped":4955,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:09:02.074: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 23 15:09:02.119: INFO: Waiting up to 5m0s for pod "pod-c19773a7-b1ca-4b54-b46a-c15133f8652f" in namespace "emptydir-5598" to be "Succeeded or Failed"
Dec 23 15:09:02.147: INFO: Pod "pod-c19773a7-b1ca-4b54-b46a-c15133f8652f": Phase="Pending", Reason="", readiness=false. Elapsed: 27.885039ms
Dec 23 15:09:04.157: INFO: Pod "pod-c19773a7-b1ca-4b54-b46a-c15133f8652f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038431738s
Dec 23 15:09:06.166: INFO: Pod "pod-c19773a7-b1ca-4b54-b46a-c15133f8652f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047029626s
STEP: Saw pod success
Dec 23 15:09:06.166: INFO: Pod "pod-c19773a7-b1ca-4b54-b46a-c15133f8652f" satisfied condition "Succeeded or Failed"
Dec 23 15:09:06.168: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-c19773a7-b1ca-4b54-b46a-c15133f8652f container test-container: <nil>
STEP: delete the pod
Dec 23 15:09:06.186: INFO: Waiting for pod pod-c19773a7-b1ca-4b54-b46a-c15133f8652f to disappear
Dec 23 15:09:06.191: INFO: Pod pod-c19773a7-b1ca-4b54-b46a-c15133f8652f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:09:06.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5598" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":287,"skipped":4956,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:09:06.199: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:09:22.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-211" for this suite.

• [SLOW TEST:16.170 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":288,"skipped":4967,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:09:22.369: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 15:09:22.409: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-3ccd13ae-7303-4ccc-aa78-7733e2d157f6" in namespace "security-context-test-1549" to be "Succeeded or Failed"
Dec 23 15:09:22.424: INFO: Pod "busybox-readonly-false-3ccd13ae-7303-4ccc-aa78-7733e2d157f6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.576239ms
Dec 23 15:09:24.431: INFO: Pod "busybox-readonly-false-3ccd13ae-7303-4ccc-aa78-7733e2d157f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022170242s
Dec 23 15:09:26.444: INFO: Pod "busybox-readonly-false-3ccd13ae-7303-4ccc-aa78-7733e2d157f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03479915s
Dec 23 15:09:26.444: INFO: Pod "busybox-readonly-false-3ccd13ae-7303-4ccc-aa78-7733e2d157f6" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:09:26.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1549" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":289,"skipped":4976,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:09:26.452: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 15:09:26.511: INFO: Create a RollingUpdate DaemonSet
Dec 23 15:09:26.515: INFO: Check that daemon pods launch on every node of the cluster
Dec 23 15:09:26.522: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:26.540: INFO: Number of nodes with available pods: 0
Dec 23 15:09:26.540: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 15:09:27.554: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:27.558: INFO: Number of nodes with available pods: 0
Dec 23 15:09:27.558: INFO: Node wt-k8s-2 is running more than one daemon pod
Dec 23 15:09:28.553: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:28.558: INFO: Number of nodes with available pods: 1
Dec 23 15:09:28.558: INFO: Node wt-k8s-3.novalocal is running more than one daemon pod
Dec 23 15:09:29.547: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:29.549: INFO: Number of nodes with available pods: 2
Dec 23 15:09:29.549: INFO: Number of running nodes: 2, number of available pods: 2
Dec 23 15:09:29.549: INFO: Update the DaemonSet to trigger a rollout
Dec 23 15:09:29.557: INFO: Updating DaemonSet daemon-set
Dec 23 15:09:39.580: INFO: Roll back the DaemonSet before rollout is complete
Dec 23 15:09:39.589: INFO: Updating DaemonSet daemon-set
Dec 23 15:09:39.589: INFO: Make sure DaemonSet rollback is complete
Dec 23 15:09:39.594: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:39.594: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:39.599: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:40.607: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:40.607: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:40.611: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:41.605: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:41.605: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:41.608: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:42.607: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:42.607: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:42.611: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:43.605: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:43.605: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:43.609: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:44.608: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:44.608: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:44.612: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:45.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:45.606: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:45.609: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:46.605: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:46.605: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:46.609: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:47.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:47.607: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:47.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:48.605: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:48.605: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:48.609: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:49.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:49.606: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:49.609: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:50.607: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:50.607: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:50.611: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:51.605: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:51.605: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:51.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:52.607: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:52.607: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:52.612: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:53.605: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:53.605: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:53.608: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:54.605: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:54.605: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:54.609: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:55.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:55.607: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:55.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:56.717: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:56.717: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:56.733: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:57.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:57.606: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:57.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:58.605: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:58.605: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:58.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:09:59.605: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:09:59.605: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:09:59.609: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:00.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:00.606: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:00.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:01.605: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:01.605: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:01.609: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:02.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:02.606: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:02.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:03.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:03.606: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:03.609: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:04.605: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:04.605: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:04.609: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:05.607: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:05.607: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:05.611: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:06.604: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:06.604: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:06.608: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:07.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:07.607: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:07.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:08.605: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:08.605: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:08.609: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:09.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:09.606: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:09.609: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:10.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:10.606: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:10.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:11.612: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:11.612: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:11.615: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:12.608: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:12.608: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:12.612: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:13.607: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:13.607: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:13.611: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:14.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:14.606: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:14.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:15.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:15.606: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:15.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:16.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:16.606: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:16.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:17.607: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:17.607: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:17.612: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:18.605: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:18.605: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:18.609: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:19.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:19.606: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:19.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:20.608: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:20.608: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:20.615: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:21.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:21.606: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:21.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:22.607: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:22.607: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:22.612: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:23.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:23.606: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:23.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:24.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:24.606: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:24.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:25.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:25.606: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:25.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:26.606: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:26.606: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:26.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:27.607: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:27.607: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:27.610: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:28.604: INFO: Wrong image for pod: daemon-set-gsfb9. Expected: abcsys.cn:5000/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 23 15:10:28.604: INFO: Pod daemon-set-gsfb9 is not available
Dec 23 15:10:28.609: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 23 15:10:29.605: INFO: Pod daemon-set-sx2l7 is not available
Dec 23 15:10:29.609: INFO: DaemonSet pods can't tolerate node wt-k8s-1.novalocal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7343, will wait for the garbage collector to delete the pods
Dec 23 15:10:29.673: INFO: Deleting DaemonSet.extensions daemon-set took: 6.096968ms
Dec 23 15:10:30.274: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.662243ms
Dec 23 15:10:39.279: INFO: Number of nodes with available pods: 0
Dec 23 15:10:39.279: INFO: Number of running nodes: 0, number of available pods: 0
Dec 23 15:10:39.282: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"108815"},"items":null}

Dec 23 15:10:39.285: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"108815"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:10:39.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7343" for this suite.

• [SLOW TEST:72.858 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":290,"skipped":4990,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:10:39.310: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 23 15:10:39.379: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1034  ebd3e156-be39-482c-89f0-a4e525ccf249 108822 0 2020-12-23 15:10:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-23 15:10:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 23 15:10:39.379: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1034  ebd3e156-be39-482c-89f0-a4e525ccf249 108823 0 2020-12-23 15:10:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-23 15:10:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 23 15:10:39.391: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1034  ebd3e156-be39-482c-89f0-a4e525ccf249 108824 0 2020-12-23 15:10:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-23 15:10:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 23 15:10:39.391: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1034  ebd3e156-be39-482c-89f0-a4e525ccf249 108825 0 2020-12-23 15:10:25 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-23 15:10:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:10:39.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1034" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":291,"skipped":4991,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:10:39.399: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8508
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8508
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8508
Dec 23 15:10:39.458: INFO: Found 0 stateful pods, waiting for 1
Dec 23 15:10:49.465: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 23 15:10:49.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-8508 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 23 15:10:50.006: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 23 15:10:50.006: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 23 15:10:50.006: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 23 15:10:50.010: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 23 15:11:00.018: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 23 15:11:00.018: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 15:11:00.037: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999422s
Dec 23 15:11:01.044: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993278129s
Dec 23 15:11:02.051: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985779661s
Dec 23 15:11:03.064: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.978668966s
Dec 23 15:11:04.070: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.966493561s
Dec 23 15:11:05.078: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.960126517s
Dec 23 15:11:06.084: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.952388232s
Dec 23 15:11:07.091: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.946100753s
Dec 23 15:11:08.098: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.939034197s
Dec 23 15:11:09.105: INFO: Verifying statefulset ss doesn't scale past 1 for another 932.003799ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8508
Dec 23 15:11:10.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-8508 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 15:11:10.320: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 23 15:11:10.320: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 23 15:11:10.320: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 23 15:11:10.323: INFO: Found 1 stateful pods, waiting for 3
Dec 23 15:11:20.336: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 15:11:20.336: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 15:11:20.336: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 23 15:11:20.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-8508 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 23 15:11:20.573: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 23 15:11:20.573: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 23 15:11:20.573: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 23 15:11:20.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-8508 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 23 15:11:20.790: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 23 15:11:20.790: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 23 15:11:20.790: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 23 15:11:20.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-8508 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 23 15:11:20.994: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 23 15:11:20.994: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 23 15:11:20.994: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 23 15:11:20.994: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 15:11:20.997: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 23 15:11:31.006: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 23 15:11:31.006: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 23 15:11:31.006: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 23 15:11:31.022: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999934s
Dec 23 15:11:32.030: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992512249s
Dec 23 15:11:33.039: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98343166s
Dec 23 15:11:34.047: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.976169633s
Dec 23 15:11:35.054: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.968259086s
Dec 23 15:11:36.061: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.95998375s
Dec 23 15:11:37.068: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.953770875s
Dec 23 15:11:38.075: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.946858895s
Dec 23 15:11:39.079: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.939647011s
Dec 23 15:11:40.089: INFO: Verifying statefulset ss doesn't scale past 3 for another 935.525305ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8508
Dec 23 15:11:41.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-8508 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 15:11:41.317: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 23 15:11:41.317: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 23 15:11:41.317: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 23 15:11:41.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-8508 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 15:11:41.573: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 23 15:11:41.573: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 23 15:11:41.573: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 23 15:11:41.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-248507315 --namespace=statefulset-8508 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 23 15:11:41.776: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 23 15:11:41.776: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 23 15:11:41.776: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 23 15:11:41.776: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Dec 23 15:12:31.797: INFO: Deleting all statefulset in ns statefulset-8508
Dec 23 15:12:31.800: INFO: Scaling statefulset ss to 0
Dec 23 15:12:31.810: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 15:12:31.812: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:12:31.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8508" for this suite.

• [SLOW TEST:112.449 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":292,"skipped":5012,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:12:31.849: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:12:48.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1036" for this suite.

• [SLOW TEST:16.177 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":293,"skipped":5021,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:12:48.027: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Dec 23 15:12:48.082: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1040  2002b7dc-5117-4864-ac2a-cff650eab6ea 109357 0 2020-12-23 15:12:34 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2020-12-23 15:12:34 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bj2jn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bj2jn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:abcsys.cn:5000/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bj2jn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 23 15:12:48.098: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Dec 23 15:12:50.106: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Dec 23 15:12:50.106: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1040 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 15:12:50.106: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Verifying customized DNS server is configured on pod...
Dec 23 15:12:50.210: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1040 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 15:12:50.211: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 15:12:50.340: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:12:50.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1040" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":294,"skipped":5047,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:12:50.375: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-69dc3a0d-78bd-4040-a343-7f0452ebccf2
STEP: Creating a pod to test consume configMaps
Dec 23 15:12:50.429: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-752ac14b-57a0-4151-ac9e-6a8f273e2f6b" in namespace "projected-2322" to be "Succeeded or Failed"
Dec 23 15:12:50.433: INFO: Pod "pod-projected-configmaps-752ac14b-57a0-4151-ac9e-6a8f273e2f6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.425793ms
Dec 23 15:12:52.442: INFO: Pod "pod-projected-configmaps-752ac14b-57a0-4151-ac9e-6a8f273e2f6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012949774s
STEP: Saw pod success
Dec 23 15:12:52.442: INFO: Pod "pod-projected-configmaps-752ac14b-57a0-4151-ac9e-6a8f273e2f6b" satisfied condition "Succeeded or Failed"
Dec 23 15:12:52.444: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-projected-configmaps-752ac14b-57a0-4151-ac9e-6a8f273e2f6b container agnhost-container: <nil>
STEP: delete the pod
Dec 23 15:12:52.506: INFO: Waiting for pod pod-projected-configmaps-752ac14b-57a0-4151-ac9e-6a8f273e2f6b to disappear
Dec 23 15:12:52.511: INFO: Pod pod-projected-configmaps-752ac14b-57a0-4151-ac9e-6a8f273e2f6b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:12:52.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2322" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":295,"skipped":5100,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:12:52.520: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 23 15:12:52.565: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:13:09.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-778" for this suite.

• [SLOW TEST:16.748 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":296,"skipped":5114,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:13:09.268: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:13:09.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5064" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":297,"skipped":5126,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:13:09.405: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:13:09.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5130" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":298,"skipped":5133,"failed":0}

------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:13:09.459: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 15:13:09.490: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:13:13.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4546" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":299,"skipped":5133,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:13:13.530: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 23 15:13:14.668: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 23 15:13:16.681: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744333180, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744333180, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744333180, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744333180, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-76bbb7fdd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 15:13:19.842: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
Dec 23 15:13:19.866: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:19.980: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:20.080: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:20.180: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:20.281: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:20.380: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:20.481: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:20.581: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:20.682: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:20.783: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:20.884: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:20.980: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:21.082: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:21.182: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:21.284: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:21.388: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:21.481: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:21.581: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:21.680: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:21.781: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:21.885: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:21.986: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:22.082: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:22.180: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:22.281: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:22.379: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:22.480: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:22.582: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:22.686: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:22.782: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:22.880: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:22.979: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:23.082: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:23.183: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:23.280: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:23.380: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:23.480: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:23.602: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:23.679: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:23.780: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:23.882: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:23.981: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:24.081: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:24.181: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:24.285: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:24.389: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:24.481: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:24.580: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:24.683: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:24.791: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:24.880: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:24.981: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:25.080: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:25.181: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:25.280: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:25.381: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:25.484: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:25.580: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:25.683: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:25.783: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:25.958: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:25.980: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:26.082: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:26.180: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:26.281: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:26.382: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:26.481: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:26.583: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:26.681: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:26.780: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:26.881: INFO: Waiting for webhook configuration to be ready...
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:13:39.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4162" for this suite.
STEP: Destroying namespace "webhook-4162-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:25.647 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":300,"skipped":5139,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:13:39.177: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Dec 23 15:13:39.211: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:13:42.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7610" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":301,"skipped":5176,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:13:42.500: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 23 15:13:42.980: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 23 15:13:44.993: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744333209, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744333209, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63744333209, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63744333209, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-76bbb7fdd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 23 15:13:48.065: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
Dec 23 15:13:48.095: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:48.209: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:48.311: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:48.411: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:48.509: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:48.611: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:48.712: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:48.815: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:48.917: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:49.049: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:49.111: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:49.210: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:49.310: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:49.409: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:49.508: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:49.610: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:49.709: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:49.810: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:49.909: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:50.009: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:50.110: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:50.208: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:50.308: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:50.410: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:50.511: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:50.614: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:50.712: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:50.811: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:50.908: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:51.011: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:51.111: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:51.210: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:51.311: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:51.410: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:51.511: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:51.612: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:51.711: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:51.809: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:51.908: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:52.010: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:52.110: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:52.209: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:52.311: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:52.409: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:52.509: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:52.611: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:52.720: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:52.811: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:52.915: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:53.014: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:53.110: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:53.211: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:53.309: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:53.409: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:53.509: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:53.610: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:53.709: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:53.809: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:53.913: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:54.009: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:54.111: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:54.209: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:54.316: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:54.414: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:54.510: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:54.610: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:54.711: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:54.821: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:54.909: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:55.010: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:55.109: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:55.208: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:55.309: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:55.409: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:55.510: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:55.609: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:55.713: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:55.815: INFO: Waiting for webhook configuration to be ready...
Dec 23 15:13:56.155: INFO: Waiting for webhook configuration to be ready...
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:13:56.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8358" for this suite.
STEP: Destroying namespace "webhook-8358-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:13.910 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":302,"skipped":5199,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:13:56.411: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-vn5j
STEP: Creating a pod to test atomic-volume-subpath
Dec 23 15:13:56.462: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-vn5j" in namespace "subpath-4127" to be "Succeeded or Failed"
Dec 23 15:13:56.464: INFO: Pod "pod-subpath-test-downwardapi-vn5j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.27963ms
Dec 23 15:13:59.146: INFO: Pod "pod-subpath-test-downwardapi-vn5j": Phase="Running", Reason="", readiness=true. Elapsed: 2.684356601s
Dec 23 15:14:01.157: INFO: Pod "pod-subpath-test-downwardapi-vn5j": Phase="Running", Reason="", readiness=true. Elapsed: 4.694565758s
Dec 23 15:14:03.168: INFO: Pod "pod-subpath-test-downwardapi-vn5j": Phase="Running", Reason="", readiness=true. Elapsed: 6.705923103s
Dec 23 15:14:05.177: INFO: Pod "pod-subpath-test-downwardapi-vn5j": Phase="Running", Reason="", readiness=true. Elapsed: 8.714495064s
Dec 23 15:14:07.186: INFO: Pod "pod-subpath-test-downwardapi-vn5j": Phase="Running", Reason="", readiness=true. Elapsed: 10.723904077s
Dec 23 15:14:09.190: INFO: Pod "pod-subpath-test-downwardapi-vn5j": Phase="Running", Reason="", readiness=true. Elapsed: 12.728090522s
Dec 23 15:14:11.198: INFO: Pod "pod-subpath-test-downwardapi-vn5j": Phase="Running", Reason="", readiness=true. Elapsed: 14.73643375s
Dec 23 15:14:13.206: INFO: Pod "pod-subpath-test-downwardapi-vn5j": Phase="Running", Reason="", readiness=true. Elapsed: 16.743916449s
Dec 23 15:14:15.214: INFO: Pod "pod-subpath-test-downwardapi-vn5j": Phase="Running", Reason="", readiness=true. Elapsed: 18.751978734s
Dec 23 15:14:17.221: INFO: Pod "pod-subpath-test-downwardapi-vn5j": Phase="Running", Reason="", readiness=true. Elapsed: 20.758714861s
Dec 23 15:14:19.225: INFO: Pod "pod-subpath-test-downwardapi-vn5j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.762703998s
STEP: Saw pod success
Dec 23 15:14:19.225: INFO: Pod "pod-subpath-test-downwardapi-vn5j" satisfied condition "Succeeded or Failed"
Dec 23 15:14:19.227: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-subpath-test-downwardapi-vn5j container test-container-subpath-downwardapi-vn5j: <nil>
STEP: delete the pod
Dec 23 15:14:19.260: INFO: Waiting for pod pod-subpath-test-downwardapi-vn5j to disappear
Dec 23 15:14:19.264: INFO: Pod pod-subpath-test-downwardapi-vn5j no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-vn5j
Dec 23 15:14:19.264: INFO: Deleting pod "pod-subpath-test-downwardapi-vn5j" in namespace "subpath-4127"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:14:19.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4127" for this suite.

• [SLOW TEST:22.862 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":303,"skipped":5213,"failed":0}
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:14:19.273: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Dec 23 15:14:19.313: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c67c2fe-230a-463d-91bd-7433e6b88853" in namespace "downward-api-9584" to be "Succeeded or Failed"
Dec 23 15:14:19.321: INFO: Pod "downwardapi-volume-5c67c2fe-230a-463d-91bd-7433e6b88853": Phase="Pending", Reason="", readiness=false. Elapsed: 7.279445ms
Dec 23 15:14:21.329: INFO: Pod "downwardapi-volume-5c67c2fe-230a-463d-91bd-7433e6b88853": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015457003s
Dec 23 15:14:23.337: INFO: Pod "downwardapi-volume-5c67c2fe-230a-463d-91bd-7433e6b88853": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023357629s
STEP: Saw pod success
Dec 23 15:14:23.337: INFO: Pod "downwardapi-volume-5c67c2fe-230a-463d-91bd-7433e6b88853" satisfied condition "Succeeded or Failed"
Dec 23 15:14:23.339: INFO: Trying to get logs from node wt-k8s-3.novalocal pod downwardapi-volume-5c67c2fe-230a-463d-91bd-7433e6b88853 container client-container: <nil>
STEP: delete the pod
Dec 23 15:14:23.371: INFO: Waiting for pod downwardapi-volume-5c67c2fe-230a-463d-91bd-7433e6b88853 to disappear
Dec 23 15:14:23.373: INFO: Pod downwardapi-volume-5c67c2fe-230a-463d-91bd-7433e6b88853 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:14:23.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9584" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":304,"skipped":5213,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:14:23.379: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 23 15:14:29.452: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-834 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 15:14:29.452: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 15:14:29.546: INFO: Exec stderr: ""
Dec 23 15:14:29.546: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-834 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 15:14:29.546: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 15:14:29.640: INFO: Exec stderr: ""
Dec 23 15:14:29.640: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-834 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 15:14:29.640: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 15:14:29.732: INFO: Exec stderr: ""
Dec 23 15:14:29.733: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-834 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 15:14:29.733: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 15:14:29.833: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 23 15:14:29.834: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-834 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 15:14:29.834: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 15:14:29.922: INFO: Exec stderr: ""
Dec 23 15:14:29.922: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-834 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 15:14:29.922: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 15:14:30.008: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 23 15:14:30.008: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-834 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 15:14:30.008: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 15:14:30.151: INFO: Exec stderr: ""
Dec 23 15:14:30.151: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-834 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 15:14:30.152: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 15:14:30.249: INFO: Exec stderr: ""
Dec 23 15:14:30.249: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-834 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 15:14:30.249: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 15:14:30.378: INFO: Exec stderr: ""
Dec 23 15:14:30.378: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-834 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 23 15:14:30.378: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 15:14:30.479: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:14:30.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-834" for this suite.

• [SLOW TEST:7.112 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":305,"skipped":5234,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:14:30.491: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-6131d079-a837-45bf-aaae-ea06a76ce1b2
STEP: Creating a pod to test consume secrets
Dec 23 15:14:30.536: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cdddafc1-71ba-407f-8412-14dba01f00e0" in namespace "projected-730" to be "Succeeded or Failed"
Dec 23 15:14:30.543: INFO: Pod "pod-projected-secrets-cdddafc1-71ba-407f-8412-14dba01f00e0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.104674ms
Dec 23 15:14:32.551: INFO: Pod "pod-projected-secrets-cdddafc1-71ba-407f-8412-14dba01f00e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015393979s
STEP: Saw pod success
Dec 23 15:14:32.551: INFO: Pod "pod-projected-secrets-cdddafc1-71ba-407f-8412-14dba01f00e0" satisfied condition "Succeeded or Failed"
Dec 23 15:14:32.554: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-projected-secrets-cdddafc1-71ba-407f-8412-14dba01f00e0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 23 15:14:32.574: INFO: Waiting for pod pod-projected-secrets-cdddafc1-71ba-407f-8412-14dba01f00e0 to disappear
Dec 23 15:14:32.576: INFO: Pod pod-projected-secrets-cdddafc1-71ba-407f-8412-14dba01f00e0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:14:32.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-730" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":306,"skipped":5253,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:14:32.590: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Dec 23 15:14:32.630: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:14:38.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6350" for this suite.

• [SLOW TEST:6.228 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":307,"skipped":5287,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:14:38.818: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 23 15:14:39.550: INFO: Waiting up to 5m0s for pod "pod-488ec3f2-a498-43c7-9887-733f9689e956" in namespace "emptydir-9865" to be "Succeeded or Failed"
Dec 23 15:14:39.618: INFO: Pod "pod-488ec3f2-a498-43c7-9887-733f9689e956": Phase="Pending", Reason="", readiness=false. Elapsed: 67.916536ms
Dec 23 15:14:41.626: INFO: Pod "pod-488ec3f2-a498-43c7-9887-733f9689e956": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076028879s
Dec 23 15:14:43.637: INFO: Pod "pod-488ec3f2-a498-43c7-9887-733f9689e956": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.086629745s
STEP: Saw pod success
Dec 23 15:14:43.637: INFO: Pod "pod-488ec3f2-a498-43c7-9887-733f9689e956" satisfied condition "Succeeded or Failed"
Dec 23 15:14:43.639: INFO: Trying to get logs from node wt-k8s-3.novalocal pod pod-488ec3f2-a498-43c7-9887-733f9689e956 container test-container: <nil>
STEP: delete the pod
Dec 23 15:14:43.655: INFO: Waiting for pod pod-488ec3f2-a498-43c7-9887-733f9689e956 to disappear
Dec 23 15:14:43.660: INFO: Pod pod-488ec3f2-a498-43c7-9887-733f9689e956 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:14:43.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9865" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":308,"skipped":5290,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:14:43.668: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 23 15:14:46.737: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:14:46.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5280" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":309,"skipped":5294,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:14:46.756: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-cada383d-e090-43a2-94b2-f68f95dee991 in namespace container-probe-8994
Dec 23 15:14:48.831: INFO: Started pod liveness-cada383d-e090-43a2-94b2-f68f95dee991 in namespace container-probe-8994
STEP: checking the pod's current state and verifying that restartCount is present
Dec 23 15:14:48.834: INFO: Initial restart count of pod liveness-cada383d-e090-43a2-94b2-f68f95dee991 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:18:50.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8994" for this suite.

• [SLOW TEST:243.512 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":310,"skipped":5297,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Dec 23 15:18:50.270: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Dec 23 15:18:50.313: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
Dec 23 15:18:54.248: INFO: >>> kubeConfig: /tmp/kubeconfig-248507315
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Dec 23 15:19:09.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8058" for this suite.

• [SLOW TEST:19.687 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":311,"skipped":5301,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSDec 23 15:19:09.958: INFO: Running AfterSuite actions on all nodes
Dec 23 15:19:09.958: INFO: Running AfterSuite actions on node 1
Dec 23 15:19:09.958: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 6859.497 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
PASS

Ginkgo ran 1 suite in 1h54m21.635039731s
Test Suite Passed
