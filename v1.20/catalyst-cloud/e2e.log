I0416 01:32:43.766965      25 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-565212062
I0416 01:32:43.766999      25 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0416 01:32:43.767097      25 e2e.go:129] Starting e2e run "25c70e31-a974-49fa-b4b1-d18d1584ad25" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1618536762 - Will randomize all specs
Will run 311 of 5667 specs

Apr 16 01:32:43.782: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 01:32:43.788: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr 16 01:32:43.873: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr 16 01:32:44.002: INFO: 37 / 37 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr 16 01:32:44.002: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
Apr 16 01:32:44.002: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr 16 01:32:44.022: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Apr 16 01:32:44.022: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
Apr 16 01:32:44.022: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'k8s-keystone-auth' (0 seconds elapsed)
Apr 16 01:32:44.022: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'magnum-auto-healer' (0 seconds elapsed)
Apr 16 01:32:44.022: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'magnum-prometheus-node-exporter' (0 seconds elapsed)
Apr 16 01:32:44.022: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'npd' (0 seconds elapsed)
Apr 16 01:32:44.022: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
Apr 16 01:32:44.022: INFO: e2e test version: v1.20.4
Apr 16 01:32:44.028: INFO: kube-apiserver version: v1.20.4
Apr 16 01:32:44.028: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 01:32:44.044: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:32:44.045: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename secrets
Apr 16 01:32:44.165: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Apr 16 01:32:44.199: INFO: PSP annotation exists on dry run pod: "magnum-grafana-test"; assuming PodSecurityPolicy is enabled
Apr 16 01:32:44.248: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7140
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-d1f1668e-a770-41c7-a461-32f0a0262ab8
STEP: Creating a pod to test consume secrets
Apr 16 01:32:44.457: INFO: Waiting up to 5m0s for pod "pod-secrets-5723d8df-0cd0-4460-99b1-e67d787092b3" in namespace "secrets-7140" to be "Succeeded or Failed"
Apr 16 01:32:44.463: INFO: Pod "pod-secrets-5723d8df-0cd0-4460-99b1-e67d787092b3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.887955ms
Apr 16 01:32:46.490: INFO: Pod "pod-secrets-5723d8df-0cd0-4460-99b1-e67d787092b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032552875s
Apr 16 01:32:48.499: INFO: Pod "pod-secrets-5723d8df-0cd0-4460-99b1-e67d787092b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041473019s
Apr 16 01:32:50.567: INFO: Pod "pod-secrets-5723d8df-0cd0-4460-99b1-e67d787092b3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.109798877s
Apr 16 01:32:52.591: INFO: Pod "pod-secrets-5723d8df-0cd0-4460-99b1-e67d787092b3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.134142243s
Apr 16 01:32:54.606: INFO: Pod "pod-secrets-5723d8df-0cd0-4460-99b1-e67d787092b3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.148767428s
Apr 16 01:32:56.624: INFO: Pod "pod-secrets-5723d8df-0cd0-4460-99b1-e67d787092b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.166633668s
STEP: Saw pod success
Apr 16 01:32:56.624: INFO: Pod "pod-secrets-5723d8df-0cd0-4460-99b1-e67d787092b3" satisfied condition "Succeeded or Failed"
Apr 16 01:32:56.629: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-secrets-5723d8df-0cd0-4460-99b1-e67d787092b3 container secret-volume-test: <nil>
STEP: delete the pod
Apr 16 01:32:56.835: INFO: Waiting for pod pod-secrets-5723d8df-0cd0-4460-99b1-e67d787092b3 to disappear
Apr 16 01:32:56.851: INFO: Pod pod-secrets-5723d8df-0cd0-4460-99b1-e67d787092b3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:32:56.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7140" for this suite.

• [SLOW TEST:12.834 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":1,"skipped":68,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:32:56.881: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1934
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Apr 16 01:32:57.145: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:33:19.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1934" for this suite.

• [SLOW TEST:22.618 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":2,"skipped":112,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:33:19.499: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3088
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-4e349aa8-a015-403b-8332-221eeffc2301
STEP: Creating a pod to test consume configMaps
Apr 16 01:33:19.812: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bbbb96d5-10f4-4604-95b0-6fbd512d714c" in namespace "projected-3088" to be "Succeeded or Failed"
Apr 16 01:33:19.834: INFO: Pod "pod-projected-configmaps-bbbb96d5-10f4-4604-95b0-6fbd512d714c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.43639ms
Apr 16 01:33:21.844: INFO: Pod "pod-projected-configmaps-bbbb96d5-10f4-4604-95b0-6fbd512d714c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031594211s
STEP: Saw pod success
Apr 16 01:33:21.844: INFO: Pod "pod-projected-configmaps-bbbb96d5-10f4-4604-95b0-6fbd512d714c" satisfied condition "Succeeded or Failed"
Apr 16 01:33:21.855: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-projected-configmaps-bbbb96d5-10f4-4604-95b0-6fbd512d714c container agnhost-container: <nil>
STEP: delete the pod
Apr 16 01:33:21.986: INFO: Waiting for pod pod-projected-configmaps-bbbb96d5-10f4-4604-95b0-6fbd512d714c to disappear
Apr 16 01:33:21.994: INFO: Pod pod-projected-configmaps-bbbb96d5-10f4-4604-95b0-6fbd512d714c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:33:21.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3088" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":3,"skipped":117,"failed":0}
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:33:22.030: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-996
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Apr 16 01:33:22.263: INFO: PodSpec: initContainers in spec.initContainers
Apr 16 01:34:18.851: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-edcdee31-2ce1-4f66-8f19-d268d1e7d365", GenerateName:"", Namespace:"init-container-996", SelfLink:"", UID:"7de4203e-a982-45d1-b27a-0ceaa81a120b", ResourceVersion:"404657", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63754133602, loc:(*time.Location)(0x797de40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"263822874"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.100.97.69/32", "cni.projectcalico.org/podIPs":"10.100.97.69/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003c0c660), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003c0c6a0)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003c0c700), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003c0c720)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003c0c760), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003c0c780)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-ddvc7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0016f6780), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ddvc7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ddvc7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ddvc7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005d18658), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-flwang-4oyd5hkwxglz-node-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00332e540), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005d186d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005d18720)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005d18728), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc005d1872c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004265ef0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754133602, loc:(*time.Location)(0x797de40)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754133602, loc:(*time.Location)(0x797de40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754133602, loc:(*time.Location)(0x797de40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754133602, loc:(*time.Location)(0x797de40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.0.68", PodIP:"10.100.97.69", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.100.97.69"}}, StartTime:(*v1.Time)(0xc003c0c7a0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00332e620)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00332e690)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://84618bf76a3cd6397f04e5deeca3909bf3ff0015232ec67ac7604fcd964451cc", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003c0c7e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003c0c7c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc005d187ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:34:18.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-996" for this suite.

• [SLOW TEST:56.875 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":4,"skipped":122,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:34:18.905: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6422
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Apr 16 01:34:19.147: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:34:22.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6422" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":5,"skipped":143,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:34:22.923: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4614
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Apr 16 01:34:23.283: INFO: Waiting up to 5m0s for pod "var-expansion-cf1770dd-6836-4290-bc90-c1e498b4c024" in namespace "var-expansion-4614" to be "Succeeded or Failed"
Apr 16 01:34:23.307: INFO: Pod "var-expansion-cf1770dd-6836-4290-bc90-c1e498b4c024": Phase="Pending", Reason="", readiness=false. Elapsed: 24.50601ms
Apr 16 01:34:25.331: INFO: Pod "var-expansion-cf1770dd-6836-4290-bc90-c1e498b4c024": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.048531321s
STEP: Saw pod success
Apr 16 01:34:25.331: INFO: Pod "var-expansion-cf1770dd-6836-4290-bc90-c1e498b4c024" satisfied condition "Succeeded or Failed"
Apr 16 01:34:25.341: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod var-expansion-cf1770dd-6836-4290-bc90-c1e498b4c024 container dapi-container: <nil>
STEP: delete the pod
Apr 16 01:34:25.400: INFO: Waiting for pod var-expansion-cf1770dd-6836-4290-bc90-c1e498b4c024 to disappear
Apr 16 01:34:25.408: INFO: Pod var-expansion-cf1770dd-6836-4290-bc90-c1e498b4c024 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:34:25.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4614" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":6,"skipped":154,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:34:25.443: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5619
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:34:38.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5619" for this suite.

• [SLOW TEST:13.569 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":7,"skipped":159,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:34:39.015: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-325
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 16 01:34:41.347: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:34:41.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-325" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":8,"skipped":178,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:34:41.423: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6226
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-5be0045f-74e5-49f9-ae5b-198a375636ff
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:34:45.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6226" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":9,"skipped":236,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:34:45.877: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5037
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-vm7z
STEP: Creating a pod to test atomic-volume-subpath
Apr 16 01:34:46.202: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vm7z" in namespace "subpath-5037" to be "Succeeded or Failed"
Apr 16 01:34:46.221: INFO: Pod "pod-subpath-test-configmap-vm7z": Phase="Pending", Reason="", readiness=false. Elapsed: 18.649333ms
Apr 16 01:34:48.237: INFO: Pod "pod-subpath-test-configmap-vm7z": Phase="Running", Reason="", readiness=true. Elapsed: 2.035013847s
Apr 16 01:34:50.252: INFO: Pod "pod-subpath-test-configmap-vm7z": Phase="Running", Reason="", readiness=true. Elapsed: 4.049972583s
Apr 16 01:34:52.271: INFO: Pod "pod-subpath-test-configmap-vm7z": Phase="Running", Reason="", readiness=true. Elapsed: 6.068861559s
Apr 16 01:34:54.289: INFO: Pod "pod-subpath-test-configmap-vm7z": Phase="Running", Reason="", readiness=true. Elapsed: 8.086185361s
Apr 16 01:34:56.309: INFO: Pod "pod-subpath-test-configmap-vm7z": Phase="Running", Reason="", readiness=true. Elapsed: 10.106516683s
Apr 16 01:34:58.328: INFO: Pod "pod-subpath-test-configmap-vm7z": Phase="Running", Reason="", readiness=true. Elapsed: 12.125395764s
Apr 16 01:35:00.344: INFO: Pod "pod-subpath-test-configmap-vm7z": Phase="Running", Reason="", readiness=true. Elapsed: 14.14124869s
Apr 16 01:35:02.361: INFO: Pod "pod-subpath-test-configmap-vm7z": Phase="Running", Reason="", readiness=true. Elapsed: 16.158670321s
Apr 16 01:35:04.378: INFO: Pod "pod-subpath-test-configmap-vm7z": Phase="Running", Reason="", readiness=true. Elapsed: 18.175037723s
Apr 16 01:35:06.392: INFO: Pod "pod-subpath-test-configmap-vm7z": Phase="Running", Reason="", readiness=true. Elapsed: 20.18939191s
Apr 16 01:35:08.409: INFO: Pod "pod-subpath-test-configmap-vm7z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.206241947s
STEP: Saw pod success
Apr 16 01:35:08.409: INFO: Pod "pod-subpath-test-configmap-vm7z" satisfied condition "Succeeded or Failed"
Apr 16 01:35:08.416: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-subpath-test-configmap-vm7z container test-container-subpath-configmap-vm7z: <nil>
STEP: delete the pod
Apr 16 01:35:08.466: INFO: Waiting for pod pod-subpath-test-configmap-vm7z to disappear
Apr 16 01:35:08.475: INFO: Pod pod-subpath-test-configmap-vm7z no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vm7z
Apr 16 01:35:08.476: INFO: Deleting pod "pod-subpath-test-configmap-vm7z" in namespace "subpath-5037"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:35:08.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5037" for this suite.

• [SLOW TEST:22.644 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":10,"skipped":264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:35:08.533: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2759
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:35:08.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2759" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":11,"skipped":299,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:35:08.853: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5437
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-0d6d8e9d-0254-420b-bd09-1266de4940d5 in namespace container-probe-5437
Apr 16 01:35:11.158: INFO: Started pod busybox-0d6d8e9d-0254-420b-bd09-1266de4940d5 in namespace container-probe-5437
STEP: checking the pod's current state and verifying that restartCount is present
Apr 16 01:35:11.168: INFO: Initial restart count of pod busybox-0d6d8e9d-0254-420b-bd09-1266de4940d5 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:39:13.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5437" for this suite.

• [SLOW TEST:244.392 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":12,"skipped":307,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:39:13.250: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9353
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 16 01:39:13.470: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 16 01:39:13.491: INFO: Waiting for terminating namespaces to be deleted...
Apr 16 01:39:13.502: INFO: 
Logging pods the apiserver thinks is on node k8s-flwang-4oyd5hkwxglz-node-0 before test
Apr 16 01:39:13.535: INFO: alertmanager-magnum-prometheus-operator-alertmanager-0 from kube-system started at 2021-04-14 03:28:17 +0000 UTC (2 container statuses recorded)
Apr 16 01:39:13.535: INFO: 	Container alertmanager ready: true, restart count 0
Apr 16 01:39:13.535: INFO: 	Container config-reloader ready: true, restart count 0
Apr 16 01:39:13.535: INFO: calico-node-8pzl7 from kube-system started at 2021-04-14 03:25:24 +0000 UTC (1 container statuses recorded)
Apr 16 01:39:13.535: INFO: 	Container calico-node ready: true, restart count 0
Apr 16 01:39:13.535: INFO: csi-cinder-nodeplugin-xngdr from kube-system started at 2021-04-14 03:25:44 +0000 UTC (2 container statuses recorded)
Apr 16 01:39:13.536: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 16 01:39:13.536: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 16 01:39:13.536: INFO: kube-dns-autoscaler-7d66dbddbc-mmpzq from kube-system started at 2021-04-14 03:25:55 +0000 UTC (1 container statuses recorded)
Apr 16 01:39:13.536: INFO: 	Container autoscaler ready: true, restart count 0
Apr 16 01:39:13.536: INFO: magnum-grafana-57b59f6cd-lmchw from kube-system started at 2021-04-14 03:27:41 +0000 UTC (2 container statuses recorded)
Apr 16 01:39:13.536: INFO: 	Container grafana ready: true, restart count 0
Apr 16 01:39:13.536: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Apr 16 01:39:13.536: INFO: magnum-kube-state-metrics-7f877f89d8-9685l from kube-system started at 2021-04-14 03:27:41 +0000 UTC (1 container statuses recorded)
Apr 16 01:39:13.536: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 16 01:39:13.536: INFO: magnum-metrics-server-6b85d69787-r9lnz from kube-system started at 2021-04-14 03:27:41 +0000 UTC (1 container statuses recorded)
Apr 16 01:39:13.536: INFO: 	Container metrics-server ready: true, restart count 0
Apr 16 01:39:13.536: INFO: magnum-prometheus-node-exporter-zs5ln from kube-system started at 2021-04-14 03:27:41 +0000 UTC (1 container statuses recorded)
Apr 16 01:39:13.537: INFO: 	Container node-exporter ready: true, restart count 0
Apr 16 01:39:13.537: INFO: magnum-prometheus-operator-operator-6b8b498699-pwg7j from kube-system started at 2021-04-14 03:27:41 +0000 UTC (2 container statuses recorded)
Apr 16 01:39:13.537: INFO: 	Container prometheus-operator ready: true, restart count 0
Apr 16 01:39:13.537: INFO: 	Container tls-proxy ready: true, restart count 0
Apr 16 01:39:13.537: INFO: npd-nxkwt from kube-system started at 2021-04-14 03:25:44 +0000 UTC (1 container statuses recorded)
Apr 16 01:39:13.537: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr 16 01:39:13.537: INFO: prometheus-magnum-prometheus-operator-prometheus-0 from kube-system started at 2021-04-14 03:28:28 +0000 UTC (3 container statuses recorded)
Apr 16 01:39:13.537: INFO: 	Container prometheus ready: true, restart count 1
Apr 16 01:39:13.537: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Apr 16 01:39:13.537: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Apr 16 01:39:13.537: INFO: sonobuoy-systemd-logs-daemon-set-d0994422a63f4009-7q9k8 from sonobuoy started at 2021-04-16 01:31:44 +0000 UTC (2 container statuses recorded)
Apr 16 01:39:13.537: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 16 01:39:13.537: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 16 01:39:13.537: INFO: busybox from test-k8s started at 2021-04-16 01:17:41 +0000 UTC (1 container statuses recorded)
Apr 16 01:39:13.537: INFO: 	Container busybox ready: true, restart count 0
Apr 16 01:39:13.537: INFO: nginx-7848d4b86f-4q96s from test-k8s started at 2021-04-16 01:16:17 +0000 UTC (1 container statuses recorded)
Apr 16 01:39:13.537: INFO: 	Container nginx ready: true, restart count 0
Apr 16 01:39:13.538: INFO: nginx-7848d4b86f-bmzz7 from test-k8s started at 2021-04-16 01:16:17 +0000 UTC (1 container statuses recorded)
Apr 16 01:39:13.538: INFO: 	Container nginx ready: true, restart count 0
Apr 16 01:39:13.538: INFO: 
Logging pods the apiserver thinks is on node k8s-flwang-4oyd5hkwxglz-node-1 before test
Apr 16 01:39:13.560: INFO: calico-node-nvrm8 from kube-system started at 2021-04-16 01:27:46 +0000 UTC (1 container statuses recorded)
Apr 16 01:39:13.560: INFO: 	Container calico-node ready: true, restart count 0
Apr 16 01:39:13.560: INFO: csi-cinder-nodeplugin-bnmpf from kube-system started at 2021-04-16 01:28:16 +0000 UTC (2 container statuses recorded)
Apr 16 01:39:13.560: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 16 01:39:13.560: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 16 01:39:13.560: INFO: magnum-prometheus-node-exporter-xz5gn from kube-system started at 2021-04-16 01:27:46 +0000 UTC (1 container statuses recorded)
Apr 16 01:39:13.560: INFO: 	Container node-exporter ready: true, restart count 0
Apr 16 01:39:13.560: INFO: npd-d29zk from kube-system started at 2021-04-16 01:28:16 +0000 UTC (1 container statuses recorded)
Apr 16 01:39:13.561: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr 16 01:39:13.561: INFO: sonobuoy from sonobuoy started at 2021-04-16 01:31:30 +0000 UTC (1 container statuses recorded)
Apr 16 01:39:13.561: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 16 01:39:13.561: INFO: sonobuoy-e2e-job-158181182f3b487c from sonobuoy started at 2021-04-16 01:31:44 +0000 UTC (2 container statuses recorded)
Apr 16 01:39:13.561: INFO: 	Container e2e ready: true, restart count 0
Apr 16 01:39:13.561: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 16 01:39:13.561: INFO: sonobuoy-systemd-logs-daemon-set-d0994422a63f4009-55xbs from sonobuoy started at 2021-04-16 01:31:44 +0000 UTC (2 container statuses recorded)
Apr 16 01:39:13.561: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 16 01:39:13.561: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a782627f-aa03-4499-b0f5-c0c9c69c8cbe 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.0.0.68 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.0.0.68 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 16 01:39:25.984: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.68 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9353 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:39:25.985: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.68, port: 54321
Apr 16 01:39:26.240: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.68:54321/hostname] Namespace:sched-pred-9353 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:39:26.240: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.68, port: 54321 UDP
Apr 16 01:39:26.484: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.0.68 54321] Namespace:sched-pred-9353 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:39:26.484: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 16 01:39:31.677: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.68 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9353 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:39:31.677: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.68, port: 54321
Apr 16 01:39:31.886: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.68:54321/hostname] Namespace:sched-pred-9353 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:39:31.886: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.68, port: 54321 UDP
Apr 16 01:39:32.078: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.0.68 54321] Namespace:sched-pred-9353 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:39:32.078: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 16 01:39:37.297: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.68 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9353 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:39:37.297: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.68, port: 54321
Apr 16 01:39:37.486: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.68:54321/hostname] Namespace:sched-pred-9353 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:39:37.486: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.68, port: 54321 UDP
Apr 16 01:39:37.679: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.0.68 54321] Namespace:sched-pred-9353 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:39:37.679: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 16 01:39:42.885: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.68 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9353 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:39:42.885: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.68, port: 54321
Apr 16 01:39:43.103: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.68:54321/hostname] Namespace:sched-pred-9353 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:39:43.103: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.68, port: 54321 UDP
Apr 16 01:39:43.317: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.0.68 54321] Namespace:sched-pred-9353 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:39:43.317: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 16 01:39:48.516: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.68 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9353 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:39:48.516: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.68, port: 54321
Apr 16 01:39:48.728: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.68:54321/hostname] Namespace:sched-pred-9353 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:39:48.728: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.68, port: 54321 UDP
Apr 16 01:39:48.938: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.0.68 54321] Namespace:sched-pred-9353 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:39:48.938: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: removing the label kubernetes.io/e2e-a782627f-aa03-4499-b0f5-c0c9c69c8cbe off the node k8s-flwang-4oyd5hkwxglz-node-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a782627f-aa03-4499-b0f5-c0c9c69c8cbe
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:39:54.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9353" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:41.018 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":13,"skipped":334,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:39:54.268: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2405
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Apr 16 01:39:54.505: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:40:02.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2405" for this suite.

• [SLOW TEST:8.763 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":14,"skipped":399,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:40:03.034: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 01:40:03.255: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr 16 01:40:03.282: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 16 01:40:08.302: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 16 01:40:24.333: INFO: Creating deployment "test-rolling-update-deployment"
Apr 16 01:40:24.360: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr 16 01:40:24.384: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr 16 01:40:26.404: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr 16 01:40:26.411: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 16 01:40:26.442: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7917  99c6db29-be73-4eb0-adce-87a1d8d5761c 406106 1 2021-04-16 01:40:24 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-04-16 01:40:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-16 01:40:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001884b98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-04-16 01:40:24 +0000 UTC,LastTransitionTime:2021-04-16 01:40:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-04-16 01:40:26 +0000 UTC,LastTransitionTime:2021-04-16 01:40:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 16 01:40:26.454: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-7917  0a76126d-e76a-4b6d-879d-30757290a83c 406095 1 2021-04-16 01:40:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 99c6db29-be73-4eb0-adce-87a1d8d5761c 0xc001885047 0xc001885048}] []  [{kube-controller-manager Update apps/v1 2021-04-16 01:40:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99c6db29-be73-4eb0-adce-87a1d8d5761c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0018850d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 16 01:40:26.454: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr 16 01:40:26.454: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7917  1ea909e9-cb1a-4cb0-8392-faa2cd1b5455 406105 2 2021-04-16 01:40:03 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 99c6db29-be73-4eb0-adce-87a1d8d5761c 0xc001884f37 0xc001884f38}] []  [{e2e.test Update apps/v1 2021-04-16 01:40:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-16 01:40:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99c6db29-be73-4eb0-adce-87a1d8d5761c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001884fd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 16 01:40:26.463: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-2qd8w" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-2qd8w test-rolling-update-deployment-6b6bf9df46- deployment-7917  1e7e3220-04c3-4bfd-ac6f-2a1eb59ecccc 406094 0 2021-04-16 01:40:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[cni.projectcalico.org/podIP:10.100.97.82/32 cni.projectcalico.org/podIPs:10.100.97.82/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 0a76126d-e76a-4b6d-879d-30757290a83c 0xc003930f57 0xc003930f58}] []  [{kube-controller-manager Update v1 2021-04-16 01:40:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a76126d-e76a-4b6d-879d-30757290a83c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-16 01:40:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-16 01:40:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.97.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rrc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rrc6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rrc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 01:40:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 01:40:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 01:40:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 01:40:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.68,PodIP:10.100.97.82,StartTime:2021-04-16 01:40:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-16 01:40:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://1963f038f53cb6cbe6877a54d56c5073eb4ba6539c66464c3a4f65725eed8ce6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.97.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:40:26.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7917" for this suite.

• [SLOW TEST:23.461 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":15,"skipped":402,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:40:26.498: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4243
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 01:40:26.756: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:40:32.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4243" for this suite.

• [SLOW TEST:6.528 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":16,"skipped":419,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:40:33.027: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5680
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 16 01:40:35.336: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:40:35.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5680" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":17,"skipped":427,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:40:35.430: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5021
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 16 01:40:36.327: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 16 01:40:38.359: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134036, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134036, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134036, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134036, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 01:40:41.416: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 01:40:41.431: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2744-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:40:42.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5021" for this suite.
STEP: Destroying namespace "webhook-5021-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.516 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":18,"skipped":442,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:40:42.946: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5029
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Apr 16 01:40:43.960: INFO: Pod name wrapped-volume-race-23367b89-da8b-496a-abc3-881944629eb4: Found 0 pods out of 5
Apr 16 01:40:49.007: INFO: Pod name wrapped-volume-race-23367b89-da8b-496a-abc3-881944629eb4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-23367b89-da8b-496a-abc3-881944629eb4 in namespace emptydir-wrapper-5029, will wait for the garbage collector to delete the pods
Apr 16 01:40:59.161: INFO: Deleting ReplicationController wrapped-volume-race-23367b89-da8b-496a-abc3-881944629eb4 took: 23.499325ms
Apr 16 01:40:59.362: INFO: Terminating ReplicationController wrapped-volume-race-23367b89-da8b-496a-abc3-881944629eb4 pods took: 200.39247ms
STEP: Creating RC which spawns configmap-volume pods
Apr 16 01:41:14.488: INFO: Pod name wrapped-volume-race-46cd4660-abe4-4b1b-bb7f-3006d734da4c: Found 0 pods out of 5
Apr 16 01:41:19.510: INFO: Pod name wrapped-volume-race-46cd4660-abe4-4b1b-bb7f-3006d734da4c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-46cd4660-abe4-4b1b-bb7f-3006d734da4c in namespace emptydir-wrapper-5029, will wait for the garbage collector to delete the pods
Apr 16 01:41:45.669: INFO: Deleting ReplicationController wrapped-volume-race-46cd4660-abe4-4b1b-bb7f-3006d734da4c took: 30.933442ms
Apr 16 01:41:46.769: INFO: Terminating ReplicationController wrapped-volume-race-46cd4660-abe4-4b1b-bb7f-3006d734da4c pods took: 1.100281415s
STEP: Creating RC which spawns configmap-volume pods
Apr 16 01:41:56.299: INFO: Pod name wrapped-volume-race-3c4600c7-fb54-4d72-8713-d4c4811a80e4: Found 0 pods out of 5
Apr 16 01:42:01.329: INFO: Pod name wrapped-volume-race-3c4600c7-fb54-4d72-8713-d4c4811a80e4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3c4600c7-fb54-4d72-8713-d4c4811a80e4 in namespace emptydir-wrapper-5029, will wait for the garbage collector to delete the pods
Apr 16 01:42:11.500: INFO: Deleting ReplicationController wrapped-volume-race-3c4600c7-fb54-4d72-8713-d4c4811a80e4 took: 23.33405ms
Apr 16 01:42:12.600: INFO: Terminating ReplicationController wrapped-volume-race-3c4600c7-fb54-4d72-8713-d4c4811a80e4 pods took: 1.100270118s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:42:25.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5029" for this suite.

• [SLOW TEST:102.758 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":19,"skipped":448,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:42:25.706: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6319
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:42:33.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6319" for this suite.

• [SLOW TEST:7.384 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":20,"skipped":456,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:42:33.092: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3773
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 16 01:42:33.383: INFO: Waiting up to 5m0s for pod "downward-api-a1b6930b-fc27-4f46-9bce-8868c815644c" in namespace "downward-api-3773" to be "Succeeded or Failed"
Apr 16 01:42:33.393: INFO: Pod "downward-api-a1b6930b-fc27-4f46-9bce-8868c815644c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.096549ms
Apr 16 01:42:35.417: INFO: Pod "downward-api-a1b6930b-fc27-4f46-9bce-8868c815644c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034794836s
STEP: Saw pod success
Apr 16 01:42:35.417: INFO: Pod "downward-api-a1b6930b-fc27-4f46-9bce-8868c815644c" satisfied condition "Succeeded or Failed"
Apr 16 01:42:35.432: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downward-api-a1b6930b-fc27-4f46-9bce-8868c815644c container dapi-container: <nil>
STEP: delete the pod
Apr 16 01:42:35.566: INFO: Waiting for pod downward-api-a1b6930b-fc27-4f46-9bce-8868c815644c to disappear
Apr 16 01:42:35.576: INFO: Pod downward-api-a1b6930b-fc27-4f46-9bce-8868c815644c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:42:35.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3773" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":21,"skipped":465,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:42:35.606: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6318
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 01:42:35.838: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:42:40.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6318" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":22,"skipped":470,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:42:40.129: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8074
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Apr 16 01:42:43.050: INFO: Successfully updated pod "annotationupdate1560d01a-0e90-4264-a48e-0d67470e9a38"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:42:45.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8074" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":23,"skipped":497,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:42:45.127: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5614
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 16 01:42:45.854: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 16 01:42:47.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134165, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134165, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134165, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134165, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 01:42:50.940: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:42:51.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5614" for this suite.
STEP: Destroying namespace "webhook-5614-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.280 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":24,"skipped":498,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:42:51.407: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-5588
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Apr 16 01:42:57.799: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5588 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:42:57.799: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 01:42:58.025: INFO: Exec stderr: ""
Apr 16 01:42:58.026: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5588 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:42:58.026: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 01:42:58.248: INFO: Exec stderr: ""
Apr 16 01:42:58.248: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5588 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:42:58.248: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 01:42:58.453: INFO: Exec stderr: ""
Apr 16 01:42:58.453: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5588 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:42:58.453: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 01:42:58.661: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Apr 16 01:42:58.661: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5588 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:42:58.661: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 01:42:58.880: INFO: Exec stderr: ""
Apr 16 01:42:58.880: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5588 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:42:58.880: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 01:42:59.072: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Apr 16 01:42:59.072: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5588 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:42:59.072: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 01:42:59.273: INFO: Exec stderr: ""
Apr 16 01:42:59.274: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5588 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:42:59.274: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 01:42:59.449: INFO: Exec stderr: ""
Apr 16 01:42:59.449: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5588 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:42:59.450: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 01:42:59.674: INFO: Exec stderr: ""
Apr 16 01:42:59.674: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5588 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:42:59.674: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 01:42:59.873: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:42:59.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5588" for this suite.

• [SLOW TEST:8.506 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":25,"skipped":509,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:42:59.914: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-9658
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Apr 16 01:43:00.191: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 16 01:44:00.290: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 01:44:00.304: INFO: Starting informer...
STEP: Starting pod...
Apr 16 01:44:00.565: INFO: Pod is running on k8s-flwang-4oyd5hkwxglz-node-1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Apr 16 01:44:00.612: INFO: Pod wasn't evicted. Proceeding
Apr 16 01:44:00.612: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Apr 16 01:45:15.672: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:45:15.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9658" for this suite.

• [SLOW TEST:135.827 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":26,"skipped":532,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:45:15.741: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8887
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Apr 16 01:45:16.023: INFO: Waiting up to 5m0s for pod "client-containers-a9462b72-0d20-4a8f-9924-36d1fd1ca98b" in namespace "containers-8887" to be "Succeeded or Failed"
Apr 16 01:45:16.049: INFO: Pod "client-containers-a9462b72-0d20-4a8f-9924-36d1fd1ca98b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.489488ms
Apr 16 01:45:18.065: INFO: Pod "client-containers-a9462b72-0d20-4a8f-9924-36d1fd1ca98b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042070209s
STEP: Saw pod success
Apr 16 01:45:18.065: INFO: Pod "client-containers-a9462b72-0d20-4a8f-9924-36d1fd1ca98b" satisfied condition "Succeeded or Failed"
Apr 16 01:45:18.074: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod client-containers-a9462b72-0d20-4a8f-9924-36d1fd1ca98b container agnhost-container: <nil>
STEP: delete the pod
Apr 16 01:45:18.284: INFO: Waiting for pod client-containers-a9462b72-0d20-4a8f-9924-36d1fd1ca98b to disappear
Apr 16 01:45:18.295: INFO: Pod client-containers-a9462b72-0d20-4a8f-9924-36d1fd1ca98b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:45:18.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8887" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":27,"skipped":542,"failed":0}
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:45:18.333: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8326
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Apr 16 01:45:21.184: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8326 pod-service-account-96ab557a-14fe-4921-a6fb-aad57b66c44e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Apr 16 01:45:22.059: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8326 pod-service-account-96ab557a-14fe-4921-a6fb-aad57b66c44e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Apr 16 01:45:22.449: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8326 pod-service-account-96ab557a-14fe-4921-a6fb-aad57b66c44e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:45:22.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8326" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":28,"skipped":549,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:45:22.874: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5442
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr 16 01:45:24.119: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Apr 16 01:45:26.154: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134324, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134324, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134324, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134324, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 01:45:29.210: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 01:45:29.227: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:45:30.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5442" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:8.069 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":29,"skipped":556,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:45:30.943: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3385
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:45:47.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3385" for this suite.

• [SLOW TEST:16.797 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":30,"skipped":567,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:45:47.742: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7502
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 01:45:48.036: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c9f0de75-c582-46f4-94c8-37c562b83f64" in namespace "downward-api-7502" to be "Succeeded or Failed"
Apr 16 01:45:48.058: INFO: Pod "downwardapi-volume-c9f0de75-c582-46f4-94c8-37c562b83f64": Phase="Pending", Reason="", readiness=false. Elapsed: 22.043242ms
Apr 16 01:45:50.072: INFO: Pod "downwardapi-volume-c9f0de75-c582-46f4-94c8-37c562b83f64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035196598s
STEP: Saw pod success
Apr 16 01:45:50.072: INFO: Pod "downwardapi-volume-c9f0de75-c582-46f4-94c8-37c562b83f64" satisfied condition "Succeeded or Failed"
Apr 16 01:45:50.078: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-c9f0de75-c582-46f4-94c8-37c562b83f64 container client-container: <nil>
STEP: delete the pod
Apr 16 01:45:50.159: INFO: Waiting for pod downwardapi-volume-c9f0de75-c582-46f4-94c8-37c562b83f64 to disappear
Apr 16 01:45:50.175: INFO: Pod downwardapi-volume-c9f0de75-c582-46f4-94c8-37c562b83f64 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:45:50.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7502" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":31,"skipped":572,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:45:50.222: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5231
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Apr 16 01:45:50.480: INFO: Waiting up to 5m0s for pod "pod-cbca7db3-ab41-47f8-9567-de40af1f3120" in namespace "emptydir-5231" to be "Succeeded or Failed"
Apr 16 01:45:50.493: INFO: Pod "pod-cbca7db3-ab41-47f8-9567-de40af1f3120": Phase="Pending", Reason="", readiness=false. Elapsed: 11.930805ms
Apr 16 01:45:52.512: INFO: Pod "pod-cbca7db3-ab41-47f8-9567-de40af1f3120": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031565226s
Apr 16 01:45:54.528: INFO: Pod "pod-cbca7db3-ab41-47f8-9567-de40af1f3120": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047717522s
STEP: Saw pod success
Apr 16 01:45:54.529: INFO: Pod "pod-cbca7db3-ab41-47f8-9567-de40af1f3120" satisfied condition "Succeeded or Failed"
Apr 16 01:45:54.535: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-cbca7db3-ab41-47f8-9567-de40af1f3120 container test-container: <nil>
STEP: delete the pod
Apr 16 01:45:54.598: INFO: Waiting for pod pod-cbca7db3-ab41-47f8-9567-de40af1f3120 to disappear
Apr 16 01:45:54.609: INFO: Pod pod-cbca7db3-ab41-47f8-9567-de40af1f3120 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:45:54.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5231" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":32,"skipped":640,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:45:54.639: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1941
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 01:45:54.902: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-5316df64-6310-43e2-b88a-225a3b3da487" in namespace "security-context-test-1941" to be "Succeeded or Failed"
Apr 16 01:45:54.915: INFO: Pod "busybox-privileged-false-5316df64-6310-43e2-b88a-225a3b3da487": Phase="Pending", Reason="", readiness=false. Elapsed: 12.907014ms
Apr 16 01:45:56.932: INFO: Pod "busybox-privileged-false-5316df64-6310-43e2-b88a-225a3b3da487": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029423897s
Apr 16 01:45:56.932: INFO: Pod "busybox-privileged-false-5316df64-6310-43e2-b88a-225a3b3da487" satisfied condition "Succeeded or Failed"
Apr 16 01:45:56.947: INFO: Got logs for pod "busybox-privileged-false-5316df64-6310-43e2-b88a-225a3b3da487": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:45:56.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1941" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":33,"skipped":651,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:45:56.980: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-5558
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 01:45:57.223: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Creating first CR 
Apr 16 01:45:57.874: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-16T01:45:57Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-16T01:45:57Z]] name:name1 resourceVersion:408747 uid:41be64dd-df41-4947-9ca0-faddfbd7eb3f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Apr 16 01:46:07.924: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-16T01:46:07Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-16T01:46:07Z]] name:name2 resourceVersion:408799 uid:1f42d2e6-0d22-4e3c-b572-fe46aa0d43eb] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Apr 16 01:46:17.996: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-16T01:45:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-16T01:46:17Z]] name:name1 resourceVersion:408825 uid:41be64dd-df41-4947-9ca0-faddfbd7eb3f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Apr 16 01:46:28.042: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-16T01:46:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-16T01:46:28Z]] name:name2 resourceVersion:408852 uid:1f42d2e6-0d22-4e3c-b572-fe46aa0d43eb] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Apr 16 01:46:38.091: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-16T01:45:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-16T01:46:17Z]] name:name1 resourceVersion:408880 uid:41be64dd-df41-4947-9ca0-faddfbd7eb3f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Apr 16 01:46:48.142: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-16T01:46:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-16T01:46:28Z]] name:name2 resourceVersion:408906 uid:1f42d2e6-0d22-4e3c-b572-fe46aa0d43eb] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:46:58.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5558" for this suite.

• [SLOW TEST:61.765 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":34,"skipped":659,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:46:58.747: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-5691
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 16 01:46:59.585: INFO: starting watch
STEP: patching
STEP: updating
Apr 16 01:46:59.640: INFO: waiting for watch events with expected annotations
Apr 16 01:46:59.640: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:46:59.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-5691" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":35,"skipped":674,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:46:59.891: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7274
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 16 01:47:00.193: INFO: Waiting up to 5m0s for pod "pod-dbc13c07-7112-4bd3-b831-53d4dfc3b227" in namespace "emptydir-7274" to be "Succeeded or Failed"
Apr 16 01:47:00.232: INFO: Pod "pod-dbc13c07-7112-4bd3-b831-53d4dfc3b227": Phase="Pending", Reason="", readiness=false. Elapsed: 38.549155ms
Apr 16 01:47:02.248: INFO: Pod "pod-dbc13c07-7112-4bd3-b831-53d4dfc3b227": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054230058s
STEP: Saw pod success
Apr 16 01:47:02.248: INFO: Pod "pod-dbc13c07-7112-4bd3-b831-53d4dfc3b227" satisfied condition "Succeeded or Failed"
Apr 16 01:47:02.255: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-dbc13c07-7112-4bd3-b831-53d4dfc3b227 container test-container: <nil>
STEP: delete the pod
Apr 16 01:47:02.304: INFO: Waiting for pod pod-dbc13c07-7112-4bd3-b831-53d4dfc3b227 to disappear
Apr 16 01:47:02.313: INFO: Pod pod-dbc13c07-7112-4bd3-b831-53d4dfc3b227 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:47:02.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7274" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":36,"skipped":680,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:47:02.343: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-214
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-d4013be1-5e33-4bff-9f97-d061c2ae8a5a
STEP: Creating a pod to test consume configMaps
Apr 16 01:47:02.634: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-211c74fd-bc1b-4f1a-a2c0-ad3dbddc4d82" in namespace "projected-214" to be "Succeeded or Failed"
Apr 16 01:47:02.643: INFO: Pod "pod-projected-configmaps-211c74fd-bc1b-4f1a-a2c0-ad3dbddc4d82": Phase="Pending", Reason="", readiness=false. Elapsed: 9.429763ms
Apr 16 01:47:04.658: INFO: Pod "pod-projected-configmaps-211c74fd-bc1b-4f1a-a2c0-ad3dbddc4d82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024153431s
Apr 16 01:47:06.679: INFO: Pod "pod-projected-configmaps-211c74fd-bc1b-4f1a-a2c0-ad3dbddc4d82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044692831s
STEP: Saw pod success
Apr 16 01:47:06.679: INFO: Pod "pod-projected-configmaps-211c74fd-bc1b-4f1a-a2c0-ad3dbddc4d82" satisfied condition "Succeeded or Failed"
Apr 16 01:47:06.687: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-projected-configmaps-211c74fd-bc1b-4f1a-a2c0-ad3dbddc4d82 container agnhost-container: <nil>
STEP: delete the pod
Apr 16 01:47:06.740: INFO: Waiting for pod pod-projected-configmaps-211c74fd-bc1b-4f1a-a2c0-ad3dbddc4d82 to disappear
Apr 16 01:47:06.749: INFO: Pod pod-projected-configmaps-211c74fd-bc1b-4f1a-a2c0-ad3dbddc4d82 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:47:06.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-214" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":37,"skipped":683,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:47:06.776: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-3982
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-3982
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3982
STEP: Deleting pre-stop pod
Apr 16 01:47:16.263: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:47:16.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3982" for this suite.

• [SLOW TEST:9.577 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":38,"skipped":709,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:47:16.354: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-8487
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1455
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2323
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:47:23.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8487" for this suite.
STEP: Destroying namespace "nsdeletetest-1455" for this suite.
Apr 16 01:47:23.190: INFO: Namespace nsdeletetest-1455 was already deleted
STEP: Destroying namespace "nsdeletetest-2323" for this suite.

• [SLOW TEST:6.851 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":39,"skipped":721,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:47:23.210: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5924
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 16 01:47:23.493: INFO: Waiting up to 5m0s for pod "downward-api-a88c26ea-bc36-4ad2-bafe-0e0cd887cdbf" in namespace "downward-api-5924" to be "Succeeded or Failed"
Apr 16 01:47:23.509: INFO: Pod "downward-api-a88c26ea-bc36-4ad2-bafe-0e0cd887cdbf": Phase="Pending", Reason="", readiness=false. Elapsed: 15.709541ms
Apr 16 01:47:25.526: INFO: Pod "downward-api-a88c26ea-bc36-4ad2-bafe-0e0cd887cdbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032621828s
STEP: Saw pod success
Apr 16 01:47:25.526: INFO: Pod "downward-api-a88c26ea-bc36-4ad2-bafe-0e0cd887cdbf" satisfied condition "Succeeded or Failed"
Apr 16 01:47:25.533: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downward-api-a88c26ea-bc36-4ad2-bafe-0e0cd887cdbf container dapi-container: <nil>
STEP: delete the pod
Apr 16 01:47:25.582: INFO: Waiting for pod downward-api-a88c26ea-bc36-4ad2-bafe-0e0cd887cdbf to disappear
Apr 16 01:47:25.592: INFO: Pod downward-api-a88c26ea-bc36-4ad2-bafe-0e0cd887cdbf no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:47:25.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5924" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":40,"skipped":727,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:47:25.621: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9980
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 16 01:47:25.887: INFO: Waiting up to 5m0s for pod "pod-26ff3ed6-932d-4b4b-a083-5b745c946bea" in namespace "emptydir-9980" to be "Succeeded or Failed"
Apr 16 01:47:25.912: INFO: Pod "pod-26ff3ed6-932d-4b4b-a083-5b745c946bea": Phase="Pending", Reason="", readiness=false. Elapsed: 24.136891ms
Apr 16 01:47:27.930: INFO: Pod "pod-26ff3ed6-932d-4b4b-a083-5b745c946bea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04172941s
Apr 16 01:47:29.944: INFO: Pod "pod-26ff3ed6-932d-4b4b-a083-5b745c946bea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056434472s
STEP: Saw pod success
Apr 16 01:47:29.945: INFO: Pod "pod-26ff3ed6-932d-4b4b-a083-5b745c946bea" satisfied condition "Succeeded or Failed"
Apr 16 01:47:29.953: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-26ff3ed6-932d-4b4b-a083-5b745c946bea container test-container: <nil>
STEP: delete the pod
Apr 16 01:47:30.012: INFO: Waiting for pod pod-26ff3ed6-932d-4b4b-a083-5b745c946bea to disappear
Apr 16 01:47:30.022: INFO: Pod pod-26ff3ed6-932d-4b4b-a083-5b745c946bea no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:47:30.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9980" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":41,"skipped":743,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:47:30.054: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1448
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Apr 16 01:47:32.453: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0416 01:47:32.452920      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0416 01:47:32.452985      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0416 01:47:32.453002      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:47:32.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1448" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":42,"skipped":752,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:47:32.494: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-2050
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr 16 01:47:33.543: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Apr 16 01:47:35.575: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134453, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134453, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134453, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134453, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 01:47:38.636: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 01:47:38.653: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:47:40.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2050" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.645 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":43,"skipped":781,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:47:40.141: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-6080
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:47:40.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6080" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":44,"skipped":804,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:47:40.901: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4763
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-4763
Apr 16 01:47:43.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-4763 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Apr 16 01:47:43.611: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Apr 16 01:47:43.611: INFO: stdout: "iptables"
Apr 16 01:47:43.611: INFO: proxyMode: iptables
Apr 16 01:47:43.668: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 16 01:47:43.687: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-4763
STEP: creating replication controller affinity-nodeport-timeout in namespace services-4763
I0416 01:47:43.744341      25 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-4763, replica count: 3
I0416 01:47:46.794881      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0416 01:47:49.795261      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0416 01:47:52.795980      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0416 01:47:55.796728      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 16 01:47:55.852: INFO: Creating new exec pod
Apr 16 01:48:00.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-4763 exec execpod-affinitypb7xp -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Apr 16 01:48:01.301: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Apr 16 01:48:01.301: INFO: stdout: ""
Apr 16 01:48:01.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-4763 exec execpod-affinitypb7xp -- /bin/sh -x -c nc -zv -t -w 2 10.254.51.121 80'
Apr 16 01:48:01.676: INFO: stderr: "+ nc -zv -t -w 2 10.254.51.121 80\nConnection to 10.254.51.121 80 port [tcp/http] succeeded!\n"
Apr 16 01:48:01.676: INFO: stdout: ""
Apr 16 01:48:01.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-4763 exec execpod-affinitypb7xp -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.13 31747'
Apr 16 01:48:02.054: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.13 31747\nConnection to 10.0.0.13 31747 port [tcp/31747] succeeded!\n"
Apr 16 01:48:02.054: INFO: stdout: ""
Apr 16 01:48:02.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-4763 exec execpod-affinitypb7xp -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.68 31747'
Apr 16 01:48:02.405: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.68 31747\nConnection to 10.0.0.68 31747 port [tcp/31747] succeeded!\n"
Apr 16 01:48:02.405: INFO: stdout: ""
Apr 16 01:48:02.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-4763 exec execpod-affinitypb7xp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.13:31747/ ; done'
Apr 16 01:48:02.917: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n"
Apr 16 01:48:02.917: INFO: stdout: "\naffinity-nodeport-timeout-rllgb\naffinity-nodeport-timeout-rllgb\naffinity-nodeport-timeout-rllgb\naffinity-nodeport-timeout-rllgb\naffinity-nodeport-timeout-rllgb\naffinity-nodeport-timeout-rllgb\naffinity-nodeport-timeout-rllgb\naffinity-nodeport-timeout-rllgb\naffinity-nodeport-timeout-rllgb\naffinity-nodeport-timeout-rllgb\naffinity-nodeport-timeout-rllgb\naffinity-nodeport-timeout-rllgb\naffinity-nodeport-timeout-rllgb\naffinity-nodeport-timeout-rllgb\naffinity-nodeport-timeout-rllgb\naffinity-nodeport-timeout-rllgb"
Apr 16 01:48:02.917: INFO: Received response from host: affinity-nodeport-timeout-rllgb
Apr 16 01:48:02.917: INFO: Received response from host: affinity-nodeport-timeout-rllgb
Apr 16 01:48:02.917: INFO: Received response from host: affinity-nodeport-timeout-rllgb
Apr 16 01:48:02.917: INFO: Received response from host: affinity-nodeport-timeout-rllgb
Apr 16 01:48:02.917: INFO: Received response from host: affinity-nodeport-timeout-rllgb
Apr 16 01:48:02.917: INFO: Received response from host: affinity-nodeport-timeout-rllgb
Apr 16 01:48:02.917: INFO: Received response from host: affinity-nodeport-timeout-rllgb
Apr 16 01:48:02.917: INFO: Received response from host: affinity-nodeport-timeout-rllgb
Apr 16 01:48:02.917: INFO: Received response from host: affinity-nodeport-timeout-rllgb
Apr 16 01:48:02.917: INFO: Received response from host: affinity-nodeport-timeout-rllgb
Apr 16 01:48:02.917: INFO: Received response from host: affinity-nodeport-timeout-rllgb
Apr 16 01:48:02.917: INFO: Received response from host: affinity-nodeport-timeout-rllgb
Apr 16 01:48:02.917: INFO: Received response from host: affinity-nodeport-timeout-rllgb
Apr 16 01:48:02.917: INFO: Received response from host: affinity-nodeport-timeout-rllgb
Apr 16 01:48:02.917: INFO: Received response from host: affinity-nodeport-timeout-rllgb
Apr 16 01:48:02.917: INFO: Received response from host: affinity-nodeport-timeout-rllgb
Apr 16 01:48:02.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-4763 exec execpod-affinitypb7xp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.13:31747/'
Apr 16 01:48:03.235: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n"
Apr 16 01:48:03.235: INFO: stdout: "affinity-nodeport-timeout-rllgb"
Apr 16 01:48:23.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-4763 exec execpod-affinitypb7xp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.13:31747/'
Apr 16 01:48:23.695: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.13:31747/\n"
Apr 16 01:48:23.695: INFO: stdout: "affinity-nodeport-timeout-n4mcg"
Apr 16 01:48:23.695: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-4763, will wait for the garbage collector to delete the pods
Apr 16 01:48:23.816: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 19.86691ms
Apr 16 01:48:24.916: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 1.100387693s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:48:36.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4763" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:55.166 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":45,"skipped":812,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:48:36.069: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3038
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:48:47.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3038" for this suite.

• [SLOW TEST:11.498 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":46,"skipped":841,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:48:47.568: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8543
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Apr 16 01:48:47.818: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Apr 16 01:48:47.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-8543 create -f -'
Apr 16 01:48:48.335: INFO: stderr: ""
Apr 16 01:48:48.335: INFO: stdout: "service/agnhost-replica created\n"
Apr 16 01:48:48.336: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Apr 16 01:48:48.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-8543 create -f -'
Apr 16 01:48:48.896: INFO: stderr: ""
Apr 16 01:48:48.896: INFO: stdout: "service/agnhost-primary created\n"
Apr 16 01:48:48.896: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr 16 01:48:48.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-8543 create -f -'
Apr 16 01:48:49.329: INFO: stderr: ""
Apr 16 01:48:49.329: INFO: stdout: "service/frontend created\n"
Apr 16 01:48:49.330: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Apr 16 01:48:49.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-8543 create -f -'
Apr 16 01:48:49.796: INFO: stderr: ""
Apr 16 01:48:49.796: INFO: stdout: "deployment.apps/frontend created\n"
Apr 16 01:48:49.797: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 16 01:48:49.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-8543 create -f -'
Apr 16 01:48:50.352: INFO: stderr: ""
Apr 16 01:48:50.352: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Apr 16 01:48:50.352: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 16 01:48:50.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-8543 create -f -'
Apr 16 01:48:50.861: INFO: stderr: ""
Apr 16 01:48:50.861: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Apr 16 01:48:50.861: INFO: Waiting for all frontend pods to be Running.
Apr 16 01:48:55.911: INFO: Waiting for frontend to serve content.
Apr 16 01:48:55.935: INFO: Trying to add a new entry to the guestbook.
Apr 16 01:48:55.961: INFO: Verifying that added entry can be retrieved.
Apr 16 01:48:55.981: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Apr 16 01:49:01.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-8543 delete --grace-period=0 --force -f -'
Apr 16 01:49:01.228: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 16 01:49:01.228: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Apr 16 01:49:01.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-8543 delete --grace-period=0 --force -f -'
Apr 16 01:49:01.394: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 16 01:49:01.394: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Apr 16 01:49:01.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-8543 delete --grace-period=0 --force -f -'
Apr 16 01:49:01.580: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 16 01:49:01.580: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 16 01:49:01.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-8543 delete --grace-period=0 --force -f -'
Apr 16 01:49:01.761: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 16 01:49:01.761: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 16 01:49:01.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-8543 delete --grace-period=0 --force -f -'
Apr 16 01:49:01.949: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 16 01:49:01.949: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Apr 16 01:49:01.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-8543 delete --grace-period=0 --force -f -'
Apr 16 01:49:02.093: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 16 01:49:02.093: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:49:02.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8543" for this suite.

• [SLOW TEST:14.564 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":47,"skipped":849,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:49:02.135: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3400
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:49:02.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3400" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":48,"skipped":906,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:49:02.647: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3077
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Apr 16 01:49:05.616: INFO: Successfully updated pod "labelsupdatef7e369a6-c726-4455-a832-9d93b70893d7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:49:09.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3077" for this suite.

• [SLOW TEST:7.074 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":49,"skipped":908,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:49:09.722: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4166
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-a7c6b8a5-434e-48ae-b0ff-c17f00c79175 in namespace container-probe-4166
Apr 16 01:49:12.016: INFO: Started pod liveness-a7c6b8a5-434e-48ae-b0ff-c17f00c79175 in namespace container-probe-4166
STEP: checking the pod's current state and verifying that restartCount is present
Apr 16 01:49:12.024: INFO: Initial restart count of pod liveness-a7c6b8a5-434e-48ae-b0ff-c17f00c79175 is 0
Apr 16 01:49:30.195: INFO: Restart count of pod container-probe-4166/liveness-a7c6b8a5-434e-48ae-b0ff-c17f00c79175 is now 1 (18.170568966s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:49:30.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4166" for this suite.

• [SLOW TEST:20.576 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":50,"skipped":927,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:49:30.304: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-361
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 01:49:30.788: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"592b372e-6c0c-4dae-b025-00e5a6bac93c", Controller:(*bool)(0xc00331dd2a), BlockOwnerDeletion:(*bool)(0xc00331dd2b)}}
Apr 16 01:49:30.819: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"006b9294-eb80-4783-b789-adfd289ae8b8", Controller:(*bool)(0xc00331de86), BlockOwnerDeletion:(*bool)(0xc00331de87)}}
Apr 16 01:49:30.842: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"eca56d35-696c-4fed-ae47-eb0c03fc396f", Controller:(*bool)(0xc000a00076), BlockOwnerDeletion:(*bool)(0xc000a00077)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:49:35.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-361" for this suite.

• [SLOW TEST:5.643 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":51,"skipped":943,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:49:35.947: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3820
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 16 01:49:36.248: INFO: Waiting up to 5m0s for pod "downward-api-30e157e1-63e8-49fb-86b9-ccffdf25f48c" in namespace "downward-api-3820" to be "Succeeded or Failed"
Apr 16 01:49:36.267: INFO: Pod "downward-api-30e157e1-63e8-49fb-86b9-ccffdf25f48c": Phase="Pending", Reason="", readiness=false. Elapsed: 19.519625ms
Apr 16 01:49:38.280: INFO: Pod "downward-api-30e157e1-63e8-49fb-86b9-ccffdf25f48c": Phase="Running", Reason="", readiness=true. Elapsed: 2.032494499s
Apr 16 01:49:40.295: INFO: Pod "downward-api-30e157e1-63e8-49fb-86b9-ccffdf25f48c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04710443s
STEP: Saw pod success
Apr 16 01:49:40.295: INFO: Pod "downward-api-30e157e1-63e8-49fb-86b9-ccffdf25f48c" satisfied condition "Succeeded or Failed"
Apr 16 01:49:40.304: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downward-api-30e157e1-63e8-49fb-86b9-ccffdf25f48c container dapi-container: <nil>
STEP: delete the pod
Apr 16 01:49:40.363: INFO: Waiting for pod downward-api-30e157e1-63e8-49fb-86b9-ccffdf25f48c to disappear
Apr 16 01:49:40.457: INFO: Pod downward-api-30e157e1-63e8-49fb-86b9-ccffdf25f48c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:49:40.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3820" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":52,"skipped":977,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:49:40.514: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8353
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Apr 16 01:49:40.785: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8353  8eec67e8-f12b-4337-a488-9bf3f351d177 410419 0 2021-04-16 01:49:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-16 01:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 16 01:49:40.786: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8353  8eec67e8-f12b-4337-a488-9bf3f351d177 410419 0 2021-04-16 01:49:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-16 01:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Apr 16 01:49:50.820: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8353  8eec67e8-f12b-4337-a488-9bf3f351d177 410476 0 2021-04-16 01:49:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-16 01:49:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 16 01:49:50.820: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8353  8eec67e8-f12b-4337-a488-9bf3f351d177 410476 0 2021-04-16 01:49:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-16 01:49:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Apr 16 01:50:00.844: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8353  8eec67e8-f12b-4337-a488-9bf3f351d177 410500 0 2021-04-16 01:49:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-16 01:49:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 16 01:50:00.844: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8353  8eec67e8-f12b-4337-a488-9bf3f351d177 410500 0 2021-04-16 01:49:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-16 01:49:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Apr 16 01:50:10.868: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8353  8eec67e8-f12b-4337-a488-9bf3f351d177 410529 0 2021-04-16 01:49:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-16 01:49:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 16 01:50:10.868: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8353  8eec67e8-f12b-4337-a488-9bf3f351d177 410529 0 2021-04-16 01:49:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-16 01:49:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Apr 16 01:50:20.988: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8353  a3ecbb8b-7735-4f07-ae98-f71b6be87b95 410555 0 2021-04-16 01:50:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-16 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 16 01:50:20.989: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8353  a3ecbb8b-7735-4f07-ae98-f71b6be87b95 410555 0 2021-04-16 01:50:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-16 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Apr 16 01:50:31.091: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8353  a3ecbb8b-7735-4f07-ae98-f71b6be87b95 410582 0 2021-04-16 01:50:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-16 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 16 01:50:31.091: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8353  a3ecbb8b-7735-4f07-ae98-f71b6be87b95 410582 0 2021-04-16 01:50:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-16 01:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:50:41.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8353" for this suite.

• [SLOW TEST:60.694 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":53,"skipped":998,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:50:41.209: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9111
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:50:46.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9111" for this suite.

• [SLOW TEST:5.421 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":54,"skipped":1008,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:50:46.631: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1151
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 16 01:50:47.524: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 16 01:50:49.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134647, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134647, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134647, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134647, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 01:50:52.604: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Apr 16 01:50:56.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=webhook-1151 attach --namespace=webhook-1151 to-be-attached-pod -i -c=container1'
Apr 16 01:50:56.934: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:50:56.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1151" for this suite.
STEP: Destroying namespace "webhook-1151-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:10.522 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":55,"skipped":1042,"failed":0}
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:50:57.153: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1772
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Apr 16 01:50:57.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-1772 create -f -'
Apr 16 01:50:58.058: INFO: stderr: ""
Apr 16 01:50:58.058: INFO: stdout: "pod/pause created\n"
Apr 16 01:50:58.058: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr 16 01:50:58.058: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1772" to be "running and ready"
Apr 16 01:50:58.075: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 16.66306ms
Apr 16 01:51:00.091: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.033067926s
Apr 16 01:51:00.091: INFO: Pod "pause" satisfied condition "running and ready"
Apr 16 01:51:00.091: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Apr 16 01:51:00.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-1772 label pods pause testing-label=testing-label-value'
Apr 16 01:51:00.290: INFO: stderr: ""
Apr 16 01:51:00.290: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Apr 16 01:51:00.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-1772 get pod pause -L testing-label'
Apr 16 01:51:00.451: INFO: stderr: ""
Apr 16 01:51:00.451: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Apr 16 01:51:00.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-1772 label pods pause testing-label-'
Apr 16 01:51:00.616: INFO: stderr: ""
Apr 16 01:51:00.616: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Apr 16 01:51:00.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-1772 get pod pause -L testing-label'
Apr 16 01:51:00.782: INFO: stderr: ""
Apr 16 01:51:00.782: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Apr 16 01:51:00.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-1772 delete --grace-period=0 --force -f -'
Apr 16 01:51:00.986: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 16 01:51:00.986: INFO: stdout: "pod \"pause\" force deleted\n"
Apr 16 01:51:00.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-1772 get rc,svc -l name=pause --no-headers'
Apr 16 01:51:01.144: INFO: stderr: "No resources found in kubectl-1772 namespace.\n"
Apr 16 01:51:01.144: INFO: stdout: ""
Apr 16 01:51:01.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-1772 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 16 01:51:01.302: INFO: stderr: ""
Apr 16 01:51:01.302: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:51:01.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1772" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":56,"skipped":1042,"failed":0}
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:51:01.397: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8526
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 16 01:51:07.798: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 16 01:51:07.811: INFO: Pod pod-with-prestop-http-hook still exists
Apr 16 01:51:09.812: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 16 01:51:09.831: INFO: Pod pod-with-prestop-http-hook still exists
Apr 16 01:51:11.812: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 16 01:51:11.823: INFO: Pod pod-with-prestop-http-hook still exists
Apr 16 01:51:13.812: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 16 01:51:13.831: INFO: Pod pod-with-prestop-http-hook still exists
Apr 16 01:51:15.812: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 16 01:51:15.827: INFO: Pod pod-with-prestop-http-hook still exists
Apr 16 01:51:17.812: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 16 01:51:17.826: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:51:17.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8526" for this suite.

• [SLOW TEST:16.564 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":57,"skipped":1044,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:51:17.962: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3402
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:51:29.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3402" for this suite.

• [SLOW TEST:11.411 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":58,"skipped":1058,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:51:29.374: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4290
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Apr 16 01:51:29.641: INFO: Waiting up to 5m0s for pod "var-expansion-4231d860-f0ff-4257-b356-64ef4774bef9" in namespace "var-expansion-4290" to be "Succeeded or Failed"
Apr 16 01:51:29.658: INFO: Pod "var-expansion-4231d860-f0ff-4257-b356-64ef4774bef9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.028894ms
Apr 16 01:51:31.668: INFO: Pod "var-expansion-4231d860-f0ff-4257-b356-64ef4774bef9": Phase="Running", Reason="", readiness=true. Elapsed: 2.026314305s
Apr 16 01:51:33.683: INFO: Pod "var-expansion-4231d860-f0ff-4257-b356-64ef4774bef9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041640066s
STEP: Saw pod success
Apr 16 01:51:33.683: INFO: Pod "var-expansion-4231d860-f0ff-4257-b356-64ef4774bef9" satisfied condition "Succeeded or Failed"
Apr 16 01:51:33.691: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod var-expansion-4231d860-f0ff-4257-b356-64ef4774bef9 container dapi-container: <nil>
STEP: delete the pod
Apr 16 01:51:33.744: INFO: Waiting for pod var-expansion-4231d860-f0ff-4257-b356-64ef4774bef9 to disappear
Apr 16 01:51:33.752: INFO: Pod var-expansion-4231d860-f0ff-4257-b356-64ef4774bef9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:51:33.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4290" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":59,"skipped":1060,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:51:33.786: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7453
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 16 01:51:34.928: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 16 01:51:36.956: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134694, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134694, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134694, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134694, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 01:51:40.004: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 01:51:40.019: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:51:41.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7453" for this suite.
STEP: Destroying namespace "webhook-7453-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.749 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":60,"skipped":1069,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:51:41.535: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-068c2c06-c1ea-44b1-b41f-60981fc22241
STEP: Creating a pod to test consume configMaps
Apr 16 01:51:42.124: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c37f6f84-6232-4f34-a3a4-f19e1b2fd418" in namespace "projected-4158" to be "Succeeded or Failed"
Apr 16 01:51:42.174: INFO: Pod "pod-projected-configmaps-c37f6f84-6232-4f34-a3a4-f19e1b2fd418": Phase="Pending", Reason="", readiness=false. Elapsed: 50.032016ms
Apr 16 01:51:44.192: INFO: Pod "pod-projected-configmaps-c37f6f84-6232-4f34-a3a4-f19e1b2fd418": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.067952361s
STEP: Saw pod success
Apr 16 01:51:44.192: INFO: Pod "pod-projected-configmaps-c37f6f84-6232-4f34-a3a4-f19e1b2fd418" satisfied condition "Succeeded or Failed"
Apr 16 01:51:44.208: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-projected-configmaps-c37f6f84-6232-4f34-a3a4-f19e1b2fd418 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 16 01:51:44.308: INFO: Waiting for pod pod-projected-configmaps-c37f6f84-6232-4f34-a3a4-f19e1b2fd418 to disappear
Apr 16 01:51:44.326: INFO: Pod pod-projected-configmaps-c37f6f84-6232-4f34-a3a4-f19e1b2fd418 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:51:44.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4158" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":61,"skipped":1082,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:51:44.364: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Apr 16 01:51:44.707: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4687  0eadb0f0-aeae-49f4-8215-91bec2f72fb6 411180 0 2021-04-16 01:51:44 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-04-16 01:51:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 16 01:51:44.707: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4687  0eadb0f0-aeae-49f4-8215-91bec2f72fb6 411181 0 2021-04-16 01:51:44 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-04-16 01:51:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:51:44.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4687" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":62,"skipped":1084,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:51:44.749: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6006
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Apr 16 01:51:45.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-6006 create -f -'
Apr 16 01:51:45.550: INFO: stderr: ""
Apr 16 01:51:45.550: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 16 01:51:45.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-6006 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 16 01:51:45.722: INFO: stderr: ""
Apr 16 01:51:45.722: INFO: stdout: "update-demo-nautilus-2xhmg update-demo-nautilus-b4wsg "
Apr 16 01:51:45.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-6006 get pods update-demo-nautilus-2xhmg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 16 01:51:45.859: INFO: stderr: ""
Apr 16 01:51:45.859: INFO: stdout: ""
Apr 16 01:51:45.859: INFO: update-demo-nautilus-2xhmg is created but not running
Apr 16 01:51:50.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-6006 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 16 01:51:51.015: INFO: stderr: ""
Apr 16 01:51:51.015: INFO: stdout: "update-demo-nautilus-2xhmg update-demo-nautilus-b4wsg "
Apr 16 01:51:51.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-6006 get pods update-demo-nautilus-2xhmg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 16 01:51:51.167: INFO: stderr: ""
Apr 16 01:51:51.167: INFO: stdout: ""
Apr 16 01:51:51.167: INFO: update-demo-nautilus-2xhmg is created but not running
Apr 16 01:51:56.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-6006 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 16 01:51:56.337: INFO: stderr: ""
Apr 16 01:51:56.337: INFO: stdout: "update-demo-nautilus-2xhmg update-demo-nautilus-b4wsg "
Apr 16 01:51:56.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-6006 get pods update-demo-nautilus-2xhmg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 16 01:51:56.484: INFO: stderr: ""
Apr 16 01:51:56.484: INFO: stdout: "true"
Apr 16 01:51:56.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-6006 get pods update-demo-nautilus-2xhmg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 16 01:51:56.629: INFO: stderr: ""
Apr 16 01:51:56.630: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 16 01:51:56.630: INFO: validating pod update-demo-nautilus-2xhmg
Apr 16 01:51:56.644: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 16 01:51:56.644: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 16 01:51:56.644: INFO: update-demo-nautilus-2xhmg is verified up and running
Apr 16 01:51:56.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-6006 get pods update-demo-nautilus-b4wsg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 16 01:51:56.819: INFO: stderr: ""
Apr 16 01:51:56.820: INFO: stdout: "true"
Apr 16 01:51:56.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-6006 get pods update-demo-nautilus-b4wsg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 16 01:51:56.960: INFO: stderr: ""
Apr 16 01:51:56.960: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 16 01:51:56.960: INFO: validating pod update-demo-nautilus-b4wsg
Apr 16 01:51:56.975: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 16 01:51:56.975: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 16 01:51:56.975: INFO: update-demo-nautilus-b4wsg is verified up and running
STEP: using delete to clean up resources
Apr 16 01:51:56.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-6006 delete --grace-period=0 --force -f -'
Apr 16 01:51:57.125: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 16 01:51:57.125: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 16 01:51:57.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-6006 get rc,svc -l name=update-demo --no-headers'
Apr 16 01:51:57.340: INFO: stderr: "No resources found in kubectl-6006 namespace.\n"
Apr 16 01:51:57.340: INFO: stdout: ""
Apr 16 01:51:57.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-6006 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 16 01:51:57.491: INFO: stderr: ""
Apr 16 01:51:57.491: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:51:57.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6006" for this suite.

• [SLOW TEST:12.777 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":63,"skipped":1087,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:51:57.526: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 16 01:51:58.370: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 16 01:52:00.405: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134718, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134718, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134718, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134718, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 01:52:03.452: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:52:03.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4373" for this suite.
STEP: Destroying namespace "webhook-4373-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.260 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":64,"skipped":1098,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:52:03.787: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8432
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 16 01:52:04.672: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 16 01:52:06.707: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134724, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134724, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134724, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134724, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 01:52:09.774: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Apr 16 01:52:09.835: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:52:09.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8432" for this suite.
STEP: Destroying namespace "webhook-8432-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.295 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":65,"skipped":1099,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:52:10.086: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-844
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:52:33.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-844" for this suite.

• [SLOW TEST:23.051 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":66,"skipped":1119,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:52:33.138: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4592
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Apr 16 01:52:33.362: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:52:43.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4592" for this suite.

• [SLOW TEST:10.336 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":67,"skipped":1129,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:52:43.475: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9858
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9858.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9858.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 16 01:53:33.815: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9858/dns-test-77f8d352-a6ad-42cb-939b-d3c55e447a1c: the server could not find the requested resource (get pods dns-test-77f8d352-a6ad-42cb-939b-d3c55e447a1c)
Apr 16 01:53:33.828: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9858/dns-test-77f8d352-a6ad-42cb-939b-d3c55e447a1c: the server could not find the requested resource (get pods dns-test-77f8d352-a6ad-42cb-939b-d3c55e447a1c)
Apr 16 01:53:33.857: INFO: Unable to read jessie_udp@PodARecord from pod dns-9858/dns-test-77f8d352-a6ad-42cb-939b-d3c55e447a1c: the server could not find the requested resource (get pods dns-test-77f8d352-a6ad-42cb-939b-d3c55e447a1c)
Apr 16 01:53:33.866: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9858/dns-test-77f8d352-a6ad-42cb-939b-d3c55e447a1c: the server could not find the requested resource (get pods dns-test-77f8d352-a6ad-42cb-939b-d3c55e447a1c)
Apr 16 01:53:33.866: INFO: Lookups using dns-9858/dns-test-77f8d352-a6ad-42cb-939b-d3c55e447a1c failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Apr 16 01:53:38.939: INFO: DNS probes using dns-9858/dns-test-77f8d352-a6ad-42cb-939b-d3c55e447a1c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:53:38.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9858" for this suite.

• [SLOW TEST:55.549 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":68,"skipped":1134,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:53:39.025: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3638
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3638
Apr 16 01:53:41.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-3638 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Apr 16 01:53:41.843: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Apr 16 01:53:41.843: INFO: stdout: "iptables"
Apr 16 01:53:41.843: INFO: proxyMode: iptables
Apr 16 01:53:41.880: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 16 01:53:41.889: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-3638
STEP: creating replication controller affinity-clusterip-timeout in namespace services-3638
I0416 01:53:41.946087      25 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-3638, replica count: 3
I0416 01:53:44.996639      25 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 16 01:53:45.026: INFO: Creating new exec pod
Apr 16 01:53:50.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-3638 exec execpod-affinityrqgxw -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Apr 16 01:53:50.438: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Apr 16 01:53:50.438: INFO: stdout: ""
Apr 16 01:53:50.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-3638 exec execpod-affinityrqgxw -- /bin/sh -x -c nc -zv -t -w 2 10.254.41.199 80'
Apr 16 01:53:50.851: INFO: stderr: "+ nc -zv -t -w 2 10.254.41.199 80\nConnection to 10.254.41.199 80 port [tcp/http] succeeded!\n"
Apr 16 01:53:50.851: INFO: stdout: ""
Apr 16 01:53:50.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-3638 exec execpod-affinityrqgxw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.41.199:80/ ; done'
Apr 16 01:53:51.327: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n"
Apr 16 01:53:51.327: INFO: stdout: "\naffinity-clusterip-timeout-jwfzg\naffinity-clusterip-timeout-jwfzg\naffinity-clusterip-timeout-jwfzg\naffinity-clusterip-timeout-jwfzg\naffinity-clusterip-timeout-jwfzg\naffinity-clusterip-timeout-jwfzg\naffinity-clusterip-timeout-jwfzg\naffinity-clusterip-timeout-jwfzg\naffinity-clusterip-timeout-jwfzg\naffinity-clusterip-timeout-jwfzg\naffinity-clusterip-timeout-jwfzg\naffinity-clusterip-timeout-jwfzg\naffinity-clusterip-timeout-jwfzg\naffinity-clusterip-timeout-jwfzg\naffinity-clusterip-timeout-jwfzg\naffinity-clusterip-timeout-jwfzg"
Apr 16 01:53:51.327: INFO: Received response from host: affinity-clusterip-timeout-jwfzg
Apr 16 01:53:51.327: INFO: Received response from host: affinity-clusterip-timeout-jwfzg
Apr 16 01:53:51.327: INFO: Received response from host: affinity-clusterip-timeout-jwfzg
Apr 16 01:53:51.327: INFO: Received response from host: affinity-clusterip-timeout-jwfzg
Apr 16 01:53:51.328: INFO: Received response from host: affinity-clusterip-timeout-jwfzg
Apr 16 01:53:51.328: INFO: Received response from host: affinity-clusterip-timeout-jwfzg
Apr 16 01:53:51.328: INFO: Received response from host: affinity-clusterip-timeout-jwfzg
Apr 16 01:53:51.328: INFO: Received response from host: affinity-clusterip-timeout-jwfzg
Apr 16 01:53:51.328: INFO: Received response from host: affinity-clusterip-timeout-jwfzg
Apr 16 01:53:51.328: INFO: Received response from host: affinity-clusterip-timeout-jwfzg
Apr 16 01:53:51.328: INFO: Received response from host: affinity-clusterip-timeout-jwfzg
Apr 16 01:53:51.328: INFO: Received response from host: affinity-clusterip-timeout-jwfzg
Apr 16 01:53:51.328: INFO: Received response from host: affinity-clusterip-timeout-jwfzg
Apr 16 01:53:51.328: INFO: Received response from host: affinity-clusterip-timeout-jwfzg
Apr 16 01:53:51.328: INFO: Received response from host: affinity-clusterip-timeout-jwfzg
Apr 16 01:53:51.328: INFO: Received response from host: affinity-clusterip-timeout-jwfzg
Apr 16 01:53:51.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-3638 exec execpod-affinityrqgxw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.41.199:80/'
Apr 16 01:53:51.666: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n"
Apr 16 01:53:51.666: INFO: stdout: "affinity-clusterip-timeout-jwfzg"
Apr 16 01:54:11.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-3638 exec execpod-affinityrqgxw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.41.199:80/'
Apr 16 01:54:12.044: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n"
Apr 16 01:54:12.044: INFO: stdout: "affinity-clusterip-timeout-jwfzg"
Apr 16 01:54:32.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-3638 exec execpod-affinityrqgxw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.41.199:80/'
Apr 16 01:54:32.458: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n"
Apr 16 01:54:32.458: INFO: stdout: "affinity-clusterip-timeout-jwfzg"
Apr 16 01:54:52.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-3638 exec execpod-affinityrqgxw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.41.199:80/'
Apr 16 01:54:52.851: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.41.199:80/\n"
Apr 16 01:54:52.851: INFO: stdout: "affinity-clusterip-timeout-9mzzm"
Apr 16 01:54:52.851: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-3638, will wait for the garbage collector to delete the pods
Apr 16 01:54:52.970: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 19.250834ms
Apr 16 01:54:54.071: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 1.100212231s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:55:06.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3638" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:87.061 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":69,"skipped":1183,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:55:06.086: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6086
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 01:55:06.409: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-57049d6d-87df-4390-a235-67a59c981303" in namespace "security-context-test-6086" to be "Succeeded or Failed"
Apr 16 01:55:06.418: INFO: Pod "busybox-readonly-false-57049d6d-87df-4390-a235-67a59c981303": Phase="Pending", Reason="", readiness=false. Elapsed: 9.078546ms
Apr 16 01:55:08.437: INFO: Pod "busybox-readonly-false-57049d6d-87df-4390-a235-67a59c981303": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027165043s
Apr 16 01:55:08.437: INFO: Pod "busybox-readonly-false-57049d6d-87df-4390-a235-67a59c981303" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:55:08.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6086" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":70,"skipped":1190,"failed":0}
S
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:55:08.474: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-5991
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Apr 16 01:55:08.707: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Apr 16 01:55:09.571: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Apr 16 01:55:11.698: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 01:55:13.709: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 01:55:15.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 01:55:17.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 01:55:19.713: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 01:55:21.712: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 01:55:23.713: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 01:55:25.717: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 01:55:27.717: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 01:55:29.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 01:55:31.713: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 01:55:33.773: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754134909, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 01:55:38.198: INFO: Waited 2.461274895s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:55:39.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5991" for this suite.

• [SLOW TEST:30.973 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":71,"skipped":1191,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:55:39.447: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8124
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 16 01:55:39.844: INFO: Waiting up to 5m0s for pod "downward-api-d82b2949-e491-49ec-9920-809163230520" in namespace "downward-api-8124" to be "Succeeded or Failed"
Apr 16 01:55:39.881: INFO: Pod "downward-api-d82b2949-e491-49ec-9920-809163230520": Phase="Pending", Reason="", readiness=false. Elapsed: 36.823305ms
Apr 16 01:55:41.900: INFO: Pod "downward-api-d82b2949-e491-49ec-9920-809163230520": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055962643s
Apr 16 01:55:43.910: INFO: Pod "downward-api-d82b2949-e491-49ec-9920-809163230520": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065873501s
STEP: Saw pod success
Apr 16 01:55:43.910: INFO: Pod "downward-api-d82b2949-e491-49ec-9920-809163230520" satisfied condition "Succeeded or Failed"
Apr 16 01:55:43.921: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downward-api-d82b2949-e491-49ec-9920-809163230520 container dapi-container: <nil>
STEP: delete the pod
Apr 16 01:55:44.039: INFO: Waiting for pod downward-api-d82b2949-e491-49ec-9920-809163230520 to disappear
Apr 16 01:55:44.048: INFO: Pod downward-api-d82b2949-e491-49ec-9920-809163230520 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:55:44.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8124" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":72,"skipped":1212,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:55:44.073: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Apr 16 01:55:44.379: INFO: observed Pod pod-test in namespace pods-8444 in phase Pending conditions []
Apr 16 01:55:44.383: INFO: observed Pod pod-test in namespace pods-8444 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 01:55:44 +0000 UTC  }]
Apr 16 01:55:44.415: INFO: observed Pod pod-test in namespace pods-8444 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 01:55:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 01:55:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 01:55:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 01:55:44 +0000 UTC  }]
Apr 16 01:55:45.137: INFO: observed Pod pod-test in namespace pods-8444 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 01:55:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 01:55:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 01:55:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 01:55:44 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Apr 16 01:55:46.557: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Apr 16 01:55:46.653: INFO: observed event type ADDED
Apr 16 01:55:46.653: INFO: observed event type MODIFIED
Apr 16 01:55:46.653: INFO: observed event type MODIFIED
Apr 16 01:55:46.654: INFO: observed event type MODIFIED
Apr 16 01:55:46.654: INFO: observed event type MODIFIED
Apr 16 01:55:46.654: INFO: observed event type MODIFIED
Apr 16 01:55:46.654: INFO: observed event type MODIFIED
Apr 16 01:55:46.655: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:55:46.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8444" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":73,"skipped":1218,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:55:46.698: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2388
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Apr 16 01:55:46.917: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 01:55:51.156: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:56:06.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2388" for this suite.

• [SLOW TEST:20.008 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":74,"skipped":1219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:56:06.715: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7034
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7034
STEP: creating service affinity-clusterip in namespace services-7034
STEP: creating replication controller affinity-clusterip in namespace services-7034
I0416 01:56:07.003282      25 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-7034, replica count: 3
I0416 01:56:10.054248      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 16 01:56:10.084: INFO: Creating new exec pod
Apr 16 01:56:13.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-7034 exec execpod-affinityqwwx7 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Apr 16 01:56:14.008: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Apr 16 01:56:14.008: INFO: stdout: ""
Apr 16 01:56:14.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-7034 exec execpod-affinityqwwx7 -- /bin/sh -x -c nc -zv -t -w 2 10.254.70.144 80'
Apr 16 01:56:14.385: INFO: stderr: "+ nc -zv -t -w 2 10.254.70.144 80\nConnection to 10.254.70.144 80 port [tcp/http] succeeded!\n"
Apr 16 01:56:14.385: INFO: stdout: ""
Apr 16 01:56:14.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-7034 exec execpod-affinityqwwx7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.70.144:80/ ; done'
Apr 16 01:56:14.883: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.70.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.70.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.70.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.70.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.70.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.70.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.70.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.70.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.70.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.70.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.70.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.70.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.70.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.70.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.70.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.70.144:80/\n"
Apr 16 01:56:14.883: INFO: stdout: "\naffinity-clusterip-94595\naffinity-clusterip-94595\naffinity-clusterip-94595\naffinity-clusterip-94595\naffinity-clusterip-94595\naffinity-clusterip-94595\naffinity-clusterip-94595\naffinity-clusterip-94595\naffinity-clusterip-94595\naffinity-clusterip-94595\naffinity-clusterip-94595\naffinity-clusterip-94595\naffinity-clusterip-94595\naffinity-clusterip-94595\naffinity-clusterip-94595\naffinity-clusterip-94595"
Apr 16 01:56:14.883: INFO: Received response from host: affinity-clusterip-94595
Apr 16 01:56:14.883: INFO: Received response from host: affinity-clusterip-94595
Apr 16 01:56:14.883: INFO: Received response from host: affinity-clusterip-94595
Apr 16 01:56:14.883: INFO: Received response from host: affinity-clusterip-94595
Apr 16 01:56:14.883: INFO: Received response from host: affinity-clusterip-94595
Apr 16 01:56:14.883: INFO: Received response from host: affinity-clusterip-94595
Apr 16 01:56:14.883: INFO: Received response from host: affinity-clusterip-94595
Apr 16 01:56:14.883: INFO: Received response from host: affinity-clusterip-94595
Apr 16 01:56:14.883: INFO: Received response from host: affinity-clusterip-94595
Apr 16 01:56:14.883: INFO: Received response from host: affinity-clusterip-94595
Apr 16 01:56:14.883: INFO: Received response from host: affinity-clusterip-94595
Apr 16 01:56:14.883: INFO: Received response from host: affinity-clusterip-94595
Apr 16 01:56:14.883: INFO: Received response from host: affinity-clusterip-94595
Apr 16 01:56:14.883: INFO: Received response from host: affinity-clusterip-94595
Apr 16 01:56:14.883: INFO: Received response from host: affinity-clusterip-94595
Apr 16 01:56:14.883: INFO: Received response from host: affinity-clusterip-94595
Apr 16 01:56:14.883: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7034, will wait for the garbage collector to delete the pods
Apr 16 01:56:15.009: INFO: Deleting ReplicationController affinity-clusterip took: 22.234218ms
Apr 16 01:56:16.409: INFO: Terminating ReplicationController affinity-clusterip pods took: 1.400429047s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:56:25.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7034" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:19.300 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":75,"skipped":1242,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:56:26.015: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8719
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 01:56:26.338: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 16 01:56:30.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-8719 --namespace=crd-publish-openapi-8719 create -f -'
Apr 16 01:56:31.659: INFO: stderr: ""
Apr 16 01:56:31.659: INFO: stdout: "e2e-test-crd-publish-openapi-2521-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 16 01:56:31.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-8719 --namespace=crd-publish-openapi-8719 delete e2e-test-crd-publish-openapi-2521-crds test-cr'
Apr 16 01:56:31.876: INFO: stderr: ""
Apr 16 01:56:31.876: INFO: stdout: "e2e-test-crd-publish-openapi-2521-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Apr 16 01:56:31.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-8719 --namespace=crd-publish-openapi-8719 apply -f -'
Apr 16 01:56:32.495: INFO: stderr: ""
Apr 16 01:56:32.495: INFO: stdout: "e2e-test-crd-publish-openapi-2521-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 16 01:56:32.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-8719 --namespace=crd-publish-openapi-8719 delete e2e-test-crd-publish-openapi-2521-crds test-cr'
Apr 16 01:56:32.671: INFO: stderr: ""
Apr 16 01:56:32.671: INFO: stdout: "e2e-test-crd-publish-openapi-2521-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr 16 01:56:32.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-8719 explain e2e-test-crd-publish-openapi-2521-crds'
Apr 16 01:56:33.029: INFO: stderr: ""
Apr 16 01:56:33.030: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2521-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:56:37.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8719" for this suite.

• [SLOW TEST:11.734 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":76,"skipped":1248,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:56:37.749: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4706
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Apr 16 01:56:37.980: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-4706 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:56:38.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4706" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":77,"skipped":1253,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:56:38.124: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1791
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-a814c0b9-a7b6-4241-aef0-ad85c856d7be
STEP: Creating a pod to test consume configMaps
Apr 16 01:56:38.412: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-262be31c-2d09-4d86-82a4-cb430f39e7d3" in namespace "projected-1791" to be "Succeeded or Failed"
Apr 16 01:56:38.431: INFO: Pod "pod-projected-configmaps-262be31c-2d09-4d86-82a4-cb430f39e7d3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.422262ms
Apr 16 01:56:40.453: INFO: Pod "pod-projected-configmaps-262be31c-2d09-4d86-82a4-cb430f39e7d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040105247s
Apr 16 01:56:42.469: INFO: Pod "pod-projected-configmaps-262be31c-2d09-4d86-82a4-cb430f39e7d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056087021s
STEP: Saw pod success
Apr 16 01:56:42.469: INFO: Pod "pod-projected-configmaps-262be31c-2d09-4d86-82a4-cb430f39e7d3" satisfied condition "Succeeded or Failed"
Apr 16 01:56:42.477: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-projected-configmaps-262be31c-2d09-4d86-82a4-cb430f39e7d3 container agnhost-container: <nil>
STEP: delete the pod
Apr 16 01:56:42.636: INFO: Waiting for pod pod-projected-configmaps-262be31c-2d09-4d86-82a4-cb430f39e7d3 to disappear
Apr 16 01:56:42.649: INFO: Pod pod-projected-configmaps-262be31c-2d09-4d86-82a4-cb430f39e7d3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:56:42.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1791" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":78,"skipped":1269,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:56:42.696: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3848
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Apr 16 01:56:42.979: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:56:55.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3848" for this suite.

• [SLOW TEST:13.175 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":79,"skipped":1275,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:56:55.871: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4412
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:56:58.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4412" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":80,"skipped":1304,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:56:58.243: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7582
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-7fe99aa5-8003-443c-9d25-e3cdaf72ec4b
STEP: Creating a pod to test consume configMaps
Apr 16 01:56:58.525: INFO: Waiting up to 5m0s for pod "pod-configmaps-decae23f-9392-4b91-959c-a1e49e3ecf90" in namespace "configmap-7582" to be "Succeeded or Failed"
Apr 16 01:56:58.546: INFO: Pod "pod-configmaps-decae23f-9392-4b91-959c-a1e49e3ecf90": Phase="Pending", Reason="", readiness=false. Elapsed: 20.111916ms
Apr 16 01:57:00.560: INFO: Pod "pod-configmaps-decae23f-9392-4b91-959c-a1e49e3ecf90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034592018s
Apr 16 01:57:02.578: INFO: Pod "pod-configmaps-decae23f-9392-4b91-959c-a1e49e3ecf90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052126409s
STEP: Saw pod success
Apr 16 01:57:02.578: INFO: Pod "pod-configmaps-decae23f-9392-4b91-959c-a1e49e3ecf90" satisfied condition "Succeeded or Failed"
Apr 16 01:57:02.587: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-configmaps-decae23f-9392-4b91-959c-a1e49e3ecf90 container agnhost-container: <nil>
STEP: delete the pod
Apr 16 01:57:02.641: INFO: Waiting for pod pod-configmaps-decae23f-9392-4b91-959c-a1e49e3ecf90 to disappear
Apr 16 01:57:02.651: INFO: Pod pod-configmaps-decae23f-9392-4b91-959c-a1e49e3ecf90 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:57:02.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7582" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":81,"skipped":1315,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:57:02.683: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2701
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Apr 16 01:57:05.018: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2701 PodName:pod-sharedvolume-723682f4-f5f6-4059-893e-3e9be4ab96b1 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 01:57:05.018: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 01:57:05.243: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:57:05.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2701" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":82,"skipped":1344,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:57:05.273: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9577
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9577.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9577.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9577.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9577.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9577.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9577.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 16 01:57:09.655: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9577/dns-test-c004b5c5-c781-4bc5-ad10-6d150f5868cd: the server could not find the requested resource (get pods dns-test-c004b5c5-c781-4bc5-ad10-6d150f5868cd)
Apr 16 01:57:09.666: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9577/dns-test-c004b5c5-c781-4bc5-ad10-6d150f5868cd: the server could not find the requested resource (get pods dns-test-c004b5c5-c781-4bc5-ad10-6d150f5868cd)
Apr 16 01:57:09.693: INFO: Unable to read jessie_udp@PodARecord from pod dns-9577/dns-test-c004b5c5-c781-4bc5-ad10-6d150f5868cd: the server could not find the requested resource (get pods dns-test-c004b5c5-c781-4bc5-ad10-6d150f5868cd)
Apr 16 01:57:09.701: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9577/dns-test-c004b5c5-c781-4bc5-ad10-6d150f5868cd: the server could not find the requested resource (get pods dns-test-c004b5c5-c781-4bc5-ad10-6d150f5868cd)
Apr 16 01:57:09.701: INFO: Lookups using dns-9577/dns-test-c004b5c5-c781-4bc5-ad10-6d150f5868cd failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Apr 16 01:57:14.778: INFO: DNS probes using dns-9577/dns-test-c004b5c5-c781-4bc5-ad10-6d150f5868cd succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:57:14.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9577" for this suite.

• [SLOW TEST:9.597 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":83,"skipped":1358,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:57:14.872: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5880
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Apr 16 01:59:15.724: INFO: Successfully updated pod "var-expansion-9395b9ef-535b-4e61-8f29-005e62af98bf"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Apr 16 01:59:17.755: INFO: Deleting pod "var-expansion-9395b9ef-535b-4e61-8f29-005e62af98bf" in namespace "var-expansion-5880"
Apr 16 01:59:17.782: INFO: Wait up to 5m0s for pod "var-expansion-9395b9ef-535b-4e61-8f29-005e62af98bf" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 01:59:57.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5880" for this suite.

• [SLOW TEST:162.967 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":84,"skipped":1372,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 01:59:57.840: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 01:59:58.125: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7845da1e-5c5c-4592-a220-1636dbbf0c86" in namespace "projected-621" to be "Succeeded or Failed"
Apr 16 01:59:58.140: INFO: Pod "downwardapi-volume-7845da1e-5c5c-4592-a220-1636dbbf0c86": Phase="Pending", Reason="", readiness=false. Elapsed: 15.119609ms
Apr 16 02:00:00.160: INFO: Pod "downwardapi-volume-7845da1e-5c5c-4592-a220-1636dbbf0c86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03467483s
Apr 16 02:00:02.176: INFO: Pod "downwardapi-volume-7845da1e-5c5c-4592-a220-1636dbbf0c86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050792841s
STEP: Saw pod success
Apr 16 02:00:02.176: INFO: Pod "downwardapi-volume-7845da1e-5c5c-4592-a220-1636dbbf0c86" satisfied condition "Succeeded or Failed"
Apr 16 02:00:02.183: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-7845da1e-5c5c-4592-a220-1636dbbf0c86 container client-container: <nil>
STEP: delete the pod
Apr 16 02:00:02.309: INFO: Waiting for pod downwardapi-volume-7845da1e-5c5c-4592-a220-1636dbbf0c86 to disappear
Apr 16 02:00:02.319: INFO: Pod downwardapi-volume-7845da1e-5c5c-4592-a220-1636dbbf0c86 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:00:02.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-621" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":85,"skipped":1382,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:00:02.354: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:00:02.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1531" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":86,"skipped":1400,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:00:02.705: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5352
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 02:00:02.957: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c8d2ef73-6b28-406c-9340-1ecd69813e8c" in namespace "projected-5352" to be "Succeeded or Failed"
Apr 16 02:00:02.972: INFO: Pod "downwardapi-volume-c8d2ef73-6b28-406c-9340-1ecd69813e8c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.410737ms
Apr 16 02:00:05.006: INFO: Pod "downwardapi-volume-c8d2ef73-6b28-406c-9340-1ecd69813e8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048469367s
Apr 16 02:00:07.019: INFO: Pod "downwardapi-volume-c8d2ef73-6b28-406c-9340-1ecd69813e8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06201778s
STEP: Saw pod success
Apr 16 02:00:07.020: INFO: Pod "downwardapi-volume-c8d2ef73-6b28-406c-9340-1ecd69813e8c" satisfied condition "Succeeded or Failed"
Apr 16 02:00:07.029: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-c8d2ef73-6b28-406c-9340-1ecd69813e8c container client-container: <nil>
STEP: delete the pod
Apr 16 02:00:07.078: INFO: Waiting for pod downwardapi-volume-c8d2ef73-6b28-406c-9340-1ecd69813e8c to disappear
Apr 16 02:00:07.088: INFO: Pod downwardapi-volume-c8d2ef73-6b28-406c-9340-1ecd69813e8c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:00:07.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5352" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":87,"skipped":1409,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:00:07.117: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6750
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 16 02:00:07.375: INFO: Waiting up to 5m0s for pod "pod-68e622e3-e188-4ab0-b113-acc40dd0d187" in namespace "emptydir-6750" to be "Succeeded or Failed"
Apr 16 02:00:07.385: INFO: Pod "pod-68e622e3-e188-4ab0-b113-acc40dd0d187": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042197ms
Apr 16 02:00:09.401: INFO: Pod "pod-68e622e3-e188-4ab0-b113-acc40dd0d187": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025752921s
STEP: Saw pod success
Apr 16 02:00:09.401: INFO: Pod "pod-68e622e3-e188-4ab0-b113-acc40dd0d187" satisfied condition "Succeeded or Failed"
Apr 16 02:00:09.408: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-68e622e3-e188-4ab0-b113-acc40dd0d187 container test-container: <nil>
STEP: delete the pod
Apr 16 02:00:09.468: INFO: Waiting for pod pod-68e622e3-e188-4ab0-b113-acc40dd0d187 to disappear
Apr 16 02:00:09.479: INFO: Pod pod-68e622e3-e188-4ab0-b113-acc40dd0d187 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:00:09.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6750" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":88,"skipped":1427,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:00:09.516: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3521
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:00:09.740: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:00:16.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3521" for this suite.

• [SLOW TEST:7.528 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":89,"skipped":1444,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:00:17.045: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4922
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:00:34.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4922" for this suite.

• [SLOW TEST:17.906 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":90,"skipped":1461,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:00:34.958: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5075
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:00:39.334: INFO: Waiting up to 5m0s for pod "client-envvars-f390c821-66be-409e-84a0-6d151d359d00" in namespace "pods-5075" to be "Succeeded or Failed"
Apr 16 02:00:39.356: INFO: Pod "client-envvars-f390c821-66be-409e-84a0-6d151d359d00": Phase="Pending", Reason="", readiness=false. Elapsed: 22.012485ms
Apr 16 02:00:41.377: INFO: Pod "client-envvars-f390c821-66be-409e-84a0-6d151d359d00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043197279s
Apr 16 02:00:43.394: INFO: Pod "client-envvars-f390c821-66be-409e-84a0-6d151d359d00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06065952s
STEP: Saw pod success
Apr 16 02:00:43.395: INFO: Pod "client-envvars-f390c821-66be-409e-84a0-6d151d359d00" satisfied condition "Succeeded or Failed"
Apr 16 02:00:43.402: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod client-envvars-f390c821-66be-409e-84a0-6d151d359d00 container env3cont: <nil>
STEP: delete the pod
Apr 16 02:00:43.451: INFO: Waiting for pod client-envvars-f390c821-66be-409e-84a0-6d151d359d00 to disappear
Apr 16 02:00:43.460: INFO: Pod client-envvars-f390c821-66be-409e-84a0-6d151d359d00 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:00:43.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5075" for this suite.

• [SLOW TEST:8.530 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":91,"skipped":1547,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:00:43.489: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6831
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Apr 16 02:00:43.738: INFO: Pod name pod-release: Found 0 pods out of 1
Apr 16 02:00:48.762: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:00:49.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6831" for this suite.

• [SLOW TEST:6.394 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":92,"skipped":1554,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:00:49.883: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6579
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:00:50.115: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Apr 16 02:00:51.248: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:00:51.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6579" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":93,"skipped":1575,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:00:51.293: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-190
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-c71dc156-bd9f-4010-bfd0-3ed34e22cc0c
STEP: Creating a pod to test consume secrets
Apr 16 02:00:51.572: INFO: Waiting up to 5m0s for pod "pod-secrets-8f1b5bd8-1d6c-40f6-8689-53db4ee3d3e2" in namespace "secrets-190" to be "Succeeded or Failed"
Apr 16 02:00:51.597: INFO: Pod "pod-secrets-8f1b5bd8-1d6c-40f6-8689-53db4ee3d3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 25.291398ms
Apr 16 02:00:53.614: INFO: Pod "pod-secrets-8f1b5bd8-1d6c-40f6-8689-53db4ee3d3e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041658654s
STEP: Saw pod success
Apr 16 02:00:53.614: INFO: Pod "pod-secrets-8f1b5bd8-1d6c-40f6-8689-53db4ee3d3e2" satisfied condition "Succeeded or Failed"
Apr 16 02:00:53.622: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-secrets-8f1b5bd8-1d6c-40f6-8689-53db4ee3d3e2 container secret-volume-test: <nil>
STEP: delete the pod
Apr 16 02:00:53.689: INFO: Waiting for pod pod-secrets-8f1b5bd8-1d6c-40f6-8689-53db4ee3d3e2 to disappear
Apr 16 02:00:53.707: INFO: Pod pod-secrets-8f1b5bd8-1d6c-40f6-8689-53db4ee3d3e2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:00:53.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-190" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":94,"skipped":1576,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:00:53.743: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1672
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 02:00:54.025: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d7da350-cc2b-4528-a342-a5728239ca84" in namespace "projected-1672" to be "Succeeded or Failed"
Apr 16 02:00:54.050: INFO: Pod "downwardapi-volume-1d7da350-cc2b-4528-a342-a5728239ca84": Phase="Pending", Reason="", readiness=false. Elapsed: 24.441493ms
Apr 16 02:00:56.068: INFO: Pod "downwardapi-volume-1d7da350-cc2b-4528-a342-a5728239ca84": Phase="Running", Reason="", readiness=true. Elapsed: 2.042708745s
Apr 16 02:00:58.085: INFO: Pod "downwardapi-volume-1d7da350-cc2b-4528-a342-a5728239ca84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059175121s
STEP: Saw pod success
Apr 16 02:00:58.085: INFO: Pod "downwardapi-volume-1d7da350-cc2b-4528-a342-a5728239ca84" satisfied condition "Succeeded or Failed"
Apr 16 02:00:58.093: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-1d7da350-cc2b-4528-a342-a5728239ca84 container client-container: <nil>
STEP: delete the pod
Apr 16 02:00:58.144: INFO: Waiting for pod downwardapi-volume-1d7da350-cc2b-4528-a342-a5728239ca84 to disappear
Apr 16 02:00:58.154: INFO: Pod downwardapi-volume-1d7da350-cc2b-4528-a342-a5728239ca84 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:00:58.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1672" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":95,"skipped":1608,"failed":0}

------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:00:58.181: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8582
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Apr 16 02:00:58.419: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-8582 proxy --unix-socket=/tmp/kubectl-proxy-unix176052361/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:00:58.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8582" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":96,"skipped":1608,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:00:58.561: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5172
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-5172
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 16 02:00:58.800: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 16 02:00:58.913: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 16 02:01:00.932: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 16 02:01:02.932: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:01:04.928: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:01:06.927: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:01:08.927: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:01:10.930: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:01:12.935: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:01:14.929: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 16 02:01:14.947: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Apr 16 02:01:17.056: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Apr 16 02:01:17.057: INFO: Going to poll 10.100.232.211 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Apr 16 02:01:17.064: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.232.211:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5172 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 02:01:17.064: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 02:01:17.307: INFO: Found all 1 expected endpoints: [netserver-0]
Apr 16 02:01:17.307: INFO: Going to poll 10.100.97.110 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Apr 16 02:01:17.317: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.97.110:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5172 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 02:01:17.318: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 02:01:17.522: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:01:17.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5172" for this suite.

• [SLOW TEST:18.999 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":97,"skipped":1641,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:01:17.563: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1538
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7705
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4179
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:01:47.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1538" for this suite.
STEP: Destroying namespace "nsdeletetest-7705" for this suite.
Apr 16 02:01:47.392: INFO: Namespace nsdeletetest-7705 was already deleted
STEP: Destroying namespace "nsdeletetest-4179" for this suite.

• [SLOW TEST:29.850 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":98,"skipped":1651,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:01:47.413: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-7639
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 16 02:01:47.716: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 16 02:02:47.817: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:02:47.829: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-2695
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:02:48.112: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Apr 16 02:02:48.125: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:02:48.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2695" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:02:48.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7639" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:61.071 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":99,"skipped":1657,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:02:48.492: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3407
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Apr 16 02:02:48.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-3407 cluster-info'
Apr 16 02:02:48.895: INFO: stderr: ""
Apr 16 02:02:48.895: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:02:48.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3407" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":100,"skipped":1666,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:02:48.921: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4329
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:02:49.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-4329 version'
Apr 16 02:02:49.302: INFO: stderr: ""
Apr 16 02:02:49.302: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.4\", GitCommit:\"e87da0bd6e03ec3fea7933c4b5263d151aafd07c\", GitTreeState:\"clean\", BuildDate:\"2021-02-18T16:12:00Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.4\", GitCommit:\"e87da0bd6e03ec3fea7933c4b5263d151aafd07c\", GitTreeState:\"clean\", BuildDate:\"2021-02-18T16:03:00Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:02:49.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4329" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":101,"skipped":1666,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:02:49.337: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1080
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-1080/secret-test-a2d732e5-9f45-4790-b3fc-40f9fe85a6a8
STEP: Creating a pod to test consume secrets
Apr 16 02:02:49.622: INFO: Waiting up to 5m0s for pod "pod-configmaps-bf1d847c-5b86-46a7-a976-dd74c7730288" in namespace "secrets-1080" to be "Succeeded or Failed"
Apr 16 02:02:49.640: INFO: Pod "pod-configmaps-bf1d847c-5b86-46a7-a976-dd74c7730288": Phase="Pending", Reason="", readiness=false. Elapsed: 17.699291ms
Apr 16 02:02:51.657: INFO: Pod "pod-configmaps-bf1d847c-5b86-46a7-a976-dd74c7730288": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034383908s
Apr 16 02:02:53.670: INFO: Pod "pod-configmaps-bf1d847c-5b86-46a7-a976-dd74c7730288": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047890448s
STEP: Saw pod success
Apr 16 02:02:53.671: INFO: Pod "pod-configmaps-bf1d847c-5b86-46a7-a976-dd74c7730288" satisfied condition "Succeeded or Failed"
Apr 16 02:02:53.682: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-configmaps-bf1d847c-5b86-46a7-a976-dd74c7730288 container env-test: <nil>
STEP: delete the pod
Apr 16 02:02:53.919: INFO: Waiting for pod pod-configmaps-bf1d847c-5b86-46a7-a976-dd74c7730288 to disappear
Apr 16 02:02:53.943: INFO: Pod pod-configmaps-bf1d847c-5b86-46a7-a976-dd74c7730288 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:02:53.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1080" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":102,"skipped":1695,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:02:54.023: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4375
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4375
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4375
I0416 02:02:54.382920      25 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4375, replica count: 2
I0416 02:02:57.433795      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 16 02:02:57.434: INFO: Creating new exec pod
Apr 16 02:03:00.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-4375 exec execpod4jg7b -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Apr 16 02:03:00.884: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 16 02:03:00.884: INFO: stdout: ""
Apr 16 02:03:00.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-4375 exec execpod4jg7b -- /bin/sh -x -c nc -zv -t -w 2 10.254.142.143 80'
Apr 16 02:03:01.233: INFO: stderr: "+ nc -zv -t -w 2 10.254.142.143 80\nConnection to 10.254.142.143 80 port [tcp/http] succeeded!\n"
Apr 16 02:03:01.233: INFO: stdout: ""
Apr 16 02:03:01.233: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:03:01.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4375" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:7.316 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":103,"skipped":1714,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:03:01.339: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7795
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:03:01.566: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 16 02:03:05.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-7795 --namespace=crd-publish-openapi-7795 create -f -'
Apr 16 02:03:06.876: INFO: stderr: ""
Apr 16 02:03:06.876: INFO: stdout: "e2e-test-crd-publish-openapi-197-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 16 02:03:06.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-7795 --namespace=crd-publish-openapi-7795 delete e2e-test-crd-publish-openapi-197-crds test-cr'
Apr 16 02:03:07.032: INFO: stderr: ""
Apr 16 02:03:07.032: INFO: stdout: "e2e-test-crd-publish-openapi-197-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Apr 16 02:03:07.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-7795 --namespace=crd-publish-openapi-7795 apply -f -'
Apr 16 02:03:07.482: INFO: stderr: ""
Apr 16 02:03:07.483: INFO: stdout: "e2e-test-crd-publish-openapi-197-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 16 02:03:07.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-7795 --namespace=crd-publish-openapi-7795 delete e2e-test-crd-publish-openapi-197-crds test-cr'
Apr 16 02:03:07.634: INFO: stderr: ""
Apr 16 02:03:07.634: INFO: stdout: "e2e-test-crd-publish-openapi-197-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Apr 16 02:03:07.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-7795 explain e2e-test-crd-publish-openapi-197-crds'
Apr 16 02:03:08.115: INFO: stderr: ""
Apr 16 02:03:08.115: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-197-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:03:12.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7795" for this suite.

• [SLOW TEST:11.542 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":104,"skipped":1716,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:03:12.884: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2106
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Apr 16 02:03:13.187: INFO: created test-pod-1
Apr 16 02:03:13.205: INFO: created test-pod-2
Apr 16 02:03:13.224: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:03:13.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2106" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":105,"skipped":1726,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:03:13.398: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7435
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 16 02:03:14.179: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 16 02:03:16.228: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135394, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135394, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135394, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135394, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 02:03:19.285: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:03:19.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7435" for this suite.
STEP: Destroying namespace "webhook-7435-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.322 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":106,"skipped":1761,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:03:19.722: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4033
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0416 02:03:21.127156      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0416 02:03:21.127316      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0416 02:03:21.127381      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 16 02:03:21.127: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:03:21.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4033" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":107,"skipped":1775,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:03:21.157: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-6996
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 16 02:03:21.510: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 16 02:04:21.617: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:04:21.629: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-905
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Apr 16 02:04:26.008: INFO: found a healthy node: k8s-flwang-4oyd5hkwxglz-node-1
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:04:46.253: INFO: pods created so far: [1 1 1]
Apr 16 02:04:46.253: INFO: length of pods created so far: 3
Apr 16 02:04:56.295: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:05:03.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-905" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:05:03.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6996" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:102.461 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":108,"skipped":1781,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:05:03.618: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1629
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Apr 16 02:05:44.011: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Apr 16 02:05:44.011: INFO: Deleting pod "simpletest.rc-2k929" in namespace "gc-1629"
W0416 02:05:44.011716      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0416 02:05:44.011782      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0416 02:05:44.011804      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 16 02:05:44.071: INFO: Deleting pod "simpletest.rc-6b8pm" in namespace "gc-1629"
Apr 16 02:05:44.117: INFO: Deleting pod "simpletest.rc-d8gfx" in namespace "gc-1629"
Apr 16 02:05:44.172: INFO: Deleting pod "simpletest.rc-h89xw" in namespace "gc-1629"
Apr 16 02:05:44.209: INFO: Deleting pod "simpletest.rc-hzvg5" in namespace "gc-1629"
Apr 16 02:05:44.273: INFO: Deleting pod "simpletest.rc-l574m" in namespace "gc-1629"
Apr 16 02:05:44.313: INFO: Deleting pod "simpletest.rc-mfxsp" in namespace "gc-1629"
Apr 16 02:05:44.345: INFO: Deleting pod "simpletest.rc-prh5t" in namespace "gc-1629"
Apr 16 02:05:44.386: INFO: Deleting pod "simpletest.rc-txmw4" in namespace "gc-1629"
Apr 16 02:05:44.421: INFO: Deleting pod "simpletest.rc-z7jqz" in namespace "gc-1629"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:05:44.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1629" for this suite.

• [SLOW TEST:40.886 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":109,"skipped":1786,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:05:44.505: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9465
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 16 02:05:45.281: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 16 02:05:47.313: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135545, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135545, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135545, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135545, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 02:05:50.354: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:06:00.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9465" for this suite.
STEP: Destroying namespace "webhook-9465-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:16.536 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":110,"skipped":1816,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:06:01.042: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4013
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ec6ac4de-4ccc-4d85-8df0-59baad041333
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-ec6ac4de-4ccc-4d85-8df0-59baad041333
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:07:20.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4013" for this suite.

• [SLOW TEST:79.146 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":111,"skipped":1817,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:07:20.191: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3955
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Apr 16 02:07:20.505: INFO: Waiting up to 5m0s for pod "test-pod-a3af8c3a-fe03-44f9-b503-9cd9dff28f79" in namespace "svcaccounts-3955" to be "Succeeded or Failed"
Apr 16 02:07:20.526: INFO: Pod "test-pod-a3af8c3a-fe03-44f9-b503-9cd9dff28f79": Phase="Pending", Reason="", readiness=false. Elapsed: 21.062327ms
Apr 16 02:07:22.542: INFO: Pod "test-pod-a3af8c3a-fe03-44f9-b503-9cd9dff28f79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036395697s
Apr 16 02:07:24.561: INFO: Pod "test-pod-a3af8c3a-fe03-44f9-b503-9cd9dff28f79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055202397s
STEP: Saw pod success
Apr 16 02:07:24.561: INFO: Pod "test-pod-a3af8c3a-fe03-44f9-b503-9cd9dff28f79" satisfied condition "Succeeded or Failed"
Apr 16 02:07:24.569: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod test-pod-a3af8c3a-fe03-44f9-b503-9cd9dff28f79 container agnhost-container: <nil>
STEP: delete the pod
Apr 16 02:07:24.631: INFO: Waiting for pod test-pod-a3af8c3a-fe03-44f9-b503-9cd9dff28f79 to disappear
Apr 16 02:07:24.640: INFO: Pod test-pod-a3af8c3a-fe03-44f9-b503-9cd9dff28f79 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:07:24.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3955" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":112,"skipped":1842,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:07:24.670: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8079
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Apr 16 02:07:27.509: INFO: Successfully updated pod "adopt-release-g96p7"
STEP: Checking that the Job readopts the Pod
Apr 16 02:07:27.510: INFO: Waiting up to 15m0s for pod "adopt-release-g96p7" in namespace "job-8079" to be "adopted"
Apr 16 02:07:27.520: INFO: Pod "adopt-release-g96p7": Phase="Running", Reason="", readiness=true. Elapsed: 9.959691ms
Apr 16 02:07:29.536: INFO: Pod "adopt-release-g96p7": Phase="Running", Reason="", readiness=true. Elapsed: 2.025364887s
Apr 16 02:07:29.536: INFO: Pod "adopt-release-g96p7" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Apr 16 02:07:30.081: INFO: Successfully updated pod "adopt-release-g96p7"
STEP: Checking that the Job releases the Pod
Apr 16 02:07:30.081: INFO: Waiting up to 15m0s for pod "adopt-release-g96p7" in namespace "job-8079" to be "released"
Apr 16 02:07:30.092: INFO: Pod "adopt-release-g96p7": Phase="Running", Reason="", readiness=true. Elapsed: 10.853662ms
Apr 16 02:07:32.111: INFO: Pod "adopt-release-g96p7": Phase="Running", Reason="", readiness=true. Elapsed: 2.029327336s
Apr 16 02:07:32.111: INFO: Pod "adopt-release-g96p7" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:07:32.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8079" for this suite.

• [SLOW TEST:7.484 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":113,"skipped":1852,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:07:32.157: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1067
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Apr 16 02:07:32.420: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 02:07:36.621: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:07:51.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1067" for this suite.

• [SLOW TEST:19.678 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":114,"skipped":1872,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:07:51.836: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9491
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-d1a192b8-5dde-4535-a75d-b213570d796a
STEP: Creating a pod to test consume configMaps
Apr 16 02:07:52.132: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-90af2ca7-aed2-4cc4-baba-79958105fe64" in namespace "projected-9491" to be "Succeeded or Failed"
Apr 16 02:07:52.151: INFO: Pod "pod-projected-configmaps-90af2ca7-aed2-4cc4-baba-79958105fe64": Phase="Pending", Reason="", readiness=false. Elapsed: 19.551748ms
Apr 16 02:07:54.168: INFO: Pod "pod-projected-configmaps-90af2ca7-aed2-4cc4-baba-79958105fe64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036527679s
STEP: Saw pod success
Apr 16 02:07:54.169: INFO: Pod "pod-projected-configmaps-90af2ca7-aed2-4cc4-baba-79958105fe64" satisfied condition "Succeeded or Failed"
Apr 16 02:07:54.176: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-projected-configmaps-90af2ca7-aed2-4cc4-baba-79958105fe64 container agnhost-container: <nil>
STEP: delete the pod
Apr 16 02:07:54.297: INFO: Waiting for pod pod-projected-configmaps-90af2ca7-aed2-4cc4-baba-79958105fe64 to disappear
Apr 16 02:07:54.311: INFO: Pod pod-projected-configmaps-90af2ca7-aed2-4cc4-baba-79958105fe64 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:07:54.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9491" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":115,"skipped":1897,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:07:54.350: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1152
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1152.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1152.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1152.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1152.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1152.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1152.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 16 02:07:58.761: INFO: DNS probes using dns-1152/dns-test-8515740c-70bd-4f47-825b-29aa4974fd8d succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:07:58.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1152" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":116,"skipped":1928,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:07:58.882: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9585
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 16 02:07:59.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9585 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Apr 16 02:07:59.302: INFO: stderr: ""
Apr 16 02:07:59.302: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Apr 16 02:07:59.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9585 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Apr 16 02:07:59.847: INFO: stderr: ""
Apr 16 02:07:59.847: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Apr 16 02:07:59.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9585 delete pods e2e-test-httpd-pod'
Apr 16 02:08:15.973: INFO: stderr: ""
Apr 16 02:08:15.973: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:08:15.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9585" for this suite.

• [SLOW TEST:17.161 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:909
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":117,"skipped":1935,"failed":0}
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:08:16.044: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2209
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 16 02:08:20.505: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 16 02:08:20.522: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 16 02:08:22.522: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 16 02:08:22.539: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 16 02:08:24.523: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 16 02:08:24.537: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:08:24.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2209" for this suite.

• [SLOW TEST:8.529 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":118,"skipped":1939,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:08:24.577: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5635
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 02:08:24.832: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f93ffa0-7d8c-4e71-9bf1-beee59e72389" in namespace "projected-5635" to be "Succeeded or Failed"
Apr 16 02:08:24.857: INFO: Pod "downwardapi-volume-6f93ffa0-7d8c-4e71-9bf1-beee59e72389": Phase="Pending", Reason="", readiness=false. Elapsed: 24.811143ms
Apr 16 02:08:26.877: INFO: Pod "downwardapi-volume-6f93ffa0-7d8c-4e71-9bf1-beee59e72389": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044663791s
Apr 16 02:08:28.894: INFO: Pod "downwardapi-volume-6f93ffa0-7d8c-4e71-9bf1-beee59e72389": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061774871s
STEP: Saw pod success
Apr 16 02:08:28.894: INFO: Pod "downwardapi-volume-6f93ffa0-7d8c-4e71-9bf1-beee59e72389" satisfied condition "Succeeded or Failed"
Apr 16 02:08:28.901: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-6f93ffa0-7d8c-4e71-9bf1-beee59e72389 container client-container: <nil>
STEP: delete the pod
Apr 16 02:08:28.975: INFO: Waiting for pod downwardapi-volume-6f93ffa0-7d8c-4e71-9bf1-beee59e72389 to disappear
Apr 16 02:08:29.013: INFO: Pod downwardapi-volume-6f93ffa0-7d8c-4e71-9bf1-beee59e72389 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:08:29.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5635" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":119,"skipped":1950,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:08:29.048: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6994
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 16 02:08:29.301: INFO: Waiting up to 5m0s for pod "pod-cfbef4d2-957c-4fa6-9f8e-bf908f6b9991" in namespace "emptydir-6994" to be "Succeeded or Failed"
Apr 16 02:08:29.319: INFO: Pod "pod-cfbef4d2-957c-4fa6-9f8e-bf908f6b9991": Phase="Pending", Reason="", readiness=false. Elapsed: 17.362758ms
Apr 16 02:08:31.331: INFO: Pod "pod-cfbef4d2-957c-4fa6-9f8e-bf908f6b9991": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029999549s
STEP: Saw pod success
Apr 16 02:08:31.331: INFO: Pod "pod-cfbef4d2-957c-4fa6-9f8e-bf908f6b9991" satisfied condition "Succeeded or Failed"
Apr 16 02:08:31.339: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-cfbef4d2-957c-4fa6-9f8e-bf908f6b9991 container test-container: <nil>
STEP: delete the pod
Apr 16 02:08:31.391: INFO: Waiting for pod pod-cfbef4d2-957c-4fa6-9f8e-bf908f6b9991 to disappear
Apr 16 02:08:31.400: INFO: Pod pod-cfbef4d2-957c-4fa6-9f8e-bf908f6b9991 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:08:31.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6994" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":120,"skipped":1950,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:08:31.433: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-6411
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 16 02:08:31.801: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Apr 16 02:08:31.826: INFO: starting watch
STEP: patching
STEP: updating
Apr 16 02:08:31.876: INFO: waiting for watch events with expected annotations
Apr 16 02:08:31.876: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:08:32.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-6411" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":121,"skipped":1988,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:08:32.063: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4913
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Apr 16 02:08:32.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-4913 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Apr 16 02:08:32.492: INFO: stderr: ""
Apr 16 02:08:32.492: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Apr 16 02:08:32.492: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Apr 16 02:08:32.492: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4913" to be "running and ready, or succeeded"
Apr 16 02:08:32.502: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.08956ms
Apr 16 02:08:34.530: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.03789999s
Apr 16 02:08:34.530: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Apr 16 02:08:34.530: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Apr 16 02:08:34.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-4913 logs logs-generator logs-generator'
Apr 16 02:08:34.823: INFO: stderr: ""
Apr 16 02:08:34.823: INFO: stdout: "I0416 02:08:33.842459       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/v9t 321\nI0416 02:08:34.042671       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/r6x 210\nI0416 02:08:34.242678       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/dl2q 525\nI0416 02:08:34.442645       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/jk2j 585\nI0416 02:08:34.642673       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/fxkj 520\n"
STEP: limiting log lines
Apr 16 02:08:34.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-4913 logs logs-generator logs-generator --tail=1'
Apr 16 02:08:35.007: INFO: stderr: ""
Apr 16 02:08:35.008: INFO: stdout: "I0416 02:08:34.842703       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/fs8p 200\n"
Apr 16 02:08:35.008: INFO: got output "I0416 02:08:34.842703       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/fs8p 200\n"
STEP: limiting log bytes
Apr 16 02:08:35.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-4913 logs logs-generator logs-generator --limit-bytes=1'
Apr 16 02:08:35.165: INFO: stderr: ""
Apr 16 02:08:35.165: INFO: stdout: "I"
Apr 16 02:08:35.165: INFO: got output "I"
STEP: exposing timestamps
Apr 16 02:08:35.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-4913 logs logs-generator logs-generator --tail=1 --timestamps'
Apr 16 02:08:35.356: INFO: stderr: ""
Apr 16 02:08:35.356: INFO: stdout: "2021-04-16T02:08:35.242795826Z I0416 02:08:35.242687       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/q2ns 281\n"
Apr 16 02:08:35.356: INFO: got output "2021-04-16T02:08:35.242795826Z I0416 02:08:35.242687       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/q2ns 281\n"
STEP: restricting to a time range
Apr 16 02:08:37.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-4913 logs logs-generator logs-generator --since=1s'
Apr 16 02:08:38.015: INFO: stderr: ""
Apr 16 02:08:38.015: INFO: stdout: "I0416 02:08:37.042881       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/wgr 529\nI0416 02:08:37.242658       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/6ft 562\nI0416 02:08:37.442674       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/7zf 261\nI0416 02:08:37.642825       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/7mk6 570\nI0416 02:08:37.842719       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/h4t8 562\n"
Apr 16 02:08:38.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-4913 logs logs-generator logs-generator --since=24h'
Apr 16 02:08:38.190: INFO: stderr: ""
Apr 16 02:08:38.190: INFO: stdout: "I0416 02:08:33.842459       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/v9t 321\nI0416 02:08:34.042671       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/r6x 210\nI0416 02:08:34.242678       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/dl2q 525\nI0416 02:08:34.442645       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/jk2j 585\nI0416 02:08:34.642673       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/fxkj 520\nI0416 02:08:34.842703       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/fs8p 200\nI0416 02:08:35.042666       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/cthj 539\nI0416 02:08:35.242687       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/q2ns 281\nI0416 02:08:35.442641       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/4bqc 325\nI0416 02:08:35.642724       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/96lb 322\nI0416 02:08:35.842856       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/st6w 290\nI0416 02:08:36.042666       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/vc2s 263\nI0416 02:08:36.242541       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/tf2 329\nI0416 02:08:36.442798       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/jjc9 354\nI0416 02:08:36.642549       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/kp72 377\nI0416 02:08:36.842704       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/z7c 244\nI0416 02:08:37.042881       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/wgr 529\nI0416 02:08:37.242658       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/6ft 562\nI0416 02:08:37.442674       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/7zf 261\nI0416 02:08:37.642825       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/7mk6 570\nI0416 02:08:37.842719       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/h4t8 562\nI0416 02:08:38.042670       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/ssxg 233\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Apr 16 02:08:38.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-4913 delete pod logs-generator'
Apr 16 02:08:40.197: INFO: stderr: ""
Apr 16 02:08:40.197: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:08:40.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4913" for this suite.

• [SLOW TEST:8.265 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":122,"skipped":1999,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:08:40.329: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3869
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:08:40.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3869" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":123,"skipped":2005,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:08:40.724: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4964
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:08:41.057: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Apr 16 02:08:41.086: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:41.086: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:41.086: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:41.095: INFO: Number of nodes with available pods: 0
Apr 16 02:08:41.095: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:08:42.121: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:42.121: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:42.121: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:42.131: INFO: Number of nodes with available pods: 0
Apr 16 02:08:42.131: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:08:43.112: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:43.112: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:43.112: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:43.120: INFO: Number of nodes with available pods: 1
Apr 16 02:08:43.120: INFO: Node k8s-flwang-4oyd5hkwxglz-node-1 is running more than one daemon pod
Apr 16 02:08:44.109: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:44.109: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:44.109: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:44.115: INFO: Number of nodes with available pods: 2
Apr 16 02:08:44.115: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Apr 16 02:08:44.200: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:44.200: INFO: Wrong image for pod: daemon-set-thz5z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:44.217: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:44.217: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:44.217: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:45.227: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:45.227: INFO: Wrong image for pod: daemon-set-thz5z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:45.237: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:45.237: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:45.237: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:46.232: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:46.232: INFO: Wrong image for pod: daemon-set-thz5z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:46.232: INFO: Pod daemon-set-thz5z is not available
Apr 16 02:08:46.242: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:46.242: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:46.242: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:47.243: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:47.243: INFO: Wrong image for pod: daemon-set-thz5z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:47.243: INFO: Pod daemon-set-thz5z is not available
Apr 16 02:08:47.253: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:47.253: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:47.253: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:48.243: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:48.243: INFO: Wrong image for pod: daemon-set-thz5z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:48.243: INFO: Pod daemon-set-thz5z is not available
Apr 16 02:08:48.253: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:48.253: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:48.253: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:49.232: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:49.232: INFO: Wrong image for pod: daemon-set-thz5z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:49.232: INFO: Pod daemon-set-thz5z is not available
Apr 16 02:08:49.243: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:49.243: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:49.243: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:50.230: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:50.230: INFO: Wrong image for pod: daemon-set-thz5z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:50.230: INFO: Pod daemon-set-thz5z is not available
Apr 16 02:08:50.241: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:50.241: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:50.242: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:51.235: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:51.235: INFO: Wrong image for pod: daemon-set-thz5z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:51.235: INFO: Pod daemon-set-thz5z is not available
Apr 16 02:08:51.246: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:51.246: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:51.246: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:52.260: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:52.260: INFO: Wrong image for pod: daemon-set-thz5z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:52.260: INFO: Pod daemon-set-thz5z is not available
Apr 16 02:08:52.268: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:52.269: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:52.269: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:53.229: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:53.229: INFO: Wrong image for pod: daemon-set-thz5z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:53.229: INFO: Pod daemon-set-thz5z is not available
Apr 16 02:08:53.238: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:53.239: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:53.239: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:54.229: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:54.229: INFO: Wrong image for pod: daemon-set-thz5z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:54.229: INFO: Pod daemon-set-thz5z is not available
Apr 16 02:08:54.239: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:54.239: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:54.239: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:55.231: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:55.231: INFO: Wrong image for pod: daemon-set-thz5z. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:55.231: INFO: Pod daemon-set-thz5z is not available
Apr 16 02:08:55.242: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:55.242: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:55.242: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:56.232: INFO: Pod daemon-set-jt45t is not available
Apr 16 02:08:56.232: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:56.242: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:56.242: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:56.242: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:57.233: INFO: Pod daemon-set-jt45t is not available
Apr 16 02:08:57.233: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:57.243: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:57.243: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:57.243: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:58.231: INFO: Pod daemon-set-jt45t is not available
Apr 16 02:08:58.231: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:58.242: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:58.242: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:58.243: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:59.231: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:08:59.241: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:59.241: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:08:59.241: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:00.242: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:09:00.242: INFO: Pod daemon-set-lndjm is not available
Apr 16 02:09:00.251: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:00.251: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:00.251: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:01.235: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:09:01.235: INFO: Pod daemon-set-lndjm is not available
Apr 16 02:09:01.249: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:01.249: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:01.250: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:02.231: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:09:02.231: INFO: Pod daemon-set-lndjm is not available
Apr 16 02:09:02.240: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:02.240: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:02.240: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:03.231: INFO: Wrong image for pod: daemon-set-lndjm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 16 02:09:03.231: INFO: Pod daemon-set-lndjm is not available
Apr 16 02:09:03.240: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:03.240: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:03.240: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:04.242: INFO: Pod daemon-set-qwq76 is not available
Apr 16 02:09:04.293: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:04.293: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:04.293: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Apr 16 02:09:04.306: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:04.306: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:04.307: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:04.316: INFO: Number of nodes with available pods: 1
Apr 16 02:09:04.316: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:09:05.337: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:05.337: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:05.337: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:05.346: INFO: Number of nodes with available pods: 1
Apr 16 02:09:05.346: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:09:06.336: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:06.336: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:06.336: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:09:06.345: INFO: Number of nodes with available pods: 2
Apr 16 02:09:06.345: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4964, will wait for the garbage collector to delete the pods
Apr 16 02:09:06.466: INFO: Deleting DaemonSet.extensions daemon-set took: 26.019736ms
Apr 16 02:09:07.566: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.100309934s
Apr 16 02:09:15.895: INFO: Number of nodes with available pods: 0
Apr 16 02:09:15.895: INFO: Number of running nodes: 0, number of available pods: 0
Apr 16 02:09:15.902: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"417585"},"items":null}

Apr 16 02:09:15.909: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"417585"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:09:15.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4964" for this suite.

• [SLOW TEST:35.261 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":124,"skipped":2029,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:09:15.985: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7741
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Apr 16 02:09:18.889: INFO: Successfully updated pod "annotationupdatea0ef2bd8-798e-484c-a18d-c70e4158b058"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:09:20.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7741" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":125,"skipped":2039,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:09:20.959: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:09:21.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4444" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":126,"skipped":2075,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:09:21.269: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4242
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-8cac6283-4e96-4f67-bcac-538cb9e9beab
STEP: Creating a pod to test consume secrets
Apr 16 02:09:21.570: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-54c6f25c-d59d-4a10-8201-e085deffcb96" in namespace "projected-4242" to be "Succeeded or Failed"
Apr 16 02:09:21.590: INFO: Pod "pod-projected-secrets-54c6f25c-d59d-4a10-8201-e085deffcb96": Phase="Pending", Reason="", readiness=false. Elapsed: 20.366166ms
Apr 16 02:09:23.607: INFO: Pod "pod-projected-secrets-54c6f25c-d59d-4a10-8201-e085deffcb96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036848445s
STEP: Saw pod success
Apr 16 02:09:23.607: INFO: Pod "pod-projected-secrets-54c6f25c-d59d-4a10-8201-e085deffcb96" satisfied condition "Succeeded or Failed"
Apr 16 02:09:23.615: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-projected-secrets-54c6f25c-d59d-4a10-8201-e085deffcb96 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 16 02:09:23.674: INFO: Waiting for pod pod-projected-secrets-54c6f25c-d59d-4a10-8201-e085deffcb96 to disappear
Apr 16 02:09:23.685: INFO: Pod pod-projected-secrets-54c6f25c-d59d-4a10-8201-e085deffcb96 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:09:23.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4242" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":127,"skipped":2117,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:09:23.720: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7082
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-2nc6
STEP: Creating a pod to test atomic-volume-subpath
Apr 16 02:09:24.085: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-2nc6" in namespace "subpath-7082" to be "Succeeded or Failed"
Apr 16 02:09:24.115: INFO: Pod "pod-subpath-test-downwardapi-2nc6": Phase="Pending", Reason="", readiness=false. Elapsed: 30.117424ms
Apr 16 02:09:26.137: INFO: Pod "pod-subpath-test-downwardapi-2nc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051251628s
Apr 16 02:09:28.154: INFO: Pod "pod-subpath-test-downwardapi-2nc6": Phase="Running", Reason="", readiness=true. Elapsed: 4.068277306s
Apr 16 02:09:30.173: INFO: Pod "pod-subpath-test-downwardapi-2nc6": Phase="Running", Reason="", readiness=true. Elapsed: 6.087365669s
Apr 16 02:09:32.188: INFO: Pod "pod-subpath-test-downwardapi-2nc6": Phase="Running", Reason="", readiness=true. Elapsed: 8.102745553s
Apr 16 02:09:34.205: INFO: Pod "pod-subpath-test-downwardapi-2nc6": Phase="Running", Reason="", readiness=true. Elapsed: 10.119957417s
Apr 16 02:09:36.234: INFO: Pod "pod-subpath-test-downwardapi-2nc6": Phase="Running", Reason="", readiness=true. Elapsed: 12.149111906s
Apr 16 02:09:38.250: INFO: Pod "pod-subpath-test-downwardapi-2nc6": Phase="Running", Reason="", readiness=true. Elapsed: 14.164613969s
Apr 16 02:09:40.278: INFO: Pod "pod-subpath-test-downwardapi-2nc6": Phase="Running", Reason="", readiness=true. Elapsed: 16.192554437s
Apr 16 02:09:42.293: INFO: Pod "pod-subpath-test-downwardapi-2nc6": Phase="Running", Reason="", readiness=true. Elapsed: 18.20799789s
Apr 16 02:09:44.309: INFO: Pod "pod-subpath-test-downwardapi-2nc6": Phase="Running", Reason="", readiness=true. Elapsed: 20.223389716s
Apr 16 02:09:46.327: INFO: Pod "pod-subpath-test-downwardapi-2nc6": Phase="Running", Reason="", readiness=true. Elapsed: 22.241969859s
Apr 16 02:09:48.345: INFO: Pod "pod-subpath-test-downwardapi-2nc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.259513371s
STEP: Saw pod success
Apr 16 02:09:48.345: INFO: Pod "pod-subpath-test-downwardapi-2nc6" satisfied condition "Succeeded or Failed"
Apr 16 02:09:48.353: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-subpath-test-downwardapi-2nc6 container test-container-subpath-downwardapi-2nc6: <nil>
STEP: delete the pod
Apr 16 02:09:48.417: INFO: Waiting for pod pod-subpath-test-downwardapi-2nc6 to disappear
Apr 16 02:09:48.428: INFO: Pod pod-subpath-test-downwardapi-2nc6 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-2nc6
Apr 16 02:09:48.428: INFO: Deleting pod "pod-subpath-test-downwardapi-2nc6" in namespace "subpath-7082"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:09:48.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7082" for this suite.

• [SLOW TEST:24.750 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":128,"skipped":2124,"failed":0}
SSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:09:48.472: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2009
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:09:48.737: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-b0f31052-3ef5-4491-99c2-e738ed9a1bf7" in namespace "security-context-test-2009" to be "Succeeded or Failed"
Apr 16 02:09:48.747: INFO: Pod "alpine-nnp-false-b0f31052-3ef5-4491-99c2-e738ed9a1bf7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.97641ms
Apr 16 02:09:50.761: INFO: Pod "alpine-nnp-false-b0f31052-3ef5-4491-99c2-e738ed9a1bf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024406131s
Apr 16 02:09:52.777: INFO: Pod "alpine-nnp-false-b0f31052-3ef5-4491-99c2-e738ed9a1bf7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040492414s
Apr 16 02:09:54.792: INFO: Pod "alpine-nnp-false-b0f31052-3ef5-4491-99c2-e738ed9a1bf7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05546315s
Apr 16 02:09:56.811: INFO: Pod "alpine-nnp-false-b0f31052-3ef5-4491-99c2-e738ed9a1bf7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.074139692s
Apr 16 02:09:58.826: INFO: Pod "alpine-nnp-false-b0f31052-3ef5-4491-99c2-e738ed9a1bf7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.089524827s
Apr 16 02:10:00.842: INFO: Pod "alpine-nnp-false-b0f31052-3ef5-4491-99c2-e738ed9a1bf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.104892092s
Apr 16 02:10:00.842: INFO: Pod "alpine-nnp-false-b0f31052-3ef5-4491-99c2-e738ed9a1bf7" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:10:00.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2009" for this suite.

• [SLOW TEST:12.500 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":129,"skipped":2129,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:10:00.972: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-8316
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Apr 16 02:10:01.277: INFO: created test-podtemplate-1
Apr 16 02:10:01.293: INFO: created test-podtemplate-2
Apr 16 02:10:01.308: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Apr 16 02:10:01.321: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Apr 16 02:10:01.375: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:10:01.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8316" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":130,"skipped":2138,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:10:01.413: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1517
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 16 02:10:05.838: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 16 02:10:05.850: INFO: Pod pod-with-poststart-http-hook still exists
Apr 16 02:10:07.850: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 16 02:10:07.865: INFO: Pod pod-with-poststart-http-hook still exists
Apr 16 02:10:09.850: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 16 02:10:09.864: INFO: Pod pod-with-poststart-http-hook still exists
Apr 16 02:10:11.850: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 16 02:10:11.863: INFO: Pod pod-with-poststart-http-hook still exists
Apr 16 02:10:13.850: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 16 02:10:13.865: INFO: Pod pod-with-poststart-http-hook still exists
Apr 16 02:10:15.851: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 16 02:10:15.869: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:10:15.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1517" for this suite.

• [SLOW TEST:14.487 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":131,"skipped":2151,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:10:15.901: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5539
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:10:20.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5539" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":132,"skipped":2186,"failed":0}
SS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:10:20.358: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-2351
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Apr 16 02:10:20.678: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Apr 16 02:10:20.744: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:10:20.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2351" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":133,"skipped":2188,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:10:20.874: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4061
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 16 02:10:22.062: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 16 02:10:24.105: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135822, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135822, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135822, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135822, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 02:10:27.190: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:10:27.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4061" for this suite.
STEP: Destroying namespace "webhook-4061-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.020 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":134,"skipped":2218,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:10:27.895: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-8282
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:10:28.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8282" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":135,"skipped":2224,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:10:28.293: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9506
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-9506/configmap-test-f3f34d17-2369-401f-a682-6c80419c2e64
STEP: Creating a pod to test consume configMaps
Apr 16 02:10:28.612: INFO: Waiting up to 5m0s for pod "pod-configmaps-83200bbc-4ebc-47a5-bfee-390db9dd0009" in namespace "configmap-9506" to be "Succeeded or Failed"
Apr 16 02:10:28.629: INFO: Pod "pod-configmaps-83200bbc-4ebc-47a5-bfee-390db9dd0009": Phase="Pending", Reason="", readiness=false. Elapsed: 17.058944ms
Apr 16 02:10:30.648: INFO: Pod "pod-configmaps-83200bbc-4ebc-47a5-bfee-390db9dd0009": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035992276s
STEP: Saw pod success
Apr 16 02:10:30.648: INFO: Pod "pod-configmaps-83200bbc-4ebc-47a5-bfee-390db9dd0009" satisfied condition "Succeeded or Failed"
Apr 16 02:10:30.657: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-configmaps-83200bbc-4ebc-47a5-bfee-390db9dd0009 container env-test: <nil>
STEP: delete the pod
Apr 16 02:10:30.719: INFO: Waiting for pod pod-configmaps-83200bbc-4ebc-47a5-bfee-390db9dd0009 to disappear
Apr 16 02:10:30.727: INFO: Pod pod-configmaps-83200bbc-4ebc-47a5-bfee-390db9dd0009 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:10:30.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9506" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":136,"skipped":2260,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:10:30.753: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9313
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:10:35.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9313" for this suite.

• [SLOW TEST:5.232 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":137,"skipped":2275,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:10:35.986: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2518
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 16 02:10:36.321: INFO: Waiting up to 5m0s for pod "pod-0f6d5cbd-ab4a-4d98-acd9-08b4086a7561" in namespace "emptydir-2518" to be "Succeeded or Failed"
Apr 16 02:10:36.339: INFO: Pod "pod-0f6d5cbd-ab4a-4d98-acd9-08b4086a7561": Phase="Pending", Reason="", readiness=false. Elapsed: 17.880458ms
Apr 16 02:10:38.360: INFO: Pod "pod-0f6d5cbd-ab4a-4d98-acd9-08b4086a7561": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038753306s
Apr 16 02:10:40.377: INFO: Pod "pod-0f6d5cbd-ab4a-4d98-acd9-08b4086a7561": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056365258s
STEP: Saw pod success
Apr 16 02:10:40.377: INFO: Pod "pod-0f6d5cbd-ab4a-4d98-acd9-08b4086a7561" satisfied condition "Succeeded or Failed"
Apr 16 02:10:40.388: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-0f6d5cbd-ab4a-4d98-acd9-08b4086a7561 container test-container: <nil>
STEP: delete the pod
Apr 16 02:10:40.446: INFO: Waiting for pod pod-0f6d5cbd-ab4a-4d98-acd9-08b4086a7561 to disappear
Apr 16 02:10:40.462: INFO: Pod pod-0f6d5cbd-ab4a-4d98-acd9-08b4086a7561 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:10:40.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2518" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":138,"skipped":2283,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:10:40.496: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3530
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3530
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3530
STEP: creating replication controller externalsvc in namespace services-3530
I0416 02:10:40.834298      25 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3530, replica count: 2
I0416 02:10:43.885087      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Apr 16 02:10:43.966: INFO: Creating new exec pod
Apr 16 02:10:46.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-3530 exec execpodljdcp -- /bin/sh -x -c nslookup clusterip-service.services-3530.svc.cluster.local'
Apr 16 02:10:46.473: INFO: stderr: "+ nslookup clusterip-service.services-3530.svc.cluster.local\n"
Apr 16 02:10:46.473: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nclusterip-service.services-3530.svc.cluster.local\tcanonical name = externalsvc.services-3530.svc.cluster.local.\nName:\texternalsvc.services-3530.svc.cluster.local\nAddress: 10.254.156.236\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3530, will wait for the garbage collector to delete the pods
Apr 16 02:10:46.561: INFO: Deleting ReplicationController externalsvc took: 23.784276ms
Apr 16 02:10:46.661: INFO: Terminating ReplicationController externalsvc pods took: 100.454645ms
Apr 16 02:10:55.942: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:10:55.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3530" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:15.535 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":139,"skipped":2306,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:10:56.033: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1331
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Apr 16 02:10:56.509: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1331  9dd848f1-109f-4087-8ce1-5f4b6924f530 418541 0 2021-04-16 02:10:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-16 02:10:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 16 02:10:56.510: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1331  9dd848f1-109f-4087-8ce1-5f4b6924f530 418543 0 2021-04-16 02:10:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-16 02:10:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 16 02:10:56.511: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1331  9dd848f1-109f-4087-8ce1-5f4b6924f530 418544 0 2021-04-16 02:10:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-16 02:10:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Apr 16 02:11:06.621: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1331  9dd848f1-109f-4087-8ce1-5f4b6924f530 418606 0 2021-04-16 02:10:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-16 02:10:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 16 02:11:06.622: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1331  9dd848f1-109f-4087-8ce1-5f4b6924f530 418607 0 2021-04-16 02:10:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-16 02:10:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 16 02:11:06.622: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1331  9dd848f1-109f-4087-8ce1-5f4b6924f530 418608 0 2021-04-16 02:10:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-16 02:10:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:11:06.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1331" for this suite.

• [SLOW TEST:10.613 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":140,"skipped":2358,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:11:06.650: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8533
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-d00b4059-981c-458e-9f63-0cda6e2a1894
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:11:06.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8533" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":141,"skipped":2369,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:11:06.906: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4557
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:11:07.218: INFO: Create a RollingUpdate DaemonSet
Apr 16 02:11:07.236: INFO: Check that daemon pods launch on every node of the cluster
Apr 16 02:11:07.249: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:07.249: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:07.249: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:07.259: INFO: Number of nodes with available pods: 0
Apr 16 02:11:07.259: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:11:08.276: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:08.276: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:08.276: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:08.284: INFO: Number of nodes with available pods: 0
Apr 16 02:11:08.284: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:11:09.274: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:09.274: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:09.274: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:09.281: INFO: Number of nodes with available pods: 2
Apr 16 02:11:09.281: INFO: Number of running nodes: 2, number of available pods: 2
Apr 16 02:11:09.281: INFO: Update the DaemonSet to trigger a rollout
Apr 16 02:11:09.309: INFO: Updating DaemonSet daemon-set
Apr 16 02:11:24.352: INFO: Roll back the DaemonSet before rollout is complete
Apr 16 02:11:24.377: INFO: Updating DaemonSet daemon-set
Apr 16 02:11:24.377: INFO: Make sure DaemonSet rollback is complete
Apr 16 02:11:24.387: INFO: Wrong image for pod: daemon-set-9bhjj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 16 02:11:24.387: INFO: Pod daemon-set-9bhjj is not available
Apr 16 02:11:24.397: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:24.397: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:24.398: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:25.412: INFO: Wrong image for pod: daemon-set-9bhjj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 16 02:11:25.412: INFO: Pod daemon-set-9bhjj is not available
Apr 16 02:11:25.424: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:25.424: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:25.425: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:26.411: INFO: Wrong image for pod: daemon-set-9bhjj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 16 02:11:26.411: INFO: Pod daemon-set-9bhjj is not available
Apr 16 02:11:26.422: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:26.422: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:26.422: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:27.412: INFO: Wrong image for pod: daemon-set-9bhjj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 16 02:11:27.412: INFO: Pod daemon-set-9bhjj is not available
Apr 16 02:11:27.425: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:27.425: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:27.425: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:28.408: INFO: Wrong image for pod: daemon-set-9bhjj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 16 02:11:28.409: INFO: Pod daemon-set-9bhjj is not available
Apr 16 02:11:28.417: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:28.417: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:28.417: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:29.412: INFO: Wrong image for pod: daemon-set-9bhjj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 16 02:11:29.412: INFO: Pod daemon-set-9bhjj is not available
Apr 16 02:11:29.422: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:29.422: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:29.422: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:30.411: INFO: Wrong image for pod: daemon-set-9bhjj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 16 02:11:30.411: INFO: Pod daemon-set-9bhjj is not available
Apr 16 02:11:30.421: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:30.421: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:30.421: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:31.411: INFO: Wrong image for pod: daemon-set-9bhjj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 16 02:11:31.411: INFO: Pod daemon-set-9bhjj is not available
Apr 16 02:11:31.422: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:31.422: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:31.422: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:32.413: INFO: Wrong image for pod: daemon-set-9bhjj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 16 02:11:32.414: INFO: Pod daemon-set-9bhjj is not available
Apr 16 02:11:32.425: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:32.425: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:32.425: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:33.417: INFO: Wrong image for pod: daemon-set-9bhjj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 16 02:11:33.417: INFO: Pod daemon-set-9bhjj is not available
Apr 16 02:11:33.432: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:33.432: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:33.432: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:34.411: INFO: Pod daemon-set-vhj65 is not available
Apr 16 02:11:34.421: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:34.421: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:11:34.421: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4557, will wait for the garbage collector to delete the pods
Apr 16 02:11:34.528: INFO: Deleting DaemonSet.extensions daemon-set took: 26.980985ms
Apr 16 02:11:35.629: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.10152322s
Apr 16 02:11:45.862: INFO: Number of nodes with available pods: 0
Apr 16 02:11:45.862: INFO: Number of running nodes: 0, number of available pods: 0
Apr 16 02:11:45.870: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"418830"},"items":null}

Apr 16 02:11:45.879: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"418830"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:11:45.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4557" for this suite.

• [SLOW TEST:39.062 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":142,"skipped":2382,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:11:45.970: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7881
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:11:46.288: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr 16 02:11:51.317: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 16 02:11:51.318: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr 16 02:11:53.341: INFO: Creating deployment "test-rollover-deployment"
Apr 16 02:11:53.371: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr 16 02:11:55.406: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr 16 02:11:55.423: INFO: Ensure that both replica sets have 1 created replica
Apr 16 02:11:55.435: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr 16 02:11:55.463: INFO: Updating deployment test-rollover-deployment
Apr 16 02:11:55.463: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr 16 02:11:57.492: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr 16 02:11:57.507: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr 16 02:11:57.523: INFO: all replica sets need to contain the pod-template-hash label
Apr 16 02:11:57.524: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135915, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 02:11:59.551: INFO: all replica sets need to contain the pod-template-hash label
Apr 16 02:11:59.552: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135917, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 02:12:01.549: INFO: all replica sets need to contain the pod-template-hash label
Apr 16 02:12:01.549: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135917, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 02:12:03.547: INFO: all replica sets need to contain the pod-template-hash label
Apr 16 02:12:03.547: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135917, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 02:12:05.546: INFO: all replica sets need to contain the pod-template-hash label
Apr 16 02:12:05.546: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135917, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 02:12:07.546: INFO: all replica sets need to contain the pod-template-hash label
Apr 16 02:12:07.547: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135917, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754135913, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 02:12:09.546: INFO: 
Apr 16 02:12:09.546: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 16 02:12:09.574: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7881  b91297f0-2110-4791-8895-44acbbf65740 419008 2 2021-04-16 02:11:53 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-04-16 02:11:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-16 02:12:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001316c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-04-16 02:11:53 +0000 UTC,LastTransitionTime:2021-04-16 02:11:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-04-16 02:12:08 +0000 UTC,LastTransitionTime:2021-04-16 02:11:53 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 16 02:12:09.584: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-7881  2bb262d7-0660-414d-8989-1867c07b5380 418997 2 2021-04-16 02:11:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b91297f0-2110-4791-8895-44acbbf65740 0xc003fc0197 0xc003fc0198}] []  [{kube-controller-manager Update apps/v1 2021-04-16 02:12:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b91297f0-2110-4791-8895-44acbbf65740\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fc0228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 16 02:12:09.584: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr 16 02:12:09.584: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7881  9098109c-ccd9-45ab-955a-d8f96f6ad9d6 419007 2 2021-04-16 02:11:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b91297f0-2110-4791-8895-44acbbf65740 0xc003fc0087 0xc003fc0088}] []  [{e2e.test Update apps/v1 2021-04-16 02:11:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-16 02:12:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b91297f0-2110-4791-8895-44acbbf65740\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003fc0128 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 16 02:12:09.585: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-7881  7e7d6120-f19a-409c-b236-c68afe04abc3 418943 2 2021-04-16 02:11:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b91297f0-2110-4791-8895-44acbbf65740 0xc003fc0297 0xc003fc0298}] []  [{kube-controller-manager Update apps/v1 2021-04-16 02:11:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b91297f0-2110-4791-8895-44acbbf65740\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fc0328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 16 02:12:09.594: INFO: Pod "test-rollover-deployment-668db69979-zb76b" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-zb76b test-rollover-deployment-668db69979- deployment-7881  503b29b6-500f-4ee9-b380-5bd760a8cd8c 418969 0 2021-04-16 02:11:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[cni.projectcalico.org/podIP:10.100.97.103/32 cni.projectcalico.org/podIPs:10.100.97.103/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 2bb262d7-0660-414d-8989-1867c07b5380 0xc001317017 0xc001317018}] []  [{kube-controller-manager Update v1 2021-04-16 02:11:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bb262d7-0660-414d-8989-1867c07b5380\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-16 02:11:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-16 02:11:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.97.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-w9qd2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-w9qd2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-w9qd2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:11:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:11:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:11:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:11:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.68,PodIP:10.100.97.103,StartTime:2021-04-16 02:11:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-16 02:11:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://4645ce5575e20962677e98bf5be7539cc3ae4ae93b5d66b8bfe88735b3860bf6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.97.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:12:09.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7881" for this suite.

• [SLOW TEST:23.657 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":143,"skipped":2415,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:12:09.636: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5334
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 16 02:12:09.912: INFO: Waiting up to 5m0s for pod "pod-10045732-6158-46da-93a8-41f406cbfbe2" in namespace "emptydir-5334" to be "Succeeded or Failed"
Apr 16 02:12:09.929: INFO: Pod "pod-10045732-6158-46da-93a8-41f406cbfbe2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.263832ms
Apr 16 02:12:11.937: INFO: Pod "pod-10045732-6158-46da-93a8-41f406cbfbe2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025465381s
Apr 16 02:12:13.953: INFO: Pod "pod-10045732-6158-46da-93a8-41f406cbfbe2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040916739s
STEP: Saw pod success
Apr 16 02:12:13.953: INFO: Pod "pod-10045732-6158-46da-93a8-41f406cbfbe2" satisfied condition "Succeeded or Failed"
Apr 16 02:12:13.960: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-10045732-6158-46da-93a8-41f406cbfbe2 container test-container: <nil>
STEP: delete the pod
Apr 16 02:12:14.089: INFO: Waiting for pod pod-10045732-6158-46da-93a8-41f406cbfbe2 to disappear
Apr 16 02:12:14.100: INFO: Pod pod-10045732-6158-46da-93a8-41f406cbfbe2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:12:14.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5334" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":144,"skipped":2455,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:12:14.131: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5848
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-hs472 in namespace proxy-5848
I0416 02:12:14.460300      25 runners.go:190] Created replication controller with name: proxy-service-hs472, namespace: proxy-5848, replica count: 1
I0416 02:12:15.510875      25 runners.go:190] proxy-service-hs472 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0416 02:12:16.511733      25 runners.go:190] proxy-service-hs472 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0416 02:12:17.512731      25 runners.go:190] proxy-service-hs472 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0416 02:12:18.513139      25 runners.go:190] proxy-service-hs472 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0416 02:12:19.513487      25 runners.go:190] proxy-service-hs472 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0416 02:12:20.513910      25 runners.go:190] proxy-service-hs472 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0416 02:12:21.514469      25 runners.go:190] proxy-service-hs472 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0416 02:12:22.515238      25 runners.go:190] proxy-service-hs472 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0416 02:12:23.515776      25 runners.go:190] proxy-service-hs472 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 16 02:12:23.536: INFO: setup took 9.130570772s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Apr 16 02:12:23.562: INFO: (0) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 25.556234ms)
Apr 16 02:12:23.584: INFO: (0) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 47.370659ms)
Apr 16 02:12:23.584: INFO: (0) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 46.657731ms)
Apr 16 02:12:23.584: INFO: (0) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 47.1475ms)
Apr 16 02:12:23.584: INFO: (0) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 46.88528ms)
Apr 16 02:12:23.584: INFO: (0) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 46.775175ms)
Apr 16 02:12:23.584: INFO: (0) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 46.672993ms)
Apr 16 02:12:23.584: INFO: (0) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 47.052544ms)
Apr 16 02:12:23.584: INFO: (0) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 47.541547ms)
Apr 16 02:12:23.585: INFO: (0) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 47.717339ms)
Apr 16 02:12:23.587: INFO: (0) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 50.549023ms)
Apr 16 02:12:23.588: INFO: (0) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 50.833384ms)
Apr 16 02:12:23.588: INFO: (0) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 50.864052ms)
Apr 16 02:12:23.589: INFO: (0) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 51.283907ms)
Apr 16 02:12:23.590: INFO: (0) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 52.538187ms)
Apr 16 02:12:23.590: INFO: (0) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 52.640815ms)
Apr 16 02:12:23.603: INFO: (1) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 12.761959ms)
Apr 16 02:12:23.608: INFO: (1) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 17.454327ms)
Apr 16 02:12:23.608: INFO: (1) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 17.140572ms)
Apr 16 02:12:23.608: INFO: (1) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 17.25408ms)
Apr 16 02:12:23.610: INFO: (1) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 19.947832ms)
Apr 16 02:12:23.611: INFO: (1) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 20.901912ms)
Apr 16 02:12:23.611: INFO: (1) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 20.951522ms)
Apr 16 02:12:23.612: INFO: (1) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 21.362654ms)
Apr 16 02:12:23.614: INFO: (1) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 23.091708ms)
Apr 16 02:12:23.614: INFO: (1) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 23.597916ms)
Apr 16 02:12:23.614: INFO: (1) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 24.057608ms)
Apr 16 02:12:23.616: INFO: (1) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 24.977131ms)
Apr 16 02:12:23.616: INFO: (1) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 25.270515ms)
Apr 16 02:12:23.616: INFO: (1) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 26.023577ms)
Apr 16 02:12:23.617: INFO: (1) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 26.541936ms)
Apr 16 02:12:23.618: INFO: (1) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 27.232055ms)
Apr 16 02:12:23.626: INFO: (2) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 8.63482ms)
Apr 16 02:12:23.631: INFO: (2) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 13.110539ms)
Apr 16 02:12:23.631: INFO: (2) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 13.120035ms)
Apr 16 02:12:23.633: INFO: (2) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 14.94338ms)
Apr 16 02:12:23.633: INFO: (2) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 15.044823ms)
Apr 16 02:12:23.634: INFO: (2) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 15.418241ms)
Apr 16 02:12:23.634: INFO: (2) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 16.337946ms)
Apr 16 02:12:23.634: INFO: (2) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 16.041143ms)
Apr 16 02:12:23.635: INFO: (2) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 16.40973ms)
Apr 16 02:12:23.635: INFO: (2) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 16.713386ms)
Apr 16 02:12:23.636: INFO: (2) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 17.449999ms)
Apr 16 02:12:23.637: INFO: (2) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 18.418593ms)
Apr 16 02:12:23.638: INFO: (2) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 19.446824ms)
Apr 16 02:12:23.638: INFO: (2) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 19.859987ms)
Apr 16 02:12:23.638: INFO: (2) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 19.975093ms)
Apr 16 02:12:23.639: INFO: (2) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 20.138466ms)
Apr 16 02:12:23.655: INFO: (3) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 15.974464ms)
Apr 16 02:12:23.655: INFO: (3) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 16.108925ms)
Apr 16 02:12:23.656: INFO: (3) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 16.203885ms)
Apr 16 02:12:23.656: INFO: (3) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 16.171301ms)
Apr 16 02:12:23.656: INFO: (3) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 16.741447ms)
Apr 16 02:12:23.655: INFO: (3) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 16.519804ms)
Apr 16 02:12:23.656: INFO: (3) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 17.215032ms)
Apr 16 02:12:23.657: INFO: (3) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 17.905823ms)
Apr 16 02:12:23.657: INFO: (3) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 18.27659ms)
Apr 16 02:12:23.658: INFO: (3) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 19.046688ms)
Apr 16 02:12:23.660: INFO: (3) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 21.464447ms)
Apr 16 02:12:23.660: INFO: (3) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 21.596478ms)
Apr 16 02:12:23.661: INFO: (3) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 22.167942ms)
Apr 16 02:12:23.663: INFO: (3) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 23.376283ms)
Apr 16 02:12:23.666: INFO: (3) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 26.747394ms)
Apr 16 02:12:23.668: INFO: (3) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 28.224665ms)
Apr 16 02:12:23.683: INFO: (4) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 13.683953ms)
Apr 16 02:12:23.685: INFO: (4) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 16.881307ms)
Apr 16 02:12:23.685: INFO: (4) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 16.583908ms)
Apr 16 02:12:23.685: INFO: (4) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 16.782466ms)
Apr 16 02:12:23.686: INFO: (4) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 18.331055ms)
Apr 16 02:12:23.686: INFO: (4) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 17.6384ms)
Apr 16 02:12:23.687: INFO: (4) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 17.922555ms)
Apr 16 02:12:23.687: INFO: (4) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 18.526221ms)
Apr 16 02:12:23.688: INFO: (4) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 19.983187ms)
Apr 16 02:12:23.689: INFO: (4) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 20.868035ms)
Apr 16 02:12:23.689: INFO: (4) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 21.159207ms)
Apr 16 02:12:23.696: INFO: (4) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 27.305805ms)
Apr 16 02:12:23.696: INFO: (4) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 28.166417ms)
Apr 16 02:12:23.696: INFO: (4) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 27.787349ms)
Apr 16 02:12:23.697: INFO: (4) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 28.535752ms)
Apr 16 02:12:23.698: INFO: (4) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 29.183113ms)
Apr 16 02:12:23.709: INFO: (5) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 11.220858ms)
Apr 16 02:12:23.712: INFO: (5) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 14.556881ms)
Apr 16 02:12:23.715: INFO: (5) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 16.427324ms)
Apr 16 02:12:23.721: INFO: (5) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 21.992694ms)
Apr 16 02:12:23.723: INFO: (5) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 24.118472ms)
Apr 16 02:12:23.723: INFO: (5) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 24.378178ms)
Apr 16 02:12:23.723: INFO: (5) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 24.792421ms)
Apr 16 02:12:23.724: INFO: (5) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 25.575007ms)
Apr 16 02:12:23.724: INFO: (5) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 25.909111ms)
Apr 16 02:12:23.725: INFO: (5) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 25.845018ms)
Apr 16 02:12:23.725: INFO: (5) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 26.439445ms)
Apr 16 02:12:23.726: INFO: (5) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 27.341742ms)
Apr 16 02:12:23.726: INFO: (5) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 27.741413ms)
Apr 16 02:12:23.727: INFO: (5) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 28.242869ms)
Apr 16 02:12:23.727: INFO: (5) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 28.182433ms)
Apr 16 02:12:23.727: INFO: (5) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 28.887362ms)
Apr 16 02:12:23.740: INFO: (6) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 12.741766ms)
Apr 16 02:12:23.742: INFO: (6) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 13.869018ms)
Apr 16 02:12:23.742: INFO: (6) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 13.651047ms)
Apr 16 02:12:23.743: INFO: (6) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 14.953192ms)
Apr 16 02:12:23.743: INFO: (6) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 15.400375ms)
Apr 16 02:12:23.743: INFO: (6) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 15.553437ms)
Apr 16 02:12:23.744: INFO: (6) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 15.811309ms)
Apr 16 02:12:23.744: INFO: (6) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 16.1022ms)
Apr 16 02:12:23.747: INFO: (6) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 18.867267ms)
Apr 16 02:12:23.748: INFO: (6) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 19.524012ms)
Apr 16 02:12:23.748: INFO: (6) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 19.68592ms)
Apr 16 02:12:23.750: INFO: (6) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 22.114351ms)
Apr 16 02:12:23.751: INFO: (6) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 22.551343ms)
Apr 16 02:12:23.751: INFO: (6) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 22.967331ms)
Apr 16 02:12:23.752: INFO: (6) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 23.46409ms)
Apr 16 02:12:23.752: INFO: (6) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 24.068592ms)
Apr 16 02:12:23.767: INFO: (7) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 14.25282ms)
Apr 16 02:12:23.767: INFO: (7) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 14.220717ms)
Apr 16 02:12:23.768: INFO: (7) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 14.895171ms)
Apr 16 02:12:23.768: INFO: (7) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 15.813098ms)
Apr 16 02:12:23.771: INFO: (7) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 17.850884ms)
Apr 16 02:12:23.771: INFO: (7) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 17.996396ms)
Apr 16 02:12:23.772: INFO: (7) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 18.346126ms)
Apr 16 02:12:23.773: INFO: (7) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 19.950081ms)
Apr 16 02:12:23.774: INFO: (7) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 20.531469ms)
Apr 16 02:12:23.774: INFO: (7) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 20.578018ms)
Apr 16 02:12:23.774: INFO: (7) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 20.926536ms)
Apr 16 02:12:23.777: INFO: (7) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 23.657869ms)
Apr 16 02:12:23.777: INFO: (7) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 23.62128ms)
Apr 16 02:12:23.777: INFO: (7) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 24.524361ms)
Apr 16 02:12:23.778: INFO: (7) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 24.892068ms)
Apr 16 02:12:23.778: INFO: (7) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 25.08503ms)
Apr 16 02:12:23.790: INFO: (8) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 12.147483ms)
Apr 16 02:12:23.791: INFO: (8) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 12.541647ms)
Apr 16 02:12:23.795: INFO: (8) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 15.506352ms)
Apr 16 02:12:23.795: INFO: (8) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 16.146677ms)
Apr 16 02:12:23.799: INFO: (8) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 19.515502ms)
Apr 16 02:12:23.802: INFO: (8) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 23.436382ms)
Apr 16 02:12:23.802: INFO: (8) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 23.830916ms)
Apr 16 02:12:23.802: INFO: (8) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 22.980387ms)
Apr 16 02:12:23.803: INFO: (8) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 23.011508ms)
Apr 16 02:12:23.803: INFO: (8) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 23.231727ms)
Apr 16 02:12:23.803: INFO: (8) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 23.619086ms)
Apr 16 02:12:23.802: INFO: (8) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 22.813674ms)
Apr 16 02:12:23.803: INFO: (8) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 24.261998ms)
Apr 16 02:12:23.804: INFO: (8) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 24.509014ms)
Apr 16 02:12:23.804: INFO: (8) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 24.343536ms)
Apr 16 02:12:23.806: INFO: (8) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 26.924603ms)
Apr 16 02:12:23.818: INFO: (9) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 12.311087ms)
Apr 16 02:12:23.818: INFO: (9) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 12.556458ms)
Apr 16 02:12:23.826: INFO: (9) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 19.359882ms)
Apr 16 02:12:23.826: INFO: (9) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 19.341748ms)
Apr 16 02:12:23.826: INFO: (9) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 19.386501ms)
Apr 16 02:12:23.826: INFO: (9) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 19.658711ms)
Apr 16 02:12:23.826: INFO: (9) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 19.662187ms)
Apr 16 02:12:23.827: INFO: (9) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 21.152853ms)
Apr 16 02:12:23.827: INFO: (9) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 21.117586ms)
Apr 16 02:12:23.827: INFO: (9) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 21.237063ms)
Apr 16 02:12:23.828: INFO: (9) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 21.208954ms)
Apr 16 02:12:23.831: INFO: (9) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 24.938069ms)
Apr 16 02:12:23.834: INFO: (9) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 27.376646ms)
Apr 16 02:12:23.834: INFO: (9) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 28.083486ms)
Apr 16 02:12:23.834: INFO: (9) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 28.42098ms)
Apr 16 02:12:23.835: INFO: (9) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 28.954503ms)
Apr 16 02:12:23.852: INFO: (10) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 17.049663ms)
Apr 16 02:12:23.852: INFO: (10) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 16.698438ms)
Apr 16 02:12:23.853: INFO: (10) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 16.995214ms)
Apr 16 02:12:23.854: INFO: (10) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 18.282156ms)
Apr 16 02:12:23.855: INFO: (10) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 19.057634ms)
Apr 16 02:12:23.855: INFO: (10) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 19.633849ms)
Apr 16 02:12:23.856: INFO: (10) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 20.953089ms)
Apr 16 02:12:23.857: INFO: (10) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 21.343615ms)
Apr 16 02:12:23.857: INFO: (10) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 21.571045ms)
Apr 16 02:12:23.858: INFO: (10) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 21.78997ms)
Apr 16 02:12:23.858: INFO: (10) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 21.908944ms)
Apr 16 02:12:23.864: INFO: (10) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 28.175129ms)
Apr 16 02:12:23.865: INFO: (10) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 29.062303ms)
Apr 16 02:12:23.866: INFO: (10) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 29.92803ms)
Apr 16 02:12:23.866: INFO: (10) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 29.740568ms)
Apr 16 02:12:23.867: INFO: (10) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 30.688583ms)
Apr 16 02:12:23.876: INFO: (11) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 9.322017ms)
Apr 16 02:12:23.881: INFO: (11) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 13.553362ms)
Apr 16 02:12:23.886: INFO: (11) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 18.696738ms)
Apr 16 02:12:23.886: INFO: (11) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 18.907211ms)
Apr 16 02:12:23.888: INFO: (11) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 20.763065ms)
Apr 16 02:12:23.888: INFO: (11) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 20.873883ms)
Apr 16 02:12:23.888: INFO: (11) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 21.028405ms)
Apr 16 02:12:23.888: INFO: (11) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 21.151876ms)
Apr 16 02:12:23.891: INFO: (11) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 23.245001ms)
Apr 16 02:12:23.891: INFO: (11) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 23.762492ms)
Apr 16 02:12:23.891: INFO: (11) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 23.989984ms)
Apr 16 02:12:23.892: INFO: (11) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 24.666764ms)
Apr 16 02:12:23.893: INFO: (11) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 25.348286ms)
Apr 16 02:12:23.893: INFO: (11) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 25.629289ms)
Apr 16 02:12:23.894: INFO: (11) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 26.41258ms)
Apr 16 02:12:23.894: INFO: (11) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 26.408397ms)
Apr 16 02:12:23.916: INFO: (12) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 21.988452ms)
Apr 16 02:12:23.916: INFO: (12) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 20.936403ms)
Apr 16 02:12:23.917: INFO: (12) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 22.0461ms)
Apr 16 02:12:23.917: INFO: (12) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 22.05056ms)
Apr 16 02:12:23.918: INFO: (12) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 22.845886ms)
Apr 16 02:12:23.919: INFO: (12) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 22.787896ms)
Apr 16 02:12:23.919: INFO: (12) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 22.922277ms)
Apr 16 02:12:23.921: INFO: (12) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 26.116963ms)
Apr 16 02:12:23.922: INFO: (12) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 26.546883ms)
Apr 16 02:12:23.922: INFO: (12) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 25.324996ms)
Apr 16 02:12:23.922: INFO: (12) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 25.404674ms)
Apr 16 02:12:23.922: INFO: (12) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 26.125872ms)
Apr 16 02:12:23.922: INFO: (12) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 25.732892ms)
Apr 16 02:12:23.923: INFO: (12) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 26.995035ms)
Apr 16 02:12:23.934: INFO: (12) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 37.610484ms)
Apr 16 02:12:23.934: INFO: (12) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 38.102717ms)
Apr 16 02:12:23.951: INFO: (13) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 15.962389ms)
Apr 16 02:12:23.953: INFO: (13) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 17.561091ms)
Apr 16 02:12:23.953: INFO: (13) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 17.679421ms)
Apr 16 02:12:23.958: INFO: (13) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 22.765009ms)
Apr 16 02:12:23.958: INFO: (13) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 23.364153ms)
Apr 16 02:12:23.958: INFO: (13) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 22.961588ms)
Apr 16 02:12:23.959: INFO: (13) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 23.762524ms)
Apr 16 02:12:23.959: INFO: (13) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 23.528493ms)
Apr 16 02:12:23.959: INFO: (13) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 23.524663ms)
Apr 16 02:12:23.959: INFO: (13) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 23.770202ms)
Apr 16 02:12:23.959: INFO: (13) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 23.534417ms)
Apr 16 02:12:23.959: INFO: (13) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 23.636404ms)
Apr 16 02:12:23.960: INFO: (13) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 24.793678ms)
Apr 16 02:12:23.961: INFO: (13) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 25.253908ms)
Apr 16 02:12:23.961: INFO: (13) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 25.641064ms)
Apr 16 02:12:23.961: INFO: (13) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 25.814435ms)
Apr 16 02:12:23.978: INFO: (14) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 16.279226ms)
Apr 16 02:12:23.982: INFO: (14) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 20.933258ms)
Apr 16 02:12:23.983: INFO: (14) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 20.919452ms)
Apr 16 02:12:23.983: INFO: (14) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 21.556491ms)
Apr 16 02:12:23.984: INFO: (14) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 22.548215ms)
Apr 16 02:12:23.984: INFO: (14) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 22.832829ms)
Apr 16 02:12:23.985: INFO: (14) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 22.802484ms)
Apr 16 02:12:23.985: INFO: (14) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 22.963941ms)
Apr 16 02:12:23.986: INFO: (14) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 23.767398ms)
Apr 16 02:12:23.986: INFO: (14) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 24.196356ms)
Apr 16 02:12:23.986: INFO: (14) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 24.067546ms)
Apr 16 02:12:23.987: INFO: (14) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 25.013503ms)
Apr 16 02:12:23.988: INFO: (14) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 26.354545ms)
Apr 16 02:12:23.988: INFO: (14) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 26.462101ms)
Apr 16 02:12:23.988: INFO: (14) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 26.879832ms)
Apr 16 02:12:23.988: INFO: (14) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 26.618052ms)
Apr 16 02:12:24.010: INFO: (15) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 20.714305ms)
Apr 16 02:12:24.010: INFO: (15) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 21.03887ms)
Apr 16 02:12:24.011: INFO: (15) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 21.624417ms)
Apr 16 02:12:24.011: INFO: (15) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 22.115556ms)
Apr 16 02:12:24.012: INFO: (15) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 22.458271ms)
Apr 16 02:12:24.012: INFO: (15) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 22.454635ms)
Apr 16 02:12:24.012: INFO: (15) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 22.257954ms)
Apr 16 02:12:24.012: INFO: (15) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 22.586809ms)
Apr 16 02:12:24.012: INFO: (15) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 22.970691ms)
Apr 16 02:12:24.013: INFO: (15) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 23.779162ms)
Apr 16 02:12:24.013: INFO: (15) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 24.80303ms)
Apr 16 02:12:24.014: INFO: (15) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 24.346405ms)
Apr 16 02:12:24.016: INFO: (15) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 26.719604ms)
Apr 16 02:12:24.017: INFO: (15) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 27.397204ms)
Apr 16 02:12:24.018: INFO: (15) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 28.813893ms)
Apr 16 02:12:24.019: INFO: (15) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 29.907134ms)
Apr 16 02:12:24.037: INFO: (16) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 16.712552ms)
Apr 16 02:12:24.038: INFO: (16) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 18.513282ms)
Apr 16 02:12:24.038: INFO: (16) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 19.143036ms)
Apr 16 02:12:24.039: INFO: (16) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 18.723512ms)
Apr 16 02:12:24.039: INFO: (16) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 18.547386ms)
Apr 16 02:12:24.039: INFO: (16) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 18.870786ms)
Apr 16 02:12:24.039: INFO: (16) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 19.100059ms)
Apr 16 02:12:24.039: INFO: (16) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 19.233875ms)
Apr 16 02:12:24.039: INFO: (16) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 19.205295ms)
Apr 16 02:12:24.039: INFO: (16) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 19.073343ms)
Apr 16 02:12:24.040: INFO: (16) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 20.649353ms)
Apr 16 02:12:24.042: INFO: (16) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 21.673191ms)
Apr 16 02:12:24.042: INFO: (16) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 22.737789ms)
Apr 16 02:12:24.042: INFO: (16) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 22.792824ms)
Apr 16 02:12:24.043: INFO: (16) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 22.981544ms)
Apr 16 02:12:24.054: INFO: (16) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 34.348365ms)
Apr 16 02:12:24.067: INFO: (17) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 12.064554ms)
Apr 16 02:12:24.067: INFO: (17) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 12.788245ms)
Apr 16 02:12:24.068: INFO: (17) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 13.8417ms)
Apr 16 02:12:24.068: INFO: (17) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 13.683379ms)
Apr 16 02:12:24.078: INFO: (17) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 22.9052ms)
Apr 16 02:12:24.078: INFO: (17) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 23.188503ms)
Apr 16 02:12:24.078: INFO: (17) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 23.308332ms)
Apr 16 02:12:24.078: INFO: (17) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 23.234423ms)
Apr 16 02:12:24.079: INFO: (17) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 23.695303ms)
Apr 16 02:12:24.079: INFO: (17) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 24.042796ms)
Apr 16 02:12:24.079: INFO: (17) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 24.532876ms)
Apr 16 02:12:24.079: INFO: (17) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 24.006867ms)
Apr 16 02:12:24.081: INFO: (17) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 25.990192ms)
Apr 16 02:12:24.081: INFO: (17) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 26.384185ms)
Apr 16 02:12:24.082: INFO: (17) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 27.175026ms)
Apr 16 02:12:24.084: INFO: (17) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 30.005879ms)
Apr 16 02:12:24.115: INFO: (18) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 30.435615ms)
Apr 16 02:12:24.123: INFO: (18) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 36.582339ms)
Apr 16 02:12:24.132: INFO: (18) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 45.70017ms)
Apr 16 02:12:24.132: INFO: (18) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 45.944469ms)
Apr 16 02:12:24.144: INFO: (18) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 58.262296ms)
Apr 16 02:12:24.145: INFO: (18) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 58.16325ms)
Apr 16 02:12:24.145: INFO: (18) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 58.554687ms)
Apr 16 02:12:24.145: INFO: (18) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 59.846623ms)
Apr 16 02:12:24.145: INFO: (18) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 59.407749ms)
Apr 16 02:12:24.150: INFO: (18) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 64.374409ms)
Apr 16 02:12:24.152: INFO: (18) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 64.647679ms)
Apr 16 02:12:24.161: INFO: (18) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 75.595549ms)
Apr 16 02:12:24.165: INFO: (18) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 78.008022ms)
Apr 16 02:12:24.165: INFO: (18) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 80.032861ms)
Apr 16 02:12:24.166: INFO: (18) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 79.426704ms)
Apr 16 02:12:24.167: INFO: (18) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 80.219608ms)
Apr 16 02:12:24.186: INFO: (19) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:1080/proxy/rewriteme">... (200; 19.435295ms)
Apr 16 02:12:24.216: INFO: (19) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:460/proxy/: tls baz (200; 49.220399ms)
Apr 16 02:12:24.217: INFO: (19) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:1080/proxy/rewriteme">test<... (200; 49.591219ms)
Apr 16 02:12:24.217: INFO: (19) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:162/proxy/: bar (200; 49.809789ms)
Apr 16 02:12:24.217: INFO: (19) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:162/proxy/: bar (200; 49.707111ms)
Apr 16 02:12:24.217: INFO: (19) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v:160/proxy/: foo (200; 50.05519ms)
Apr 16 02:12:24.217: INFO: (19) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:443/proxy/tlsrewritem... (200; 50.289045ms)
Apr 16 02:12:24.217: INFO: (19) /api/v1/namespaces/proxy-5848/pods/https:proxy-service-hs472-2mg6v:462/proxy/: tls qux (200; 50.100169ms)
Apr 16 02:12:24.217: INFO: (19) /api/v1/namespaces/proxy-5848/pods/http:proxy-service-hs472-2mg6v:160/proxy/: foo (200; 50.03591ms)
Apr 16 02:12:24.217: INFO: (19) /api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/: <a href="/api/v1/namespaces/proxy-5848/pods/proxy-service-hs472-2mg6v/proxy/rewriteme">test</a> (200; 50.499799ms)
Apr 16 02:12:24.221: INFO: (19) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname1/proxy/: tls baz (200; 53.554127ms)
Apr 16 02:12:24.221: INFO: (19) /api/v1/namespaces/proxy-5848/services/https:proxy-service-hs472:tlsportname2/proxy/: tls qux (200; 53.898186ms)
Apr 16 02:12:24.222: INFO: (19) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname2/proxy/: bar (200; 55.41121ms)
Apr 16 02:12:24.222: INFO: (19) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname1/proxy/: foo (200; 55.368103ms)
Apr 16 02:12:24.223: INFO: (19) /api/v1/namespaces/proxy-5848/services/proxy-service-hs472:portname2/proxy/: bar (200; 55.480661ms)
Apr 16 02:12:24.227: INFO: (19) /api/v1/namespaces/proxy-5848/services/http:proxy-service-hs472:portname1/proxy/: foo (200; 60.49692ms)
STEP: deleting ReplicationController proxy-service-hs472 in namespace proxy-5848, will wait for the garbage collector to delete the pods
Apr 16 02:12:24.341: INFO: Deleting ReplicationController proxy-service-hs472 took: 32.68865ms
Apr 16 02:12:24.441: INFO: Terminating ReplicationController proxy-service-hs472 pods took: 100.456936ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:12:35.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5848" for this suite.

• [SLOW TEST:21.868 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":145,"skipped":2476,"failed":0}
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:12:36.005: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2654
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 16 02:12:36.378: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:36.378: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:36.378: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:36.388: INFO: Number of nodes with available pods: 0
Apr 16 02:12:36.388: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:12:37.415: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:37.415: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:37.415: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:37.426: INFO: Number of nodes with available pods: 0
Apr 16 02:12:37.426: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:12:38.404: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:38.404: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:38.404: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:38.412: INFO: Number of nodes with available pods: 1
Apr 16 02:12:38.412: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:12:39.405: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:39.405: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:39.405: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:39.415: INFO: Number of nodes with available pods: 2
Apr 16 02:12:39.415: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Apr 16 02:12:39.475: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:39.475: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:39.476: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:39.489: INFO: Number of nodes with available pods: 1
Apr 16 02:12:39.491: INFO: Node k8s-flwang-4oyd5hkwxglz-node-1 is running more than one daemon pod
Apr 16 02:12:40.510: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:40.510: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:40.510: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:40.521: INFO: Number of nodes with available pods: 1
Apr 16 02:12:40.521: INFO: Node k8s-flwang-4oyd5hkwxglz-node-1 is running more than one daemon pod
Apr 16 02:12:41.512: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:41.512: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:41.512: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:12:41.525: INFO: Number of nodes with available pods: 2
Apr 16 02:12:41.525: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2654, will wait for the garbage collector to delete the pods
Apr 16 02:12:41.628: INFO: Deleting DaemonSet.extensions daemon-set took: 28.580603ms
Apr 16 02:12:42.728: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.100716904s
Apr 16 02:12:55.949: INFO: Number of nodes with available pods: 0
Apr 16 02:12:55.949: INFO: Number of running nodes: 0, number of available pods: 0
Apr 16 02:12:55.962: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"419345"},"items":null}

Apr 16 02:12:55.970: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"419345"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:12:55.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2654" for this suite.

• [SLOW TEST:20.016 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":146,"skipped":2476,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:12:56.023: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8619
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:13:07.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8619" for this suite.

• [SLOW TEST:11.520 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":147,"skipped":2516,"failed":0}
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:13:07.543: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7155
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Apr 16 02:13:07.860: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:13:07.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7155" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":148,"skipped":2516,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:13:07.971: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6353
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:13:10.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6353" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":149,"skipped":2528,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:13:10.544: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5858
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:13:10.797: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Apr 16 02:13:15.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-5858 --namespace=crd-publish-openapi-5858 create -f -'
Apr 16 02:13:16.077: INFO: stderr: ""
Apr 16 02:13:16.077: INFO: stdout: "e2e-test-crd-publish-openapi-5874-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 16 02:13:16.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-5858 --namespace=crd-publish-openapi-5858 delete e2e-test-crd-publish-openapi-5874-crds test-foo'
Apr 16 02:13:16.358: INFO: stderr: ""
Apr 16 02:13:16.358: INFO: stdout: "e2e-test-crd-publish-openapi-5874-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Apr 16 02:13:16.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-5858 --namespace=crd-publish-openapi-5858 apply -f -'
Apr 16 02:13:16.775: INFO: stderr: ""
Apr 16 02:13:16.776: INFO: stdout: "e2e-test-crd-publish-openapi-5874-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 16 02:13:16.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-5858 --namespace=crd-publish-openapi-5858 delete e2e-test-crd-publish-openapi-5874-crds test-foo'
Apr 16 02:13:16.958: INFO: stderr: ""
Apr 16 02:13:16.958: INFO: stdout: "e2e-test-crd-publish-openapi-5874-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Apr 16 02:13:16.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-5858 --namespace=crd-publish-openapi-5858 create -f -'
Apr 16 02:13:17.408: INFO: rc: 1
Apr 16 02:13:17.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-5858 --namespace=crd-publish-openapi-5858 apply -f -'
Apr 16 02:13:17.872: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Apr 16 02:13:17.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-5858 --namespace=crd-publish-openapi-5858 create -f -'
Apr 16 02:13:18.362: INFO: rc: 1
Apr 16 02:13:18.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-5858 --namespace=crd-publish-openapi-5858 apply -f -'
Apr 16 02:13:18.808: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Apr 16 02:13:18.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-5858 explain e2e-test-crd-publish-openapi-5874-crds'
Apr 16 02:13:19.173: INFO: stderr: ""
Apr 16 02:13:19.173: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5874-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Apr 16 02:13:19.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-5858 explain e2e-test-crd-publish-openapi-5874-crds.metadata'
Apr 16 02:13:19.494: INFO: stderr: ""
Apr 16 02:13:19.494: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5874-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Apr 16 02:13:19.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-5858 explain e2e-test-crd-publish-openapi-5874-crds.spec'
Apr 16 02:13:19.981: INFO: stderr: ""
Apr 16 02:13:19.981: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5874-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Apr 16 02:13:19.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-5858 explain e2e-test-crd-publish-openapi-5874-crds.spec.bars'
Apr 16 02:13:20.366: INFO: stderr: ""
Apr 16 02:13:20.366: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5874-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Apr 16 02:13:20.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-5858 explain e2e-test-crd-publish-openapi-5874-crds.spec.bars2'
Apr 16 02:13:20.835: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:13:24.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5858" for this suite.

• [SLOW TEST:14.112 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":150,"skipped":2548,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:13:24.674: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4902
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Apr 16 02:13:28.989: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4902 PodName:var-expansion-cddd01bb-da93-4d8f-87bc-a2b2f5f03e94 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 02:13:28.989: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: test for file in mounted path
Apr 16 02:13:29.200: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4902 PodName:var-expansion-cddd01bb-da93-4d8f-87bc-a2b2f5f03e94 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 02:13:29.200: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: updating the annotation value
Apr 16 02:13:29.963: INFO: Successfully updated pod "var-expansion-cddd01bb-da93-4d8f-87bc-a2b2f5f03e94"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Apr 16 02:13:29.979: INFO: Deleting pod "var-expansion-cddd01bb-da93-4d8f-87bc-a2b2f5f03e94" in namespace "var-expansion-4902"
Apr 16 02:13:30.001: INFO: Wait up to 5m0s for pod "var-expansion-cddd01bb-da93-4d8f-87bc-a2b2f5f03e94" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:14:04.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4902" for this suite.

• [SLOW TEST:39.386 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":151,"skipped":2614,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:14:04.061: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1502
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:14:04.300: INFO: Creating ReplicaSet my-hostname-basic-8686276c-270e-407c-9a56-acda59f30083
Apr 16 02:14:04.327: INFO: Pod name my-hostname-basic-8686276c-270e-407c-9a56-acda59f30083: Found 0 pods out of 1
Apr 16 02:14:09.350: INFO: Pod name my-hostname-basic-8686276c-270e-407c-9a56-acda59f30083: Found 1 pods out of 1
Apr 16 02:14:09.350: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8686276c-270e-407c-9a56-acda59f30083" is running
Apr 16 02:14:09.362: INFO: Pod "my-hostname-basic-8686276c-270e-407c-9a56-acda59f30083-h2hcv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-16 02:14:04 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-16 02:14:06 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-16 02:14:06 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-16 02:14:04 +0000 UTC Reason: Message:}])
Apr 16 02:14:09.364: INFO: Trying to dial the pod
Apr 16 02:14:14.395: INFO: Controller my-hostname-basic-8686276c-270e-407c-9a56-acda59f30083: Got expected result from replica 1 [my-hostname-basic-8686276c-270e-407c-9a56-acda59f30083-h2hcv]: "my-hostname-basic-8686276c-270e-407c-9a56-acda59f30083-h2hcv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:14:14.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1502" for this suite.

• [SLOW TEST:10.362 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":152,"skipped":2620,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:14:14.424: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8958
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-8958
STEP: creating replication controller nodeport-test in namespace services-8958
I0416 02:14:14.738265      25 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-8958, replica count: 2
I0416 02:14:17.789240      25 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 16 02:14:17.789: INFO: Creating new exec pod
Apr 16 02:14:20.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-8958 exec execpod4jxns -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Apr 16 02:14:21.269: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Apr 16 02:14:21.269: INFO: stdout: ""
Apr 16 02:14:21.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-8958 exec execpod4jxns -- /bin/sh -x -c nc -zv -t -w 2 10.254.55.47 80'
Apr 16 02:14:21.620: INFO: stderr: "+ nc -zv -t -w 2 10.254.55.47 80\nConnection to 10.254.55.47 80 port [tcp/http] succeeded!\n"
Apr 16 02:14:21.620: INFO: stdout: ""
Apr 16 02:14:21.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-8958 exec execpod4jxns -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.13 30187'
Apr 16 02:14:21.964: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.13 30187\nConnection to 10.0.0.13 30187 port [tcp/30187] succeeded!\n"
Apr 16 02:14:21.964: INFO: stdout: ""
Apr 16 02:14:21.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-8958 exec execpod4jxns -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.68 30187'
Apr 16 02:14:22.303: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.68 30187\nConnection to 10.0.0.68 30187 port [tcp/30187] succeeded!\n"
Apr 16 02:14:22.303: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:14:22.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8958" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:7.920 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":153,"skipped":2695,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:14:22.345: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7539
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 16 02:14:23.245: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 16 02:14:25.278: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754136063, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754136063, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754136063, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754136063, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 02:14:28.335: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:14:28.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7539" for this suite.
STEP: Destroying namespace "webhook-7539-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.486 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":154,"skipped":2733,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:14:28.833: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7985
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-7985
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 16 02:14:29.079: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 16 02:14:29.209: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 16 02:14:31.227: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:14:33.220: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:14:35.226: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:14:37.224: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:14:39.225: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:14:41.225: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:14:43.309: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:14:45.224: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:14:47.223: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:14:49.227: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 16 02:14:49.245: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Apr 16 02:14:53.413: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Apr 16 02:14:53.413: INFO: Going to poll 10.100.232.224 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Apr 16 02:14:53.419: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.232.224 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7985 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 02:14:53.420: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 02:14:54.645: INFO: Found all 1 expected endpoints: [netserver-0]
Apr 16 02:14:54.645: INFO: Going to poll 10.100.97.112 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Apr 16 02:14:54.661: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.97.112 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7985 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 02:14:54.661: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 02:14:55.856: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:14:55.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7985" for this suite.

• [SLOW TEST:27.082 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":155,"skipped":2741,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:14:55.915: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6194
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 16 02:14:56.303: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:14:56.303: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:14:56.304: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:14:56.318: INFO: Number of nodes with available pods: 0
Apr 16 02:14:56.318: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:14:57.333: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:14:57.333: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:14:57.333: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:14:57.341: INFO: Number of nodes with available pods: 0
Apr 16 02:14:57.341: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:14:58.333: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:14:58.333: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:14:58.333: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:14:58.342: INFO: Number of nodes with available pods: 1
Apr 16 02:14:58.342: INFO: Node k8s-flwang-4oyd5hkwxglz-node-1 is running more than one daemon pod
Apr 16 02:14:59.337: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:14:59.337: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:14:59.337: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:14:59.348: INFO: Number of nodes with available pods: 2
Apr 16 02:14:59.348: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Apr 16 02:14:59.403: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:14:59.404: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:14:59.404: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:14:59.414: INFO: Number of nodes with available pods: 1
Apr 16 02:14:59.414: INFO: Node k8s-flwang-4oyd5hkwxglz-node-1 is running more than one daemon pod
Apr 16 02:15:00.430: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:00.431: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:00.432: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:00.440: INFO: Number of nodes with available pods: 1
Apr 16 02:15:00.440: INFO: Node k8s-flwang-4oyd5hkwxglz-node-1 is running more than one daemon pod
Apr 16 02:15:01.437: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:01.437: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:01.437: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:01.447: INFO: Number of nodes with available pods: 1
Apr 16 02:15:01.447: INFO: Node k8s-flwang-4oyd5hkwxglz-node-1 is running more than one daemon pod
Apr 16 02:15:02.427: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:02.427: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:02.427: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:02.435: INFO: Number of nodes with available pods: 1
Apr 16 02:15:02.435: INFO: Node k8s-flwang-4oyd5hkwxglz-node-1 is running more than one daemon pod
Apr 16 02:15:03.449: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:03.450: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:03.450: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:03.459: INFO: Number of nodes with available pods: 1
Apr 16 02:15:03.459: INFO: Node k8s-flwang-4oyd5hkwxglz-node-1 is running more than one daemon pod
Apr 16 02:15:04.432: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:04.432: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:04.432: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:04.445: INFO: Number of nodes with available pods: 1
Apr 16 02:15:04.445: INFO: Node k8s-flwang-4oyd5hkwxglz-node-1 is running more than one daemon pod
Apr 16 02:15:05.428: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:05.428: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:05.428: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:05.435: INFO: Number of nodes with available pods: 1
Apr 16 02:15:05.435: INFO: Node k8s-flwang-4oyd5hkwxglz-node-1 is running more than one daemon pod
Apr 16 02:15:06.429: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:06.429: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:06.429: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:06.438: INFO: Number of nodes with available pods: 1
Apr 16 02:15:06.438: INFO: Node k8s-flwang-4oyd5hkwxglz-node-1 is running more than one daemon pod
Apr 16 02:15:07.430: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:07.430: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:07.430: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:07.439: INFO: Number of nodes with available pods: 1
Apr 16 02:15:07.439: INFO: Node k8s-flwang-4oyd5hkwxglz-node-1 is running more than one daemon pod
Apr 16 02:15:08.434: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:08.434: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:08.434: INFO: DaemonSet pods can't tolerate node k8s-flwang-4oyd5hkwxglz-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 16 02:15:08.443: INFO: Number of nodes with available pods: 2
Apr 16 02:15:08.443: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6194, will wait for the garbage collector to delete the pods
Apr 16 02:15:08.536: INFO: Deleting DaemonSet.extensions daemon-set took: 24.771315ms
Apr 16 02:15:09.637: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.100399838s
Apr 16 02:15:15.858: INFO: Number of nodes with available pods: 0
Apr 16 02:15:15.858: INFO: Number of running nodes: 0, number of available pods: 0
Apr 16 02:15:15.865: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"420276"},"items":null}

Apr 16 02:15:15.872: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"420276"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:15:15.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6194" for this suite.

• [SLOW TEST:20.019 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":156,"skipped":2761,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:15:15.936: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4576
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4576
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Apr 16 02:15:16.302: INFO: Found 0 stateful pods, waiting for 3
Apr 16 02:15:26.326: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 16 02:15:26.327: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 16 02:15:26.327: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr 16 02:15:26.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-4576 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 16 02:15:26.744: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 16 02:15:26.744: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 16 02:15:26.744: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Apr 16 02:15:36.848: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Apr 16 02:15:46.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-4576 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 16 02:15:47.381: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 16 02:15:47.381: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 16 02:15:47.381: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 16 02:15:57.445: INFO: Waiting for StatefulSet statefulset-4576/ss2 to complete update
Apr 16 02:15:57.445: INFO: Waiting for Pod statefulset-4576/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 16 02:15:57.445: INFO: Waiting for Pod statefulset-4576/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 16 02:16:07.474: INFO: Waiting for StatefulSet statefulset-4576/ss2 to complete update
Apr 16 02:16:07.474: INFO: Waiting for Pod statefulset-4576/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 16 02:16:07.474: INFO: Waiting for Pod statefulset-4576/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 16 02:16:17.474: INFO: Waiting for StatefulSet statefulset-4576/ss2 to complete update
Apr 16 02:16:17.474: INFO: Waiting for Pod statefulset-4576/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 16 02:16:17.474: INFO: Waiting for Pod statefulset-4576/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 16 02:16:27.482: INFO: Waiting for StatefulSet statefulset-4576/ss2 to complete update
Apr 16 02:16:27.483: INFO: Waiting for Pod statefulset-4576/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 16 02:16:37.473: INFO: Waiting for StatefulSet statefulset-4576/ss2 to complete update
Apr 16 02:16:37.474: INFO: Waiting for Pod statefulset-4576/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 16 02:16:47.473: INFO: Waiting for StatefulSet statefulset-4576/ss2 to complete update
Apr 16 02:16:47.474: INFO: Waiting for Pod statefulset-4576/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 16 02:16:57.474: INFO: Waiting for StatefulSet statefulset-4576/ss2 to complete update
STEP: Rolling back to a previous revision
Apr 16 02:17:07.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-4576 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 16 02:17:07.863: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 16 02:17:07.863: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 16 02:17:07.863: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 16 02:17:17.945: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Apr 16 02:17:28.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-4576 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 16 02:17:28.387: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 16 02:17:28.387: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 16 02:17:28.387: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 16 02:17:58.454: INFO: Waiting for StatefulSet statefulset-4576/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 16 02:18:08.484: INFO: Deleting all statefulset in ns statefulset-4576
Apr 16 02:18:08.493: INFO: Scaling statefulset ss2 to 0
Apr 16 02:18:28.551: INFO: Waiting for statefulset status.replicas updated to 0
Apr 16 02:18:28.558: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:18:28.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4576" for this suite.

• [SLOW TEST:192.698 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":157,"skipped":2783,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:18:28.637: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3171
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-7635d0c1-1e58-4808-a6fa-8ddda3106b5e
STEP: Creating a pod to test consume secrets
Apr 16 02:18:28.912: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-86d8ff09-ae4b-4ea7-be80-fa12d2ba7c2d" in namespace "projected-3171" to be "Succeeded or Failed"
Apr 16 02:18:28.921: INFO: Pod "pod-projected-secrets-86d8ff09-ae4b-4ea7-be80-fa12d2ba7c2d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.154779ms
Apr 16 02:18:30.936: INFO: Pod "pod-projected-secrets-86d8ff09-ae4b-4ea7-be80-fa12d2ba7c2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023573557s
Apr 16 02:18:32.952: INFO: Pod "pod-projected-secrets-86d8ff09-ae4b-4ea7-be80-fa12d2ba7c2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040057041s
STEP: Saw pod success
Apr 16 02:18:32.952: INFO: Pod "pod-projected-secrets-86d8ff09-ae4b-4ea7-be80-fa12d2ba7c2d" satisfied condition "Succeeded or Failed"
Apr 16 02:18:32.963: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-projected-secrets-86d8ff09-ae4b-4ea7-be80-fa12d2ba7c2d container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 16 02:18:33.147: INFO: Waiting for pod pod-projected-secrets-86d8ff09-ae4b-4ea7-be80-fa12d2ba7c2d to disappear
Apr 16 02:18:33.158: INFO: Pod pod-projected-secrets-86d8ff09-ae4b-4ea7-be80-fa12d2ba7c2d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:18:33.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3171" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":158,"skipped":2810,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:18:33.193: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1680
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 16 02:18:33.476: INFO: Waiting up to 5m0s for pod "pod-8ec6d7c1-ff20-4364-acdf-5b51b7eda208" in namespace "emptydir-1680" to be "Succeeded or Failed"
Apr 16 02:18:33.499: INFO: Pod "pod-8ec6d7c1-ff20-4364-acdf-5b51b7eda208": Phase="Pending", Reason="", readiness=false. Elapsed: 22.371772ms
Apr 16 02:18:35.518: INFO: Pod "pod-8ec6d7c1-ff20-4364-acdf-5b51b7eda208": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041879735s
STEP: Saw pod success
Apr 16 02:18:35.518: INFO: Pod "pod-8ec6d7c1-ff20-4364-acdf-5b51b7eda208" satisfied condition "Succeeded or Failed"
Apr 16 02:18:35.535: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-8ec6d7c1-ff20-4364-acdf-5b51b7eda208 container test-container: <nil>
STEP: delete the pod
Apr 16 02:18:35.615: INFO: Waiting for pod pod-8ec6d7c1-ff20-4364-acdf-5b51b7eda208 to disappear
Apr 16 02:18:35.625: INFO: Pod pod-8ec6d7c1-ff20-4364-acdf-5b51b7eda208 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:18:35.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1680" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":159,"skipped":2836,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:18:35.660: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5152
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Apr 16 02:18:35.913: INFO: created test-event-1
Apr 16 02:18:35.924: INFO: created test-event-2
Apr 16 02:18:35.936: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Apr 16 02:18:35.951: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Apr 16 02:18:36.019: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:18:36.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5152" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":160,"skipped":2871,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:18:36.067: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6385
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Apr 16 02:18:36.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-6385 create -f -'
Apr 16 02:18:36.992: INFO: stderr: ""
Apr 16 02:18:36.992: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 16 02:18:38.009: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 16 02:18:38.009: INFO: Found 0 / 1
Apr 16 02:18:39.005: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 16 02:18:39.005: INFO: Found 0 / 1
Apr 16 02:18:40.008: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 16 02:18:40.008: INFO: Found 1 / 1
Apr 16 02:18:40.008: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Apr 16 02:18:40.018: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 16 02:18:40.018: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 16 02:18:40.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-6385 patch pod agnhost-primary-grnvt -p {"metadata":{"annotations":{"x":"y"}}}'
Apr 16 02:18:40.229: INFO: stderr: ""
Apr 16 02:18:40.229: INFO: stdout: "pod/agnhost-primary-grnvt patched\n"
STEP: checking annotations
Apr 16 02:18:40.239: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 16 02:18:40.239: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:18:40.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6385" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":161,"skipped":2871,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:18:40.280: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9440
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-88c9
STEP: Creating a pod to test atomic-volume-subpath
Apr 16 02:18:40.607: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-88c9" in namespace "subpath-9440" to be "Succeeded or Failed"
Apr 16 02:18:40.630: INFO: Pod "pod-subpath-test-configmap-88c9": Phase="Pending", Reason="", readiness=false. Elapsed: 22.118049ms
Apr 16 02:18:42.661: INFO: Pod "pod-subpath-test-configmap-88c9": Phase="Running", Reason="", readiness=true. Elapsed: 2.052966079s
Apr 16 02:18:44.678: INFO: Pod "pod-subpath-test-configmap-88c9": Phase="Running", Reason="", readiness=true. Elapsed: 4.070525101s
Apr 16 02:18:46.693: INFO: Pod "pod-subpath-test-configmap-88c9": Phase="Running", Reason="", readiness=true. Elapsed: 6.085370317s
Apr 16 02:18:48.710: INFO: Pod "pod-subpath-test-configmap-88c9": Phase="Running", Reason="", readiness=true. Elapsed: 8.102235744s
Apr 16 02:18:50.730: INFO: Pod "pod-subpath-test-configmap-88c9": Phase="Running", Reason="", readiness=true. Elapsed: 10.121983415s
Apr 16 02:18:52.746: INFO: Pod "pod-subpath-test-configmap-88c9": Phase="Running", Reason="", readiness=true. Elapsed: 12.138116834s
Apr 16 02:18:54.764: INFO: Pod "pod-subpath-test-configmap-88c9": Phase="Running", Reason="", readiness=true. Elapsed: 14.156708783s
Apr 16 02:18:56.783: INFO: Pod "pod-subpath-test-configmap-88c9": Phase="Running", Reason="", readiness=true. Elapsed: 16.175628689s
Apr 16 02:18:58.801: INFO: Pod "pod-subpath-test-configmap-88c9": Phase="Running", Reason="", readiness=true. Elapsed: 18.193942719s
Apr 16 02:19:00.824: INFO: Pod "pod-subpath-test-configmap-88c9": Phase="Running", Reason="", readiness=true. Elapsed: 20.216450795s
Apr 16 02:19:02.839: INFO: Pod "pod-subpath-test-configmap-88c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.231472389s
STEP: Saw pod success
Apr 16 02:19:02.839: INFO: Pod "pod-subpath-test-configmap-88c9" satisfied condition "Succeeded or Failed"
Apr 16 02:19:02.849: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-subpath-test-configmap-88c9 container test-container-subpath-configmap-88c9: <nil>
STEP: delete the pod
Apr 16 02:19:03.150: INFO: Waiting for pod pod-subpath-test-configmap-88c9 to disappear
Apr 16 02:19:03.164: INFO: Pod pod-subpath-test-configmap-88c9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-88c9
Apr 16 02:19:03.164: INFO: Deleting pod "pod-subpath-test-configmap-88c9" in namespace "subpath-9440"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:19:03.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9440" for this suite.

• [SLOW TEST:22.932 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":162,"skipped":2892,"failed":0}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:19:03.211: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1232
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1232
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-1232
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1232
Apr 16 02:19:03.512: INFO: Found 0 stateful pods, waiting for 1
Apr 16 02:19:13.548: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Apr 16 02:19:13.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-1232 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 16 02:19:13.927: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 16 02:19:13.927: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 16 02:19:13.927: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 16 02:19:13.939: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 16 02:19:23.974: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 16 02:19:23.975: INFO: Waiting for statefulset status.replicas updated to 0
Apr 16 02:19:24.018: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 16 02:19:24.018: INFO: ss-0  k8s-flwang-4oyd5hkwxglz-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:03 +0000 UTC  }]
Apr 16 02:19:24.018: INFO: 
Apr 16 02:19:24.018: INFO: StatefulSet ss has not reached scale 3, at 1
Apr 16 02:19:25.031: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991263788s
Apr 16 02:19:26.043: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.978419623s
Apr 16 02:19:27.063: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.966140175s
Apr 16 02:19:28.081: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.946530094s
Apr 16 02:19:29.097: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.92828284s
Apr 16 02:19:30.114: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.912172368s
Apr 16 02:19:31.126: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.895845123s
Apr 16 02:19:32.139: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.883706771s
Apr 16 02:19:33.154: INFO: Verifying statefulset ss doesn't scale past 3 for another 870.424654ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1232
Apr 16 02:19:34.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-1232 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 16 02:19:34.536: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 16 02:19:34.536: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 16 02:19:34.536: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 16 02:19:34.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-1232 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 16 02:19:34.918: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 16 02:19:34.918: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 16 02:19:34.918: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 16 02:19:34.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-1232 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 16 02:19:35.331: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 16 02:19:35.331: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 16 02:19:35.331: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 16 02:19:35.347: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 16 02:19:35.347: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 16 02:19:35.347: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Apr 16 02:19:35.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-1232 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 16 02:19:35.742: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 16 02:19:35.742: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 16 02:19:35.742: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 16 02:19:35.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-1232 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 16 02:19:36.178: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 16 02:19:36.178: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 16 02:19:36.178: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 16 02:19:36.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-1232 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 16 02:19:36.535: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 16 02:19:36.535: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 16 02:19:36.535: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 16 02:19:36.535: INFO: Waiting for statefulset status.replicas updated to 0
Apr 16 02:19:36.549: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr 16 02:19:46.567: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 16 02:19:46.567: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 16 02:19:46.567: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 16 02:19:46.602: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 16 02:19:46.602: INFO: ss-0  k8s-flwang-4oyd5hkwxglz-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:03 +0000 UTC  }]
Apr 16 02:19:46.602: INFO: ss-1  k8s-flwang-4oyd5hkwxglz-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:46.602: INFO: ss-2  k8s-flwang-4oyd5hkwxglz-node-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:46.603: INFO: 
Apr 16 02:19:46.603: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 16 02:19:47.622: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 16 02:19:47.622: INFO: ss-0  k8s-flwang-4oyd5hkwxglz-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:03 +0000 UTC  }]
Apr 16 02:19:47.622: INFO: ss-1  k8s-flwang-4oyd5hkwxglz-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:47.622: INFO: ss-2  k8s-flwang-4oyd5hkwxglz-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:47.622: INFO: 
Apr 16 02:19:47.622: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 16 02:19:48.637: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 16 02:19:48.637: INFO: ss-1  k8s-flwang-4oyd5hkwxglz-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:48.637: INFO: ss-2  k8s-flwang-4oyd5hkwxglz-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:48.637: INFO: 
Apr 16 02:19:48.637: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 16 02:19:49.651: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 16 02:19:49.651: INFO: ss-1  k8s-flwang-4oyd5hkwxglz-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:49.651: INFO: ss-2  k8s-flwang-4oyd5hkwxglz-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:49.651: INFO: 
Apr 16 02:19:49.651: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 16 02:19:50.665: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 16 02:19:50.665: INFO: ss-1  k8s-flwang-4oyd5hkwxglz-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:50.665: INFO: ss-2  k8s-flwang-4oyd5hkwxglz-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:50.665: INFO: 
Apr 16 02:19:50.666: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 16 02:19:51.679: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 16 02:19:51.679: INFO: ss-1  k8s-flwang-4oyd5hkwxglz-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:51.679: INFO: ss-2  k8s-flwang-4oyd5hkwxglz-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:51.679: INFO: 
Apr 16 02:19:51.679: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 16 02:19:52.691: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 16 02:19:52.691: INFO: ss-1  k8s-flwang-4oyd5hkwxglz-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:52.692: INFO: ss-2  k8s-flwang-4oyd5hkwxglz-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:52.692: INFO: 
Apr 16 02:19:52.692: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 16 02:19:53.705: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 16 02:19:53.705: INFO: ss-1  k8s-flwang-4oyd5hkwxglz-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:53.705: INFO: ss-2  k8s-flwang-4oyd5hkwxglz-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:53.705: INFO: 
Apr 16 02:19:53.705: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 16 02:19:54.720: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 16 02:19:54.720: INFO: ss-1  k8s-flwang-4oyd5hkwxglz-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:54.720: INFO: 
Apr 16 02:19:54.720: INFO: StatefulSet ss has not reached scale 0, at 1
Apr 16 02:19:55.737: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Apr 16 02:19:55.737: INFO: ss-1  k8s-flwang-4oyd5hkwxglz-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-16 02:19:24 +0000 UTC  }]
Apr 16 02:19:55.737: INFO: 
Apr 16 02:19:55.737: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1232
Apr 16 02:19:56.750: INFO: Scaling statefulset ss to 0
Apr 16 02:19:56.782: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 16 02:19:56.791: INFO: Deleting all statefulset in ns statefulset-1232
Apr 16 02:19:56.801: INFO: Scaling statefulset ss to 0
Apr 16 02:19:56.823: INFO: Waiting for statefulset status.replicas updated to 0
Apr 16 02:19:56.829: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:19:56.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1232" for this suite.

• [SLOW TEST:53.697 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":163,"skipped":2892,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:19:56.919: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1608
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-dcc77e87-aa13-4fca-a3b3-37036feb8303
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-dcc77e87-aa13-4fca-a3b3-37036feb8303
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:21:11.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1608" for this suite.

• [SLOW TEST:75.071 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":164,"skipped":2905,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:21:11.995: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6873
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Apr 16 02:21:12.296: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6873  17958597-4ad2-450c-8d23-1771bd375c9a 422168 0 2021-04-16 02:21:12 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-04-16 02:21:12 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7h6dq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7h6dq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7h6dq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 16 02:21:12.323: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Apr 16 02:21:14.338: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Apr 16 02:21:16.339: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Apr 16 02:21:16.339: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6873 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 02:21:16.339: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Verifying customized DNS server is configured on pod...
Apr 16 02:21:16.629: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6873 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 02:21:16.629: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 02:21:16.839: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:21:16.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6873" for this suite.

• [SLOW TEST:5.031 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":165,"skipped":2930,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:21:17.029: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6671
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6671
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6671
STEP: Creating statefulset with conflicting port in namespace statefulset-6671
STEP: Waiting until pod test-pod will start running in namespace statefulset-6671
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6671
Apr 16 02:21:21.441: INFO: Observed stateful pod in namespace: statefulset-6671, name: ss-0, uid: 61a03d09-3292-47d2-92ce-4cae01c7c4dd, status phase: Pending. Waiting for statefulset controller to delete.
Apr 16 02:21:21.982: INFO: Observed stateful pod in namespace: statefulset-6671, name: ss-0, uid: 61a03d09-3292-47d2-92ce-4cae01c7c4dd, status phase: Failed. Waiting for statefulset controller to delete.
Apr 16 02:21:22.004: INFO: Observed stateful pod in namespace: statefulset-6671, name: ss-0, uid: 61a03d09-3292-47d2-92ce-4cae01c7c4dd, status phase: Failed. Waiting for statefulset controller to delete.
Apr 16 02:21:22.023: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6671
STEP: Removing pod with conflicting port in namespace statefulset-6671
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6671 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 16 02:21:26.120: INFO: Deleting all statefulset in ns statefulset-6671
Apr 16 02:21:26.134: INFO: Scaling statefulset ss to 0
Apr 16 02:21:36.220: INFO: Waiting for statefulset status.replicas updated to 0
Apr 16 02:21:36.237: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:21:36.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6671" for this suite.

• [SLOW TEST:19.300 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":166,"skipped":2934,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:21:36.337: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-25
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-91d90ecf-6928-43bf-80d9-25c07ee5eb8b
STEP: Creating a pod to test consume secrets
Apr 16 02:21:36.611: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-47dabd60-adb8-4753-8ca0-6392acb5cfff" in namespace "projected-25" to be "Succeeded or Failed"
Apr 16 02:21:36.632: INFO: Pod "pod-projected-secrets-47dabd60-adb8-4753-8ca0-6392acb5cfff": Phase="Pending", Reason="", readiness=false. Elapsed: 21.105706ms
Apr 16 02:21:38.650: INFO: Pod "pod-projected-secrets-47dabd60-adb8-4753-8ca0-6392acb5cfff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038972081s
Apr 16 02:21:40.667: INFO: Pod "pod-projected-secrets-47dabd60-adb8-4753-8ca0-6392acb5cfff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056192501s
STEP: Saw pod success
Apr 16 02:21:40.667: INFO: Pod "pod-projected-secrets-47dabd60-adb8-4753-8ca0-6392acb5cfff" satisfied condition "Succeeded or Failed"
Apr 16 02:21:40.676: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-projected-secrets-47dabd60-adb8-4753-8ca0-6392acb5cfff container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 16 02:21:40.732: INFO: Waiting for pod pod-projected-secrets-47dabd60-adb8-4753-8ca0-6392acb5cfff to disappear
Apr 16 02:21:40.743: INFO: Pod pod-projected-secrets-47dabd60-adb8-4753-8ca0-6392acb5cfff no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:21:40.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-25" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":167,"skipped":2950,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:21:40.772: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-3131
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 16 02:21:41.046: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 16 02:22:41.134: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Apr 16 02:22:41.211: INFO: Created pod: pod0-sched-preemption-low-priority
Apr 16 02:22:41.265: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:22:57.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3131" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:76.746 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":168,"skipped":2968,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:22:57.523: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6716
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6716.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6716.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6716.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6716.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6716.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6716.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6716.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6716.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6716.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6716.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6716.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 14.159.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.159.14_udp@PTR;check="$$(dig +tcp +noall +answer +search 14.159.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.159.14_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6716.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6716.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6716.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6716.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6716.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6716.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6716.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6716.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6716.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6716.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6716.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 14.159.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.159.14_udp@PTR;check="$$(dig +tcp +noall +answer +search 14.159.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.159.14_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 16 02:23:00.027: INFO: Unable to read wheezy_udp@dns-test-service.dns-6716.svc.cluster.local from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:00.040: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6716.svc.cluster.local from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:00.054: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:00.064: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:00.099: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:00.125: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:00.177: INFO: Unable to read jessie_udp@dns-test-service.dns-6716.svc.cluster.local from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:00.188: INFO: Unable to read jessie_tcp@dns-test-service.dns-6716.svc.cluster.local from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:00.203: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:00.215: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:00.263: INFO: Unable to read jessie_udp@PodARecord from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:00.272: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:00.288: INFO: Lookups using dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e failed for: [wheezy_udp@dns-test-service.dns-6716.svc.cluster.local wheezy_tcp@dns-test-service.dns-6716.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-6716.svc.cluster.local jessie_tcp@dns-test-service.dns-6716.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Apr 16 02:23:05.303: INFO: Unable to read wheezy_udp@dns-test-service.dns-6716.svc.cluster.local from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:05.314: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6716.svc.cluster.local from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:05.322: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:05.331: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:05.367: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:05.378: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:05.427: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local from pod dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e: the server could not find the requested resource (get pods dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e)
Apr 16 02:23:05.498: INFO: Lookups using dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e failed for: [wheezy_udp@dns-test-service.dns-6716.svc.cluster.local wheezy_tcp@dns-test-service.dns-6716.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@_http._tcp.dns-test-service.dns-6716.svc.cluster.local]

Apr 16 02:23:10.509: INFO: DNS probes using dns-6716/dns-test-b7906367-ba68-474d-8a4f-9a79c2613f9e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:23:10.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6716" for this suite.

• [SLOW TEST:13.234 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":169,"skipped":2974,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:23:10.758: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5267
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 16 02:23:11.800: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 16 02:23:13.840: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754136591, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754136591, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754136591, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754136591, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 02:23:16.898: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:23:17.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5267" for this suite.
STEP: Destroying namespace "webhook-5267-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.513 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":170,"skipped":2981,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:23:17.271: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4861
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 16 02:23:21.687: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 16 02:23:21.701: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 16 02:23:23.701: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 16 02:23:23.720: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 16 02:23:25.701: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 16 02:23:25.720: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 16 02:23:27.701: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 16 02:23:27.715: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 16 02:23:29.701: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 16 02:23:29.717: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 16 02:23:31.701: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 16 02:23:31.716: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 16 02:23:33.701: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 16 02:23:33.716: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 16 02:23:35.701: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 16 02:23:35.716: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 16 02:23:37.701: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 16 02:23:37.719: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:23:37.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4861" for this suite.

• [SLOW TEST:20.657 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":171,"skipped":3000,"failed":0}
S
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:23:37.930: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4044
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-bca7da08-6b7b-4dd7-9bd6-180f8c3aa564
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:23:38.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4044" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":172,"skipped":3001,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:23:38.310: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8182
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 16 02:23:38.542: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 16 02:23:38.568: INFO: Waiting for terminating namespaces to be deleted...
Apr 16 02:23:38.578: INFO: 
Logging pods the apiserver thinks is on node k8s-flwang-4oyd5hkwxglz-node-0 before test
Apr 16 02:23:38.600: INFO: alertmanager-magnum-prometheus-operator-alertmanager-0 from kube-system started at 2021-04-14 03:28:17 +0000 UTC (2 container statuses recorded)
Apr 16 02:23:38.600: INFO: 	Container alertmanager ready: true, restart count 0
Apr 16 02:23:38.600: INFO: 	Container config-reloader ready: true, restart count 0
Apr 16 02:23:38.600: INFO: calico-node-8pzl7 from kube-system started at 2021-04-14 03:25:24 +0000 UTC (1 container statuses recorded)
Apr 16 02:23:38.600: INFO: 	Container calico-node ready: true, restart count 0
Apr 16 02:23:38.601: INFO: csi-cinder-nodeplugin-xngdr from kube-system started at 2021-04-14 03:25:44 +0000 UTC (2 container statuses recorded)
Apr 16 02:23:38.601: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 16 02:23:38.601: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 16 02:23:38.601: INFO: kube-dns-autoscaler-7d66dbddbc-mmpzq from kube-system started at 2021-04-14 03:25:55 +0000 UTC (1 container statuses recorded)
Apr 16 02:23:38.601: INFO: 	Container autoscaler ready: true, restart count 0
Apr 16 02:23:38.601: INFO: magnum-grafana-57b59f6cd-lmchw from kube-system started at 2021-04-14 03:27:41 +0000 UTC (2 container statuses recorded)
Apr 16 02:23:38.602: INFO: 	Container grafana ready: true, restart count 0
Apr 16 02:23:38.602: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Apr 16 02:23:38.602: INFO: magnum-kube-state-metrics-7f877f89d8-9685l from kube-system started at 2021-04-14 03:27:41 +0000 UTC (1 container statuses recorded)
Apr 16 02:23:38.602: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 16 02:23:38.602: INFO: magnum-metrics-server-6b85d69787-r9lnz from kube-system started at 2021-04-14 03:27:41 +0000 UTC (1 container statuses recorded)
Apr 16 02:23:38.602: INFO: 	Container metrics-server ready: true, restart count 0
Apr 16 02:23:38.602: INFO: magnum-prometheus-node-exporter-zs5ln from kube-system started at 2021-04-14 03:27:41 +0000 UTC (1 container statuses recorded)
Apr 16 02:23:38.602: INFO: 	Container node-exporter ready: true, restart count 0
Apr 16 02:23:38.603: INFO: magnum-prometheus-operator-operator-6b8b498699-pwg7j from kube-system started at 2021-04-14 03:27:41 +0000 UTC (2 container statuses recorded)
Apr 16 02:23:38.603: INFO: 	Container prometheus-operator ready: true, restart count 0
Apr 16 02:23:38.614: INFO: 	Container tls-proxy ready: true, restart count 0
Apr 16 02:23:38.615: INFO: npd-nxkwt from kube-system started at 2021-04-14 03:25:44 +0000 UTC (1 container statuses recorded)
Apr 16 02:23:38.615: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr 16 02:23:38.615: INFO: prometheus-magnum-prometheus-operator-prometheus-0 from kube-system started at 2021-04-14 03:28:28 +0000 UTC (3 container statuses recorded)
Apr 16 02:23:38.615: INFO: 	Container prometheus ready: true, restart count 1
Apr 16 02:23:38.615: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Apr 16 02:23:38.615: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Apr 16 02:23:38.615: INFO: sonobuoy-systemd-logs-daemon-set-d0994422a63f4009-7q9k8 from sonobuoy started at 2021-04-16 01:31:44 +0000 UTC (2 container statuses recorded)
Apr 16 02:23:38.615: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 16 02:23:38.615: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 16 02:23:38.615: INFO: busybox from test-k8s started at 2021-04-16 01:17:41 +0000 UTC (1 container statuses recorded)
Apr 16 02:23:38.615: INFO: 	Container busybox ready: true, restart count 1
Apr 16 02:23:38.615: INFO: nginx-7848d4b86f-4q96s from test-k8s started at 2021-04-16 01:16:17 +0000 UTC (1 container statuses recorded)
Apr 16 02:23:38.615: INFO: 	Container nginx ready: true, restart count 0
Apr 16 02:23:38.615: INFO: nginx-7848d4b86f-bmzz7 from test-k8s started at 2021-04-16 01:16:17 +0000 UTC (1 container statuses recorded)
Apr 16 02:23:38.615: INFO: 	Container nginx ready: true, restart count 0
Apr 16 02:23:38.615: INFO: 
Logging pods the apiserver thinks is on node k8s-flwang-4oyd5hkwxglz-node-1 before test
Apr 16 02:23:38.632: INFO: pod-handle-http-request from container-lifecycle-hook-4861 started at 2021-04-16 02:23:17 +0000 UTC (1 container statuses recorded)
Apr 16 02:23:38.632: INFO: 	Container agnhost-container ready: true, restart count 0
Apr 16 02:23:38.632: INFO: calico-node-nvrm8 from kube-system started at 2021-04-16 01:27:46 +0000 UTC (1 container statuses recorded)
Apr 16 02:23:38.632: INFO: 	Container calico-node ready: true, restart count 0
Apr 16 02:23:38.632: INFO: csi-cinder-nodeplugin-md46f from kube-system started at 2021-04-16 01:44:02 +0000 UTC (2 container statuses recorded)
Apr 16 02:23:38.632: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 16 02:23:38.632: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 16 02:23:38.632: INFO: magnum-prometheus-node-exporter-fcbqk from kube-system started at 2021-04-16 01:44:02 +0000 UTC (1 container statuses recorded)
Apr 16 02:23:38.632: INFO: 	Container node-exporter ready: true, restart count 0
Apr 16 02:23:38.632: INFO: npd-d29zk from kube-system started at 2021-04-16 01:28:16 +0000 UTC (1 container statuses recorded)
Apr 16 02:23:38.632: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr 16 02:23:38.632: INFO: sonobuoy from sonobuoy started at 2021-04-16 01:31:30 +0000 UTC (1 container statuses recorded)
Apr 16 02:23:38.632: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 16 02:23:38.632: INFO: sonobuoy-e2e-job-158181182f3b487c from sonobuoy started at 2021-04-16 01:31:44 +0000 UTC (2 container statuses recorded)
Apr 16 02:23:38.632: INFO: 	Container e2e ready: true, restart count 0
Apr 16 02:23:38.632: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 16 02:23:38.632: INFO: sonobuoy-systemd-logs-daemon-set-d0994422a63f4009-55xbs from sonobuoy started at 2021-04-16 01:31:44 +0000 UTC (2 container statuses recorded)
Apr 16 02:23:38.632: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 16 02:23:38.632: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-3469a76d-6d47-4c97-bbff-2c68574039c0 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.0.68 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-3469a76d-6d47-4c97-bbff-2c68574039c0 off the node k8s-flwang-4oyd5hkwxglz-node-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3469a76d-6d47-4c97-bbff-2c68574039c0
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:28:45.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8182" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:306.753 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":173,"skipped":3017,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:28:45.068: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1705
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-1705
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1705 to expose endpoints map[]
Apr 16 02:28:45.396: INFO: successfully validated that service endpoint-test2 in namespace services-1705 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1705
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1705 to expose endpoints map[pod1:[80]]
Apr 16 02:28:47.501: INFO: successfully validated that service endpoint-test2 in namespace services-1705 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-1705
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1705 to expose endpoints map[pod1:[80] pod2:[80]]
Apr 16 02:28:49.584: INFO: successfully validated that service endpoint-test2 in namespace services-1705 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-1705
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1705 to expose endpoints map[pod2:[80]]
Apr 16 02:28:49.657: INFO: successfully validated that service endpoint-test2 in namespace services-1705 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-1705
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1705 to expose endpoints map[]
Apr 16 02:28:49.715: INFO: successfully validated that service endpoint-test2 in namespace services-1705 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:28:49.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1705" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":174,"skipped":3021,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:28:49.814: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-7421
STEP: Creating secret with name secret-test-6d13bb29-8e42-48c1-b0ad-9f7277df39d9
STEP: Creating a pod to test consume secrets
Apr 16 02:28:50.430: INFO: Waiting up to 5m0s for pod "pod-secrets-b33448f3-cb6c-4e0f-881b-897a5b82051a" in namespace "secrets-103" to be "Succeeded or Failed"
Apr 16 02:28:50.444: INFO: Pod "pod-secrets-b33448f3-cb6c-4e0f-881b-897a5b82051a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.298116ms
Apr 16 02:28:52.460: INFO: Pod "pod-secrets-b33448f3-cb6c-4e0f-881b-897a5b82051a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029306551s
STEP: Saw pod success
Apr 16 02:28:52.460: INFO: Pod "pod-secrets-b33448f3-cb6c-4e0f-881b-897a5b82051a" satisfied condition "Succeeded or Failed"
Apr 16 02:28:52.468: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-secrets-b33448f3-cb6c-4e0f-881b-897a5b82051a container secret-volume-test: <nil>
STEP: delete the pod
Apr 16 02:28:52.668: INFO: Waiting for pod pod-secrets-b33448f3-cb6c-4e0f-881b-897a5b82051a to disappear
Apr 16 02:28:52.685: INFO: Pod pod-secrets-b33448f3-cb6c-4e0f-881b-897a5b82051a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:28:52.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-103" for this suite.
STEP: Destroying namespace "secret-namespace-7421" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":175,"skipped":3027,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:28:52.735: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 02:28:53.033: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b2c20f1d-aeeb-4978-86fe-a5afd0d80fe9" in namespace "downward-api-5165" to be "Succeeded or Failed"
Apr 16 02:28:53.043: INFO: Pod "downwardapi-volume-b2c20f1d-aeeb-4978-86fe-a5afd0d80fe9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.343477ms
Apr 16 02:28:55.056: INFO: Pod "downwardapi-volume-b2c20f1d-aeeb-4978-86fe-a5afd0d80fe9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022446809s
STEP: Saw pod success
Apr 16 02:28:55.056: INFO: Pod "downwardapi-volume-b2c20f1d-aeeb-4978-86fe-a5afd0d80fe9" satisfied condition "Succeeded or Failed"
Apr 16 02:28:55.064: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-b2c20f1d-aeeb-4978-86fe-a5afd0d80fe9 container client-container: <nil>
STEP: delete the pod
Apr 16 02:28:55.122: INFO: Waiting for pod downwardapi-volume-b2c20f1d-aeeb-4978-86fe-a5afd0d80fe9 to disappear
Apr 16 02:28:55.136: INFO: Pod downwardapi-volume-b2c20f1d-aeeb-4978-86fe-a5afd0d80fe9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:28:55.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5165" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":176,"skipped":3040,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:28:55.192: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3146
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:28:55.443: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:28:56.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3146" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":177,"skipped":3062,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:28:56.669: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9903
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9903
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Apr 16 02:28:56.961: INFO: Found 0 stateful pods, waiting for 3
Apr 16 02:29:06.988: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 16 02:29:06.989: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 16 02:29:06.989: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Apr 16 02:29:07.053: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Apr 16 02:29:17.144: INFO: Updating stateful set ss2
Apr 16 02:29:17.165: INFO: Waiting for Pod statefulset-9903/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Apr 16 02:29:27.304: INFO: Found 1 stateful pods, waiting for 3
Apr 16 02:29:37.332: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 16 02:29:37.333: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 16 02:29:37.333: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Apr 16 02:29:37.389: INFO: Updating stateful set ss2
Apr 16 02:29:37.405: INFO: Waiting for Pod statefulset-9903/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 16 02:29:47.476: INFO: Updating stateful set ss2
Apr 16 02:29:47.498: INFO: Waiting for StatefulSet statefulset-9903/ss2 to complete update
Apr 16 02:29:47.498: INFO: Waiting for Pod statefulset-9903/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 16 02:29:57.537: INFO: Waiting for StatefulSet statefulset-9903/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 16 02:30:07.535: INFO: Deleting all statefulset in ns statefulset-9903
Apr 16 02:30:07.543: INFO: Scaling statefulset ss2 to 0
Apr 16 02:30:27.617: INFO: Waiting for statefulset status.replicas updated to 0
Apr 16 02:30:27.625: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:30:27.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9903" for this suite.

• [SLOW TEST:91.049 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":178,"skipped":3095,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:30:27.719: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9804
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9804
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-9804
I0416 02:30:28.085657      25 runners.go:190] Created replication controller with name: externalname-service, namespace: services-9804, replica count: 2
I0416 02:30:31.137566      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 16 02:30:31.138: INFO: Creating new exec pod
Apr 16 02:30:36.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-9804 exec execpodcv4gh -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Apr 16 02:30:37.175: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 16 02:30:37.175: INFO: stdout: ""
Apr 16 02:30:37.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-9804 exec execpodcv4gh -- /bin/sh -x -c nc -zv -t -w 2 10.254.51.76 80'
Apr 16 02:30:37.545: INFO: stderr: "+ nc -zv -t -w 2 10.254.51.76 80\nConnection to 10.254.51.76 80 port [tcp/http] succeeded!\n"
Apr 16 02:30:37.545: INFO: stdout: ""
Apr 16 02:30:37.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-9804 exec execpodcv4gh -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.13 31171'
Apr 16 02:30:37.924: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.13 31171\nConnection to 10.0.0.13 31171 port [tcp/31171] succeeded!\n"
Apr 16 02:30:37.924: INFO: stdout: ""
Apr 16 02:30:37.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-9804 exec execpodcv4gh -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.68 31171'
Apr 16 02:30:38.320: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.68 31171\nConnection to 10.0.0.68 31171 port [tcp/31171] succeeded!\n"
Apr 16 02:30:38.320: INFO: stdout: ""
Apr 16 02:30:38.320: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:30:38.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9804" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:10.727 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":179,"skipped":3096,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:30:38.446: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8496
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-8496
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 16 02:30:38.689: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 16 02:30:38.804: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 16 02:30:40.825: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 16 02:30:42.819: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:30:44.821: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:30:46.821: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:30:48.819: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:30:50.821: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:30:52.819: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:30:54.820: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:30:56.822: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:30:58.831: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 16 02:30:58.849: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Apr 16 02:31:00.921: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Apr 16 02:31:00.921: INFO: Breadth first check of 10.100.232.236 on host 10.0.0.13...
Apr 16 02:31:00.929: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.97.94:9080/dial?request=hostname&protocol=udp&host=10.100.232.236&port=8081&tries=1'] Namespace:pod-network-test-8496 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 02:31:00.929: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 02:31:01.172: INFO: Waiting for responses: map[]
Apr 16 02:31:01.172: INFO: reached 10.100.232.236 after 0/1 tries
Apr 16 02:31:01.172: INFO: Breadth first check of 10.100.97.93 on host 10.0.0.68...
Apr 16 02:31:01.183: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.97.94:9080/dial?request=hostname&protocol=udp&host=10.100.97.93&port=8081&tries=1'] Namespace:pod-network-test-8496 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 02:31:01.183: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 02:31:01.380: INFO: Waiting for responses: map[]
Apr 16 02:31:01.380: INFO: reached 10.100.97.93 after 0/1 tries
Apr 16 02:31:01.380: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:31:01.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8496" for this suite.

• [SLOW TEST:22.970 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":180,"skipped":3097,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:31:01.417: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5738
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:31:01.715: INFO: Waiting up to 5m0s for pod "busybox-user-65534-7312938f-dbc6-440c-b67a-9f90975ad131" in namespace "security-context-test-5738" to be "Succeeded or Failed"
Apr 16 02:31:01.730: INFO: Pod "busybox-user-65534-7312938f-dbc6-440c-b67a-9f90975ad131": Phase="Pending", Reason="", readiness=false. Elapsed: 14.521558ms
Apr 16 02:31:03.746: INFO: Pod "busybox-user-65534-7312938f-dbc6-440c-b67a-9f90975ad131": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030901386s
Apr 16 02:31:03.746: INFO: Pod "busybox-user-65534-7312938f-dbc6-440c-b67a-9f90975ad131" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:31:03.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5738" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":181,"skipped":3127,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:31:03.783: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 16 02:31:04.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-8466 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Apr 16 02:31:04.199: INFO: stderr: ""
Apr 16 02:31:04.200: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Apr 16 02:31:09.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-8466 get pod e2e-test-httpd-pod -o json'
Apr 16 02:31:09.394: INFO: stderr: ""
Apr 16 02:31:09.394: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.100.97.92/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.100.97.92/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2021-04-16T02:31:04Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-04-16T02:31:04Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-04-16T02:31:04Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.100.97.92\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-04-16T02:31:06Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8466\",\n        \"resourceVersion\": \"425135\",\n        \"uid\": \"69cd9bcf-475d-4f73-b962-25604d4bb326\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-hjm52\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-flwang-4oyd5hkwxglz-node-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-hjm52\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-hjm52\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-16T02:31:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-16T02:31:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-16T02:31:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-16T02:31:04Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://e96c1e462bb7e6468793615410576b637b87bce70248d91f12e6f22bec21a342\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-04-16T02:31:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.68\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.100.97.92\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.100.97.92\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-04-16T02:31:04Z\"\n    }\n}\n"
STEP: replace the image in the pod
Apr 16 02:31:09.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-8466 replace -f -'
Apr 16 02:31:09.929: INFO: stderr: ""
Apr 16 02:31:09.929: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Apr 16 02:31:09.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-8466 delete pods e2e-test-httpd-pod'
Apr 16 02:31:15.818: INFO: stderr: ""
Apr 16 02:31:15.818: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:31:15.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8466" for this suite.

• [SLOW TEST:12.118 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":182,"skipped":3152,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:31:15.902: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6293
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-c328ec0a-ac76-4f58-8b0e-07026f9919b4
STEP: Creating a pod to test consume configMaps
Apr 16 02:31:16.264: INFO: Waiting up to 5m0s for pod "pod-configmaps-1c881727-986f-486b-885a-f57b991f6318" in namespace "configmap-6293" to be "Succeeded or Failed"
Apr 16 02:31:16.277: INFO: Pod "pod-configmaps-1c881727-986f-486b-885a-f57b991f6318": Phase="Pending", Reason="", readiness=false. Elapsed: 12.229456ms
Apr 16 02:31:18.291: INFO: Pod "pod-configmaps-1c881727-986f-486b-885a-f57b991f6318": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02652835s
Apr 16 02:31:20.310: INFO: Pod "pod-configmaps-1c881727-986f-486b-885a-f57b991f6318": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045229629s
STEP: Saw pod success
Apr 16 02:31:20.310: INFO: Pod "pod-configmaps-1c881727-986f-486b-885a-f57b991f6318" satisfied condition "Succeeded or Failed"
Apr 16 02:31:20.321: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-configmaps-1c881727-986f-486b-885a-f57b991f6318 container agnhost-container: <nil>
STEP: delete the pod
Apr 16 02:31:20.456: INFO: Waiting for pod pod-configmaps-1c881727-986f-486b-885a-f57b991f6318 to disappear
Apr 16 02:31:20.465: INFO: Pod pod-configmaps-1c881727-986f-486b-885a-f57b991f6318 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:31:20.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6293" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":183,"skipped":3153,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:31:20.523: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 16 02:31:23.384: INFO: Successfully updated pod "pod-update-activedeadlineseconds-444b7be5-1c27-4e10-92ea-04e1387d01ee"
Apr 16 02:31:23.384: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-444b7be5-1c27-4e10-92ea-04e1387d01ee" in namespace "pods-8797" to be "terminated due to deadline exceeded"
Apr 16 02:31:23.399: INFO: Pod "pod-update-activedeadlineseconds-444b7be5-1c27-4e10-92ea-04e1387d01ee": Phase="Running", Reason="", readiness=true. Elapsed: 14.337238ms
Apr 16 02:31:25.416: INFO: Pod "pod-update-activedeadlineseconds-444b7be5-1c27-4e10-92ea-04e1387d01ee": Phase="Running", Reason="", readiness=true. Elapsed: 2.03159654s
Apr 16 02:31:27.431: INFO: Pod "pod-update-activedeadlineseconds-444b7be5-1c27-4e10-92ea-04e1387d01ee": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.046561548s
Apr 16 02:31:27.431: INFO: Pod "pod-update-activedeadlineseconds-444b7be5-1c27-4e10-92ea-04e1387d01ee" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:31:27.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8797" for this suite.

• [SLOW TEST:6.938 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":184,"skipped":3206,"failed":0}
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:31:27.462: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6700
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-01d2525f-aa63-40d9-a847-169b6bb70522-216
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:31:28.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6700" for this suite.
STEP: Destroying namespace "nspatchtest-01d2525f-aa63-40d9-a847-169b6bb70522-216" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":185,"skipped":3206,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:31:28.067: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5206
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:31:32.456: INFO: Deleting pod "var-expansion-e4e87dad-812e-46e3-884f-10ddfe8930da" in namespace "var-expansion-5206"
Apr 16 02:31:32.479: INFO: Wait up to 5m0s for pod "var-expansion-e4e87dad-812e-46e3-884f-10ddfe8930da" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:31:36.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5206" for this suite.

• [SLOW TEST:8.476 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":186,"skipped":3257,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:31:36.543: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4409
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Apr 16 02:31:36.800: INFO: Waiting up to 5m0s for pod "var-expansion-31bd3b53-eaae-429f-af02-c05abda7c2e6" in namespace "var-expansion-4409" to be "Succeeded or Failed"
Apr 16 02:31:36.811: INFO: Pod "var-expansion-31bd3b53-eaae-429f-af02-c05abda7c2e6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.575923ms
Apr 16 02:31:38.829: INFO: Pod "var-expansion-31bd3b53-eaae-429f-af02-c05abda7c2e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028966451s
STEP: Saw pod success
Apr 16 02:31:38.829: INFO: Pod "var-expansion-31bd3b53-eaae-429f-af02-c05abda7c2e6" satisfied condition "Succeeded or Failed"
Apr 16 02:31:38.837: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod var-expansion-31bd3b53-eaae-429f-af02-c05abda7c2e6 container dapi-container: <nil>
STEP: delete the pod
Apr 16 02:31:38.907: INFO: Waiting for pod var-expansion-31bd3b53-eaae-429f-af02-c05abda7c2e6 to disappear
Apr 16 02:31:38.916: INFO: Pod var-expansion-31bd3b53-eaae-429f-af02-c05abda7c2e6 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:31:38.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4409" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":187,"skipped":3259,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:31:38.944: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4140
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-fc6f83e3-50b2-4cc4-867c-e8357edf1676
STEP: Creating a pod to test consume configMaps
Apr 16 02:31:39.248: INFO: Waiting up to 5m0s for pod "pod-configmaps-d7cb6130-2df9-4852-954f-4c293c112f13" in namespace "configmap-4140" to be "Succeeded or Failed"
Apr 16 02:31:39.264: INFO: Pod "pod-configmaps-d7cb6130-2df9-4852-954f-4c293c112f13": Phase="Pending", Reason="", readiness=false. Elapsed: 16.172289ms
Apr 16 02:31:41.283: INFO: Pod "pod-configmaps-d7cb6130-2df9-4852-954f-4c293c112f13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034582514s
Apr 16 02:31:43.299: INFO: Pod "pod-configmaps-d7cb6130-2df9-4852-954f-4c293c112f13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050309733s
STEP: Saw pod success
Apr 16 02:31:43.299: INFO: Pod "pod-configmaps-d7cb6130-2df9-4852-954f-4c293c112f13" satisfied condition "Succeeded or Failed"
Apr 16 02:31:43.306: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-configmaps-d7cb6130-2df9-4852-954f-4c293c112f13 container agnhost-container: <nil>
STEP: delete the pod
Apr 16 02:31:43.373: INFO: Waiting for pod pod-configmaps-d7cb6130-2df9-4852-954f-4c293c112f13 to disappear
Apr 16 02:31:43.383: INFO: Pod pod-configmaps-d7cb6130-2df9-4852-954f-4c293c112f13 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:31:43.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4140" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":188,"skipped":3265,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:31:43.417: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8821
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:31:59.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8821" for this suite.

• [SLOW TEST:16.577 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":189,"skipped":3275,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:31:59.996: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6171
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:32:02.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6171" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":190,"skipped":3302,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:32:02.482: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8590
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-135acc93-cfb8-4e65-b35d-61a7fdbd5e10
STEP: Creating a pod to test consume secrets
Apr 16 02:32:02.743: INFO: Waiting up to 5m0s for pod "pod-secrets-0ad41ae0-7e26-43ca-8f12-a96f2fb333e0" in namespace "secrets-8590" to be "Succeeded or Failed"
Apr 16 02:32:02.761: INFO: Pod "pod-secrets-0ad41ae0-7e26-43ca-8f12-a96f2fb333e0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.037642ms
Apr 16 02:32:04.776: INFO: Pod "pod-secrets-0ad41ae0-7e26-43ca-8f12-a96f2fb333e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032383738s
STEP: Saw pod success
Apr 16 02:32:04.776: INFO: Pod "pod-secrets-0ad41ae0-7e26-43ca-8f12-a96f2fb333e0" satisfied condition "Succeeded or Failed"
Apr 16 02:32:04.783: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-secrets-0ad41ae0-7e26-43ca-8f12-a96f2fb333e0 container secret-volume-test: <nil>
STEP: delete the pod
Apr 16 02:32:04.839: INFO: Waiting for pod pod-secrets-0ad41ae0-7e26-43ca-8f12-a96f2fb333e0 to disappear
Apr 16 02:32:04.851: INFO: Pod pod-secrets-0ad41ae0-7e26-43ca-8f12-a96f2fb333e0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:32:04.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8590" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":191,"skipped":3305,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:32:04.891: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5685
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:32:05.138: INFO: Creating deployment "webserver-deployment"
Apr 16 02:32:05.153: INFO: Waiting for observed generation 1
Apr 16 02:32:07.187: INFO: Waiting for all required pods to come up
Apr 16 02:32:07.198: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Apr 16 02:32:09.243: INFO: Waiting for deployment "webserver-deployment" to complete
Apr 16 02:32:09.258: INFO: Updating deployment "webserver-deployment" with a non-existent image
Apr 16 02:32:09.289: INFO: Updating deployment webserver-deployment
Apr 16 02:32:09.289: INFO: Waiting for observed generation 2
Apr 16 02:32:11.320: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr 16 02:32:11.332: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr 16 02:32:11.340: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 16 02:32:11.360: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr 16 02:32:11.360: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr 16 02:32:11.369: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 16 02:32:11.382: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Apr 16 02:32:11.382: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Apr 16 02:32:11.405: INFO: Updating deployment webserver-deployment
Apr 16 02:32:11.405: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Apr 16 02:32:11.426: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr 16 02:32:11.448: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 16 02:32:11.481: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5685  cb01c8de-a587-4872-b93f-9011428f153c 425971 3 2021-04-16 02:32:05 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-04-16 02:32:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-16 02:32:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034bfd58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-04-16 02:32:08 +0000 UTC,LastTransitionTime:2021-04-16 02:32:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-04-16 02:32:09 +0000 UTC,LastTransitionTime:2021-04-16 02:32:05 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Apr 16 02:32:11.496: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-5685  767d3fe9-d781-4221-a4e9-e75f3d249612 425974 3 2021-04-16 02:32:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment cb01c8de-a587-4872-b93f-9011428f153c 0xc003e62417 0xc003e62418}] []  [{kube-controller-manager Update apps/v1 2021-04-16 02:32:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb01c8de-a587-4872-b93f-9011428f153c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e62558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 16 02:32:11.496: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Apr 16 02:32:11.497: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-5685  e02d08f2-7f37-44ef-a69c-4f415a03dba5 425972 3 2021-04-16 02:32:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment cb01c8de-a587-4872-b93f-9011428f153c 0xc003e62687 0xc003e62688}] []  [{kube-controller-manager Update apps/v1 2021-04-16 02:32:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb01c8de-a587-4872-b93f-9011428f153c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e626f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Apr 16 02:32:11.539: INFO: Pod "webserver-deployment-795d758f88-577gc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-577gc webserver-deployment-795d758f88- deployment-5685  cae3d7d2-c6ca-4de7-b9ca-117f16355291 425947 0 2021-04-16 02:32:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.232.242/32 cni.projectcalico.org/podIPs:10.100.232.242/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 767d3fe9-d781-4221-a4e9-e75f3d249612 0xc0062a8807 0xc0062a8808}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"767d3fe9-d781-4221-a4e9-e75f3d249612\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-16 02:32:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-16 02:32:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d4qxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d4qxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d4qxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:,StartTime:2021-04-16 02:32:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 16 02:32:11.539: INFO: Pod "webserver-deployment-795d758f88-5rzw7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5rzw7 webserver-deployment-795d758f88- deployment-5685  16cc2c7b-5b9c-4a35-aa57-b7cd607c6e9a 425928 0 2021-04-16 02:32:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.232.239/32 cni.projectcalico.org/podIPs:10.100.232.239/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 767d3fe9-d781-4221-a4e9-e75f3d249612 0xc0062a8a80 0xc0062a8a81}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"767d3fe9-d781-4221-a4e9-e75f3d249612\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-16 02:32:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-16 02:32:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d4qxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d4qxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d4qxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:,StartTime:2021-04-16 02:32:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 16 02:32:11.539: INFO: Pod "webserver-deployment-795d758f88-bz99j" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bz99j webserver-deployment-795d758f88- deployment-5685  46982320-feca-4eaa-9ffb-181126178a59 425943 0 2021-04-16 02:32:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.97.107/32 cni.projectcalico.org/podIPs:10.100.97.107/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 767d3fe9-d781-4221-a4e9-e75f3d249612 0xc0062a8c50 0xc0062a8c51}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"767d3fe9-d781-4221-a4e9-e75f3d249612\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-16 02:32:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-16 02:32:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d4qxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d4qxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d4qxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.68,PodIP:,StartTime:2021-04-16 02:32:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 16 02:32:11.540: INFO: Pod "webserver-deployment-795d758f88-ch72l" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ch72l webserver-deployment-795d758f88- deployment-5685  7375dcf4-c37b-49d7-b05d-87f61d519dff 425983 0 2021-04-16 02:32:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 767d3fe9-d781-4221-a4e9-e75f3d249612 0xc0062a8e00 0xc0062a8e01}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"767d3fe9-d781-4221-a4e9-e75f3d249612\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d4qxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d4qxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d4qxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 16 02:32:11.540: INFO: Pod "webserver-deployment-795d758f88-ltcnz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ltcnz webserver-deployment-795d758f88- deployment-5685  32f7eac9-d036-43be-ba66-a7afb30cc48f 425948 0 2021-04-16 02:32:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.97.110/32 cni.projectcalico.org/podIPs:10.100.97.110/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 767d3fe9-d781-4221-a4e9-e75f3d249612 0xc0062a8fc0 0xc0062a8fc1}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"767d3fe9-d781-4221-a4e9-e75f3d249612\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-16 02:32:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-16 02:32:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d4qxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d4qxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d4qxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.68,PodIP:,StartTime:2021-04-16 02:32:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 16 02:32:11.541: INFO: Pod "webserver-deployment-795d758f88-wbqjd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wbqjd webserver-deployment-795d758f88- deployment-5685  ffa62eee-193f-429d-a381-2ae8710ca0f2 425955 0 2021-04-16 02:32:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.97.114/32 cni.projectcalico.org/podIPs:10.100.97.114/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 767d3fe9-d781-4221-a4e9-e75f3d249612 0xc0062a9190 0xc0062a9191}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"767d3fe9-d781-4221-a4e9-e75f3d249612\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-16 02:32:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-16 02:32:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d4qxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d4qxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d4qxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.68,PodIP:,StartTime:2021-04-16 02:32:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 16 02:32:11.541: INFO: Pod "webserver-deployment-dd94f59b7-2jxmp" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2jxmp webserver-deployment-dd94f59b7- deployment-5685  5ef11b5d-2864-4af5-9ab7-278faa6a6a19 425846 0 2021-04-16 02:32:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.232.240/32 cni.projectcalico.org/podIPs:10.100.232.240/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e02d08f2-7f37-44ef-a69c-4f415a03dba5 0xc0062a9340 0xc0062a9341}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e02d08f2-7f37-44ef-a69c-4f415a03dba5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-16 02:32:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-16 02:32:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.232.240\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d4qxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d4qxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d4qxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:10.100.232.240,StartTime:2021-04-16 02:32:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-16 02:32:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://f28c90764f0e6961655d65072ec94a9ca8414fd23fcb3baa12e938ac7fb20bbd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.232.240,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 16 02:32:11.542: INFO: Pod "webserver-deployment-dd94f59b7-4jzvg" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4jzvg webserver-deployment-dd94f59b7- deployment-5685  7bdebd11-fe42-40f2-981c-3a10012f67c5 425849 0 2021-04-16 02:32:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.232.238/32 cni.projectcalico.org/podIPs:10.100.232.238/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e02d08f2-7f37-44ef-a69c-4f415a03dba5 0xc0062a94f0 0xc0062a94f1}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e02d08f2-7f37-44ef-a69c-4f415a03dba5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-16 02:32:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-16 02:32:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.232.238\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d4qxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d4qxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d4qxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:10.100.232.238,StartTime:2021-04-16 02:32:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-16 02:32:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://523f4e690ecb04f76dce15c5916a2cae3d2adfa4baddf497c3695c8cc92c8858,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.232.238,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 16 02:32:11.542: INFO: Pod "webserver-deployment-dd94f59b7-6bqfm" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6bqfm webserver-deployment-dd94f59b7- deployment-5685  8b240f7b-47f9-4582-9004-e1f454687147 425809 0 2021-04-16 02:32:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.232.237/32 cni.projectcalico.org/podIPs:10.100.232.237/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e02d08f2-7f37-44ef-a69c-4f415a03dba5 0xc0062a96b0 0xc0062a96b1}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e02d08f2-7f37-44ef-a69c-4f415a03dba5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-16 02:32:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-16 02:32:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.232.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d4qxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d4qxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d4qxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:10.100.232.237,StartTime:2021-04-16 02:32:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-16 02:32:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://9d0377d38ec4a7dd1d882bc1109f9e8ac7703701d2ccaa08c9d5aa7f8e6c7c8b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.232.237,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 16 02:32:11.542: INFO: Pod "webserver-deployment-dd94f59b7-7mvwf" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7mvwf webserver-deployment-dd94f59b7- deployment-5685  7568c4e1-a531-4723-8141-07973b955f76 425979 0 2021-04-16 02:32:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e02d08f2-7f37-44ef-a69c-4f415a03dba5 0xc0062a9860 0xc0062a9861}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e02d08f2-7f37-44ef-a69c-4f415a03dba5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d4qxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d4qxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d4qxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 16 02:32:11.543: INFO: Pod "webserver-deployment-dd94f59b7-7xzx6" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7xzx6 webserver-deployment-dd94f59b7- deployment-5685  39091f17-1f2b-4d21-af9e-75bb85f0fd6a 425812 0 2021-04-16 02:32:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.232.235/32 cni.projectcalico.org/podIPs:10.100.232.235/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e02d08f2-7f37-44ef-a69c-4f415a03dba5 0xc0062a9980 0xc0062a9981}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e02d08f2-7f37-44ef-a69c-4f415a03dba5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-16 02:32:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-16 02:32:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.232.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d4qxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d4qxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d4qxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.13,PodIP:10.100.232.235,StartTime:2021-04-16 02:32:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-16 02:32:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://ae3ee3446db9de909d49003b0e77b70988645029ba206c2b4d98741377495eb7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.232.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 16 02:32:11.543: INFO: Pod "webserver-deployment-dd94f59b7-8567s" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8567s webserver-deployment-dd94f59b7- deployment-5685  bbc6e4f0-dadd-455a-8742-edcdb27f1e03 425830 0 2021-04-16 02:32:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.97.108/32 cni.projectcalico.org/podIPs:10.100.97.108/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e02d08f2-7f37-44ef-a69c-4f415a03dba5 0xc0062a9b50 0xc0062a9b51}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e02d08f2-7f37-44ef-a69c-4f415a03dba5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-16 02:32:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-16 02:32:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.97.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d4qxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d4qxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d4qxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.68,PodIP:10.100.97.108,StartTime:2021-04-16 02:32:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-16 02:32:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://2c56855087e5c9fe3e208f18064c82e1bab8eb4f498313a3b9fc324c0f70b0df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.97.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 16 02:32:11.543: INFO: Pod "webserver-deployment-dd94f59b7-b4jrq" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-b4jrq webserver-deployment-dd94f59b7- deployment-5685  e87db40e-33bf-4219-89b4-a1b527fe0442 425827 0 2021-04-16 02:32:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.97.102/32 cni.projectcalico.org/podIPs:10.100.97.102/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e02d08f2-7f37-44ef-a69c-4f415a03dba5 0xc0062a9d20 0xc0062a9d21}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e02d08f2-7f37-44ef-a69c-4f415a03dba5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-16 02:32:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-16 02:32:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.97.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d4qxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d4qxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d4qxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.68,PodIP:10.100.97.102,StartTime:2021-04-16 02:32:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-16 02:32:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://c78fc405fcca52cb17c40ef8eacfba21adb5406b0cd24682926db09211751d59,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.97.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 16 02:32:11.544: INFO: Pod "webserver-deployment-dd94f59b7-cgwt5" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-cgwt5 webserver-deployment-dd94f59b7- deployment-5685  2caa18e9-2cb1-425a-80c8-aa87d72d1194 425858 0 2021-04-16 02:32:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.97.109/32 cni.projectcalico.org/podIPs:10.100.97.109/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e02d08f2-7f37-44ef-a69c-4f415a03dba5 0xc0062a9ef0 0xc0062a9ef1}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e02d08f2-7f37-44ef-a69c-4f415a03dba5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-16 02:32:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-16 02:32:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.97.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d4qxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d4qxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d4qxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.68,PodIP:10.100.97.109,StartTime:2021-04-16 02:32:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-16 02:32:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://e44b85c0537f05954c4796d66a8bfd438f62e6aa06d15b49f9ebc4cfbe13b291,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.97.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 16 02:32:11.544: INFO: Pod "webserver-deployment-dd94f59b7-d7v68" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-d7v68 webserver-deployment-dd94f59b7- deployment-5685  5bac7e59-91f1-46f8-b8e3-f20e73b07e59 425834 0 2021-04-16 02:32:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.97.104/32 cni.projectcalico.org/podIPs:10.100.97.104/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e02d08f2-7f37-44ef-a69c-4f415a03dba5 0xc0067980e0 0xc0067980e1}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e02d08f2-7f37-44ef-a69c-4f415a03dba5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-16 02:32:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-16 02:32:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.97.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d4qxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d4qxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d4qxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.68,PodIP:10.100.97.104,StartTime:2021-04-16 02:32:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-16 02:32:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://6f19a1d57f1f038e099b0df22e7e5ce704d8f520b08cfcae27390f12eab9a6b6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.97.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 16 02:32:11.545: INFO: Pod "webserver-deployment-dd94f59b7-gdxgf" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gdxgf webserver-deployment-dd94f59b7- deployment-5685  4f092987-b8ea-403c-b295-acc0637bcecb 425984 0 2021-04-16 02:32:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e02d08f2-7f37-44ef-a69c-4f415a03dba5 0xc0067982b0 0xc0067982b1}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e02d08f2-7f37-44ef-a69c-4f415a03dba5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d4qxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d4qxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d4qxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 16 02:32:11.545: INFO: Pod "webserver-deployment-dd94f59b7-rhsb6" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rhsb6 webserver-deployment-dd94f59b7- deployment-5685  a9ebf7cd-8217-41f8-ac35-7f8811129481 425978 0 2021-04-16 02:32:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 e02d08f2-7f37-44ef-a69c-4f415a03dba5 0xc0067983d0 0xc0067983d1}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e02d08f2-7f37-44ef-a69c-4f415a03dba5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d4qxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d4qxz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d4qxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:32:11.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5685" for this suite.

• [SLOW TEST:6.738 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":192,"skipped":3333,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:32:11.632: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2944
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:32:12.195: INFO: The status of Pod test-webserver-461de428-0c7e-4b88-baf6-2b37eff89fe9 is Pending, waiting for it to be Running (with Ready = true)
Apr 16 02:32:14.210: INFO: The status of Pod test-webserver-461de428-0c7e-4b88-baf6-2b37eff89fe9 is Pending, waiting for it to be Running (with Ready = true)
Apr 16 02:32:16.209: INFO: The status of Pod test-webserver-461de428-0c7e-4b88-baf6-2b37eff89fe9 is Pending, waiting for it to be Running (with Ready = true)
Apr 16 02:32:18.210: INFO: The status of Pod test-webserver-461de428-0c7e-4b88-baf6-2b37eff89fe9 is Running (Ready = false)
Apr 16 02:32:20.233: INFO: The status of Pod test-webserver-461de428-0c7e-4b88-baf6-2b37eff89fe9 is Running (Ready = false)
Apr 16 02:32:22.209: INFO: The status of Pod test-webserver-461de428-0c7e-4b88-baf6-2b37eff89fe9 is Running (Ready = false)
Apr 16 02:32:24.209: INFO: The status of Pod test-webserver-461de428-0c7e-4b88-baf6-2b37eff89fe9 is Running (Ready = false)
Apr 16 02:32:26.209: INFO: The status of Pod test-webserver-461de428-0c7e-4b88-baf6-2b37eff89fe9 is Running (Ready = false)
Apr 16 02:32:28.218: INFO: The status of Pod test-webserver-461de428-0c7e-4b88-baf6-2b37eff89fe9 is Running (Ready = false)
Apr 16 02:32:30.208: INFO: The status of Pod test-webserver-461de428-0c7e-4b88-baf6-2b37eff89fe9 is Running (Ready = false)
Apr 16 02:32:32.209: INFO: The status of Pod test-webserver-461de428-0c7e-4b88-baf6-2b37eff89fe9 is Running (Ready = false)
Apr 16 02:32:34.212: INFO: The status of Pod test-webserver-461de428-0c7e-4b88-baf6-2b37eff89fe9 is Running (Ready = false)
Apr 16 02:32:36.212: INFO: The status of Pod test-webserver-461de428-0c7e-4b88-baf6-2b37eff89fe9 is Running (Ready = false)
Apr 16 02:32:38.209: INFO: The status of Pod test-webserver-461de428-0c7e-4b88-baf6-2b37eff89fe9 is Running (Ready = false)
Apr 16 02:32:40.210: INFO: The status of Pod test-webserver-461de428-0c7e-4b88-baf6-2b37eff89fe9 is Running (Ready = true)
Apr 16 02:32:40.280: INFO: Container started at 2021-04-16 02:32:14 +0000 UTC, pod became ready at 2021-04-16 02:32:38 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:32:40.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2944" for this suite.

• [SLOW TEST:28.710 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":193,"skipped":3368,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:32:40.345: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7675
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-7675/configmap-test-73b5f9c5-f423-4664-b926-9039f8d9d450
STEP: Creating a pod to test consume configMaps
Apr 16 02:32:40.678: INFO: Waiting up to 5m0s for pod "pod-configmaps-2c84783d-a9c7-45ab-aa33-3511672bf47e" in namespace "configmap-7675" to be "Succeeded or Failed"
Apr 16 02:32:40.702: INFO: Pod "pod-configmaps-2c84783d-a9c7-45ab-aa33-3511672bf47e": Phase="Pending", Reason="", readiness=false. Elapsed: 23.518603ms
Apr 16 02:32:42.732: INFO: Pod "pod-configmaps-2c84783d-a9c7-45ab-aa33-3511672bf47e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053766476s
Apr 16 02:32:44.748: INFO: Pod "pod-configmaps-2c84783d-a9c7-45ab-aa33-3511672bf47e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069816225s
STEP: Saw pod success
Apr 16 02:32:44.748: INFO: Pod "pod-configmaps-2c84783d-a9c7-45ab-aa33-3511672bf47e" satisfied condition "Succeeded or Failed"
Apr 16 02:32:44.760: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-configmaps-2c84783d-a9c7-45ab-aa33-3511672bf47e container env-test: <nil>
STEP: delete the pod
Apr 16 02:32:44.817: INFO: Waiting for pod pod-configmaps-2c84783d-a9c7-45ab-aa33-3511672bf47e to disappear
Apr 16 02:32:44.827: INFO: Pod pod-configmaps-2c84783d-a9c7-45ab-aa33-3511672bf47e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:32:44.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7675" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":194,"skipped":3380,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:32:44.869: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1667
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 16 02:32:45.171: INFO: Waiting up to 5m0s for pod "pod-f256b66c-66ea-481c-884f-f0abc5bf5882" in namespace "emptydir-1667" to be "Succeeded or Failed"
Apr 16 02:32:45.188: INFO: Pod "pod-f256b66c-66ea-481c-884f-f0abc5bf5882": Phase="Pending", Reason="", readiness=false. Elapsed: 17.806802ms
Apr 16 02:32:47.214: INFO: Pod "pod-f256b66c-66ea-481c-884f-f0abc5bf5882": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043389351s
STEP: Saw pod success
Apr 16 02:32:47.214: INFO: Pod "pod-f256b66c-66ea-481c-884f-f0abc5bf5882" satisfied condition "Succeeded or Failed"
Apr 16 02:32:47.231: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-f256b66c-66ea-481c-884f-f0abc5bf5882 container test-container: <nil>
STEP: delete the pod
Apr 16 02:32:47.297: INFO: Waiting for pod pod-f256b66c-66ea-481c-884f-f0abc5bf5882 to disappear
Apr 16 02:32:47.305: INFO: Pod pod-f256b66c-66ea-481c-884f-f0abc5bf5882 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:32:47.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1667" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":195,"skipped":3406,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:32:47.333: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6786
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 16 02:32:49.657: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:32:49.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6786" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":196,"skipped":3420,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:32:49.747: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 02:32:50.032: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a201059-98a2-4149-be1b-dc1206091c91" in namespace "downward-api-2773" to be "Succeeded or Failed"
Apr 16 02:32:50.052: INFO: Pod "downwardapi-volume-5a201059-98a2-4149-be1b-dc1206091c91": Phase="Pending", Reason="", readiness=false. Elapsed: 19.834571ms
Apr 16 02:32:52.080: INFO: Pod "downwardapi-volume-5a201059-98a2-4149-be1b-dc1206091c91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047881645s
STEP: Saw pod success
Apr 16 02:32:52.080: INFO: Pod "downwardapi-volume-5a201059-98a2-4149-be1b-dc1206091c91" satisfied condition "Succeeded or Failed"
Apr 16 02:32:52.088: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-5a201059-98a2-4149-be1b-dc1206091c91 container client-container: <nil>
STEP: delete the pod
Apr 16 02:32:52.154: INFO: Waiting for pod downwardapi-volume-5a201059-98a2-4149-be1b-dc1206091c91 to disappear
Apr 16 02:32:52.168: INFO: Pod downwardapi-volume-5a201059-98a2-4149-be1b-dc1206091c91 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:32:52.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2773" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":197,"skipped":3436,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:32:52.211: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-586
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Apr 16 02:32:52.619: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 16 02:32:52.619: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 16 02:32:52.649: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 16 02:32:52.649: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 16 02:32:52.690: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 16 02:32:52.690: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 16 02:32:52.751: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 16 02:32:52.752: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 16 02:32:54.121: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr 16 02:32:54.121: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr 16 02:32:55.124: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Apr 16 02:32:55.164: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Apr 16 02:32:55.171: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 0
Apr 16 02:32:55.171: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 0
Apr 16 02:32:55.171: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 0
Apr 16 02:32:55.171: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 0
Apr 16 02:32:55.172: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 0
Apr 16 02:32:55.172: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 0
Apr 16 02:32:55.172: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 0
Apr 16 02:32:55.172: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 0
Apr 16 02:32:55.173: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1
Apr 16 02:32:55.173: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1
Apr 16 02:32:55.173: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 2
Apr 16 02:32:55.173: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 2
Apr 16 02:32:55.173: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 2
Apr 16 02:32:55.173: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 2
Apr 16 02:32:55.184: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 2
Apr 16 02:32:55.185: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 2
Apr 16 02:32:55.234: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 2
Apr 16 02:32:55.234: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 2
Apr 16 02:32:55.294: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1
STEP: listing Deployments
Apr 16 02:32:55.313: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Apr 16 02:32:55.340: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Apr 16 02:32:55.371: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 16 02:32:55.406: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 16 02:32:55.449: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 16 02:32:55.525: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 16 02:32:55.566: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 16 02:32:55.594: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 16 02:32:55.643: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 16 02:32:55.655: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Apr 16 02:32:57.303: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1
Apr 16 02:32:57.303: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1
Apr 16 02:32:57.304: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1
Apr 16 02:32:57.304: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1
Apr 16 02:32:57.304: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1
Apr 16 02:32:57.305: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1
Apr 16 02:32:57.305: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1
Apr 16 02:32:57.305: INFO: observed Deployment test-deployment in namespace deployment-586 with ReadyReplicas 1
STEP: deleting the Deployment
Apr 16 02:32:57.350: INFO: observed event type MODIFIED
Apr 16 02:32:57.351: INFO: observed event type MODIFIED
Apr 16 02:32:57.351: INFO: observed event type MODIFIED
Apr 16 02:32:57.351: INFO: observed event type MODIFIED
Apr 16 02:32:57.351: INFO: observed event type MODIFIED
Apr 16 02:32:57.351: INFO: observed event type MODIFIED
Apr 16 02:32:57.351: INFO: observed event type MODIFIED
Apr 16 02:32:57.352: INFO: observed event type MODIFIED
Apr 16 02:32:57.352: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 16 02:32:57.360: INFO: Log out all the ReplicaSets if there is no deployment created
Apr 16 02:32:57.370: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-586  fefe134d-9588-4664-a458-a388563ca9af 426904 3 2021-04-16 02:32:55 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 52f8c6ef-8dd3-40ee-9c1c-27fba9d8cbf3 0xc005619107 0xc005619108}] []  [{kube-controller-manager Update apps/v1 2021-04-16 02:32:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52f8c6ef-8dd3-40ee-9c1c-27fba9d8cbf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005619170 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Apr 16 02:32:57.386: INFO: pod: "test-deployment-768947d6f5-pw8sv":
&Pod{ObjectMeta:{test-deployment-768947d6f5-pw8sv test-deployment-768947d6f5- deployment-586  51189ba4-3fa7-4507-9175-a4aa72c7b390 426893 0 2021-04-16 02:32:55 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[cni.projectcalico.org/podIP:10.100.97.125/32 cni.projectcalico.org/podIPs:10.100.97.125/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-768947d6f5 fefe134d-9588-4664-a458-a388563ca9af 0xc00331d9c7 0xc00331d9c8}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fefe134d-9588-4664-a458-a388563ca9af\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-16 02:32:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-16 02:32:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.97.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qqlw2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qqlw2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qqlw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.68,PodIP:10.100.97.125,StartTime:2021-04-16 02:32:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-16 02:32:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://04baa77f48d9cf8f15843e8566e22c7e512d8e44abae4efcffb14cdd9606db2d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.97.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 16 02:32:57.387: INFO: pod: "test-deployment-768947d6f5-zpwqf":
&Pod{ObjectMeta:{test-deployment-768947d6f5-zpwqf test-deployment-768947d6f5- deployment-586  0ddcd63a-7af1-486c-b5d9-6d892f4c6dc8 426908 0 2021-04-16 02:32:57 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-768947d6f5 fefe134d-9588-4664-a458-a388563ca9af 0xc00331db87 0xc00331db88}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fefe134d-9588-4664-a458-a388563ca9af\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qqlw2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qqlw2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qqlw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 16 02:32:57.388: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-586  c9498aea-0fb3-4222-b6b8-2c7f902e5564 426900 4 2021-04-16 02:32:55 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 52f8c6ef-8dd3-40ee-9c1c-27fba9d8cbf3 0xc0056191d7 0xc0056191d8}] []  [{kube-controller-manager Update apps/v1 2021-04-16 02:32:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52f8c6ef-8dd3-40ee-9c1c-27fba9d8cbf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005619258 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Apr 16 02:32:57.401: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-586  e39f5d6e-d228-49a3-9626-937e4d672bb4 426832 2 2021-04-16 02:32:52 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 52f8c6ef-8dd3-40ee-9c1c-27fba9d8cbf3 0xc0056192b7 0xc0056192b8}] []  [{kube-controller-manager Update apps/v1 2021-04-16 02:32:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52f8c6ef-8dd3-40ee-9c1c-27fba9d8cbf3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005619320 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Apr 16 02:32:57.413: INFO: pod: "test-deployment-8b6954bfb-fl2bh":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-fl2bh test-deployment-8b6954bfb- deployment-586  bf63897b-4102-4ccc-bdd7-1a0dd8e38412 426788 0 2021-04-16 02:32:52 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[cni.projectcalico.org/podIP:10.100.97.122/32 cni.projectcalico.org/podIPs:10.100.97.122/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-8b6954bfb e39f5d6e-d228-49a3-9626-937e4d672bb4 0xc0005ff6b7 0xc0005ff6b8}] []  [{kube-controller-manager Update v1 2021-04-16 02:32:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e39f5d6e-d228-49a3-9626-937e4d672bb4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-16 02:32:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-16 02:32:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.97.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qqlw2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qqlw2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qqlw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:32:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.68,PodIP:10.100.97.122,StartTime:2021-04-16 02:32:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-16 02:32:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://296797a79d0fba1653637d157737e6c1737b353141b78e92601bc803589e0dcb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.97.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:32:57.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-586" for this suite.

• [SLOW TEST:5.254 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":198,"skipped":3441,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:32:57.466: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9653
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Apr 16 02:32:57.726: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Apr 16 02:33:14.208: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 02:33:18.388: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:33:34.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9653" for this suite.

• [SLOW TEST:37.093 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":199,"skipped":3446,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:33:34.561: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-5062
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:33:34.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5062" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":200,"skipped":3475,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:33:34.947: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4059
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:33:39.259: INFO: Deleting pod "var-expansion-05ba2a6f-b6c2-4b0f-b0b5-2c64ad35f1a8" in namespace "var-expansion-4059"
Apr 16 02:33:39.295: INFO: Wait up to 5m0s for pod "var-expansion-05ba2a6f-b6c2-4b0f-b0b5-2c64ad35f1a8" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:33:41.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4059" for this suite.

• [SLOW TEST:6.401 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":201,"skipped":3476,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:33:41.357: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9948
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Apr 16 02:33:41.584: INFO: namespace kubectl-9948
Apr 16 02:33:41.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9948 create -f -'
Apr 16 02:33:42.151: INFO: stderr: ""
Apr 16 02:33:42.151: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 16 02:33:43.166: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 16 02:33:43.166: INFO: Found 0 / 1
Apr 16 02:33:44.167: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 16 02:33:44.167: INFO: Found 0 / 1
Apr 16 02:33:45.164: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 16 02:33:45.164: INFO: Found 1 / 1
Apr 16 02:33:45.164: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 16 02:33:45.172: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 16 02:33:45.172: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 16 02:33:45.172: INFO: wait on agnhost-primary startup in kubectl-9948 
Apr 16 02:33:45.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9948 logs agnhost-primary-x4wmc agnhost-primary'
Apr 16 02:33:45.345: INFO: stderr: ""
Apr 16 02:33:45.346: INFO: stdout: "Paused\n"
STEP: exposing RC
Apr 16 02:33:45.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9948 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Apr 16 02:33:45.538: INFO: stderr: ""
Apr 16 02:33:45.538: INFO: stdout: "service/rm2 exposed\n"
Apr 16 02:33:45.551: INFO: Service rm2 in namespace kubectl-9948 found.
STEP: exposing service
Apr 16 02:33:47.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9948 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Apr 16 02:33:47.795: INFO: stderr: ""
Apr 16 02:33:47.795: INFO: stdout: "service/rm3 exposed\n"
Apr 16 02:33:47.812: INFO: Service rm3 in namespace kubectl-9948 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:33:49.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9948" for this suite.

• [SLOW TEST:8.517 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":202,"skipped":3513,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:33:49.874: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7360
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Apr 16 02:33:50.147: INFO: Waiting up to 5m0s for pod "client-containers-7dd92431-b585-407e-8d37-dbd8a97f617f" in namespace "containers-7360" to be "Succeeded or Failed"
Apr 16 02:33:50.171: INFO: Pod "client-containers-7dd92431-b585-407e-8d37-dbd8a97f617f": Phase="Pending", Reason="", readiness=false. Elapsed: 23.993277ms
Apr 16 02:33:52.192: INFO: Pod "client-containers-7dd92431-b585-407e-8d37-dbd8a97f617f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045095235s
Apr 16 02:33:54.208: INFO: Pod "client-containers-7dd92431-b585-407e-8d37-dbd8a97f617f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060887334s
STEP: Saw pod success
Apr 16 02:33:54.208: INFO: Pod "client-containers-7dd92431-b585-407e-8d37-dbd8a97f617f" satisfied condition "Succeeded or Failed"
Apr 16 02:33:54.215: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod client-containers-7dd92431-b585-407e-8d37-dbd8a97f617f container agnhost-container: <nil>
STEP: delete the pod
Apr 16 02:33:54.292: INFO: Waiting for pod client-containers-7dd92431-b585-407e-8d37-dbd8a97f617f to disappear
Apr 16 02:33:54.301: INFO: Pod client-containers-7dd92431-b585-407e-8d37-dbd8a97f617f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:33:54.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7360" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":203,"skipped":3523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:33:54.331: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-959
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 16 02:33:54.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-959 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Apr 16 02:33:54.726: INFO: stderr: ""
Apr 16 02:33:54.726: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Apr 16 02:33:54.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-959 delete pods e2e-test-httpd-pod'
Apr 16 02:34:05.872: INFO: stderr: ""
Apr 16 02:34:05.872: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:34:05.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-959" for this suite.

• [SLOW TEST:11.575 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":204,"skipped":3577,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:34:05.907: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1109
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Apr 16 02:34:06.231: INFO: Waiting up to 5m0s for pod "var-expansion-c86b2f4b-b778-4d84-983e-a74d5b3e81ee" in namespace "var-expansion-1109" to be "Succeeded or Failed"
Apr 16 02:34:06.257: INFO: Pod "var-expansion-c86b2f4b-b778-4d84-983e-a74d5b3e81ee": Phase="Pending", Reason="", readiness=false. Elapsed: 26.447441ms
Apr 16 02:34:08.272: INFO: Pod "var-expansion-c86b2f4b-b778-4d84-983e-a74d5b3e81ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041212195s
Apr 16 02:34:10.293: INFO: Pod "var-expansion-c86b2f4b-b778-4d84-983e-a74d5b3e81ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062738431s
STEP: Saw pod success
Apr 16 02:34:10.294: INFO: Pod "var-expansion-c86b2f4b-b778-4d84-983e-a74d5b3e81ee" satisfied condition "Succeeded or Failed"
Apr 16 02:34:10.301: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod var-expansion-c86b2f4b-b778-4d84-983e-a74d5b3e81ee container dapi-container: <nil>
STEP: delete the pod
Apr 16 02:34:10.356: INFO: Waiting for pod var-expansion-c86b2f4b-b778-4d84-983e-a74d5b3e81ee to disappear
Apr 16 02:34:10.369: INFO: Pod var-expansion-c86b2f4b-b778-4d84-983e-a74d5b3e81ee no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:34:10.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1109" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":205,"skipped":3589,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:34:10.411: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3673
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-52e1465a-9e3b-4a34-a030-c74895dd0364
STEP: Creating configMap with name cm-test-opt-upd-b67ddf8a-377e-4070-8863-548da6367b84
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-52e1465a-9e3b-4a34-a030-c74895dd0364
STEP: Updating configmap cm-test-opt-upd-b67ddf8a-377e-4070-8863-548da6367b84
STEP: Creating configMap with name cm-test-opt-create-c450297e-b789-47ba-ae05-c4eefb30b506
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:35:41.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3673" for this suite.

• [SLOW TEST:91.368 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":206,"skipped":3598,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:35:41.779: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8562
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Apr 16 02:35:44.162: INFO: Pod pod-hostip-bf8f5484-8c23-4f42-bd44-ee324ce844d3 has hostIP: 10.0.0.68
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:35:44.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8562" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":207,"skipped":3615,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:35:44.206: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7399
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Apr 16 02:35:49.178: INFO: Successfully updated pod "labelsupdate5ab2714c-0a16-487a-8474-80bd45af28e7"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:35:51.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7399" for this suite.

• [SLOW TEST:7.040 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":208,"skipped":3652,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:35:51.245: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 02:35:51.526: INFO: Waiting up to 5m0s for pod "downwardapi-volume-313322bd-7ee0-4ff5-b297-ce08f924ac06" in namespace "projected-7541" to be "Succeeded or Failed"
Apr 16 02:35:51.542: INFO: Pod "downwardapi-volume-313322bd-7ee0-4ff5-b297-ce08f924ac06": Phase="Pending", Reason="", readiness=false. Elapsed: 16.242756ms
Apr 16 02:35:53.556: INFO: Pod "downwardapi-volume-313322bd-7ee0-4ff5-b297-ce08f924ac06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029733216s
Apr 16 02:35:55.570: INFO: Pod "downwardapi-volume-313322bd-7ee0-4ff5-b297-ce08f924ac06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044421318s
STEP: Saw pod success
Apr 16 02:35:55.570: INFO: Pod "downwardapi-volume-313322bd-7ee0-4ff5-b297-ce08f924ac06" satisfied condition "Succeeded or Failed"
Apr 16 02:35:55.580: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-313322bd-7ee0-4ff5-b297-ce08f924ac06 container client-container: <nil>
STEP: delete the pod
Apr 16 02:35:55.629: INFO: Waiting for pod downwardapi-volume-313322bd-7ee0-4ff5-b297-ce08f924ac06 to disappear
Apr 16 02:35:55.641: INFO: Pod downwardapi-volume-313322bd-7ee0-4ff5-b297-ce08f924ac06 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:35:55.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7541" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":209,"skipped":3652,"failed":0}
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:35:55.686: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 16 02:35:58.043: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:35:58.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2108" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":210,"skipped":3654,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:35:58.142: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6129
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 16 02:35:58.421: INFO: Waiting up to 5m0s for pod "pod-1ee3d0cd-80ad-41a0-b216-cefb6ab94441" in namespace "emptydir-6129" to be "Succeeded or Failed"
Apr 16 02:35:58.433: INFO: Pod "pod-1ee3d0cd-80ad-41a0-b216-cefb6ab94441": Phase="Pending", Reason="", readiness=false. Elapsed: 12.43359ms
Apr 16 02:36:00.460: INFO: Pod "pod-1ee3d0cd-80ad-41a0-b216-cefb6ab94441": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039016629s
Apr 16 02:36:02.476: INFO: Pod "pod-1ee3d0cd-80ad-41a0-b216-cefb6ab94441": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055882882s
STEP: Saw pod success
Apr 16 02:36:02.477: INFO: Pod "pod-1ee3d0cd-80ad-41a0-b216-cefb6ab94441" satisfied condition "Succeeded or Failed"
Apr 16 02:36:02.484: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-1ee3d0cd-80ad-41a0-b216-cefb6ab94441 container test-container: <nil>
STEP: delete the pod
Apr 16 02:36:02.539: INFO: Waiting for pod pod-1ee3d0cd-80ad-41a0-b216-cefb6ab94441 to disappear
Apr 16 02:36:02.550: INFO: Pod pod-1ee3d0cd-80ad-41a0-b216-cefb6ab94441 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:36:02.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6129" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":211,"skipped":3658,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:36:02.588: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4047
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-71e0c540-63c2-4d43-852e-93f5d9233267
STEP: Creating a pod to test consume secrets
Apr 16 02:36:02.904: INFO: Waiting up to 5m0s for pod "pod-secrets-f248a06c-afcf-44d0-a5ca-96b3fda5cebc" in namespace "secrets-4047" to be "Succeeded or Failed"
Apr 16 02:36:02.921: INFO: Pod "pod-secrets-f248a06c-afcf-44d0-a5ca-96b3fda5cebc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.807349ms
Apr 16 02:36:04.932: INFO: Pod "pod-secrets-f248a06c-afcf-44d0-a5ca-96b3fda5cebc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027707838s
STEP: Saw pod success
Apr 16 02:36:04.932: INFO: Pod "pod-secrets-f248a06c-afcf-44d0-a5ca-96b3fda5cebc" satisfied condition "Succeeded or Failed"
Apr 16 02:36:04.939: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-secrets-f248a06c-afcf-44d0-a5ca-96b3fda5cebc container secret-env-test: <nil>
STEP: delete the pod
Apr 16 02:36:05.042: INFO: Waiting for pod pod-secrets-f248a06c-afcf-44d0-a5ca-96b3fda5cebc to disappear
Apr 16 02:36:05.052: INFO: Pod pod-secrets-f248a06c-afcf-44d0-a5ca-96b3fda5cebc no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:36:05.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4047" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":212,"skipped":3666,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:36:05.084: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4437
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 16 02:36:05.331: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 16 02:36:05.357: INFO: Waiting for terminating namespaces to be deleted...
Apr 16 02:36:05.372: INFO: 
Logging pods the apiserver thinks is on node k8s-flwang-4oyd5hkwxglz-node-0 before test
Apr 16 02:36:05.399: INFO: alertmanager-magnum-prometheus-operator-alertmanager-0 from kube-system started at 2021-04-14 03:28:17 +0000 UTC (2 container statuses recorded)
Apr 16 02:36:05.400: INFO: 	Container alertmanager ready: true, restart count 0
Apr 16 02:36:05.400: INFO: 	Container config-reloader ready: true, restart count 0
Apr 16 02:36:05.400: INFO: calico-node-8pzl7 from kube-system started at 2021-04-14 03:25:24 +0000 UTC (1 container statuses recorded)
Apr 16 02:36:05.400: INFO: 	Container calico-node ready: true, restart count 0
Apr 16 02:36:05.401: INFO: csi-cinder-nodeplugin-xngdr from kube-system started at 2021-04-14 03:25:44 +0000 UTC (2 container statuses recorded)
Apr 16 02:36:05.401: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 16 02:36:05.401: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 16 02:36:05.401: INFO: kube-dns-autoscaler-7d66dbddbc-mmpzq from kube-system started at 2021-04-14 03:25:55 +0000 UTC (1 container statuses recorded)
Apr 16 02:36:05.401: INFO: 	Container autoscaler ready: true, restart count 0
Apr 16 02:36:05.402: INFO: magnum-grafana-57b59f6cd-lmchw from kube-system started at 2021-04-14 03:27:41 +0000 UTC (2 container statuses recorded)
Apr 16 02:36:05.402: INFO: 	Container grafana ready: true, restart count 0
Apr 16 02:36:05.402: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Apr 16 02:36:05.403: INFO: magnum-kube-state-metrics-7f877f89d8-9685l from kube-system started at 2021-04-14 03:27:41 +0000 UTC (1 container statuses recorded)
Apr 16 02:36:05.403: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 16 02:36:05.403: INFO: magnum-metrics-server-6b85d69787-r9lnz from kube-system started at 2021-04-14 03:27:41 +0000 UTC (1 container statuses recorded)
Apr 16 02:36:05.403: INFO: 	Container metrics-server ready: true, restart count 0
Apr 16 02:36:05.403: INFO: magnum-prometheus-node-exporter-zs5ln from kube-system started at 2021-04-14 03:27:41 +0000 UTC (1 container statuses recorded)
Apr 16 02:36:05.403: INFO: 	Container node-exporter ready: true, restart count 0
Apr 16 02:36:05.403: INFO: magnum-prometheus-operator-operator-6b8b498699-pwg7j from kube-system started at 2021-04-14 03:27:41 +0000 UTC (2 container statuses recorded)
Apr 16 02:36:05.403: INFO: 	Container prometheus-operator ready: true, restart count 0
Apr 16 02:36:05.403: INFO: 	Container tls-proxy ready: true, restart count 0
Apr 16 02:36:05.404: INFO: npd-nxkwt from kube-system started at 2021-04-14 03:25:44 +0000 UTC (1 container statuses recorded)
Apr 16 02:36:05.404: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr 16 02:36:05.404: INFO: prometheus-magnum-prometheus-operator-prometheus-0 from kube-system started at 2021-04-14 03:28:28 +0000 UTC (3 container statuses recorded)
Apr 16 02:36:05.404: INFO: 	Container prometheus ready: true, restart count 1
Apr 16 02:36:05.404: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Apr 16 02:36:05.404: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Apr 16 02:36:05.404: INFO: sonobuoy-systemd-logs-daemon-set-d0994422a63f4009-7q9k8 from sonobuoy started at 2021-04-16 01:31:44 +0000 UTC (2 container statuses recorded)
Apr 16 02:36:05.405: INFO: 	Container sonobuoy-worker ready: false, restart count 5
Apr 16 02:36:05.405: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 16 02:36:05.405: INFO: busybox from test-k8s started at 2021-04-16 01:17:41 +0000 UTC (1 container statuses recorded)
Apr 16 02:36:05.405: INFO: 	Container busybox ready: true, restart count 1
Apr 16 02:36:05.405: INFO: nginx-7848d4b86f-4q96s from test-k8s started at 2021-04-16 01:16:17 +0000 UTC (1 container statuses recorded)
Apr 16 02:36:05.405: INFO: 	Container nginx ready: true, restart count 0
Apr 16 02:36:05.405: INFO: nginx-7848d4b86f-bmzz7 from test-k8s started at 2021-04-16 01:16:17 +0000 UTC (1 container statuses recorded)
Apr 16 02:36:05.405: INFO: 	Container nginx ready: true, restart count 0
Apr 16 02:36:05.406: INFO: 
Logging pods the apiserver thinks is on node k8s-flwang-4oyd5hkwxglz-node-1 before test
Apr 16 02:36:05.424: INFO: calico-node-nvrm8 from kube-system started at 2021-04-16 01:27:46 +0000 UTC (1 container statuses recorded)
Apr 16 02:36:05.425: INFO: 	Container calico-node ready: true, restart count 0
Apr 16 02:36:05.425: INFO: csi-cinder-nodeplugin-md46f from kube-system started at 2021-04-16 01:44:02 +0000 UTC (2 container statuses recorded)
Apr 16 02:36:05.425: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 16 02:36:05.426: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 16 02:36:05.426: INFO: magnum-prometheus-node-exporter-fcbqk from kube-system started at 2021-04-16 01:44:02 +0000 UTC (1 container statuses recorded)
Apr 16 02:36:05.426: INFO: 	Container node-exporter ready: true, restart count 0
Apr 16 02:36:05.426: INFO: npd-d29zk from kube-system started at 2021-04-16 01:28:16 +0000 UTC (1 container statuses recorded)
Apr 16 02:36:05.426: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr 16 02:36:05.426: INFO: labelsupdate5ab2714c-0a16-487a-8474-80bd45af28e7 from projected-7399 started at 2021-04-16 02:35:44 +0000 UTC (1 container statuses recorded)
Apr 16 02:36:05.426: INFO: 	Container client-container ready: false, restart count 0
Apr 16 02:36:05.427: INFO: sonobuoy from sonobuoy started at 2021-04-16 01:31:30 +0000 UTC (1 container statuses recorded)
Apr 16 02:36:05.427: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 16 02:36:05.427: INFO: sonobuoy-e2e-job-158181182f3b487c from sonobuoy started at 2021-04-16 01:31:44 +0000 UTC (2 container statuses recorded)
Apr 16 02:36:05.427: INFO: 	Container e2e ready: true, restart count 0
Apr 16 02:36:05.427: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 16 02:36:05.427: INFO: sonobuoy-systemd-logs-daemon-set-d0994422a63f4009-55xbs from sonobuoy started at 2021-04-16 01:31:44 +0000 UTC (2 container statuses recorded)
Apr 16 02:36:05.428: INFO: 	Container sonobuoy-worker ready: false, restart count 5
Apr 16 02:36:05.428: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node k8s-flwang-4oyd5hkwxglz-node-0
STEP: verifying the node has the label node k8s-flwang-4oyd5hkwxglz-node-1
Apr 16 02:36:05.580: INFO: Pod alertmanager-magnum-prometheus-operator-alertmanager-0 requesting resource cpu=100m on Node k8s-flwang-4oyd5hkwxglz-node-0
Apr 16 02:36:05.580: INFO: Pod calico-node-8pzl7 requesting resource cpu=250m on Node k8s-flwang-4oyd5hkwxglz-node-0
Apr 16 02:36:05.580: INFO: Pod calico-node-nvrm8 requesting resource cpu=250m on Node k8s-flwang-4oyd5hkwxglz-node-1
Apr 16 02:36:05.581: INFO: Pod csi-cinder-nodeplugin-md46f requesting resource cpu=0m on Node k8s-flwang-4oyd5hkwxglz-node-1
Apr 16 02:36:05.581: INFO: Pod csi-cinder-nodeplugin-xngdr requesting resource cpu=0m on Node k8s-flwang-4oyd5hkwxglz-node-0
Apr 16 02:36:05.581: INFO: Pod kube-dns-autoscaler-7d66dbddbc-mmpzq requesting resource cpu=20m on Node k8s-flwang-4oyd5hkwxglz-node-0
Apr 16 02:36:05.582: INFO: Pod magnum-grafana-57b59f6cd-lmchw requesting resource cpu=100m on Node k8s-flwang-4oyd5hkwxglz-node-0
Apr 16 02:36:05.582: INFO: Pod magnum-kube-state-metrics-7f877f89d8-9685l requesting resource cpu=50m on Node k8s-flwang-4oyd5hkwxglz-node-0
Apr 16 02:36:05.582: INFO: Pod magnum-metrics-server-6b85d69787-r9lnz requesting resource cpu=0m on Node k8s-flwang-4oyd5hkwxglz-node-0
Apr 16 02:36:05.583: INFO: Pod magnum-prometheus-node-exporter-fcbqk requesting resource cpu=20m on Node k8s-flwang-4oyd5hkwxglz-node-1
Apr 16 02:36:05.583: INFO: Pod magnum-prometheus-node-exporter-zs5ln requesting resource cpu=20m on Node k8s-flwang-4oyd5hkwxglz-node-0
Apr 16 02:36:05.583: INFO: Pod magnum-prometheus-operator-operator-6b8b498699-pwg7j requesting resource cpu=0m on Node k8s-flwang-4oyd5hkwxglz-node-0
Apr 16 02:36:05.583: INFO: Pod npd-d29zk requesting resource cpu=20m on Node k8s-flwang-4oyd5hkwxglz-node-1
Apr 16 02:36:05.584: INFO: Pod npd-nxkwt requesting resource cpu=20m on Node k8s-flwang-4oyd5hkwxglz-node-0
Apr 16 02:36:05.584: INFO: Pod prometheus-magnum-prometheus-operator-prometheus-0 requesting resource cpu=342m on Node k8s-flwang-4oyd5hkwxglz-node-0
Apr 16 02:36:05.584: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-flwang-4oyd5hkwxglz-node-1
Apr 16 02:36:05.584: INFO: Pod sonobuoy-e2e-job-158181182f3b487c requesting resource cpu=0m on Node k8s-flwang-4oyd5hkwxglz-node-1
Apr 16 02:36:05.584: INFO: Pod sonobuoy-systemd-logs-daemon-set-d0994422a63f4009-55xbs requesting resource cpu=0m on Node k8s-flwang-4oyd5hkwxglz-node-1
Apr 16 02:36:05.584: INFO: Pod sonobuoy-systemd-logs-daemon-set-d0994422a63f4009-7q9k8 requesting resource cpu=0m on Node k8s-flwang-4oyd5hkwxglz-node-0
Apr 16 02:36:05.584: INFO: Pod busybox requesting resource cpu=0m on Node k8s-flwang-4oyd5hkwxglz-node-0
Apr 16 02:36:05.584: INFO: Pod nginx-7848d4b86f-4q96s requesting resource cpu=0m on Node k8s-flwang-4oyd5hkwxglz-node-0
Apr 16 02:36:05.584: INFO: Pod nginx-7848d4b86f-bmzz7 requesting resource cpu=0m on Node k8s-flwang-4oyd5hkwxglz-node-0
STEP: Starting Pods to consume most of the cluster CPU.
Apr 16 02:36:05.585: INFO: Creating a pod which consumes cpu=2168m on Node k8s-flwang-4oyd5hkwxglz-node-0
Apr 16 02:36:05.614: INFO: Creating a pod which consumes cpu=2597m on Node k8s-flwang-4oyd5hkwxglz-node-1
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a3d912b0-bedc-4080-b2a0-9f79d2703a63.16763611ad570d0e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4437/filler-pod-a3d912b0-bedc-4080-b2a0-9f79d2703a63 to k8s-flwang-4oyd5hkwxglz-node-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a3d912b0-bedc-4080-b2a0-9f79d2703a63.16763611fe5a6678], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a3d912b0-bedc-4080-b2a0-9f79d2703a63.1676361201648e99], Reason = [Created], Message = [Created container filler-pod-a3d912b0-bedc-4080-b2a0-9f79d2703a63]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a3d912b0-bedc-4080-b2a0-9f79d2703a63.16763612095d2be0], Reason = [Started], Message = [Started container filler-pod-a3d912b0-bedc-4080-b2a0-9f79d2703a63]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d0cf2f2d-ab1b-4742-b8ad-81e6d3348165.16763611aecebd90], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4437/filler-pod-d0cf2f2d-ab1b-4742-b8ad-81e6d3348165 to k8s-flwang-4oyd5hkwxglz-node-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d0cf2f2d-ab1b-4742-b8ad-81e6d3348165.16763611f3635f83], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d0cf2f2d-ab1b-4742-b8ad-81e6d3348165.16763611f6388de4], Reason = [Created], Message = [Created container filler-pod-d0cf2f2d-ab1b-4742-b8ad-81e6d3348165]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d0cf2f2d-ab1b-4742-b8ad-81e6d3348165.16763611fdbad16c], Reason = [Started], Message = [Started container filler-pod-d0cf2f2d-ab1b-4742-b8ad-81e6d3348165]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16763612a2c5544f], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: removing the label node off the node k8s-flwang-4oyd5hkwxglz-node-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-flwang-4oyd5hkwxglz-node-1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:36:10.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4437" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:5.809 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":213,"skipped":3677,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:36:10.902: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4835
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:36:11.137: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 16 02:36:15.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-4835 --namespace=crd-publish-openapi-4835 create -f -'
Apr 16 02:36:16.333: INFO: stderr: ""
Apr 16 02:36:16.333: INFO: stdout: "e2e-test-crd-publish-openapi-6253-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 16 02:36:16.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-4835 --namespace=crd-publish-openapi-4835 delete e2e-test-crd-publish-openapi-6253-crds test-cr'
Apr 16 02:36:16.615: INFO: stderr: ""
Apr 16 02:36:16.615: INFO: stdout: "e2e-test-crd-publish-openapi-6253-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Apr 16 02:36:16.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-4835 --namespace=crd-publish-openapi-4835 apply -f -'
Apr 16 02:36:17.301: INFO: stderr: ""
Apr 16 02:36:17.301: INFO: stdout: "e2e-test-crd-publish-openapi-6253-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 16 02:36:17.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-4835 --namespace=crd-publish-openapi-4835 delete e2e-test-crd-publish-openapi-6253-crds test-cr'
Apr 16 02:36:17.485: INFO: stderr: ""
Apr 16 02:36:17.485: INFO: stdout: "e2e-test-crd-publish-openapi-6253-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr 16 02:36:17.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=crd-publish-openapi-4835 explain e2e-test-crd-publish-openapi-6253-crds'
Apr 16 02:36:18.017: INFO: stderr: ""
Apr 16 02:36:18.017: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6253-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:36:22.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4835" for this suite.

• [SLOW TEST:11.447 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":214,"skipped":3699,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:36:22.351: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5027
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-098581f0-0a0d-46ce-b821-b0a5689ab6f0 in namespace container-probe-5027
Apr 16 02:36:26.641: INFO: Started pod liveness-098581f0-0a0d-46ce-b821-b0a5689ab6f0 in namespace container-probe-5027
STEP: checking the pod's current state and verifying that restartCount is present
Apr 16 02:36:26.648: INFO: Initial restart count of pod liveness-098581f0-0a0d-46ce-b821-b0a5689ab6f0 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:40:26.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5027" for this suite.

• [SLOW TEST:244.560 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":215,"skipped":3721,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:40:26.913: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2269
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Apr 16 02:40:27.742: INFO: created pod pod-service-account-defaultsa
Apr 16 02:40:27.742: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr 16 02:40:27.769: INFO: created pod pod-service-account-mountsa
Apr 16 02:40:27.769: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr 16 02:40:27.790: INFO: created pod pod-service-account-nomountsa
Apr 16 02:40:27.790: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr 16 02:40:27.807: INFO: created pod pod-service-account-defaultsa-mountspec
Apr 16 02:40:27.807: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr 16 02:40:27.833: INFO: created pod pod-service-account-mountsa-mountspec
Apr 16 02:40:27.833: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr 16 02:40:27.848: INFO: created pod pod-service-account-nomountsa-mountspec
Apr 16 02:40:27.849: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr 16 02:40:27.864: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr 16 02:40:27.864: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr 16 02:40:27.878: INFO: created pod pod-service-account-mountsa-nomountspec
Apr 16 02:40:27.878: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr 16 02:40:27.910: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr 16 02:40:27.910: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:40:27.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2269" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":216,"skipped":3729,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:40:27.949: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9858
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 16 02:40:29.316: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 16 02:40:31.350: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137629, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137629, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137629, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137629, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 16 02:40:33.366: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137629, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137629, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137629, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137629, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 02:40:36.408: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:40:36.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9858" for this suite.
STEP: Destroying namespace "webhook-9858-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:8.813 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":217,"skipped":3729,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:40:36.765: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:41:37.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2096" for this suite.

• [SLOW TEST:60.348 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":218,"skipped":3758,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:41:37.120: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-627
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-ce0c54cc-56e0-4f01-bfe4-54bb34287aa9
STEP: Creating a pod to test consume configMaps
Apr 16 02:41:37.505: INFO: Waiting up to 5m0s for pod "pod-configmaps-54c95074-dd62-4d83-a9e1-ded1f2c5dc8e" in namespace "configmap-627" to be "Succeeded or Failed"
Apr 16 02:41:37.523: INFO: Pod "pod-configmaps-54c95074-dd62-4d83-a9e1-ded1f2c5dc8e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.73211ms
Apr 16 02:41:39.541: INFO: Pod "pod-configmaps-54c95074-dd62-4d83-a9e1-ded1f2c5dc8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036471908s
Apr 16 02:41:41.560: INFO: Pod "pod-configmaps-54c95074-dd62-4d83-a9e1-ded1f2c5dc8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054760142s
STEP: Saw pod success
Apr 16 02:41:41.560: INFO: Pod "pod-configmaps-54c95074-dd62-4d83-a9e1-ded1f2c5dc8e" satisfied condition "Succeeded or Failed"
Apr 16 02:41:41.568: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-configmaps-54c95074-dd62-4d83-a9e1-ded1f2c5dc8e container configmap-volume-test: <nil>
STEP: delete the pod
Apr 16 02:41:41.787: INFO: Waiting for pod pod-configmaps-54c95074-dd62-4d83-a9e1-ded1f2c5dc8e to disappear
Apr 16 02:41:41.797: INFO: Pod pod-configmaps-54c95074-dd62-4d83-a9e1-ded1f2c5dc8e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:41:41.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-627" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":219,"skipped":3791,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:41:41.839: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9236
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 02:41:42.101: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56bdec49-2eb3-4c8e-8291-72441014098b" in namespace "downward-api-9236" to be "Succeeded or Failed"
Apr 16 02:41:42.120: INFO: Pod "downwardapi-volume-56bdec49-2eb3-4c8e-8291-72441014098b": Phase="Pending", Reason="", readiness=false. Elapsed: 19.170252ms
Apr 16 02:41:44.142: INFO: Pod "downwardapi-volume-56bdec49-2eb3-4c8e-8291-72441014098b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041512081s
STEP: Saw pod success
Apr 16 02:41:44.143: INFO: Pod "downwardapi-volume-56bdec49-2eb3-4c8e-8291-72441014098b" satisfied condition "Succeeded or Failed"
Apr 16 02:41:44.151: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-56bdec49-2eb3-4c8e-8291-72441014098b container client-container: <nil>
STEP: delete the pod
Apr 16 02:41:44.242: INFO: Waiting for pod downwardapi-volume-56bdec49-2eb3-4c8e-8291-72441014098b to disappear
Apr 16 02:41:44.253: INFO: Pod downwardapi-volume-56bdec49-2eb3-4c8e-8291-72441014098b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:41:44.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9236" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":220,"skipped":3796,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:41:44.311: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3493
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 16 02:41:45.708: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 16 02:41:47.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137705, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137705, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137705, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137705, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 02:41:50.786: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:41:50.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3493" for this suite.
STEP: Destroying namespace "webhook-3493-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.709 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":221,"skipped":3806,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:41:51.022: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:41:51.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-3988 create -f -'
Apr 16 02:41:51.775: INFO: stderr: ""
Apr 16 02:41:51.775: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Apr 16 02:41:51.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-3988 create -f -'
Apr 16 02:41:52.379: INFO: stderr: ""
Apr 16 02:41:52.379: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 16 02:41:53.401: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 16 02:41:53.401: INFO: Found 0 / 1
Apr 16 02:41:54.391: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 16 02:41:54.391: INFO: Found 1 / 1
Apr 16 02:41:54.391: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 16 02:41:54.396: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 16 02:41:54.396: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 16 02:41:54.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-3988 describe pod agnhost-primary-fp4vl'
Apr 16 02:41:54.548: INFO: stderr: ""
Apr 16 02:41:54.548: INFO: stdout: "Name:         agnhost-primary-fp4vl\nNamespace:    kubectl-3988\nPriority:     0\nNode:         k8s-flwang-4oyd5hkwxglz-node-1/10.0.0.68\nStart Time:   Fri, 16 Apr 2021 02:41:51 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 10.100.97.92/32\n              cni.projectcalico.org/podIPs: 10.100.97.92/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           10.100.97.92\nIPs:\n  IP:           10.100.97.92\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://3e889f9c2d1f1a33cbccebf579ae2e890a9e35eb6d0bbc53987127e45165740e\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 16 Apr 2021 02:41:53 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rn99z (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-rn99z:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-rn99z\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-3988/agnhost-primary-fp4vl to k8s-flwang-4oyd5hkwxglz-node-1\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Apr 16 02:41:54.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-3988 describe rc agnhost-primary'
Apr 16 02:41:54.777: INFO: stderr: ""
Apr 16 02:41:54.777: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3988\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-fp4vl\n"
Apr 16 02:41:54.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-3988 describe service agnhost-primary'
Apr 16 02:41:54.952: INFO: stderr: ""
Apr 16 02:41:54.952: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3988\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.254.163.177\nIPs:               10.254.163.177\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.100.97.92:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr 16 02:41:54.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-3988 describe node k8s-flwang-4oyd5hkwxglz-master-0'
Apr 16 02:41:55.193: INFO: stderr: ""
Apr 16 02:41:55.193: INFO: stdout: "Name:               k8s-flwang-4oyd5hkwxglz-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=c1.c2r4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=nz-hlz-1\n                    failure-domain.beta.kubernetes.io/zone=nz-hlz-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-flwang-4oyd5hkwxglz-master-0\n                    kubernetes.io/os=linux\n                    magnum.openstack.org/nodegroup=default-master\n                    magnum.openstack.org/role=master\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=c1.c2r4\n                    topology.kubernetes.io/region=nz-hlz-1\n                    topology.kubernetes.io/zone=nz-hlz-1a\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.0.12/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 14 Apr 2021 03:21:24 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-flwang-4oyd5hkwxglz-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 16 Apr 2021 02:41:45 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 14 Apr 2021 03:23:29 +0000   Wed, 14 Apr 2021 03:23:29 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 16 Apr 2021 02:38:04 +0000   Wed, 14 Apr 2021 03:21:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 16 Apr 2021 02:38:04 +0000   Wed, 14 Apr 2021 03:21:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 16 Apr 2021 02:38:04 +0000   Wed, 14 Apr 2021 03:21:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 16 Apr 2021 02:38:04 +0000   Wed, 14 Apr 2021 03:22:45 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.0.12\n  Hostname:    k8s-flwang-4oyd5hkwxglz-master-0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  20435948Ki\n  hugepages-2Mi:      0\n  memory:             4022256Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  18833769646\n  hugepages-2Mi:      0\n  memory:             3919856Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 325f80d634c34efb9b7161dc1ddee957\n  System UUID:                325f80d6-34c3-4efb-9b71-61dc1ddee957\n  Boot ID:                    20d27189-c63d-40c1-b4e0-47ac1077b2a1\n  Kernel Version:             5.6.14-300.fc32.x86_64\n  OS Image:                   Fedora CoreOS 32.20200601.3.0\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.4.4\n  Kubelet Version:            v1.20.4\n  Kube-Proxy Version:         v1.20.4\nPodCIDR:                      10.100.0.0/24\nPodCIDRs:                     10.100.0.0/24\nProviderID:                   openstack:///325f80d6-34c3-4efb-9b71-61dc1ddee957\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-bhhj7                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         47h\n  kube-system                 csi-cinder-controllerplugin-0                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         47h\n  kube-system                 k8s-keystone-auth-jpjc2                                    200m (10%)    0 (0%)      0 (0%)           0 (0%)         47h\n  kube-system                 magnum-auto-healer-nd85t                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         47h\n  kube-system                 magnum-prometheus-node-exporter-jn5qj                      20m (1%)      20m (1%)    20M (0%)         20M (0%)       47h\n  kube-system                 octavia-ingress-controller-0                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         47h\n  kube-system                 openstack-cloud-controller-manager-hp9lg                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         47h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-d0994422a63f4009-67nqj    0 (0%)        0 (0%)      0 (0%)           0 (0%)         70m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                470m (23%)  20m (1%)\n  memory             20M (0%)    20M (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Apr 16 02:41:55.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-3988 describe namespace kubectl-3988'
Apr 16 02:41:55.354: INFO: stderr: ""
Apr 16 02:41:55.354: INFO: stdout: "Name:         kubectl-3988\nLabels:       e2e-framework=kubectl\n              e2e-run=25c70e31-a974-49fa-b4b1-d18d1584ad25\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:41:55.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3988" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":222,"skipped":3813,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:41:55.394: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6884
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 16 02:41:56.261: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 16 02:41:58.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137716, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137716, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137716, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137716, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 02:42:01.343: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:42:01.363: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8151-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:42:02.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6884" for this suite.
STEP: Destroying namespace "webhook-6884-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.486 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":223,"skipped":3866,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:42:02.881: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6105
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-77ec4593-4715-4361-aaf1-784d2b5e49cf
STEP: Creating a pod to test consume configMaps
Apr 16 02:42:03.310: INFO: Waiting up to 5m0s for pod "pod-configmaps-9609c277-1722-4c6c-9ebe-2e04e39d6bcd" in namespace "configmap-6105" to be "Succeeded or Failed"
Apr 16 02:42:03.324: INFO: Pod "pod-configmaps-9609c277-1722-4c6c-9ebe-2e04e39d6bcd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.723298ms
Apr 16 02:42:05.339: INFO: Pod "pod-configmaps-9609c277-1722-4c6c-9ebe-2e04e39d6bcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028693842s
Apr 16 02:42:07.355: INFO: Pod "pod-configmaps-9609c277-1722-4c6c-9ebe-2e04e39d6bcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044678705s
STEP: Saw pod success
Apr 16 02:42:07.355: INFO: Pod "pod-configmaps-9609c277-1722-4c6c-9ebe-2e04e39d6bcd" satisfied condition "Succeeded or Failed"
Apr 16 02:42:07.362: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-configmaps-9609c277-1722-4c6c-9ebe-2e04e39d6bcd container agnhost-container: <nil>
STEP: delete the pod
Apr 16 02:42:07.411: INFO: Waiting for pod pod-configmaps-9609c277-1722-4c6c-9ebe-2e04e39d6bcd to disappear
Apr 16 02:42:07.421: INFO: Pod pod-configmaps-9609c277-1722-4c6c-9ebe-2e04e39d6bcd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:42:07.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6105" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":224,"skipped":3876,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:42:07.454: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8015
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:42:07.778: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr 16 02:42:12.812: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 16 02:42:12.812: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 16 02:42:16.933: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8015  420552b1-3af5-4406-9074-ef3f3ac57538 429989 1 2021-04-16 02:42:12 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-04-16 02:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-16 02:42:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000c89908 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-04-16 02:42:12 +0000 UTC,LastTransitionTime:2021-04-16 02:42:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-685c4f8568" has successfully progressed.,LastUpdateTime:2021-04-16 02:42:15 +0000 UTC,LastTransitionTime:2021-04-16 02:42:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 16 02:42:16.941: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-8015  1e01eb65-c299-4893-9397-dceddfa233e4 429978 1 2021-04-16 02:42:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 420552b1-3af5-4406-9074-ef3f3ac57538 0xc000c89ca7 0xc000c89ca8}] []  [{kube-controller-manager Update apps/v1 2021-04-16 02:42:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"420552b1-3af5-4406-9074-ef3f3ac57538\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000c89d38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 16 02:42:16.951: INFO: Pod "test-cleanup-deployment-685c4f8568-zht9t" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-685c4f8568-zht9t test-cleanup-deployment-685c4f8568- deployment-8015  0aa9e723-cfab-4a81-90ea-904acfb04107 429977 0 2021-04-16 02:42:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[cni.projectcalico.org/podIP:10.100.97.98/32 cni.projectcalico.org/podIPs:10.100.97.98/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-685c4f8568 1e01eb65-c299-4893-9397-dceddfa233e4 0xc0016a07d7 0xc0016a07d8}] []  [{kube-controller-manager Update v1 2021-04-16 02:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e01eb65-c299-4893-9397-dceddfa233e4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-16 02:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-16 02:42:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.97.98\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5kgzk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5kgzk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5kgzk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:42:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:42:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:42:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:42:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.68,PodIP:10.100.97.98,StartTime:2021-04-16 02:42:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-16 02:42:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://6c67fbd16aa64d765cf0136e157e19222c79d9ccb1f0e704bed0c2096616cfd5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.97.98,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:42:16.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8015" for this suite.

• [SLOW TEST:9.560 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":225,"skipped":3884,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:42:17.015: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6561
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6561
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6561
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6561
Apr 16 02:42:17.395: INFO: Found 0 stateful pods, waiting for 1
Apr 16 02:42:27.439: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Apr 16 02:42:27.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-6561 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 16 02:42:27.843: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 16 02:42:27.843: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 16 02:42:27.843: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 16 02:42:27.853: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 16 02:42:37.904: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 16 02:42:37.904: INFO: Waiting for statefulset status.replicas updated to 0
Apr 16 02:42:37.971: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999429s
Apr 16 02:42:38.984: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.990955497s
Apr 16 02:42:39.997: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.978414529s
Apr 16 02:42:41.008: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.96509699s
Apr 16 02:42:42.019: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.954302844s
Apr 16 02:42:43.031: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.943421399s
Apr 16 02:42:44.044: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.930710517s
Apr 16 02:42:45.057: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.918322522s
Apr 16 02:42:46.072: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.904939876s
Apr 16 02:42:47.086: INFO: Verifying statefulset ss doesn't scale past 1 for another 889.562614ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6561
Apr 16 02:42:48.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-6561 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 16 02:42:48.485: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 16 02:42:48.485: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 16 02:42:48.485: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 16 02:42:48.495: INFO: Found 1 stateful pods, waiting for 3
Apr 16 02:42:58.510: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 16 02:42:58.510: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 16 02:42:58.510: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Apr 16 02:42:58.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-6561 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 16 02:42:59.096: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 16 02:42:59.096: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 16 02:42:59.096: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 16 02:42:59.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-6561 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 16 02:42:59.450: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 16 02:42:59.450: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 16 02:42:59.450: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 16 02:42:59.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-6561 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 16 02:42:59.883: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 16 02:42:59.883: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 16 02:42:59.883: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 16 02:42:59.883: INFO: Waiting for statefulset status.replicas updated to 0
Apr 16 02:42:59.894: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr 16 02:43:09.922: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 16 02:43:09.922: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 16 02:43:09.922: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 16 02:43:09.956: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999208s
Apr 16 02:43:10.970: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99173921s
Apr 16 02:43:11.987: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.978150986s
Apr 16 02:43:13.004: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.961447893s
Apr 16 02:43:14.018: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.944091759s
Apr 16 02:43:15.032: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.929917673s
Apr 16 02:43:16.049: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.915836199s
Apr 16 02:43:17.065: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.8987652s
Apr 16 02:43:18.080: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.883132813s
Apr 16 02:43:19.095: INFO: Verifying statefulset ss doesn't scale past 3 for another 868.329905ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6561
Apr 16 02:43:20.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-6561 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 16 02:43:20.460: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 16 02:43:20.460: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 16 02:43:20.460: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 16 02:43:20.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-6561 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 16 02:43:20.846: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 16 02:43:20.846: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 16 02:43:20.846: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 16 02:43:20.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=statefulset-6561 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 16 02:43:21.248: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 16 02:43:21.248: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 16 02:43:21.248: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 16 02:43:21.248: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 16 02:43:41.316: INFO: Deleting all statefulset in ns statefulset-6561
Apr 16 02:43:41.328: INFO: Scaling statefulset ss to 0
Apr 16 02:43:41.358: INFO: Waiting for statefulset status.replicas updated to 0
Apr 16 02:43:41.364: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:43:41.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6561" for this suite.

• [SLOW TEST:84.419 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":226,"skipped":3892,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:43:41.434: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8880
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 16 02:43:42.527: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 16 02:43:44.557: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137822, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137822, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137822, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137822, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 02:43:47.617: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:43:48.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8880" for this suite.
STEP: Destroying namespace "webhook-8880-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.993 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":227,"skipped":3915,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:43:48.429: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9472
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-x49x
STEP: Creating a pod to test atomic-volume-subpath
Apr 16 02:43:48.804: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-x49x" in namespace "subpath-9472" to be "Succeeded or Failed"
Apr 16 02:43:48.825: INFO: Pod "pod-subpath-test-secret-x49x": Phase="Pending", Reason="", readiness=false. Elapsed: 20.130525ms
Apr 16 02:43:50.842: INFO: Pod "pod-subpath-test-secret-x49x": Phase="Running", Reason="", readiness=true. Elapsed: 2.0373005s
Apr 16 02:43:52.859: INFO: Pod "pod-subpath-test-secret-x49x": Phase="Running", Reason="", readiness=true. Elapsed: 4.054289553s
Apr 16 02:43:54.880: INFO: Pod "pod-subpath-test-secret-x49x": Phase="Running", Reason="", readiness=true. Elapsed: 6.075750892s
Apr 16 02:43:56.895: INFO: Pod "pod-subpath-test-secret-x49x": Phase="Running", Reason="", readiness=true. Elapsed: 8.090988752s
Apr 16 02:43:58.905: INFO: Pod "pod-subpath-test-secret-x49x": Phase="Running", Reason="", readiness=true. Elapsed: 10.100320588s
Apr 16 02:44:00.919: INFO: Pod "pod-subpath-test-secret-x49x": Phase="Running", Reason="", readiness=true. Elapsed: 12.114901293s
Apr 16 02:44:02.942: INFO: Pod "pod-subpath-test-secret-x49x": Phase="Running", Reason="", readiness=true. Elapsed: 14.137995334s
Apr 16 02:44:04.956: INFO: Pod "pod-subpath-test-secret-x49x": Phase="Running", Reason="", readiness=true. Elapsed: 16.151918198s
Apr 16 02:44:06.972: INFO: Pod "pod-subpath-test-secret-x49x": Phase="Running", Reason="", readiness=true. Elapsed: 18.167146239s
Apr 16 02:44:08.994: INFO: Pod "pod-subpath-test-secret-x49x": Phase="Running", Reason="", readiness=true. Elapsed: 20.189186987s
Apr 16 02:44:11.012: INFO: Pod "pod-subpath-test-secret-x49x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.20717493s
STEP: Saw pod success
Apr 16 02:44:11.012: INFO: Pod "pod-subpath-test-secret-x49x" satisfied condition "Succeeded or Failed"
Apr 16 02:44:11.026: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-subpath-test-secret-x49x container test-container-subpath-secret-x49x: <nil>
STEP: delete the pod
Apr 16 02:44:11.152: INFO: Waiting for pod pod-subpath-test-secret-x49x to disappear
Apr 16 02:44:11.168: INFO: Pod pod-subpath-test-secret-x49x no longer exists
STEP: Deleting pod pod-subpath-test-secret-x49x
Apr 16 02:44:11.168: INFO: Deleting pod "pod-subpath-test-secret-x49x" in namespace "subpath-9472"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:44:11.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9472" for this suite.

• [SLOW TEST:22.774 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":228,"skipped":3926,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:44:11.205: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5972
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:44:15.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5972" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":229,"skipped":3983,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:44:15.630: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-332
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:44:21.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-332" for this suite.

• [SLOW TEST:5.715 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":230,"skipped":3996,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:44:21.347: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1413
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 16 02:44:22.142: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 16 02:44:24.172: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137862, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137862, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137862, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754137862, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 02:44:27.223: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:44:39.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1413" for this suite.
STEP: Destroying namespace "webhook-1413-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:18.425 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":231,"skipped":4007,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:44:39.775: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5936
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 16 02:44:44.612: INFO: Successfully updated pod "pod-update-f0f58e4c-a0c1-4696-9f54-2742d647a06b"
STEP: verifying the updated pod is in kubernetes
Apr 16 02:44:44.628: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:44:44.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5936" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":232,"skipped":4011,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:44:44.656: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1109
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Apr 16 02:44:44.939: INFO: Waiting up to 5m0s for pod "client-containers-659974fb-f054-4d78-8cc7-7d4685c91617" in namespace "containers-1109" to be "Succeeded or Failed"
Apr 16 02:44:44.951: INFO: Pod "client-containers-659974fb-f054-4d78-8cc7-7d4685c91617": Phase="Pending", Reason="", readiness=false. Elapsed: 12.125199ms
Apr 16 02:44:46.969: INFO: Pod "client-containers-659974fb-f054-4d78-8cc7-7d4685c91617": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029925704s
STEP: Saw pod success
Apr 16 02:44:46.969: INFO: Pod "client-containers-659974fb-f054-4d78-8cc7-7d4685c91617" satisfied condition "Succeeded or Failed"
Apr 16 02:44:46.977: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod client-containers-659974fb-f054-4d78-8cc7-7d4685c91617 container agnhost-container: <nil>
STEP: delete the pod
Apr 16 02:44:47.027: INFO: Waiting for pod client-containers-659974fb-f054-4d78-8cc7-7d4685c91617 to disappear
Apr 16 02:44:47.040: INFO: Pod client-containers-659974fb-f054-4d78-8cc7-7d4685c91617 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:44:47.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1109" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":233,"skipped":4028,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:44:47.075: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:44:47.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9344" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":234,"skipped":4036,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:44:47.391: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3099
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:44:47.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3099" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":235,"skipped":4038,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:44:47.767: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9476
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:44:52.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9476" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":236,"skipped":4055,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:44:52.190: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6260
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-662a1b1b-33c8-404c-a292-22db6f327c8c
STEP: Creating a pod to test consume configMaps
Apr 16 02:44:52.638: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7557972f-7b6a-4b97-aabb-1d5cfca0878f" in namespace "projected-6260" to be "Succeeded or Failed"
Apr 16 02:44:52.660: INFO: Pod "pod-projected-configmaps-7557972f-7b6a-4b97-aabb-1d5cfca0878f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.436114ms
Apr 16 02:44:54.685: INFO: Pod "pod-projected-configmaps-7557972f-7b6a-4b97-aabb-1d5cfca0878f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046993263s
STEP: Saw pod success
Apr 16 02:44:54.685: INFO: Pod "pod-projected-configmaps-7557972f-7b6a-4b97-aabb-1d5cfca0878f" satisfied condition "Succeeded or Failed"
Apr 16 02:44:54.696: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-projected-configmaps-7557972f-7b6a-4b97-aabb-1d5cfca0878f container agnhost-container: <nil>
STEP: delete the pod
Apr 16 02:44:54.749: INFO: Waiting for pod pod-projected-configmaps-7557972f-7b6a-4b97-aabb-1d5cfca0878f to disappear
Apr 16 02:44:54.764: INFO: Pod pod-projected-configmaps-7557972f-7b6a-4b97-aabb-1d5cfca0878f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:44:54.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6260" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":237,"skipped":4080,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:44:54.795: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2327
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-f3ff6e09-6b2b-4853-9e89-8b326adf026d
STEP: Creating a pod to test consume configMaps
Apr 16 02:44:55.079: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7289fa3b-5553-4d4e-a90f-702b24d74f30" in namespace "projected-2327" to be "Succeeded or Failed"
Apr 16 02:44:55.097: INFO: Pod "pod-projected-configmaps-7289fa3b-5553-4d4e-a90f-702b24d74f30": Phase="Pending", Reason="", readiness=false. Elapsed: 17.431445ms
Apr 16 02:44:57.113: INFO: Pod "pod-projected-configmaps-7289fa3b-5553-4d4e-a90f-702b24d74f30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03367433s
Apr 16 02:44:59.131: INFO: Pod "pod-projected-configmaps-7289fa3b-5553-4d4e-a90f-702b24d74f30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051659223s
STEP: Saw pod success
Apr 16 02:44:59.131: INFO: Pod "pod-projected-configmaps-7289fa3b-5553-4d4e-a90f-702b24d74f30" satisfied condition "Succeeded or Failed"
Apr 16 02:44:59.165: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-projected-configmaps-7289fa3b-5553-4d4e-a90f-702b24d74f30 container agnhost-container: <nil>
STEP: delete the pod
Apr 16 02:44:59.216: INFO: Waiting for pod pod-projected-configmaps-7289fa3b-5553-4d4e-a90f-702b24d74f30 to disappear
Apr 16 02:44:59.226: INFO: Pod pod-projected-configmaps-7289fa3b-5553-4d4e-a90f-702b24d74f30 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:44:59.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2327" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":238,"skipped":4098,"failed":0}
SS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:44:59.257: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-7564
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:44:59.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7564" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":239,"skipped":4100,"failed":0}
SSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:44:59.522: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1058
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-cd14ffce-242b-4d1f-8e08-24e3b9f69b3f
STEP: Creating secret with name secret-projected-all-test-volume-0990b226-dd1e-45b9-9925-e96229569263
STEP: Creating a pod to test Check all projections for projected volume plugin
Apr 16 02:44:59.826: INFO: Waiting up to 5m0s for pod "projected-volume-8827d387-09c5-4acc-9a4e-9e181306decf" in namespace "projected-1058" to be "Succeeded or Failed"
Apr 16 02:44:59.839: INFO: Pod "projected-volume-8827d387-09c5-4acc-9a4e-9e181306decf": Phase="Pending", Reason="", readiness=false. Elapsed: 12.077136ms
Apr 16 02:45:01.856: INFO: Pod "projected-volume-8827d387-09c5-4acc-9a4e-9e181306decf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029295833s
STEP: Saw pod success
Apr 16 02:45:01.856: INFO: Pod "projected-volume-8827d387-09c5-4acc-9a4e-9e181306decf" satisfied condition "Succeeded or Failed"
Apr 16 02:45:01.865: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod projected-volume-8827d387-09c5-4acc-9a4e-9e181306decf container projected-all-volume-test: <nil>
STEP: delete the pod
Apr 16 02:45:01.921: INFO: Waiting for pod projected-volume-8827d387-09c5-4acc-9a4e-9e181306decf to disappear
Apr 16 02:45:01.929: INFO: Pod projected-volume-8827d387-09c5-4acc-9a4e-9e181306decf no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:45:01.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1058" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":240,"skipped":4103,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:45:01.962: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3362
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3362 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3362;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3362 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3362;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3362.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3362.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3362.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3362.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3362.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3362.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3362.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3362.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3362.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3362.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3362.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3362.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3362.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 173.29.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.29.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.29.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.29.173_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3362 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3362;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3362 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3362;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3362.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3362.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3362.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3362.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3362.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3362.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3362.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3362.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3362.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3362.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3362.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3362.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3362.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 173.29.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.29.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.29.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.29.173_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 16 02:45:06.386: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.397: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.407: INFO: Unable to read wheezy_udp@dns-test-service.dns-3362 from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.416: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3362 from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.424: INFO: Unable to read wheezy_udp@dns-test-service.dns-3362.svc from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.434: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3362.svc from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.442: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3362.svc from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.450: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3362.svc from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.483: INFO: Unable to read wheezy_udp@PodARecord from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.492: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.520: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.528: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.537: INFO: Unable to read jessie_udp@dns-test-service.dns-3362 from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.547: INFO: Unable to read jessie_tcp@dns-test-service.dns-3362 from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.558: INFO: Unable to read jessie_udp@dns-test-service.dns-3362.svc from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.567: INFO: Unable to read jessie_tcp@dns-test-service.dns-3362.svc from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.578: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3362.svc from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.586: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3362.svc from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.615: INFO: Unable to read jessie_udp@PodARecord from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.624: INFO: Unable to read jessie_tcp@PodARecord from pod dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6: the server could not find the requested resource (get pods dns-test-7661bee6-d071-4297-b455-13332be79ed6)
Apr 16 02:45:06.641: INFO: Lookups using dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3362 wheezy_tcp@dns-test-service.dns-3362 wheezy_udp@dns-test-service.dns-3362.svc wheezy_tcp@dns-test-service.dns-3362.svc wheezy_udp@_http._tcp.dns-test-service.dns-3362.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3362.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3362 jessie_tcp@dns-test-service.dns-3362 jessie_udp@dns-test-service.dns-3362.svc jessie_tcp@dns-test-service.dns-3362.svc jessie_udp@_http._tcp.dns-test-service.dns-3362.svc jessie_tcp@_http._tcp.dns-test-service.dns-3362.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Apr 16 02:45:11.889: INFO: DNS probes using dns-3362/dns-test-7661bee6-d071-4297-b455-13332be79ed6 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:45:12.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3362" for this suite.

• [SLOW TEST:10.161 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":241,"skipped":4126,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:45:12.124: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5831
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 16 02:45:12.452: INFO: Waiting up to 5m0s for pod "pod-5abbcb2d-ca12-408a-8db3-5b25710b43e2" in namespace "emptydir-5831" to be "Succeeded or Failed"
Apr 16 02:45:12.463: INFO: Pod "pod-5abbcb2d-ca12-408a-8db3-5b25710b43e2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.230412ms
Apr 16 02:45:14.486: INFO: Pod "pod-5abbcb2d-ca12-408a-8db3-5b25710b43e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033556074s
Apr 16 02:45:16.503: INFO: Pod "pod-5abbcb2d-ca12-408a-8db3-5b25710b43e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051114206s
STEP: Saw pod success
Apr 16 02:45:16.503: INFO: Pod "pod-5abbcb2d-ca12-408a-8db3-5b25710b43e2" satisfied condition "Succeeded or Failed"
Apr 16 02:45:16.511: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-5abbcb2d-ca12-408a-8db3-5b25710b43e2 container test-container: <nil>
STEP: delete the pod
Apr 16 02:45:16.567: INFO: Waiting for pod pod-5abbcb2d-ca12-408a-8db3-5b25710b43e2 to disappear
Apr 16 02:45:16.577: INFO: Pod pod-5abbcb2d-ca12-408a-8db3-5b25710b43e2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:45:16.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5831" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":242,"skipped":4132,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:45:16.605: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9252
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 16 02:45:16.879: INFO: Waiting up to 5m0s for pod "pod-6b0660cf-3c3d-49a5-b76a-2346e7f12026" in namespace "emptydir-9252" to be "Succeeded or Failed"
Apr 16 02:45:16.889: INFO: Pod "pod-6b0660cf-3c3d-49a5-b76a-2346e7f12026": Phase="Pending", Reason="", readiness=false. Elapsed: 10.203548ms
Apr 16 02:45:18.911: INFO: Pod "pod-6b0660cf-3c3d-49a5-b76a-2346e7f12026": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031712691s
STEP: Saw pod success
Apr 16 02:45:18.911: INFO: Pod "pod-6b0660cf-3c3d-49a5-b76a-2346e7f12026" satisfied condition "Succeeded or Failed"
Apr 16 02:45:18.918: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-6b0660cf-3c3d-49a5-b76a-2346e7f12026 container test-container: <nil>
STEP: delete the pod
Apr 16 02:45:18.985: INFO: Waiting for pod pod-6b0660cf-3c3d-49a5-b76a-2346e7f12026 to disappear
Apr 16 02:45:19.006: INFO: Pod pod-6b0660cf-3c3d-49a5-b76a-2346e7f12026 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:45:19.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9252" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":243,"skipped":4134,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:45:19.056: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2592
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Apr 16 02:45:25.537: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:45:25.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0416 02:45:25.536988      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0416 02:45:25.537025      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0416 02:45:25.537034      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-2592" for this suite.

• [SLOW TEST:6.540 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":244,"skipped":4169,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:45:25.597: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 02:45:25.892: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c42c84c5-1ee1-481c-86bd-3b9b5d428d39" in namespace "projected-9444" to be "Succeeded or Failed"
Apr 16 02:45:25.901: INFO: Pod "downwardapi-volume-c42c84c5-1ee1-481c-86bd-3b9b5d428d39": Phase="Pending", Reason="", readiness=false. Elapsed: 9.301463ms
Apr 16 02:45:27.919: INFO: Pod "downwardapi-volume-c42c84c5-1ee1-481c-86bd-3b9b5d428d39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02675329s
STEP: Saw pod success
Apr 16 02:45:27.919: INFO: Pod "downwardapi-volume-c42c84c5-1ee1-481c-86bd-3b9b5d428d39" satisfied condition "Succeeded or Failed"
Apr 16 02:45:27.928: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-c42c84c5-1ee1-481c-86bd-3b9b5d428d39 container client-container: <nil>
STEP: delete the pod
Apr 16 02:45:27.988: INFO: Waiting for pod downwardapi-volume-c42c84c5-1ee1-481c-86bd-3b9b5d428d39 to disappear
Apr 16 02:45:27.997: INFO: Pod downwardapi-volume-c42c84c5-1ee1-481c-86bd-3b9b5d428d39 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:45:27.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9444" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":245,"skipped":4184,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:45:28.033: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2583
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-6f1640ef-7cbb-4204-8b7c-4fe4db2a1608
STEP: Creating a pod to test consume secrets
Apr 16 02:45:28.385: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c799fb6e-2d3f-4957-a863-eeac7d08f2b0" in namespace "projected-2583" to be "Succeeded or Failed"
Apr 16 02:45:28.402: INFO: Pod "pod-projected-secrets-c799fb6e-2d3f-4957-a863-eeac7d08f2b0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.796389ms
Apr 16 02:45:30.413: INFO: Pod "pod-projected-secrets-c799fb6e-2d3f-4957-a863-eeac7d08f2b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.028150523s
Apr 16 02:45:32.431: INFO: Pod "pod-projected-secrets-c799fb6e-2d3f-4957-a863-eeac7d08f2b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046024659s
STEP: Saw pod success
Apr 16 02:45:32.431: INFO: Pod "pod-projected-secrets-c799fb6e-2d3f-4957-a863-eeac7d08f2b0" satisfied condition "Succeeded or Failed"
Apr 16 02:45:32.438: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-projected-secrets-c799fb6e-2d3f-4957-a863-eeac7d08f2b0 container secret-volume-test: <nil>
STEP: delete the pod
Apr 16 02:45:32.483: INFO: Waiting for pod pod-projected-secrets-c799fb6e-2d3f-4957-a863-eeac7d08f2b0 to disappear
Apr 16 02:45:32.493: INFO: Pod pod-projected-secrets-c799fb6e-2d3f-4957-a863-eeac7d08f2b0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:45:32.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2583" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":246,"skipped":4184,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:45:32.523: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:45:32.764: INFO: Creating deployment "test-recreate-deployment"
Apr 16 02:45:32.779: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr 16 02:45:32.816: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Apr 16 02:45:34.849: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr 16 02:45:34.856: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr 16 02:45:34.881: INFO: Updating deployment test-recreate-deployment
Apr 16 02:45:34.881: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 16 02:45:35.131: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4228  df0d2446-f90f-4f68-9df0-8788e5615abc 432093 2 2021-04-16 02:45:32 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-04-16 02:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-16 02:45:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000c89038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-04-16 02:45:35 +0000 UTC,LastTransitionTime:2021-04-16 02:45:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-04-16 02:45:35 +0000 UTC,LastTransitionTime:2021-04-16 02:45:32 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Apr 16 02:45:35.141: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-4228  29a8feb5-3c9a-4b1c-adf4-36acca5471a3 432090 1 2021-04-16 02:45:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment df0d2446-f90f-4f68-9df0-8788e5615abc 0xc004444a60 0xc004444a61}] []  [{kube-controller-manager Update apps/v1 2021-04-16 02:45:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df0d2446-f90f-4f68-9df0-8788e5615abc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004444ad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 16 02:45:35.142: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr 16 02:45:35.142: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-4228  6b641c10-d8e9-483d-b92f-8e1205399329 432081 2 2021-04-16 02:45:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment df0d2446-f90f-4f68-9df0-8788e5615abc 0xc004444967 0xc004444968}] []  [{kube-controller-manager Update apps/v1 2021-04-16 02:45:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df0d2446-f90f-4f68-9df0-8788e5615abc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044449f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 16 02:45:35.155: INFO: Pod "test-recreate-deployment-f79dd4667-bdjh2" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-bdjh2 test-recreate-deployment-f79dd4667- deployment-4228  fb87969d-050f-48f5-abf7-521b631720a5 432092 0 2021-04-16 02:45:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 29a8feb5-3c9a-4b1c-adf4-36acca5471a3 0xc004444ef0 0xc004444ef1}] []  [{kube-controller-manager Update v1 2021-04-16 02:45:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"29a8feb5-3c9a-4b1c-adf4-36acca5471a3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-16 02:45:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zfh7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zfh7z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zfh7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:45:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:45:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:45:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:45:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.68,PodIP:,StartTime:2021-04-16 02:45:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:45:35.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4228" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":247,"skipped":4188,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:45:35.189: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5964
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-1bce96e2-591c-4339-82cd-41bfc6369e90
STEP: Creating a pod to test consume configMaps
Apr 16 02:45:35.530: INFO: Waiting up to 5m0s for pod "pod-configmaps-c64e0328-6bb5-435b-a9dc-61e95c06feb2" in namespace "configmap-5964" to be "Succeeded or Failed"
Apr 16 02:45:35.547: INFO: Pod "pod-configmaps-c64e0328-6bb5-435b-a9dc-61e95c06feb2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.170918ms
Apr 16 02:45:37.563: INFO: Pod "pod-configmaps-c64e0328-6bb5-435b-a9dc-61e95c06feb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032832942s
Apr 16 02:45:39.573: INFO: Pod "pod-configmaps-c64e0328-6bb5-435b-a9dc-61e95c06feb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043635394s
STEP: Saw pod success
Apr 16 02:45:39.574: INFO: Pod "pod-configmaps-c64e0328-6bb5-435b-a9dc-61e95c06feb2" satisfied condition "Succeeded or Failed"
Apr 16 02:45:39.582: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-configmaps-c64e0328-6bb5-435b-a9dc-61e95c06feb2 container agnhost-container: <nil>
STEP: delete the pod
Apr 16 02:45:39.632: INFO: Waiting for pod pod-configmaps-c64e0328-6bb5-435b-a9dc-61e95c06feb2 to disappear
Apr 16 02:45:39.644: INFO: Pod pod-configmaps-c64e0328-6bb5-435b-a9dc-61e95c06feb2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:45:39.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5964" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":248,"skipped":4200,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:45:39.680: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1010
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 02:45:39.930: INFO: Waiting up to 5m0s for pod "downwardapi-volume-664a2cf9-5c16-4b65-901c-2a9dca90e7bd" in namespace "projected-1010" to be "Succeeded or Failed"
Apr 16 02:45:39.943: INFO: Pod "downwardapi-volume-664a2cf9-5c16-4b65-901c-2a9dca90e7bd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.909736ms
Apr 16 02:45:41.960: INFO: Pod "downwardapi-volume-664a2cf9-5c16-4b65-901c-2a9dca90e7bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029820154s
STEP: Saw pod success
Apr 16 02:45:41.961: INFO: Pod "downwardapi-volume-664a2cf9-5c16-4b65-901c-2a9dca90e7bd" satisfied condition "Succeeded or Failed"
Apr 16 02:45:41.971: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-664a2cf9-5c16-4b65-901c-2a9dca90e7bd container client-container: <nil>
STEP: delete the pod
Apr 16 02:45:42.024: INFO: Waiting for pod downwardapi-volume-664a2cf9-5c16-4b65-901c-2a9dca90e7bd to disappear
Apr 16 02:45:42.032: INFO: Pod downwardapi-volume-664a2cf9-5c16-4b65-901c-2a9dca90e7bd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:45:42.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1010" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":249,"skipped":4201,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:45:42.073: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4494
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4494
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4494
STEP: creating replication controller externalsvc in namespace services-4494
I0416 02:45:42.411338      25 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4494, replica count: 2
I0416 02:45:45.461922      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Apr 16 02:45:45.560: INFO: Creating new exec pod
Apr 16 02:45:49.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-4494 exec execpodkd8ht -- /bin/sh -x -c nslookup nodeport-service.services-4494.svc.cluster.local'
Apr 16 02:45:50.154: INFO: stderr: "+ nslookup nodeport-service.services-4494.svc.cluster.local\n"
Apr 16 02:45:50.154: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nnodeport-service.services-4494.svc.cluster.local\tcanonical name = externalsvc.services-4494.svc.cluster.local.\nName:\texternalsvc.services-4494.svc.cluster.local\nAddress: 10.254.106.61\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4494, will wait for the garbage collector to delete the pods
Apr 16 02:45:50.246: INFO: Deleting ReplicationController externalsvc took: 29.263206ms
Apr 16 02:45:50.346: INFO: Terminating ReplicationController externalsvc pods took: 100.221318ms
Apr 16 02:46:06.045: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:46:06.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4494" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:24.091 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":250,"skipped":4209,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:46:06.165: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7359
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:46:06.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7359" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":251,"skipped":4215,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:46:06.479: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-2918
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Apr 16 02:46:06.730: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Apr 16 02:46:06.760: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 16 02:46:06.760: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Apr 16 02:46:06.794: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 16 02:46:06.794: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Apr 16 02:46:06.840: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Apr 16 02:46:06.840: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Apr 16 02:46:13.963: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:46:13.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-2918" for this suite.

• [SLOW TEST:7.540 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":252,"skipped":4263,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:46:14.021: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-184
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 02:46:14.291: INFO: Waiting up to 5m0s for pod "downwardapi-volume-677e44fd-c745-494f-9f03-ebcac8f4a91f" in namespace "downward-api-184" to be "Succeeded or Failed"
Apr 16 02:46:14.311: INFO: Pod "downwardapi-volume-677e44fd-c745-494f-9f03-ebcac8f4a91f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.085247ms
Apr 16 02:46:16.329: INFO: Pod "downwardapi-volume-677e44fd-c745-494f-9f03-ebcac8f4a91f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.037322317s
STEP: Saw pod success
Apr 16 02:46:16.329: INFO: Pod "downwardapi-volume-677e44fd-c745-494f-9f03-ebcac8f4a91f" satisfied condition "Succeeded or Failed"
Apr 16 02:46:16.336: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-677e44fd-c745-494f-9f03-ebcac8f4a91f container client-container: <nil>
STEP: delete the pod
Apr 16 02:46:16.403: INFO: Waiting for pod downwardapi-volume-677e44fd-c745-494f-9f03-ebcac8f4a91f to disappear
Apr 16 02:46:16.422: INFO: Pod downwardapi-volume-677e44fd-c745-494f-9f03-ebcac8f4a91f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:46:16.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-184" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":253,"skipped":4284,"failed":0}
SSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:46:16.455: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-4654
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 16 02:46:16.775: INFO: starting watch
STEP: patching
STEP: updating
Apr 16 02:46:16.810: INFO: waiting for watch events with expected annotations
Apr 16 02:46:16.810: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:46:16.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-4654" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":254,"skipped":4287,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:46:16.931: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1320
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Apr 16 02:46:17.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-1320 create -f -'
Apr 16 02:46:18.192: INFO: stderr: ""
Apr 16 02:46:18.192: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Apr 16 02:46:18.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-1320 diff -f -'
Apr 16 02:46:18.903: INFO: rc: 1
Apr 16 02:46:18.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-1320 delete -f -'
Apr 16 02:46:19.195: INFO: stderr: ""
Apr 16 02:46:19.195: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:46:19.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1320" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":255,"skipped":4301,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:46:19.246: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7173
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Apr 16 02:46:19.528: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7173  e0e77ecc-095d-45e0-98f6-b6b1ae6912b5 432577 0 2021-04-16 02:46:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-16 02:46:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 16 02:46:19.529: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7173  e0e77ecc-095d-45e0-98f6-b6b1ae6912b5 432578 0 2021-04-16 02:46:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-16 02:46:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Apr 16 02:46:19.570: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7173  e0e77ecc-095d-45e0-98f6-b6b1ae6912b5 432579 0 2021-04-16 02:46:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-16 02:46:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 16 02:46:19.570: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7173  e0e77ecc-095d-45e0-98f6-b6b1ae6912b5 432580 0 2021-04-16 02:46:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-16 02:46:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:46:19.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7173" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":256,"skipped":4305,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:46:19.606: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2319
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 16 02:46:19.963: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 16 02:46:19.979: INFO: Waiting for terminating namespaces to be deleted...
Apr 16 02:46:19.987: INFO: 
Logging pods the apiserver thinks is on node k8s-flwang-4oyd5hkwxglz-node-0 before test
Apr 16 02:46:20.012: INFO: alertmanager-magnum-prometheus-operator-alertmanager-0 from kube-system started at 2021-04-14 03:28:17 +0000 UTC (2 container statuses recorded)
Apr 16 02:46:20.012: INFO: 	Container alertmanager ready: true, restart count 0
Apr 16 02:46:20.012: INFO: 	Container config-reloader ready: true, restart count 0
Apr 16 02:46:20.013: INFO: calico-node-8pzl7 from kube-system started at 2021-04-14 03:25:24 +0000 UTC (1 container statuses recorded)
Apr 16 02:46:20.013: INFO: 	Container calico-node ready: true, restart count 0
Apr 16 02:46:20.013: INFO: csi-cinder-nodeplugin-xngdr from kube-system started at 2021-04-14 03:25:44 +0000 UTC (2 container statuses recorded)
Apr 16 02:46:20.013: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 16 02:46:20.013: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 16 02:46:20.013: INFO: kube-dns-autoscaler-7d66dbddbc-mmpzq from kube-system started at 2021-04-14 03:25:55 +0000 UTC (1 container statuses recorded)
Apr 16 02:46:20.013: INFO: 	Container autoscaler ready: true, restart count 0
Apr 16 02:46:20.013: INFO: magnum-grafana-57b59f6cd-lmchw from kube-system started at 2021-04-14 03:27:41 +0000 UTC (2 container statuses recorded)
Apr 16 02:46:20.013: INFO: 	Container grafana ready: true, restart count 0
Apr 16 02:46:20.013: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Apr 16 02:46:20.013: INFO: magnum-kube-state-metrics-7f877f89d8-9685l from kube-system started at 2021-04-14 03:27:41 +0000 UTC (1 container statuses recorded)
Apr 16 02:46:20.013: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 16 02:46:20.013: INFO: magnum-metrics-server-6b85d69787-r9lnz from kube-system started at 2021-04-14 03:27:41 +0000 UTC (1 container statuses recorded)
Apr 16 02:46:20.013: INFO: 	Container metrics-server ready: true, restart count 0
Apr 16 02:46:20.013: INFO: magnum-prometheus-node-exporter-zs5ln from kube-system started at 2021-04-14 03:27:41 +0000 UTC (1 container statuses recorded)
Apr 16 02:46:20.014: INFO: 	Container node-exporter ready: true, restart count 0
Apr 16 02:46:20.014: INFO: magnum-prometheus-operator-operator-6b8b498699-pwg7j from kube-system started at 2021-04-14 03:27:41 +0000 UTC (2 container statuses recorded)
Apr 16 02:46:20.014: INFO: 	Container prometheus-operator ready: true, restart count 0
Apr 16 02:46:20.014: INFO: 	Container tls-proxy ready: true, restart count 0
Apr 16 02:46:20.014: INFO: npd-nxkwt from kube-system started at 2021-04-14 03:25:44 +0000 UTC (1 container statuses recorded)
Apr 16 02:46:20.014: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr 16 02:46:20.014: INFO: prometheus-magnum-prometheus-operator-prometheus-0 from kube-system started at 2021-04-14 03:28:28 +0000 UTC (3 container statuses recorded)
Apr 16 02:46:20.014: INFO: 	Container prometheus ready: true, restart count 1
Apr 16 02:46:20.014: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Apr 16 02:46:20.015: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Apr 16 02:46:20.015: INFO: sonobuoy-systemd-logs-daemon-set-d0994422a63f4009-7q9k8 from sonobuoy started at 2021-04-16 01:31:44 +0000 UTC (2 container statuses recorded)
Apr 16 02:46:20.015: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Apr 16 02:46:20.015: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 16 02:46:20.015: INFO: busybox from test-k8s started at 2021-04-16 01:17:41 +0000 UTC (1 container statuses recorded)
Apr 16 02:46:20.015: INFO: 	Container busybox ready: true, restart count 1
Apr 16 02:46:20.018: INFO: nginx-7848d4b86f-4q96s from test-k8s started at 2021-04-16 01:16:17 +0000 UTC (1 container statuses recorded)
Apr 16 02:46:20.020: INFO: 	Container nginx ready: true, restart count 0
Apr 16 02:46:20.021: INFO: nginx-7848d4b86f-bmzz7 from test-k8s started at 2021-04-16 01:16:17 +0000 UTC (1 container statuses recorded)
Apr 16 02:46:20.021: INFO: 	Container nginx ready: true, restart count 0
Apr 16 02:46:20.021: INFO: 
Logging pods the apiserver thinks is on node k8s-flwang-4oyd5hkwxglz-node-1 before test
Apr 16 02:46:20.037: INFO: calico-node-nvrm8 from kube-system started at 2021-04-16 01:27:46 +0000 UTC (1 container statuses recorded)
Apr 16 02:46:20.037: INFO: 	Container calico-node ready: true, restart count 0
Apr 16 02:46:20.037: INFO: csi-cinder-nodeplugin-md46f from kube-system started at 2021-04-16 01:44:02 +0000 UTC (2 container statuses recorded)
Apr 16 02:46:20.037: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 16 02:46:20.037: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 16 02:46:20.037: INFO: magnum-prometheus-node-exporter-fcbqk from kube-system started at 2021-04-16 01:44:02 +0000 UTC (1 container statuses recorded)
Apr 16 02:46:20.037: INFO: 	Container node-exporter ready: true, restart count 0
Apr 16 02:46:20.037: INFO: npd-d29zk from kube-system started at 2021-04-16 01:28:16 +0000 UTC (1 container statuses recorded)
Apr 16 02:46:20.037: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr 16 02:46:20.037: INFO: httpd-deployment-86bff9b6d7-ttmtk from kubectl-1320 started at 2021-04-16 02:46:18 +0000 UTC (1 container statuses recorded)
Apr 16 02:46:20.037: INFO: 	Container httpd ready: false, restart count 0
Apr 16 02:46:20.037: INFO: sonobuoy from sonobuoy started at 2021-04-16 01:31:30 +0000 UTC (1 container statuses recorded)
Apr 16 02:46:20.037: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 16 02:46:20.037: INFO: sonobuoy-e2e-job-158181182f3b487c from sonobuoy started at 2021-04-16 01:31:44 +0000 UTC (2 container statuses recorded)
Apr 16 02:46:20.037: INFO: 	Container e2e ready: true, restart count 0
Apr 16 02:46:20.037: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 16 02:46:20.037: INFO: sonobuoy-systemd-logs-daemon-set-d0994422a63f4009-55xbs from sonobuoy started at 2021-04-16 01:31:44 +0000 UTC (2 container statuses recorded)
Apr 16 02:46:20.037: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Apr 16 02:46:20.037: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-4d197d4d-94bf-49c3-b66a-e98125346a01 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-4d197d4d-94bf-49c3-b66a-e98125346a01 off the node k8s-flwang-4oyd5hkwxglz-node-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4d197d4d-94bf-49c3-b66a-e98125346a01
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:46:26.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2319" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:6.782 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":257,"skipped":4330,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:46:26.388: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2777
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 02:46:26.641: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5ac3eb2-bf9d-4c0c-a804-f5817290b142" in namespace "projected-2777" to be "Succeeded or Failed"
Apr 16 02:46:26.660: INFO: Pod "downwardapi-volume-c5ac3eb2-bf9d-4c0c-a804-f5817290b142": Phase="Pending", Reason="", readiness=false. Elapsed: 18.773005ms
Apr 16 02:46:28.676: INFO: Pod "downwardapi-volume-c5ac3eb2-bf9d-4c0c-a804-f5817290b142": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034319025s
STEP: Saw pod success
Apr 16 02:46:28.676: INFO: Pod "downwardapi-volume-c5ac3eb2-bf9d-4c0c-a804-f5817290b142" satisfied condition "Succeeded or Failed"
Apr 16 02:46:28.684: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-c5ac3eb2-bf9d-4c0c-a804-f5817290b142 container client-container: <nil>
STEP: delete the pod
Apr 16 02:46:28.743: INFO: Waiting for pod downwardapi-volume-c5ac3eb2-bf9d-4c0c-a804-f5817290b142 to disappear
Apr 16 02:46:28.752: INFO: Pod downwardapi-volume-c5ac3eb2-bf9d-4c0c-a804-f5817290b142 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:46:28.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2777" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":258,"skipped":4363,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:46:28.783: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8922
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-ee49069d-f730-45f9-a77c-cd3d3686b9e5
STEP: Creating secret with name s-test-opt-upd-ac35291d-075d-4078-b500-ad728882fd5d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ee49069d-f730-45f9-a77c-cd3d3686b9e5
STEP: Updating secret s-test-opt-upd-ac35291d-075d-4078-b500-ad728882fd5d
STEP: Creating secret with name s-test-opt-create-9954a7a1-4f65-4613-b283-3d31ec92ec0e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:48:04.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8922" for this suite.

• [SLOW TEST:95.558 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":259,"skipped":4387,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:48:04.342: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-193
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-193
STEP: creating service affinity-clusterip-transition in namespace services-193
STEP: creating replication controller affinity-clusterip-transition in namespace services-193
I0416 02:48:04.646686      25 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-193, replica count: 3
I0416 02:48:07.697434      25 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 16 02:48:07.735: INFO: Creating new exec pod
Apr 16 02:48:10.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-193 exec execpod-affinitygq9r7 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Apr 16 02:48:11.226: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Apr 16 02:48:11.227: INFO: stdout: ""
Apr 16 02:48:11.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-193 exec execpod-affinitygq9r7 -- /bin/sh -x -c nc -zv -t -w 2 10.254.108.172 80'
Apr 16 02:48:11.566: INFO: stderr: "+ nc -zv -t -w 2 10.254.108.172 80\nConnection to 10.254.108.172 80 port [tcp/http] succeeded!\n"
Apr 16 02:48:11.566: INFO: stdout: ""
Apr 16 02:48:11.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-193 exec execpod-affinitygq9r7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.108.172:80/ ; done'
Apr 16 02:48:12.088: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n"
Apr 16 02:48:12.088: INFO: stdout: "\naffinity-clusterip-transition-9kbrr\naffinity-clusterip-transition-m8xpq\naffinity-clusterip-transition-9kbrr\naffinity-clusterip-transition-9kbrr\naffinity-clusterip-transition-x28jb\naffinity-clusterip-transition-m8xpq\naffinity-clusterip-transition-9kbrr\naffinity-clusterip-transition-m8xpq\naffinity-clusterip-transition-m8xpq\naffinity-clusterip-transition-9kbrr\naffinity-clusterip-transition-9kbrr\naffinity-clusterip-transition-9kbrr\naffinity-clusterip-transition-x28jb\naffinity-clusterip-transition-9kbrr\naffinity-clusterip-transition-9kbrr\naffinity-clusterip-transition-9kbrr"
Apr 16 02:48:12.088: INFO: Received response from host: affinity-clusterip-transition-9kbrr
Apr 16 02:48:12.088: INFO: Received response from host: affinity-clusterip-transition-m8xpq
Apr 16 02:48:12.088: INFO: Received response from host: affinity-clusterip-transition-9kbrr
Apr 16 02:48:12.088: INFO: Received response from host: affinity-clusterip-transition-9kbrr
Apr 16 02:48:12.088: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.088: INFO: Received response from host: affinity-clusterip-transition-m8xpq
Apr 16 02:48:12.088: INFO: Received response from host: affinity-clusterip-transition-9kbrr
Apr 16 02:48:12.088: INFO: Received response from host: affinity-clusterip-transition-m8xpq
Apr 16 02:48:12.088: INFO: Received response from host: affinity-clusterip-transition-m8xpq
Apr 16 02:48:12.088: INFO: Received response from host: affinity-clusterip-transition-9kbrr
Apr 16 02:48:12.088: INFO: Received response from host: affinity-clusterip-transition-9kbrr
Apr 16 02:48:12.088: INFO: Received response from host: affinity-clusterip-transition-9kbrr
Apr 16 02:48:12.088: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.088: INFO: Received response from host: affinity-clusterip-transition-9kbrr
Apr 16 02:48:12.088: INFO: Received response from host: affinity-clusterip-transition-9kbrr
Apr 16 02:48:12.088: INFO: Received response from host: affinity-clusterip-transition-9kbrr
Apr 16 02:48:12.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-193 exec execpod-affinitygq9r7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.108.172:80/ ; done'
Apr 16 02:48:12.689: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.108.172:80/\n"
Apr 16 02:48:12.689: INFO: stdout: "\naffinity-clusterip-transition-x28jb\naffinity-clusterip-transition-x28jb\naffinity-clusterip-transition-x28jb\naffinity-clusterip-transition-x28jb\naffinity-clusterip-transition-x28jb\naffinity-clusterip-transition-x28jb\naffinity-clusterip-transition-x28jb\naffinity-clusterip-transition-x28jb\naffinity-clusterip-transition-x28jb\naffinity-clusterip-transition-x28jb\naffinity-clusterip-transition-x28jb\naffinity-clusterip-transition-x28jb\naffinity-clusterip-transition-x28jb\naffinity-clusterip-transition-x28jb\naffinity-clusterip-transition-x28jb\naffinity-clusterip-transition-x28jb"
Apr 16 02:48:12.689: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.689: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.689: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.689: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.689: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.689: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.689: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.689: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.689: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.689: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.689: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.689: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.689: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.689: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.689: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.689: INFO: Received response from host: affinity-clusterip-transition-x28jb
Apr 16 02:48:12.689: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-193, will wait for the garbage collector to delete the pods
Apr 16 02:48:12.834: INFO: Deleting ReplicationController affinity-clusterip-transition took: 28.902397ms
Apr 16 02:48:13.935: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 1.100890653s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:48:26.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-193" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:21.738 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":260,"skipped":4436,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:48:26.081: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8567
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:48:26.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8567" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":261,"skipped":4495,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:48:26.477: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2719
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-qsrg
STEP: Creating a pod to test atomic-volume-subpath
Apr 16 02:48:26.783: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-qsrg" in namespace "subpath-2719" to be "Succeeded or Failed"
Apr 16 02:48:26.797: INFO: Pod "pod-subpath-test-projected-qsrg": Phase="Pending", Reason="", readiness=false. Elapsed: 13.86564ms
Apr 16 02:48:28.815: INFO: Pod "pod-subpath-test-projected-qsrg": Phase="Running", Reason="", readiness=true. Elapsed: 2.031514487s
Apr 16 02:48:30.830: INFO: Pod "pod-subpath-test-projected-qsrg": Phase="Running", Reason="", readiness=true. Elapsed: 4.046506099s
Apr 16 02:48:32.850: INFO: Pod "pod-subpath-test-projected-qsrg": Phase="Running", Reason="", readiness=true. Elapsed: 6.067311643s
Apr 16 02:48:34.865: INFO: Pod "pod-subpath-test-projected-qsrg": Phase="Running", Reason="", readiness=true. Elapsed: 8.082364185s
Apr 16 02:48:36.883: INFO: Pod "pod-subpath-test-projected-qsrg": Phase="Running", Reason="", readiness=true. Elapsed: 10.099679374s
Apr 16 02:48:38.906: INFO: Pod "pod-subpath-test-projected-qsrg": Phase="Running", Reason="", readiness=true. Elapsed: 12.12263682s
Apr 16 02:48:40.919: INFO: Pod "pod-subpath-test-projected-qsrg": Phase="Running", Reason="", readiness=true. Elapsed: 14.135494431s
Apr 16 02:48:42.933: INFO: Pod "pod-subpath-test-projected-qsrg": Phase="Running", Reason="", readiness=true. Elapsed: 16.14989238s
Apr 16 02:48:44.956: INFO: Pod "pod-subpath-test-projected-qsrg": Phase="Running", Reason="", readiness=true. Elapsed: 18.172561421s
Apr 16 02:48:46.976: INFO: Pod "pod-subpath-test-projected-qsrg": Phase="Running", Reason="", readiness=true. Elapsed: 20.192704194s
Apr 16 02:48:49.000: INFO: Pod "pod-subpath-test-projected-qsrg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.217120498s
STEP: Saw pod success
Apr 16 02:48:49.000: INFO: Pod "pod-subpath-test-projected-qsrg" satisfied condition "Succeeded or Failed"
Apr 16 02:48:49.010: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-subpath-test-projected-qsrg container test-container-subpath-projected-qsrg: <nil>
STEP: delete the pod
Apr 16 02:48:49.067: INFO: Waiting for pod pod-subpath-test-projected-qsrg to disappear
Apr 16 02:48:49.089: INFO: Pod pod-subpath-test-projected-qsrg no longer exists
STEP: Deleting pod pod-subpath-test-projected-qsrg
Apr 16 02:48:49.089: INFO: Deleting pod "pod-subpath-test-projected-qsrg" in namespace "subpath-2719"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:48:49.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2719" for this suite.

• [SLOW TEST:22.664 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":262,"skipped":4496,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:48:49.141: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9505
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:48:57.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9505" for this suite.

• [SLOW TEST:8.353 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":263,"skipped":4499,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:48:57.495: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8359
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Apr 16 02:49:08.036: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Apr 16 02:49:08.036: INFO: Deleting pod "simpletest-rc-to-be-deleted-65kjc" in namespace "gc-8359"
W0416 02:49:08.036640      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0416 02:49:08.036681      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0416 02:49:08.036695      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 16 02:49:08.089: INFO: Deleting pod "simpletest-rc-to-be-deleted-897bh" in namespace "gc-8359"
Apr 16 02:49:08.123: INFO: Deleting pod "simpletest-rc-to-be-deleted-8c6hf" in namespace "gc-8359"
Apr 16 02:49:08.162: INFO: Deleting pod "simpletest-rc-to-be-deleted-dxct5" in namespace "gc-8359"
Apr 16 02:49:08.273: INFO: Deleting pod "simpletest-rc-to-be-deleted-f27th" in namespace "gc-8359"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:49:08.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8359" for this suite.

• [SLOW TEST:10.868 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":264,"skipped":4510,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:49:08.363: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1362
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 16 02:49:08.717: INFO: Waiting up to 5m0s for pod "pod-0e86e41b-4e76-4745-8272-ce6f55afe71d" in namespace "emptydir-1362" to be "Succeeded or Failed"
Apr 16 02:49:08.731: INFO: Pod "pod-0e86e41b-4e76-4745-8272-ce6f55afe71d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.658761ms
Apr 16 02:49:10.747: INFO: Pod "pod-0e86e41b-4e76-4745-8272-ce6f55afe71d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02991742s
STEP: Saw pod success
Apr 16 02:49:10.747: INFO: Pod "pod-0e86e41b-4e76-4745-8272-ce6f55afe71d" satisfied condition "Succeeded or Failed"
Apr 16 02:49:10.755: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-0e86e41b-4e76-4745-8272-ce6f55afe71d container test-container: <nil>
STEP: delete the pod
Apr 16 02:49:10.806: INFO: Waiting for pod pod-0e86e41b-4e76-4745-8272-ce6f55afe71d to disappear
Apr 16 02:49:10.815: INFO: Pod pod-0e86e41b-4e76-4745-8272-ce6f55afe71d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:49:10.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1362" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":265,"skipped":4524,"failed":0}
S
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:49:10.848: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9934
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-9934
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9934 to expose endpoints map[]
Apr 16 02:49:11.166: INFO: successfully validated that service multi-endpoint-test in namespace services-9934 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9934
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9934 to expose endpoints map[pod1:[100]]
Apr 16 02:49:13.254: INFO: successfully validated that service multi-endpoint-test in namespace services-9934 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9934
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9934 to expose endpoints map[pod1:[100] pod2:[101]]
Apr 16 02:49:16.342: INFO: successfully validated that service multi-endpoint-test in namespace services-9934 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-9934
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9934 to expose endpoints map[pod2:[101]]
Apr 16 02:49:16.459: INFO: successfully validated that service multi-endpoint-test in namespace services-9934 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9934
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9934 to expose endpoints map[]
Apr 16 02:49:16.536: INFO: successfully validated that service multi-endpoint-test in namespace services-9934 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:49:16.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9934" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:5.791 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":266,"skipped":4525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:49:16.639: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1806
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-1806
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 16 02:49:16.887: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 16 02:49:17.003: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 16 02:49:19.018: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:49:21.025: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:49:23.016: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:49:25.020: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:49:27.019: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:49:29.023: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:49:31.019: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:49:33.016: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:49:35.023: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 16 02:49:37.019: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 16 02:49:37.038: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Apr 16 02:49:39.105: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Apr 16 02:49:39.105: INFO: Breadth first check of 10.100.232.205 on host 10.0.0.13...
Apr 16 02:49:39.113: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.97.93:9080/dial?request=hostname&protocol=http&host=10.100.232.205&port=8080&tries=1'] Namespace:pod-network-test-1806 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 02:49:39.113: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 02:49:39.336: INFO: Waiting for responses: map[]
Apr 16 02:49:39.336: INFO: reached 10.100.232.205 after 0/1 tries
Apr 16 02:49:39.336: INFO: Breadth first check of 10.100.97.94 on host 10.0.0.68...
Apr 16 02:49:39.349: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.97.93:9080/dial?request=hostname&protocol=http&host=10.100.97.94&port=8080&tries=1'] Namespace:pod-network-test-1806 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 16 02:49:39.349: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
Apr 16 02:49:39.565: INFO: Waiting for responses: map[]
Apr 16 02:49:39.565: INFO: reached 10.100.97.94 after 0/1 tries
Apr 16 02:49:39.565: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:49:39.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1806" for this suite.

• [SLOW TEST:22.958 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":267,"skipped":4549,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:49:39.599: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-87
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:49:39.851: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:49:41.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-87" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":268,"skipped":4563,"failed":0}

------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:49:41.252: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-4501
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:49:42.283: INFO: Checking APIGroup: apiregistration.k8s.io
Apr 16 02:49:42.286: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Apr 16 02:49:42.286: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Apr 16 02:49:42.286: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Apr 16 02:49:42.286: INFO: Checking APIGroup: apps
Apr 16 02:49:42.292: INFO: PreferredVersion.GroupVersion: apps/v1
Apr 16 02:49:42.292: INFO: Versions found [{apps/v1 v1}]
Apr 16 02:49:42.292: INFO: apps/v1 matches apps/v1
Apr 16 02:49:42.292: INFO: Checking APIGroup: events.k8s.io
Apr 16 02:49:42.296: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Apr 16 02:49:42.296: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Apr 16 02:49:42.296: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Apr 16 02:49:42.296: INFO: Checking APIGroup: authentication.k8s.io
Apr 16 02:49:42.299: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Apr 16 02:49:42.299: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Apr 16 02:49:42.299: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Apr 16 02:49:42.299: INFO: Checking APIGroup: authorization.k8s.io
Apr 16 02:49:42.303: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Apr 16 02:49:42.303: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Apr 16 02:49:42.303: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Apr 16 02:49:42.303: INFO: Checking APIGroup: autoscaling
Apr 16 02:49:42.307: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Apr 16 02:49:42.307: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Apr 16 02:49:42.307: INFO: autoscaling/v1 matches autoscaling/v1
Apr 16 02:49:42.307: INFO: Checking APIGroup: batch
Apr 16 02:49:42.311: INFO: PreferredVersion.GroupVersion: batch/v1
Apr 16 02:49:42.311: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1} {batch/v2alpha1 v2alpha1}]
Apr 16 02:49:42.311: INFO: batch/v1 matches batch/v1
Apr 16 02:49:42.311: INFO: Checking APIGroup: certificates.k8s.io
Apr 16 02:49:42.315: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Apr 16 02:49:42.315: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Apr 16 02:49:42.315: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Apr 16 02:49:42.315: INFO: Checking APIGroup: networking.k8s.io
Apr 16 02:49:42.319: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Apr 16 02:49:42.319: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Apr 16 02:49:42.319: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Apr 16 02:49:42.319: INFO: Checking APIGroup: extensions
Apr 16 02:49:42.322: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Apr 16 02:49:42.322: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Apr 16 02:49:42.323: INFO: extensions/v1beta1 matches extensions/v1beta1
Apr 16 02:49:42.323: INFO: Checking APIGroup: policy
Apr 16 02:49:42.328: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Apr 16 02:49:42.328: INFO: Versions found [{policy/v1beta1 v1beta1}]
Apr 16 02:49:42.328: INFO: policy/v1beta1 matches policy/v1beta1
Apr 16 02:49:42.328: INFO: Checking APIGroup: rbac.authorization.k8s.io
Apr 16 02:49:42.331: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Apr 16 02:49:42.331: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1} {rbac.authorization.k8s.io/v1alpha1 v1alpha1}]
Apr 16 02:49:42.331: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Apr 16 02:49:42.331: INFO: Checking APIGroup: storage.k8s.io
Apr 16 02:49:42.334: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Apr 16 02:49:42.334: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1} {storage.k8s.io/v1alpha1 v1alpha1}]
Apr 16 02:49:42.334: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Apr 16 02:49:42.334: INFO: Checking APIGroup: admissionregistration.k8s.io
Apr 16 02:49:42.337: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Apr 16 02:49:42.337: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Apr 16 02:49:42.337: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Apr 16 02:49:42.337: INFO: Checking APIGroup: apiextensions.k8s.io
Apr 16 02:49:42.341: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Apr 16 02:49:42.341: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Apr 16 02:49:42.341: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Apr 16 02:49:42.341: INFO: Checking APIGroup: scheduling.k8s.io
Apr 16 02:49:42.344: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Apr 16 02:49:42.344: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1} {scheduling.k8s.io/v1alpha1 v1alpha1}]
Apr 16 02:49:42.344: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Apr 16 02:49:42.344: INFO: Checking APIGroup: coordination.k8s.io
Apr 16 02:49:42.348: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Apr 16 02:49:42.348: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Apr 16 02:49:42.348: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Apr 16 02:49:42.348: INFO: Checking APIGroup: node.k8s.io
Apr 16 02:49:42.352: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Apr 16 02:49:42.352: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1} {node.k8s.io/v1alpha1 v1alpha1}]
Apr 16 02:49:42.352: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Apr 16 02:49:42.352: INFO: Checking APIGroup: discovery.k8s.io
Apr 16 02:49:42.356: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Apr 16 02:49:42.356: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Apr 16 02:49:42.356: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Apr 16 02:49:42.356: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Apr 16 02:49:42.360: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Apr 16 02:49:42.360: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1} {flowcontrol.apiserver.k8s.io/v1alpha1 v1alpha1}]
Apr 16 02:49:42.360: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Apr 16 02:49:42.360: INFO: Checking APIGroup: internal.apiserver.k8s.io
Apr 16 02:49:42.364: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
Apr 16 02:49:42.364: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
Apr 16 02:49:42.364: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
Apr 16 02:49:42.364: INFO: Checking APIGroup: crd.projectcalico.org
Apr 16 02:49:42.368: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Apr 16 02:49:42.368: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Apr 16 02:49:42.368: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Apr 16 02:49:42.368: INFO: Checking APIGroup: monitoring.coreos.com
Apr 16 02:49:42.371: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Apr 16 02:49:42.371: INFO: Versions found [{monitoring.coreos.com/v1 v1}]
Apr 16 02:49:42.371: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Apr 16 02:49:42.371: INFO: Checking APIGroup: snapshot.storage.k8s.io
Apr 16 02:49:42.375: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1alpha1
Apr 16 02:49:42.375: INFO: Versions found [{snapshot.storage.k8s.io/v1alpha1 v1alpha1}]
Apr 16 02:49:42.375: INFO: snapshot.storage.k8s.io/v1alpha1 matches snapshot.storage.k8s.io/v1alpha1
Apr 16 02:49:42.375: INFO: Checking APIGroup: metrics.k8s.io
Apr 16 02:49:42.378: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Apr 16 02:49:42.378: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Apr 16 02:49:42.378: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:49:42.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-4501" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":269,"skipped":4563,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:49:42.407: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9037
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:49:42.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9037" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":270,"skipped":4601,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:49:42.826: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3204
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Apr 16 02:49:43.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-3204 api-versions'
Apr 16 02:49:43.235: INFO: stderr: ""
Apr 16 02:49:43.236: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1alpha1\nflowcontrol.apiserver.k8s.io/v1beta1\ninternal.apiserver.k8s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1alpha1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1alpha1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsnapshot.storage.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1alpha1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:49:43.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3204" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":271,"skipped":4612,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:49:43.271: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3003
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:49:43.508: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:49:44.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3003" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":272,"skipped":4617,"failed":0}

------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:49:44.383: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-8161
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Apr 16 02:49:48.794: INFO: &Pod{ObjectMeta:{send-events-7ebd5669-7b9f-4651-bba8-0c58efc0adb2  events-8161  e7f150ff-d05f-4d02-959b-3ef9321c08cc 434309 0 2021-04-16 02:49:44 +0000 UTC <nil> <nil> map[name:foo time:651406531] map[cni.projectcalico.org/podIP:10.100.97.92/32 cni.projectcalico.org/podIPs:10.100.97.92/32 kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-04-16 02:49:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-16 02:49:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-16 02:49:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.97.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8xbwd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8xbwd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8xbwd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-flwang-4oyd5hkwxglz-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:49:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:49:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:49:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-16 02:49:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.68,PodIP:10.100.97.92,StartTime:2021-04-16 02:49:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-16 02:49:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://739776a5de25a11814abac2d409086d64ca45c163343feb02dc3c89e1ddea915,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.97.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Apr 16 02:49:50.817: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Apr 16 02:49:52.833: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:49:52.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8161" for this suite.

• [SLOW TEST:8.521 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":273,"skipped":4617,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:49:52.904: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1390
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-87cb1fef-1ebe-43d1-8dd4-46eade782498
STEP: Creating a pod to test consume secrets
Apr 16 02:49:53.200: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e86e022-b411-4ed7-a24c-0d647f32a83d" in namespace "projected-1390" to be "Succeeded or Failed"
Apr 16 02:49:53.210: INFO: Pod "pod-projected-secrets-8e86e022-b411-4ed7-a24c-0d647f32a83d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.109097ms
Apr 16 02:49:55.228: INFO: Pod "pod-projected-secrets-8e86e022-b411-4ed7-a24c-0d647f32a83d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027774475s
STEP: Saw pod success
Apr 16 02:49:55.228: INFO: Pod "pod-projected-secrets-8e86e022-b411-4ed7-a24c-0d647f32a83d" satisfied condition "Succeeded or Failed"
Apr 16 02:49:55.239: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-projected-secrets-8e86e022-b411-4ed7-a24c-0d647f32a83d container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 16 02:49:55.292: INFO: Waiting for pod pod-projected-secrets-8e86e022-b411-4ed7-a24c-0d647f32a83d to disappear
Apr 16 02:49:55.302: INFO: Pod pod-projected-secrets-8e86e022-b411-4ed7-a24c-0d647f32a83d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:49:55.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1390" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":274,"skipped":4621,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:49:55.333: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-2cfb5390-b6e4-4582-a57e-2894bf1e6dbc in namespace container-probe-3749
Apr 16 02:49:59.674: INFO: Started pod liveness-2cfb5390-b6e4-4582-a57e-2894bf1e6dbc in namespace container-probe-3749
STEP: checking the pod's current state and verifying that restartCount is present
Apr 16 02:49:59.683: INFO: Initial restart count of pod liveness-2cfb5390-b6e4-4582-a57e-2894bf1e6dbc is 0
Apr 16 02:50:17.959: INFO: Restart count of pod container-probe-3749/liveness-2cfb5390-b6e4-4582-a57e-2894bf1e6dbc is now 1 (18.275211494s elapsed)
Apr 16 02:50:36.163: INFO: Restart count of pod container-probe-3749/liveness-2cfb5390-b6e4-4582-a57e-2894bf1e6dbc is now 2 (36.479287675s elapsed)
Apr 16 02:50:56.383: INFO: Restart count of pod container-probe-3749/liveness-2cfb5390-b6e4-4582-a57e-2894bf1e6dbc is now 3 (56.699599815s elapsed)
Apr 16 02:51:16.547: INFO: Restart count of pod container-probe-3749/liveness-2cfb5390-b6e4-4582-a57e-2894bf1e6dbc is now 4 (1m16.862641127s elapsed)
Apr 16 02:52:19.098: INFO: Restart count of pod container-probe-3749/liveness-2cfb5390-b6e4-4582-a57e-2894bf1e6dbc is now 5 (2m19.413878166s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:52:19.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3749" for this suite.

• [SLOW TEST:143.851 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":275,"skipped":4639,"failed":0}
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:52:19.184: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-5731
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Apr 16 02:52:19.416: INFO: Major version: 1
STEP: Confirm minor version
Apr 16 02:52:19.416: INFO: cleanMinorVersion: 20
Apr 16 02:52:19.416: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:52:19.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-5731" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":276,"skipped":4639,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:52:19.457: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9220
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9220
STEP: creating service affinity-nodeport-transition in namespace services-9220
STEP: creating replication controller affinity-nodeport-transition in namespace services-9220
I0416 02:52:19.768880      25 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-9220, replica count: 3
I0416 02:52:22.819382      25 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 16 02:52:22.860: INFO: Creating new exec pod
Apr 16 02:52:25.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-9220 exec execpod-affinityfzq4s -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Apr 16 02:52:26.351: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Apr 16 02:52:26.351: INFO: stdout: ""
Apr 16 02:52:26.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-9220 exec execpod-affinityfzq4s -- /bin/sh -x -c nc -zv -t -w 2 10.254.244.253 80'
Apr 16 02:52:26.707: INFO: stderr: "+ nc -zv -t -w 2 10.254.244.253 80\nConnection to 10.254.244.253 80 port [tcp/http] succeeded!\n"
Apr 16 02:52:26.707: INFO: stdout: ""
Apr 16 02:52:26.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-9220 exec execpod-affinityfzq4s -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.13 32245'
Apr 16 02:52:27.055: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.13 32245\nConnection to 10.0.0.13 32245 port [tcp/32245] succeeded!\n"
Apr 16 02:52:27.055: INFO: stdout: ""
Apr 16 02:52:27.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-9220 exec execpod-affinityfzq4s -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.68 32245'
Apr 16 02:52:27.406: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.68 32245\nConnection to 10.0.0.68 32245 port [tcp/32245] succeeded!\n"
Apr 16 02:52:27.406: INFO: stdout: ""
Apr 16 02:52:27.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-9220 exec execpod-affinityfzq4s -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.13:32245/ ; done'
Apr 16 02:52:28.086: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n"
Apr 16 02:52:28.086: INFO: stdout: "\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-59dfg\naffinity-nodeport-transition-g7sfk\naffinity-nodeport-transition-g7sfk\naffinity-nodeport-transition-g7sfk\naffinity-nodeport-transition-g7sfk\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-59dfg\naffinity-nodeport-transition-g7sfk\naffinity-nodeport-transition-g7sfk\naffinity-nodeport-transition-59dfg\naffinity-nodeport-transition-g7sfk\naffinity-nodeport-transition-59dfg\naffinity-nodeport-transition-59dfg\naffinity-nodeport-transition-g7sfk"
Apr 16 02:52:28.086: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.086: INFO: Received response from host: affinity-nodeport-transition-59dfg
Apr 16 02:52:28.086: INFO: Received response from host: affinity-nodeport-transition-g7sfk
Apr 16 02:52:28.086: INFO: Received response from host: affinity-nodeport-transition-g7sfk
Apr 16 02:52:28.086: INFO: Received response from host: affinity-nodeport-transition-g7sfk
Apr 16 02:52:28.086: INFO: Received response from host: affinity-nodeport-transition-g7sfk
Apr 16 02:52:28.086: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.086: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.086: INFO: Received response from host: affinity-nodeport-transition-59dfg
Apr 16 02:52:28.086: INFO: Received response from host: affinity-nodeport-transition-g7sfk
Apr 16 02:52:28.086: INFO: Received response from host: affinity-nodeport-transition-g7sfk
Apr 16 02:52:28.086: INFO: Received response from host: affinity-nodeport-transition-59dfg
Apr 16 02:52:28.086: INFO: Received response from host: affinity-nodeport-transition-g7sfk
Apr 16 02:52:28.086: INFO: Received response from host: affinity-nodeport-transition-59dfg
Apr 16 02:52:28.086: INFO: Received response from host: affinity-nodeport-transition-59dfg
Apr 16 02:52:28.087: INFO: Received response from host: affinity-nodeport-transition-g7sfk
Apr 16 02:52:28.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-9220 exec execpod-affinityfzq4s -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.13:32245/ ; done'
Apr 16 02:52:28.735: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32245/\n"
Apr 16 02:52:28.735: INFO: stdout: "\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-8ck4q\naffinity-nodeport-transition-8ck4q"
Apr 16 02:52:28.735: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.735: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.735: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.735: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.735: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.735: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.735: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.735: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.735: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.735: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.735: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.735: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.735: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.735: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.735: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.735: INFO: Received response from host: affinity-nodeport-transition-8ck4q
Apr 16 02:52:28.735: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9220, will wait for the garbage collector to delete the pods
Apr 16 02:52:28.877: INFO: Deleting ReplicationController affinity-nodeport-transition took: 25.699078ms
Apr 16 02:52:32.477: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 3.600323091s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:52:36.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9220" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:17.268 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":277,"skipped":4669,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:52:36.728: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6948
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 16 02:52:36.968: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 16 02:52:37.015: INFO: Waiting for terminating namespaces to be deleted...
Apr 16 02:52:37.029: INFO: 
Logging pods the apiserver thinks is on node k8s-flwang-4oyd5hkwxglz-node-0 before test
Apr 16 02:52:37.049: INFO: alertmanager-magnum-prometheus-operator-alertmanager-0 from kube-system started at 2021-04-14 03:28:17 +0000 UTC (2 container statuses recorded)
Apr 16 02:52:37.049: INFO: 	Container alertmanager ready: true, restart count 0
Apr 16 02:52:37.049: INFO: 	Container config-reloader ready: true, restart count 0
Apr 16 02:52:37.049: INFO: calico-node-8pzl7 from kube-system started at 2021-04-14 03:25:24 +0000 UTC (1 container statuses recorded)
Apr 16 02:52:37.049: INFO: 	Container calico-node ready: true, restart count 0
Apr 16 02:52:37.049: INFO: csi-cinder-nodeplugin-xngdr from kube-system started at 2021-04-14 03:25:44 +0000 UTC (2 container statuses recorded)
Apr 16 02:52:37.049: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 16 02:52:37.049: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 16 02:52:37.050: INFO: kube-dns-autoscaler-7d66dbddbc-mmpzq from kube-system started at 2021-04-14 03:25:55 +0000 UTC (1 container statuses recorded)
Apr 16 02:52:37.050: INFO: 	Container autoscaler ready: true, restart count 0
Apr 16 02:52:37.050: INFO: magnum-grafana-57b59f6cd-lmchw from kube-system started at 2021-04-14 03:27:41 +0000 UTC (2 container statuses recorded)
Apr 16 02:52:37.050: INFO: 	Container grafana ready: true, restart count 0
Apr 16 02:52:37.050: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Apr 16 02:52:37.050: INFO: magnum-kube-state-metrics-7f877f89d8-9685l from kube-system started at 2021-04-14 03:27:41 +0000 UTC (1 container statuses recorded)
Apr 16 02:52:37.050: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 16 02:52:37.050: INFO: magnum-metrics-server-6b85d69787-r9lnz from kube-system started at 2021-04-14 03:27:41 +0000 UTC (1 container statuses recorded)
Apr 16 02:52:37.050: INFO: 	Container metrics-server ready: true, restart count 0
Apr 16 02:52:37.050: INFO: magnum-prometheus-node-exporter-zs5ln from kube-system started at 2021-04-14 03:27:41 +0000 UTC (1 container statuses recorded)
Apr 16 02:52:37.051: INFO: 	Container node-exporter ready: true, restart count 0
Apr 16 02:52:37.051: INFO: magnum-prometheus-operator-operator-6b8b498699-pwg7j from kube-system started at 2021-04-14 03:27:41 +0000 UTC (2 container statuses recorded)
Apr 16 02:52:37.051: INFO: 	Container prometheus-operator ready: true, restart count 0
Apr 16 02:52:37.051: INFO: 	Container tls-proxy ready: true, restart count 0
Apr 16 02:52:37.051: INFO: npd-nxkwt from kube-system started at 2021-04-14 03:25:44 +0000 UTC (1 container statuses recorded)
Apr 16 02:52:37.051: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr 16 02:52:37.051: INFO: prometheus-magnum-prometheus-operator-prometheus-0 from kube-system started at 2021-04-14 03:28:28 +0000 UTC (3 container statuses recorded)
Apr 16 02:52:37.051: INFO: 	Container prometheus ready: true, restart count 1
Apr 16 02:52:37.051: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Apr 16 02:52:37.051: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Apr 16 02:52:37.051: INFO: sonobuoy-systemd-logs-daemon-set-d0994422a63f4009-7q9k8 from sonobuoy started at 2021-04-16 01:31:44 +0000 UTC (2 container statuses recorded)
Apr 16 02:52:37.052: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Apr 16 02:52:37.052: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 16 02:52:37.052: INFO: busybox from test-k8s started at 2021-04-16 01:17:41 +0000 UTC (1 container statuses recorded)
Apr 16 02:52:37.052: INFO: 	Container busybox ready: true, restart count 1
Apr 16 02:52:37.052: INFO: nginx-7848d4b86f-4q96s from test-k8s started at 2021-04-16 01:16:17 +0000 UTC (1 container statuses recorded)
Apr 16 02:52:37.052: INFO: 	Container nginx ready: true, restart count 0
Apr 16 02:52:37.052: INFO: nginx-7848d4b86f-bmzz7 from test-k8s started at 2021-04-16 01:16:17 +0000 UTC (1 container statuses recorded)
Apr 16 02:52:37.052: INFO: 	Container nginx ready: true, restart count 0
Apr 16 02:52:37.052: INFO: 
Logging pods the apiserver thinks is on node k8s-flwang-4oyd5hkwxglz-node-1 before test
Apr 16 02:52:37.072: INFO: calico-node-nvrm8 from kube-system started at 2021-04-16 01:27:46 +0000 UTC (1 container statuses recorded)
Apr 16 02:52:37.072: INFO: 	Container calico-node ready: true, restart count 0
Apr 16 02:52:37.072: INFO: csi-cinder-nodeplugin-md46f from kube-system started at 2021-04-16 01:44:02 +0000 UTC (2 container statuses recorded)
Apr 16 02:52:37.072: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Apr 16 02:52:37.072: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 16 02:52:37.072: INFO: magnum-prometheus-node-exporter-fcbqk from kube-system started at 2021-04-16 01:44:02 +0000 UTC (1 container statuses recorded)
Apr 16 02:52:37.072: INFO: 	Container node-exporter ready: true, restart count 0
Apr 16 02:52:37.072: INFO: npd-d29zk from kube-system started at 2021-04-16 01:28:16 +0000 UTC (1 container statuses recorded)
Apr 16 02:52:37.072: INFO: 	Container node-problem-detector ready: true, restart count 0
Apr 16 02:52:37.072: INFO: sonobuoy from sonobuoy started at 2021-04-16 01:31:30 +0000 UTC (1 container statuses recorded)
Apr 16 02:52:37.073: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 16 02:52:37.073: INFO: sonobuoy-e2e-job-158181182f3b487c from sonobuoy started at 2021-04-16 01:31:44 +0000 UTC (2 container statuses recorded)
Apr 16 02:52:37.073: INFO: 	Container e2e ready: true, restart count 0
Apr 16 02:52:37.073: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 16 02:52:37.073: INFO: sonobuoy-systemd-logs-daemon-set-d0994422a63f4009-55xbs from sonobuoy started at 2021-04-16 01:31:44 +0000 UTC (2 container statuses recorded)
Apr 16 02:52:37.073: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Apr 16 02:52:37.073: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.167636f8881095ec], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) didn't match Pod's node affinity, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:52:38.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6948" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":278,"skipped":4689,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:52:38.196: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-9358
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Apr 16 02:52:38.447: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 16 02:53:38.532: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:53:38.544: INFO: Starting informer...
STEP: Starting pods...
Apr 16 02:53:38.814: INFO: Pod1 is running on k8s-flwang-4oyd5hkwxglz-node-1. Tainting Node
Apr 16 02:53:41.090: INFO: Pod2 is running on k8s-flwang-4oyd5hkwxglz-node-1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Apr 16 02:53:55.825: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Apr 16 02:54:15.840: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:54:15.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9358" for this suite.

• [SLOW TEST:97.762 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":279,"skipped":4717,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:54:15.958: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3400
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-33dcdda3-2555-4047-9bdb-4befbaa36d7e
STEP: Creating a pod to test consume secrets
Apr 16 02:54:16.384: INFO: Waiting up to 5m0s for pod "pod-secrets-032f3726-c7df-424c-abf9-fbd548dafbb8" in namespace "secrets-3400" to be "Succeeded or Failed"
Apr 16 02:54:16.403: INFO: Pod "pod-secrets-032f3726-c7df-424c-abf9-fbd548dafbb8": Phase="Pending", Reason="", readiness=false. Elapsed: 19.473587ms
Apr 16 02:54:18.419: INFO: Pod "pod-secrets-032f3726-c7df-424c-abf9-fbd548dafbb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035389099s
Apr 16 02:54:20.437: INFO: Pod "pod-secrets-032f3726-c7df-424c-abf9-fbd548dafbb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052979601s
STEP: Saw pod success
Apr 16 02:54:20.437: INFO: Pod "pod-secrets-032f3726-c7df-424c-abf9-fbd548dafbb8" satisfied condition "Succeeded or Failed"
Apr 16 02:54:20.445: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-secrets-032f3726-c7df-424c-abf9-fbd548dafbb8 container secret-volume-test: <nil>
STEP: delete the pod
Apr 16 02:54:20.633: INFO: Waiting for pod pod-secrets-032f3726-c7df-424c-abf9-fbd548dafbb8 to disappear
Apr 16 02:54:20.646: INFO: Pod pod-secrets-032f3726-c7df-424c-abf9-fbd548dafbb8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:54:20.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3400" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":280,"skipped":4743,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:54:20.683: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8805
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 02:54:20.936: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d6375c35-b113-4ab4-8ed0-c1d2e1387c59" in namespace "projected-8805" to be "Succeeded or Failed"
Apr 16 02:54:20.956: INFO: Pod "downwardapi-volume-d6375c35-b113-4ab4-8ed0-c1d2e1387c59": Phase="Pending", Reason="", readiness=false. Elapsed: 19.704041ms
Apr 16 02:54:22.975: INFO: Pod "downwardapi-volume-d6375c35-b113-4ab4-8ed0-c1d2e1387c59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03814423s
STEP: Saw pod success
Apr 16 02:54:22.975: INFO: Pod "downwardapi-volume-d6375c35-b113-4ab4-8ed0-c1d2e1387c59" satisfied condition "Succeeded or Failed"
Apr 16 02:54:22.983: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-d6375c35-b113-4ab4-8ed0-c1d2e1387c59 container client-container: <nil>
STEP: delete the pod
Apr 16 02:54:23.045: INFO: Waiting for pod downwardapi-volume-d6375c35-b113-4ab4-8ed0-c1d2e1387c59 to disappear
Apr 16 02:54:23.054: INFO: Pod downwardapi-volume-d6375c35-b113-4ab4-8ed0-c1d2e1387c59 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:54:23.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8805" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":281,"skipped":4777,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:54:23.087: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9683
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Apr 16 02:54:23.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 create -f -'
Apr 16 02:54:23.883: INFO: stderr: ""
Apr 16 02:54:23.883: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 16 02:54:23.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 16 02:54:24.095: INFO: stderr: ""
Apr 16 02:54:24.095: INFO: stdout: "update-demo-nautilus-lwlqj update-demo-nautilus-qq4f2 "
Apr 16 02:54:24.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods update-demo-nautilus-lwlqj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 16 02:54:24.290: INFO: stderr: ""
Apr 16 02:54:24.290: INFO: stdout: ""
Apr 16 02:54:24.290: INFO: update-demo-nautilus-lwlqj is created but not running
Apr 16 02:54:29.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 16 02:54:29.468: INFO: stderr: ""
Apr 16 02:54:29.468: INFO: stdout: "update-demo-nautilus-lwlqj update-demo-nautilus-qq4f2 "
Apr 16 02:54:29.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods update-demo-nautilus-lwlqj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 16 02:54:29.618: INFO: stderr: ""
Apr 16 02:54:29.618: INFO: stdout: "true"
Apr 16 02:54:29.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods update-demo-nautilus-lwlqj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 16 02:54:29.753: INFO: stderr: ""
Apr 16 02:54:29.753: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 16 02:54:29.753: INFO: validating pod update-demo-nautilus-lwlqj
Apr 16 02:54:29.768: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 16 02:54:29.768: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 16 02:54:29.769: INFO: update-demo-nautilus-lwlqj is verified up and running
Apr 16 02:54:29.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods update-demo-nautilus-qq4f2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 16 02:54:29.924: INFO: stderr: ""
Apr 16 02:54:29.924: INFO: stdout: "true"
Apr 16 02:54:29.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods update-demo-nautilus-qq4f2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 16 02:54:30.092: INFO: stderr: ""
Apr 16 02:54:30.092: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 16 02:54:30.092: INFO: validating pod update-demo-nautilus-qq4f2
Apr 16 02:54:30.114: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 16 02:54:30.114: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 16 02:54:30.114: INFO: update-demo-nautilus-qq4f2 is verified up and running
STEP: scaling down the replication controller
Apr 16 02:54:30.118: INFO: scanned /root for discovery docs: <nil>
Apr 16 02:54:30.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Apr 16 02:54:30.375: INFO: stderr: ""
Apr 16 02:54:30.375: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 16 02:54:30.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 16 02:54:30.521: INFO: stderr: ""
Apr 16 02:54:30.521: INFO: stdout: "update-demo-nautilus-lwlqj update-demo-nautilus-qq4f2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 16 02:54:35.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 16 02:54:35.671: INFO: stderr: ""
Apr 16 02:54:35.671: INFO: stdout: "update-demo-nautilus-lwlqj update-demo-nautilus-qq4f2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 16 02:54:40.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 16 02:54:40.852: INFO: stderr: ""
Apr 16 02:54:40.852: INFO: stdout: "update-demo-nautilus-qq4f2 "
Apr 16 02:54:40.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods update-demo-nautilus-qq4f2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 16 02:54:41.015: INFO: stderr: ""
Apr 16 02:54:41.015: INFO: stdout: "true"
Apr 16 02:54:41.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods update-demo-nautilus-qq4f2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 16 02:54:41.168: INFO: stderr: ""
Apr 16 02:54:41.168: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 16 02:54:41.168: INFO: validating pod update-demo-nautilus-qq4f2
Apr 16 02:54:41.180: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 16 02:54:41.180: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 16 02:54:41.180: INFO: update-demo-nautilus-qq4f2 is verified up and running
STEP: scaling up the replication controller
Apr 16 02:54:41.187: INFO: scanned /root for discovery docs: <nil>
Apr 16 02:54:41.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Apr 16 02:54:42.396: INFO: stderr: ""
Apr 16 02:54:42.396: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 16 02:54:42.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 16 02:54:42.588: INFO: stderr: ""
Apr 16 02:54:42.588: INFO: stdout: "update-demo-nautilus-qq4f2 update-demo-nautilus-t4wb6 "
Apr 16 02:54:42.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods update-demo-nautilus-qq4f2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 16 02:54:42.764: INFO: stderr: ""
Apr 16 02:54:42.764: INFO: stdout: "true"
Apr 16 02:54:42.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods update-demo-nautilus-qq4f2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 16 02:54:42.925: INFO: stderr: ""
Apr 16 02:54:42.925: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 16 02:54:42.925: INFO: validating pod update-demo-nautilus-qq4f2
Apr 16 02:54:42.937: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 16 02:54:42.937: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 16 02:54:42.937: INFO: update-demo-nautilus-qq4f2 is verified up and running
Apr 16 02:54:42.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods update-demo-nautilus-t4wb6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 16 02:54:43.084: INFO: stderr: ""
Apr 16 02:54:43.084: INFO: stdout: "true"
Apr 16 02:54:43.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods update-demo-nautilus-t4wb6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 16 02:54:43.257: INFO: stderr: ""
Apr 16 02:54:43.257: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 16 02:54:43.257: INFO: validating pod update-demo-nautilus-t4wb6
Apr 16 02:54:43.272: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 16 02:54:43.272: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 16 02:54:43.272: INFO: update-demo-nautilus-t4wb6 is verified up and running
STEP: using delete to clean up resources
Apr 16 02:54:43.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 delete --grace-period=0 --force -f -'
Apr 16 02:54:43.467: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 16 02:54:43.467: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 16 02:54:43.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get rc,svc -l name=update-demo --no-headers'
Apr 16 02:54:43.662: INFO: stderr: "No resources found in kubectl-9683 namespace.\n"
Apr 16 02:54:43.662: INFO: stdout: ""
Apr 16 02:54:43.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 16 02:54:43.816: INFO: stderr: ""
Apr 16 02:54:43.816: INFO: stdout: "update-demo-nautilus-qq4f2\nupdate-demo-nautilus-t4wb6\n"
Apr 16 02:54:44.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get rc,svc -l name=update-demo --no-headers'
Apr 16 02:54:44.507: INFO: stderr: "No resources found in kubectl-9683 namespace.\n"
Apr 16 02:54:44.507: INFO: stdout: ""
Apr 16 02:54:44.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=kubectl-9683 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 16 02:54:44.643: INFO: stderr: ""
Apr 16 02:54:44.643: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:54:44.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9683" for this suite.

• [SLOW TEST:21.591 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":282,"skipped":4857,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:54:44.679: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4802
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-5c407dd7-92d9-4713-8d16-b87701e71640
STEP: Creating a pod to test consume secrets
Apr 16 02:54:44.952: INFO: Waiting up to 5m0s for pod "pod-secrets-a39190b0-9172-40c6-bbe5-797bdc5db098" in namespace "secrets-4802" to be "Succeeded or Failed"
Apr 16 02:54:44.969: INFO: Pod "pod-secrets-a39190b0-9172-40c6-bbe5-797bdc5db098": Phase="Pending", Reason="", readiness=false. Elapsed: 16.769208ms
Apr 16 02:54:46.985: INFO: Pod "pod-secrets-a39190b0-9172-40c6-bbe5-797bdc5db098": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033236492s
STEP: Saw pod success
Apr 16 02:54:46.985: INFO: Pod "pod-secrets-a39190b0-9172-40c6-bbe5-797bdc5db098" satisfied condition "Succeeded or Failed"
Apr 16 02:54:46.993: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-secrets-a39190b0-9172-40c6-bbe5-797bdc5db098 container secret-volume-test: <nil>
STEP: delete the pod
Apr 16 02:54:47.057: INFO: Waiting for pod pod-secrets-a39190b0-9172-40c6-bbe5-797bdc5db098 to disappear
Apr 16 02:54:47.067: INFO: Pod pod-secrets-a39190b0-9172-40c6-bbe5-797bdc5db098 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:54:47.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4802" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":283,"skipped":4883,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:54:47.096: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2421
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Apr 16 02:54:47.416: INFO: Waiting up to 5m0s for pod "pod-be611261-567e-49e4-9936-debc9d1f28bc" in namespace "emptydir-2421" to be "Succeeded or Failed"
Apr 16 02:54:47.438: INFO: Pod "pod-be611261-567e-49e4-9936-debc9d1f28bc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.807339ms
Apr 16 02:54:49.451: INFO: Pod "pod-be611261-567e-49e4-9936-debc9d1f28bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034434985s
STEP: Saw pod success
Apr 16 02:54:49.451: INFO: Pod "pod-be611261-567e-49e4-9936-debc9d1f28bc" satisfied condition "Succeeded or Failed"
Apr 16 02:54:49.459: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-be611261-567e-49e4-9936-debc9d1f28bc container test-container: <nil>
STEP: delete the pod
Apr 16 02:54:49.515: INFO: Waiting for pod pod-be611261-567e-49e4-9936-debc9d1f28bc to disappear
Apr 16 02:54:49.527: INFO: Pod pod-be611261-567e-49e4-9936-debc9d1f28bc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:54:49.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2421" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":284,"skipped":4907,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:54:49.558: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 16 02:54:50.638: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 16 02:54:52.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754138490, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754138490, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754138490, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754138490, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 16 02:54:55.725: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:54:55.744: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2576-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:54:57.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6909" for this suite.
STEP: Destroying namespace "webhook-6909-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.827 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":285,"skipped":4914,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:54:57.386: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1022
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 02:54:57.811: INFO: Waiting up to 5m0s for pod "downwardapi-volume-86c20931-7e9f-40e5-b42d-26ad18188b0e" in namespace "downward-api-1022" to be "Succeeded or Failed"
Apr 16 02:54:57.832: INFO: Pod "downwardapi-volume-86c20931-7e9f-40e5-b42d-26ad18188b0e": Phase="Pending", Reason="", readiness=false. Elapsed: 21.367115ms
Apr 16 02:54:59.846: INFO: Pod "downwardapi-volume-86c20931-7e9f-40e5-b42d-26ad18188b0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035275137s
Apr 16 02:55:01.884: INFO: Pod "downwardapi-volume-86c20931-7e9f-40e5-b42d-26ad18188b0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072932655s
STEP: Saw pod success
Apr 16 02:55:01.884: INFO: Pod "downwardapi-volume-86c20931-7e9f-40e5-b42d-26ad18188b0e" satisfied condition "Succeeded or Failed"
Apr 16 02:55:01.903: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-86c20931-7e9f-40e5-b42d-26ad18188b0e container client-container: <nil>
STEP: delete the pod
Apr 16 02:55:01.973: INFO: Waiting for pod downwardapi-volume-86c20931-7e9f-40e5-b42d-26ad18188b0e to disappear
Apr 16 02:55:01.982: INFO: Pod downwardapi-volume-86c20931-7e9f-40e5-b42d-26ad18188b0e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:55:01.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1022" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":286,"skipped":4931,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:55:02.013: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-259
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-392d575c-9338-4421-8383-b56efcfa9364
STEP: Creating a pod to test consume secrets
Apr 16 02:55:02.299: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0472cae6-26a3-4e27-b185-5efc6b3a4875" in namespace "projected-259" to be "Succeeded or Failed"
Apr 16 02:55:02.312: INFO: Pod "pod-projected-secrets-0472cae6-26a3-4e27-b185-5efc6b3a4875": Phase="Pending", Reason="", readiness=false. Elapsed: 12.850522ms
Apr 16 02:55:04.428: INFO: Pod "pod-projected-secrets-0472cae6-26a3-4e27-b185-5efc6b3a4875": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.128692671s
STEP: Saw pod success
Apr 16 02:55:04.428: INFO: Pod "pod-projected-secrets-0472cae6-26a3-4e27-b185-5efc6b3a4875" satisfied condition "Succeeded or Failed"
Apr 16 02:55:04.441: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-projected-secrets-0472cae6-26a3-4e27-b185-5efc6b3a4875 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 16 02:55:04.499: INFO: Waiting for pod pod-projected-secrets-0472cae6-26a3-4e27-b185-5efc6b3a4875 to disappear
Apr 16 02:55:04.508: INFO: Pod pod-projected-secrets-0472cae6-26a3-4e27-b185-5efc6b3a4875 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:55:04.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-259" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":287,"skipped":4945,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:55:04.545: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4361
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-f5417328-d9e4-475b-94af-d7f4d15b1db0
Apr 16 02:55:04.804: INFO: Pod name my-hostname-basic-f5417328-d9e4-475b-94af-d7f4d15b1db0: Found 0 pods out of 1
Apr 16 02:55:09.839: INFO: Pod name my-hostname-basic-f5417328-d9e4-475b-94af-d7f4d15b1db0: Found 1 pods out of 1
Apr 16 02:55:09.839: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f5417328-d9e4-475b-94af-d7f4d15b1db0" are running
Apr 16 02:55:09.849: INFO: Pod "my-hostname-basic-f5417328-d9e4-475b-94af-d7f4d15b1db0-vbfbp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-16 02:55:04 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-16 02:55:06 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-16 02:55:06 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-16 02:55:04 +0000 UTC Reason: Message:}])
Apr 16 02:55:09.850: INFO: Trying to dial the pod
Apr 16 02:55:14.882: INFO: Controller my-hostname-basic-f5417328-d9e4-475b-94af-d7f4d15b1db0: Got expected result from replica 1 [my-hostname-basic-f5417328-d9e4-475b-94af-d7f4d15b1db0-vbfbp]: "my-hostname-basic-f5417328-d9e4-475b-94af-d7f4d15b1db0-vbfbp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:55:14.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4361" for this suite.

• [SLOW TEST:10.378 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":288,"skipped":4946,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:55:14.925: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2014
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-1b92d1c0-5277-4a4b-bc65-2519ed175101
STEP: Creating secret with name s-test-opt-upd-d53387c5-4778-40bb-a48f-989a620583e4
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-1b92d1c0-5277-4a4b-bc65-2519ed175101
STEP: Updating secret s-test-opt-upd-d53387c5-4778-40bb-a48f-989a620583e4
STEP: Creating secret with name s-test-opt-create-06fc0da5-f5d2-4ad8-bef2-dfe379982231
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:55:19.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2014" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":289,"skipped":4951,"failed":0}

------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:55:19.509: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1696
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Apr 16 02:55:24.889: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:55:25.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1696" for this suite.

• [SLOW TEST:6.477 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":290,"skipped":4951,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:55:25.986: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1354
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1354.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1354.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1354.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1354.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 16 02:55:30.430: INFO: DNS probes using dns-test-37a7b46c-3da1-49a4-b1ab-a821d4d7cc6d succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1354.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1354.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1354.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1354.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 16 02:55:34.596: INFO: DNS probes using dns-test-a7bd5eb1-975a-4c01-8f8b-0b5b4db722d4 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1354.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1354.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1354.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1354.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 16 02:55:38.934: INFO: DNS probes using dns-test-7b2c3b0c-da02-4a76-9d1b-f047d8b56dee succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:55:39.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1354" for this suite.

• [SLOW TEST:13.073 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":291,"skipped":4969,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:55:39.061: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3298
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-06e14524-c770-4c65-ba4f-f038e1b504b6
STEP: Creating a pod to test consume configMaps
Apr 16 02:55:39.379: INFO: Waiting up to 5m0s for pod "pod-configmaps-febc4869-a845-4197-acfd-d1b0cb486258" in namespace "configmap-3298" to be "Succeeded or Failed"
Apr 16 02:55:39.400: INFO: Pod "pod-configmaps-febc4869-a845-4197-acfd-d1b0cb486258": Phase="Pending", Reason="", readiness=false. Elapsed: 20.786339ms
Apr 16 02:55:41.414: INFO: Pod "pod-configmaps-febc4869-a845-4197-acfd-d1b0cb486258": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03508587s
STEP: Saw pod success
Apr 16 02:55:41.414: INFO: Pod "pod-configmaps-febc4869-a845-4197-acfd-d1b0cb486258" satisfied condition "Succeeded or Failed"
Apr 16 02:55:41.422: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-configmaps-febc4869-a845-4197-acfd-d1b0cb486258 container agnhost-container: <nil>
STEP: delete the pod
Apr 16 02:55:41.515: INFO: Waiting for pod pod-configmaps-febc4869-a845-4197-acfd-d1b0cb486258 to disappear
Apr 16 02:55:41.524: INFO: Pod pod-configmaps-febc4869-a845-4197-acfd-d1b0cb486258 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:55:41.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3298" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":292,"skipped":4983,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:55:41.559: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8600
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:55:41.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8600" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":293,"skipped":4995,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:55:42.031: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2087
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2087.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2087.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2087.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2087.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2087.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2087.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2087.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2087.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2087.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2087.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 16 02:55:44.441: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:44.454: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:44.464: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2087.svc.cluster.local from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:44.475: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2087.svc.cluster.local from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:44.488: INFO: Unable to read wheezy_udp@PodARecord from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:44.503: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:44.521: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:44.529: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:44.536: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2087.svc.cluster.local from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:44.545: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2087.svc.cluster.local from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:44.559: INFO: Unable to read jessie_udp@PodARecord from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:44.569: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:44.569: INFO: Lookups using dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2087.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2087.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local jessie_udp@dns-test-service-2.dns-2087.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2087.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Apr 16 02:55:49.588: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:49.596: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2087.svc.cluster.local from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:49.606: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2087.svc.cluster.local from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:49.628: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:49.635: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:49.645: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2087.svc.cluster.local from pod dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b: the server could not find the requested resource (get pods dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b)
Apr 16 02:55:49.677: INFO: Lookups using dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b failed for: [wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2087.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2087.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2087.svc.cluster.local jessie_udp@dns-test-service-2.dns-2087.svc.cluster.local]

Apr 16 02:55:54.696: INFO: DNS probes using dns-2087/dns-test-d4a0a075-6c36-40a8-84e8-1737e6df1d9b succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:55:54.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2087" for this suite.

• [SLOW TEST:12.912 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":294,"skipped":5045,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:55:54.945: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8154
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 16 02:55:55.211: INFO: Waiting up to 5m0s for pod "downward-api-ff834a2f-53ea-4921-8965-026232834f8c" in namespace "downward-api-8154" to be "Succeeded or Failed"
Apr 16 02:55:55.237: INFO: Pod "downward-api-ff834a2f-53ea-4921-8965-026232834f8c": Phase="Pending", Reason="", readiness=false. Elapsed: 26.082081ms
Apr 16 02:55:57.254: INFO: Pod "downward-api-ff834a2f-53ea-4921-8965-026232834f8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042965989s
Apr 16 02:55:59.271: INFO: Pod "downward-api-ff834a2f-53ea-4921-8965-026232834f8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05989119s
STEP: Saw pod success
Apr 16 02:55:59.271: INFO: Pod "downward-api-ff834a2f-53ea-4921-8965-026232834f8c" satisfied condition "Succeeded or Failed"
Apr 16 02:55:59.280: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downward-api-ff834a2f-53ea-4921-8965-026232834f8c container dapi-container: <nil>
STEP: delete the pod
Apr 16 02:55:59.334: INFO: Waiting for pod downward-api-ff834a2f-53ea-4921-8965-026232834f8c to disappear
Apr 16 02:55:59.345: INFO: Pod downward-api-ff834a2f-53ea-4921-8965-026232834f8c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:55:59.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8154" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":295,"skipped":5064,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:55:59.389: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2143
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-bc0a97a2-55c2-4cf4-99be-641f21492776 in namespace container-probe-2143
Apr 16 02:56:01.689: INFO: Started pod busybox-bc0a97a2-55c2-4cf4-99be-641f21492776 in namespace container-probe-2143
STEP: checking the pod's current state and verifying that restartCount is present
Apr 16 02:56:01.698: INFO: Initial restart count of pod busybox-bc0a97a2-55c2-4cf4-99be-641f21492776 is 0
Apr 16 02:56:48.095: INFO: Restart count of pod container-probe-2143/busybox-bc0a97a2-55c2-4cf4-99be-641f21492776 is now 1 (46.397158384s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:56:48.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2143" for this suite.

• [SLOW TEST:48.797 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":296,"skipped":5073,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:56:48.187: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3696
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:57:16.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3696" for this suite.

• [SLOW TEST:28.475 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":297,"skipped":5075,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:57:16.665: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9431
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 02:57:16.976: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Apr 16 02:57:17.027: INFO: Number of nodes with available pods: 0
Apr 16 02:57:17.027: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Apr 16 02:57:17.091: INFO: Number of nodes with available pods: 0
Apr 16 02:57:17.092: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:18.106: INFO: Number of nodes with available pods: 0
Apr 16 02:57:18.106: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:19.104: INFO: Number of nodes with available pods: 0
Apr 16 02:57:19.104: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:20.108: INFO: Number of nodes with available pods: 1
Apr 16 02:57:20.108: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Apr 16 02:57:20.174: INFO: Number of nodes with available pods: 1
Apr 16 02:57:20.174: INFO: Number of running nodes: 0, number of available pods: 1
Apr 16 02:57:21.191: INFO: Number of nodes with available pods: 0
Apr 16 02:57:21.192: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Apr 16 02:57:21.227: INFO: Number of nodes with available pods: 0
Apr 16 02:57:21.227: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:22.238: INFO: Number of nodes with available pods: 0
Apr 16 02:57:22.238: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:23.251: INFO: Number of nodes with available pods: 0
Apr 16 02:57:23.251: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:24.252: INFO: Number of nodes with available pods: 0
Apr 16 02:57:24.252: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:25.241: INFO: Number of nodes with available pods: 0
Apr 16 02:57:25.241: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:26.244: INFO: Number of nodes with available pods: 0
Apr 16 02:57:26.244: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:27.241: INFO: Number of nodes with available pods: 0
Apr 16 02:57:27.241: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:28.254: INFO: Number of nodes with available pods: 0
Apr 16 02:57:28.254: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:29.239: INFO: Number of nodes with available pods: 0
Apr 16 02:57:29.239: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:30.240: INFO: Number of nodes with available pods: 0
Apr 16 02:57:30.240: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:31.242: INFO: Number of nodes with available pods: 0
Apr 16 02:57:31.242: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:32.240: INFO: Number of nodes with available pods: 0
Apr 16 02:57:32.240: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:33.240: INFO: Number of nodes with available pods: 0
Apr 16 02:57:33.240: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:34.255: INFO: Number of nodes with available pods: 0
Apr 16 02:57:34.255: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:35.244: INFO: Number of nodes with available pods: 0
Apr 16 02:57:35.244: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:36.258: INFO: Number of nodes with available pods: 0
Apr 16 02:57:36.258: INFO: Node k8s-flwang-4oyd5hkwxglz-node-0 is running more than one daemon pod
Apr 16 02:57:37.242: INFO: Number of nodes with available pods: 1
Apr 16 02:57:37.242: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9431, will wait for the garbage collector to delete the pods
Apr 16 02:57:37.353: INFO: Deleting DaemonSet.extensions daemon-set took: 28.016143ms
Apr 16 02:57:38.453: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.100364442s
Apr 16 02:57:40.591: INFO: Number of nodes with available pods: 0
Apr 16 02:57:40.591: INFO: Number of running nodes: 0, number of available pods: 0
Apr 16 02:57:40.602: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"437014"},"items":null}

Apr 16 02:57:40.609: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"437014"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 02:57:40.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9431" for this suite.

• [SLOW TEST:24.047 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":298,"skipped":5110,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 02:57:40.712: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-727f68cb-fe13-4035-990f-2ed129e2a606 in namespace container-probe-9696
Apr 16 02:57:43.069: INFO: Started pod test-webserver-727f68cb-fe13-4035-990f-2ed129e2a606 in namespace container-probe-9696
STEP: checking the pod's current state and verifying that restartCount is present
Apr 16 02:57:43.076: INFO: Initial restart count of pod test-webserver-727f68cb-fe13-4035-990f-2ed129e2a606 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 03:01:43.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9696" for this suite.

• [SLOW TEST:242.584 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":299,"skipped":5118,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 03:01:43.300: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3861
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3861, will wait for the garbage collector to delete the pods
Apr 16 03:01:45.649: INFO: Deleting Job.batch foo took: 27.057911ms
Apr 16 03:01:46.749: INFO: Terminating Job.batch foo pods took: 1.100238724s
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 03:02:25.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3861" for this suite.

• [SLOW TEST:42.718 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":300,"skipped":5129,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 03:02:26.033: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-779
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 16 03:02:26.308: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: creating replication controller svc-latency-rc in namespace svc-latency-779
I0416 03:02:26.347722      25 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-779, replica count: 1
I0416 03:02:27.398289      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0416 03:02:28.398857      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 16 03:02:28.538: INFO: Created: latency-svc-n2962
Apr 16 03:02:28.565: INFO: Got endpoints: latency-svc-n2962 [66.760083ms]
Apr 16 03:02:28.604: INFO: Created: latency-svc-zhpkz
Apr 16 03:02:28.612: INFO: Got endpoints: latency-svc-zhpkz [46.089557ms]
Apr 16 03:02:28.629: INFO: Created: latency-svc-5bf8p
Apr 16 03:02:28.645: INFO: Got endpoints: latency-svc-5bf8p [79.519871ms]
Apr 16 03:02:28.657: INFO: Created: latency-svc-nvd48
Apr 16 03:02:28.680: INFO: Got endpoints: latency-svc-nvd48 [113.938081ms]
Apr 16 03:02:28.687: INFO: Created: latency-svc-vsddl
Apr 16 03:02:28.710: INFO: Got endpoints: latency-svc-vsddl [143.285917ms]
Apr 16 03:02:28.720: INFO: Created: latency-svc-gcbvg
Apr 16 03:02:28.733: INFO: Got endpoints: latency-svc-gcbvg [167.765674ms]
Apr 16 03:02:28.751: INFO: Created: latency-svc-2jrxd
Apr 16 03:02:28.754: INFO: Got endpoints: latency-svc-2jrxd [188.11763ms]
Apr 16 03:02:28.775: INFO: Created: latency-svc-2kxqc
Apr 16 03:02:28.803: INFO: Got endpoints: latency-svc-2kxqc [236.545723ms]
Apr 16 03:02:28.803: INFO: Created: latency-svc-lqr9c
Apr 16 03:02:28.840: INFO: Got endpoints: latency-svc-lqr9c [274.560326ms]
Apr 16 03:02:28.846: INFO: Created: latency-svc-jj66v
Apr 16 03:02:28.861: INFO: Got endpoints: latency-svc-jj66v [295.328317ms]
Apr 16 03:02:28.869: INFO: Created: latency-svc-msdcl
Apr 16 03:02:28.883: INFO: Got endpoints: latency-svc-msdcl [317.150294ms]
Apr 16 03:02:28.891: INFO: Created: latency-svc-r56s4
Apr 16 03:02:28.904: INFO: Got endpoints: latency-svc-r56s4 [337.543148ms]
Apr 16 03:02:28.918: INFO: Created: latency-svc-hdbw2
Apr 16 03:02:28.933: INFO: Got endpoints: latency-svc-hdbw2 [366.500051ms]
Apr 16 03:02:28.944: INFO: Created: latency-svc-sf49r
Apr 16 03:02:28.965: INFO: Got endpoints: latency-svc-sf49r [398.892018ms]
Apr 16 03:02:28.974: INFO: Created: latency-svc-tpsrq
Apr 16 03:02:29.020: INFO: Got endpoints: latency-svc-tpsrq [453.834241ms]
Apr 16 03:02:29.043: INFO: Created: latency-svc-xhmfd
Apr 16 03:02:29.051: INFO: Got endpoints: latency-svc-xhmfd [485.353585ms]
Apr 16 03:02:29.070: INFO: Created: latency-svc-hwx6d
Apr 16 03:02:29.085: INFO: Got endpoints: latency-svc-hwx6d [473.024954ms]
Apr 16 03:02:29.093: INFO: Created: latency-svc-nnqxl
Apr 16 03:02:29.105: INFO: Got endpoints: latency-svc-nnqxl [459.519001ms]
Apr 16 03:02:29.115: INFO: Created: latency-svc-q7s4k
Apr 16 03:02:29.127: INFO: Got endpoints: latency-svc-q7s4k [447.264602ms]
Apr 16 03:02:29.135: INFO: Created: latency-svc-qhp4w
Apr 16 03:02:29.159: INFO: Got endpoints: latency-svc-qhp4w [449.784574ms]
Apr 16 03:02:29.168: INFO: Created: latency-svc-zchpr
Apr 16 03:02:29.187: INFO: Got endpoints: latency-svc-zchpr [453.659255ms]
Apr 16 03:02:29.199: INFO: Created: latency-svc-krkn5
Apr 16 03:02:29.215: INFO: Got endpoints: latency-svc-krkn5 [460.81898ms]
Apr 16 03:02:29.223: INFO: Created: latency-svc-ppkgs
Apr 16 03:02:29.242: INFO: Got endpoints: latency-svc-ppkgs [439.33564ms]
Apr 16 03:02:29.249: INFO: Created: latency-svc-7wkjv
Apr 16 03:02:29.270: INFO: Got endpoints: latency-svc-7wkjv [429.040048ms]
Apr 16 03:02:29.275: INFO: Created: latency-svc-v4wtx
Apr 16 03:02:29.292: INFO: Got endpoints: latency-svc-v4wtx [430.216804ms]
Apr 16 03:02:29.309: INFO: Created: latency-svc-lpf6p
Apr 16 03:02:29.320: INFO: Got endpoints: latency-svc-lpf6p [436.333948ms]
Apr 16 03:02:29.331: INFO: Created: latency-svc-8jdxb
Apr 16 03:02:29.345: INFO: Got endpoints: latency-svc-8jdxb [440.936833ms]
Apr 16 03:02:29.354: INFO: Created: latency-svc-wllbv
Apr 16 03:02:29.382: INFO: Created: latency-svc-2jnjr
Apr 16 03:02:29.391: INFO: Got endpoints: latency-svc-wllbv [458.066976ms]
Apr 16 03:02:29.415: INFO: Created: latency-svc-vb4t4
Apr 16 03:02:29.418: INFO: Got endpoints: latency-svc-2jnjr [453.396522ms]
Apr 16 03:02:29.435: INFO: Got endpoints: latency-svc-vb4t4 [414.718761ms]
Apr 16 03:02:29.445: INFO: Created: latency-svc-b6rzp
Apr 16 03:02:29.455: INFO: Got endpoints: latency-svc-b6rzp [403.046202ms]
Apr 16 03:02:29.469: INFO: Created: latency-svc-42l6p
Apr 16 03:02:29.484: INFO: Got endpoints: latency-svc-42l6p [398.669229ms]
Apr 16 03:02:29.491: INFO: Created: latency-svc-6sbf2
Apr 16 03:02:29.502: INFO: Got endpoints: latency-svc-6sbf2 [397.136505ms]
Apr 16 03:02:29.520: INFO: Created: latency-svc-g5vbf
Apr 16 03:02:29.546: INFO: Got endpoints: latency-svc-g5vbf [418.575211ms]
Apr 16 03:02:29.550: INFO: Created: latency-svc-f8cr9
Apr 16 03:02:29.573: INFO: Got endpoints: latency-svc-f8cr9 [413.765052ms]
Apr 16 03:02:29.584: INFO: Created: latency-svc-62qjm
Apr 16 03:02:29.606: INFO: Got endpoints: latency-svc-62qjm [418.603759ms]
Apr 16 03:02:29.606: INFO: Created: latency-svc-l8wwm
Apr 16 03:02:29.631: INFO: Created: latency-svc-jnj99
Apr 16 03:02:29.640: INFO: Got endpoints: latency-svc-jnj99 [398.066732ms]
Apr 16 03:02:29.640: INFO: Got endpoints: latency-svc-l8wwm [424.617387ms]
Apr 16 03:02:29.656: INFO: Created: latency-svc-9nr2f
Apr 16 03:02:29.687: INFO: Created: latency-svc-jt275
Apr 16 03:02:29.714: INFO: Created: latency-svc-jr5xt
Apr 16 03:02:29.735: INFO: Created: latency-svc-f7fj5
Apr 16 03:02:29.753: INFO: Created: latency-svc-h2b7q
Apr 16 03:02:29.773: INFO: Created: latency-svc-9bfh8
Apr 16 03:02:29.775: INFO: Got endpoints: latency-svc-jt275 [483.144493ms]
Apr 16 03:02:29.776: INFO: Got endpoints: latency-svc-9nr2f [506.751893ms]
Apr 16 03:02:29.780: INFO: Got endpoints: latency-svc-jr5xt [460.672728ms]
Apr 16 03:02:29.799: INFO: Got endpoints: latency-svc-f7fj5 [453.945073ms]
Apr 16 03:02:29.807: INFO: Created: latency-svc-lhdg5
Apr 16 03:02:29.808: INFO: Got endpoints: latency-svc-h2b7q [417.412396ms]
Apr 16 03:02:29.823: INFO: Got endpoints: latency-svc-9bfh8 [404.701265ms]
Apr 16 03:02:29.832: INFO: Got endpoints: latency-svc-lhdg5 [396.738277ms]
Apr 16 03:02:29.836: INFO: Created: latency-svc-cgwd9
Apr 16 03:02:29.852: INFO: Got endpoints: latency-svc-cgwd9 [397.240671ms]
Apr 16 03:02:29.860: INFO: Created: latency-svc-kdtpk
Apr 16 03:02:29.875: INFO: Got endpoints: latency-svc-kdtpk [391.674997ms]
Apr 16 03:02:29.877: INFO: Created: latency-svc-t7js5
Apr 16 03:02:29.893: INFO: Got endpoints: latency-svc-t7js5 [390.304456ms]
Apr 16 03:02:29.901: INFO: Created: latency-svc-gt9qg
Apr 16 03:02:29.913: INFO: Got endpoints: latency-svc-gt9qg [366.620547ms]
Apr 16 03:02:29.920: INFO: Created: latency-svc-tccwk
Apr 16 03:02:29.946: INFO: Got endpoints: latency-svc-tccwk [372.789295ms]
Apr 16 03:02:29.947: INFO: Created: latency-svc-74xzw
Apr 16 03:02:29.958: INFO: Got endpoints: latency-svc-74xzw [351.916353ms]
Apr 16 03:02:29.968: INFO: Created: latency-svc-rvhcg
Apr 16 03:02:29.985: INFO: Got endpoints: latency-svc-rvhcg [344.506939ms]
Apr 16 03:02:29.991: INFO: Created: latency-svc-2s7pn
Apr 16 03:02:30.003: INFO: Got endpoints: latency-svc-2s7pn [363.03581ms]
Apr 16 03:02:30.018: INFO: Created: latency-svc-zxlxv
Apr 16 03:02:30.029: INFO: Got endpoints: latency-svc-zxlxv [253.610389ms]
Apr 16 03:02:30.045: INFO: Created: latency-svc-9rbmq
Apr 16 03:02:30.054: INFO: Got endpoints: latency-svc-9rbmq [273.473123ms]
Apr 16 03:02:30.070: INFO: Created: latency-svc-qb4z6
Apr 16 03:02:30.090: INFO: Got endpoints: latency-svc-qb4z6 [313.172746ms]
Apr 16 03:02:30.090: INFO: Created: latency-svc-52rvv
Apr 16 03:02:30.121: INFO: Got endpoints: latency-svc-52rvv [322.353954ms]
Apr 16 03:02:30.132: INFO: Created: latency-svc-h4l7k
Apr 16 03:02:30.133: INFO: Got endpoints: latency-svc-h4l7k [324.303928ms]
Apr 16 03:02:30.142: INFO: Created: latency-svc-mvzd2
Apr 16 03:02:30.173: INFO: Got endpoints: latency-svc-mvzd2 [349.323342ms]
Apr 16 03:02:30.192: INFO: Created: latency-svc-xtmlh
Apr 16 03:02:30.205: INFO: Got endpoints: latency-svc-xtmlh [373.27714ms]
Apr 16 03:02:30.225: INFO: Created: latency-svc-2jfsf
Apr 16 03:02:30.239: INFO: Created: latency-svc-g8h8k
Apr 16 03:02:30.247: INFO: Got endpoints: latency-svc-2jfsf [394.628684ms]
Apr 16 03:02:30.266: INFO: Got endpoints: latency-svc-g8h8k [390.320515ms]
Apr 16 03:02:30.270: INFO: Created: latency-svc-whzzn
Apr 16 03:02:30.283: INFO: Got endpoints: latency-svc-whzzn [390.803952ms]
Apr 16 03:02:30.294: INFO: Created: latency-svc-s6j29
Apr 16 03:02:30.313: INFO: Created: latency-svc-rbwr7
Apr 16 03:02:30.316: INFO: Got endpoints: latency-svc-s6j29 [403.57663ms]
Apr 16 03:02:30.338: INFO: Got endpoints: latency-svc-rbwr7 [392.136191ms]
Apr 16 03:02:30.340: INFO: Created: latency-svc-vqsdh
Apr 16 03:02:30.355: INFO: Created: latency-svc-xrsz4
Apr 16 03:02:30.369: INFO: Got endpoints: latency-svc-vqsdh [411.50791ms]
Apr 16 03:02:30.381: INFO: Created: latency-svc-89d8z
Apr 16 03:02:30.417: INFO: Created: latency-svc-69zb5
Apr 16 03:02:30.425: INFO: Got endpoints: latency-svc-xrsz4 [440.358847ms]
Apr 16 03:02:30.432: INFO: Created: latency-svc-gt2ml
Apr 16 03:02:30.465: INFO: Got endpoints: latency-svc-89d8z [461.252312ms]
Apr 16 03:02:30.480: INFO: Created: latency-svc-2vqcw
Apr 16 03:02:30.501: INFO: Created: latency-svc-wrhbz
Apr 16 03:02:30.518: INFO: Got endpoints: latency-svc-69zb5 [488.866225ms]
Apr 16 03:02:30.533: INFO: Created: latency-svc-npfw2
Apr 16 03:02:30.580: INFO: Got endpoints: latency-svc-gt2ml [526.119217ms]
Apr 16 03:02:30.590: INFO: Created: latency-svc-m44w5
Apr 16 03:02:30.617: INFO: Got endpoints: latency-svc-2vqcw [527.642737ms]
Apr 16 03:02:30.634: INFO: Created: latency-svc-p2dlg
Apr 16 03:02:30.664: INFO: Created: latency-svc-9mz78
Apr 16 03:02:30.672: INFO: Got endpoints: latency-svc-wrhbz [551.075608ms]
Apr 16 03:02:30.695: INFO: Created: latency-svc-2nf5g
Apr 16 03:02:30.711: INFO: Created: latency-svc-jtg22
Apr 16 03:02:30.715: INFO: Got endpoints: latency-svc-npfw2 [581.719652ms]
Apr 16 03:02:30.745: INFO: Created: latency-svc-lhxhq
Apr 16 03:02:30.758: INFO: Created: latency-svc-llgjh
Apr 16 03:02:30.777: INFO: Created: latency-svc-8cplq
Apr 16 03:02:30.800: INFO: Got endpoints: latency-svc-m44w5 [627.2999ms]
Apr 16 03:02:30.807: INFO: Created: latency-svc-psxvp
Apr 16 03:02:30.828: INFO: Got endpoints: latency-svc-p2dlg [622.913272ms]
Apr 16 03:02:30.842: INFO: Created: latency-svc-726sl
Apr 16 03:02:30.875: INFO: Got endpoints: latency-svc-9mz78 [627.94911ms]
Apr 16 03:02:30.879: INFO: Created: latency-svc-ggspb
Apr 16 03:02:30.910: INFO: Created: latency-svc-fkkgc
Apr 16 03:02:30.915: INFO: Got endpoints: latency-svc-2nf5g [649.29679ms]
Apr 16 03:02:30.946: INFO: Created: latency-svc-577zp
Apr 16 03:02:30.969: INFO: Got endpoints: latency-svc-jtg22 [685.145811ms]
Apr 16 03:02:30.972: INFO: Created: latency-svc-tqlrh
Apr 16 03:02:30.999: INFO: Created: latency-svc-xvh8l
Apr 16 03:02:31.013: INFO: Got endpoints: latency-svc-lhxhq [697.030053ms]
Apr 16 03:02:31.023: INFO: Created: latency-svc-lbzvc
Apr 16 03:02:31.051: INFO: Created: latency-svc-vt8w7
Apr 16 03:02:31.063: INFO: Got endpoints: latency-svc-llgjh [724.648742ms]
Apr 16 03:02:31.069: INFO: Created: latency-svc-lcf6v
Apr 16 03:02:31.096: INFO: Created: latency-svc-vrkpw
Apr 16 03:02:31.123: INFO: Got endpoints: latency-svc-8cplq [753.033607ms]
Apr 16 03:02:31.130: INFO: Created: latency-svc-67cvb
Apr 16 03:02:31.166: INFO: Created: latency-svc-k9jdp
Apr 16 03:02:31.168: INFO: Got endpoints: latency-svc-psxvp [742.585059ms]
Apr 16 03:02:31.181: INFO: Created: latency-svc-b9r8b
Apr 16 03:02:31.196: INFO: Created: latency-svc-hvz65
Apr 16 03:02:31.219: INFO: Created: latency-svc-z59l9
Apr 16 03:02:31.227: INFO: Got endpoints: latency-svc-726sl [762.495336ms]
Apr 16 03:02:31.266: INFO: Created: latency-svc-lmt4b
Apr 16 03:02:31.271: INFO: Got endpoints: latency-svc-ggspb [753.511931ms]
Apr 16 03:02:31.300: INFO: Created: latency-svc-r2v4f
Apr 16 03:02:31.335: INFO: Got endpoints: latency-svc-fkkgc [754.416091ms]
Apr 16 03:02:31.365: INFO: Got endpoints: latency-svc-577zp [747.275629ms]
Apr 16 03:02:31.371: INFO: Created: latency-svc-l9cwl
Apr 16 03:02:31.403: INFO: Created: latency-svc-245kf
Apr 16 03:02:31.416: INFO: Got endpoints: latency-svc-tqlrh [743.918239ms]
Apr 16 03:02:31.447: INFO: Created: latency-svc-2lsrc
Apr 16 03:02:31.470: INFO: Got endpoints: latency-svc-xvh8l [754.919467ms]
Apr 16 03:02:31.506: INFO: Created: latency-svc-q5xqv
Apr 16 03:02:31.522: INFO: Got endpoints: latency-svc-lbzvc [721.442732ms]
Apr 16 03:02:31.562: INFO: Created: latency-svc-k6b2p
Apr 16 03:02:31.575: INFO: Got endpoints: latency-svc-vt8w7 [746.866949ms]
Apr 16 03:02:31.609: INFO: Created: latency-svc-5vb4v
Apr 16 03:02:31.614: INFO: Got endpoints: latency-svc-lcf6v [738.965425ms]
Apr 16 03:02:31.642: INFO: Created: latency-svc-v5gvc
Apr 16 03:02:31.665: INFO: Got endpoints: latency-svc-vrkpw [749.46688ms]
Apr 16 03:02:31.705: INFO: Created: latency-svc-wz6sx
Apr 16 03:02:31.716: INFO: Got endpoints: latency-svc-67cvb [747.370503ms]
Apr 16 03:02:31.751: INFO: Created: latency-svc-njvpj
Apr 16 03:02:31.766: INFO: Got endpoints: latency-svc-k9jdp [752.913294ms]
Apr 16 03:02:31.797: INFO: Created: latency-svc-wkw56
Apr 16 03:02:31.812: INFO: Got endpoints: latency-svc-b9r8b [748.571152ms]
Apr 16 03:02:31.845: INFO: Created: latency-svc-4knsz
Apr 16 03:02:31.862: INFO: Got endpoints: latency-svc-hvz65 [739.343553ms]
Apr 16 03:02:31.892: INFO: Created: latency-svc-jsfg7
Apr 16 03:02:31.915: INFO: Got endpoints: latency-svc-z59l9 [747.212368ms]
Apr 16 03:02:31.941: INFO: Created: latency-svc-l86pm
Apr 16 03:02:31.969: INFO: Got endpoints: latency-svc-lmt4b [741.461881ms]
Apr 16 03:02:32.003: INFO: Created: latency-svc-cfq6q
Apr 16 03:02:32.022: INFO: Got endpoints: latency-svc-r2v4f [751.187229ms]
Apr 16 03:02:32.059: INFO: Created: latency-svc-t4zsb
Apr 16 03:02:32.069: INFO: Got endpoints: latency-svc-l9cwl [733.784846ms]
Apr 16 03:02:32.107: INFO: Created: latency-svc-4n6rk
Apr 16 03:02:32.122: INFO: Got endpoints: latency-svc-245kf [757.450669ms]
Apr 16 03:02:32.153: INFO: Created: latency-svc-cgsqg
Apr 16 03:02:32.167: INFO: Got endpoints: latency-svc-2lsrc [750.670788ms]
Apr 16 03:02:32.212: INFO: Created: latency-svc-cksdf
Apr 16 03:02:32.242: INFO: Got endpoints: latency-svc-q5xqv [772.4643ms]
Apr 16 03:02:32.272: INFO: Got endpoints: latency-svc-k6b2p [750.432963ms]
Apr 16 03:02:32.302: INFO: Created: latency-svc-v6bsz
Apr 16 03:02:32.322: INFO: Got endpoints: latency-svc-5vb4v [747.226207ms]
Apr 16 03:02:32.323: INFO: Created: latency-svc-ss6pj
Apr 16 03:02:32.353: INFO: Created: latency-svc-b2f5d
Apr 16 03:02:32.371: INFO: Got endpoints: latency-svc-v5gvc [757.125573ms]
Apr 16 03:02:32.398: INFO: Created: latency-svc-g4sfd
Apr 16 03:02:32.415: INFO: Got endpoints: latency-svc-wz6sx [750.606518ms]
Apr 16 03:02:32.448: INFO: Created: latency-svc-x9kxb
Apr 16 03:02:32.465: INFO: Got endpoints: latency-svc-njvpj [748.99209ms]
Apr 16 03:02:32.493: INFO: Created: latency-svc-75wcf
Apr 16 03:02:32.515: INFO: Got endpoints: latency-svc-wkw56 [748.147608ms]
Apr 16 03:02:32.553: INFO: Created: latency-svc-lg794
Apr 16 03:02:32.575: INFO: Got endpoints: latency-svc-4knsz [763.45903ms]
Apr 16 03:02:32.618: INFO: Got endpoints: latency-svc-jsfg7 [755.3271ms]
Apr 16 03:02:32.622: INFO: Created: latency-svc-m79fz
Apr 16 03:02:32.680: INFO: Got endpoints: latency-svc-l86pm [764.389493ms]
Apr 16 03:02:32.680: INFO: Created: latency-svc-jrzv7
Apr 16 03:02:32.711: INFO: Got endpoints: latency-svc-cfq6q [742.074451ms]
Apr 16 03:02:32.719: INFO: Created: latency-svc-r49hj
Apr 16 03:02:32.745: INFO: Created: latency-svc-5h24z
Apr 16 03:02:32.760: INFO: Got endpoints: latency-svc-t4zsb [737.383339ms]
Apr 16 03:02:32.790: INFO: Created: latency-svc-xvk45
Apr 16 03:02:32.814: INFO: Got endpoints: latency-svc-4n6rk [745.667035ms]
Apr 16 03:02:32.849: INFO: Created: latency-svc-gmg22
Apr 16 03:02:32.865: INFO: Got endpoints: latency-svc-cgsqg [742.528991ms]
Apr 16 03:02:32.893: INFO: Created: latency-svc-mgjns
Apr 16 03:02:32.921: INFO: Got endpoints: latency-svc-cksdf [753.630487ms]
Apr 16 03:02:32.964: INFO: Created: latency-svc-9hqtj
Apr 16 03:02:32.974: INFO: Got endpoints: latency-svc-v6bsz [731.737575ms]
Apr 16 03:02:33.022: INFO: Got endpoints: latency-svc-ss6pj [750.172205ms]
Apr 16 03:02:33.052: INFO: Created: latency-svc-rwfdk
Apr 16 03:02:33.072: INFO: Got endpoints: latency-svc-b2f5d [749.17606ms]
Apr 16 03:02:33.095: INFO: Created: latency-svc-l5bht
Apr 16 03:02:33.117: INFO: Created: latency-svc-hpbbh
Apr 16 03:02:33.117: INFO: Got endpoints: latency-svc-g4sfd [745.620024ms]
Apr 16 03:02:33.150: INFO: Created: latency-svc-pfw44
Apr 16 03:02:33.165: INFO: Got endpoints: latency-svc-x9kxb [748.978798ms]
Apr 16 03:02:33.200: INFO: Created: latency-svc-fp649
Apr 16 03:02:33.215: INFO: Got endpoints: latency-svc-75wcf [749.294652ms]
Apr 16 03:02:33.246: INFO: Created: latency-svc-64hdw
Apr 16 03:02:33.265: INFO: Got endpoints: latency-svc-lg794 [749.951132ms]
Apr 16 03:02:33.297: INFO: Created: latency-svc-7mwkk
Apr 16 03:02:33.310: INFO: Got endpoints: latency-svc-m79fz [734.392652ms]
Apr 16 03:02:33.342: INFO: Created: latency-svc-b7tsk
Apr 16 03:02:33.360: INFO: Got endpoints: latency-svc-jrzv7 [741.739654ms]
Apr 16 03:02:33.397: INFO: Created: latency-svc-wcp6h
Apr 16 03:02:33.417: INFO: Got endpoints: latency-svc-r49hj [736.57557ms]
Apr 16 03:02:33.446: INFO: Created: latency-svc-f88j7
Apr 16 03:02:33.463: INFO: Got endpoints: latency-svc-5h24z [751.984781ms]
Apr 16 03:02:33.496: INFO: Created: latency-svc-gsg54
Apr 16 03:02:33.512: INFO: Got endpoints: latency-svc-xvk45 [752.340438ms]
Apr 16 03:02:33.545: INFO: Created: latency-svc-9rm4p
Apr 16 03:02:33.569: INFO: Got endpoints: latency-svc-gmg22 [754.417624ms]
Apr 16 03:02:33.599: INFO: Created: latency-svc-9wtvc
Apr 16 03:02:33.612: INFO: Got endpoints: latency-svc-mgjns [746.758396ms]
Apr 16 03:02:33.640: INFO: Created: latency-svc-68tcb
Apr 16 03:02:33.671: INFO: Got endpoints: latency-svc-9hqtj [750.27845ms]
Apr 16 03:02:33.717: INFO: Created: latency-svc-wlknt
Apr 16 03:02:33.723: INFO: Got endpoints: latency-svc-rwfdk [748.539222ms]
Apr 16 03:02:33.752: INFO: Created: latency-svc-6sslf
Apr 16 03:02:33.771: INFO: Got endpoints: latency-svc-l5bht [748.346041ms]
Apr 16 03:02:33.803: INFO: Created: latency-svc-f9rq2
Apr 16 03:02:33.817: INFO: Got endpoints: latency-svc-hpbbh [745.271807ms]
Apr 16 03:02:33.847: INFO: Created: latency-svc-rx7qg
Apr 16 03:02:33.863: INFO: Got endpoints: latency-svc-pfw44 [746.433268ms]
Apr 16 03:02:33.898: INFO: Created: latency-svc-t9tc9
Apr 16 03:02:33.913: INFO: Got endpoints: latency-svc-fp649 [748.858152ms]
Apr 16 03:02:33.941: INFO: Created: latency-svc-7v77n
Apr 16 03:02:33.962: INFO: Got endpoints: latency-svc-64hdw [747.119674ms]
Apr 16 03:02:33.990: INFO: Created: latency-svc-88wnt
Apr 16 03:02:34.014: INFO: Got endpoints: latency-svc-7mwkk [749.23451ms]
Apr 16 03:02:34.043: INFO: Created: latency-svc-jrctc
Apr 16 03:02:34.063: INFO: Got endpoints: latency-svc-b7tsk [752.873348ms]
Apr 16 03:02:34.092: INFO: Created: latency-svc-4z9pb
Apr 16 03:02:34.119: INFO: Got endpoints: latency-svc-wcp6h [758.9131ms]
Apr 16 03:02:34.148: INFO: Created: latency-svc-h88gl
Apr 16 03:02:34.161: INFO: Got endpoints: latency-svc-f88j7 [744.064752ms]
Apr 16 03:02:34.193: INFO: Created: latency-svc-wm7p6
Apr 16 03:02:34.214: INFO: Got endpoints: latency-svc-gsg54 [750.010621ms]
Apr 16 03:02:34.241: INFO: Created: latency-svc-stsn5
Apr 16 03:02:34.261: INFO: Got endpoints: latency-svc-9rm4p [748.175394ms]
Apr 16 03:02:34.285: INFO: Created: latency-svc-5kw4p
Apr 16 03:02:34.314: INFO: Got endpoints: latency-svc-9wtvc [745.400779ms]
Apr 16 03:02:34.342: INFO: Created: latency-svc-cbgkz
Apr 16 03:02:34.370: INFO: Got endpoints: latency-svc-68tcb [757.564765ms]
Apr 16 03:02:34.398: INFO: Created: latency-svc-vpjft
Apr 16 03:02:34.414: INFO: Got endpoints: latency-svc-wlknt [742.798657ms]
Apr 16 03:02:34.441: INFO: Created: latency-svc-m2fl6
Apr 16 03:02:34.463: INFO: Got endpoints: latency-svc-6sslf [739.654037ms]
Apr 16 03:02:34.488: INFO: Created: latency-svc-kscfd
Apr 16 03:02:34.510: INFO: Got endpoints: latency-svc-f9rq2 [739.075008ms]
Apr 16 03:02:34.545: INFO: Created: latency-svc-lzgqm
Apr 16 03:02:34.563: INFO: Got endpoints: latency-svc-rx7qg [746.439256ms]
Apr 16 03:02:34.600: INFO: Created: latency-svc-4rbsr
Apr 16 03:02:34.620: INFO: Got endpoints: latency-svc-t9tc9 [756.234754ms]
Apr 16 03:02:34.650: INFO: Created: latency-svc-qh7zt
Apr 16 03:02:34.674: INFO: Got endpoints: latency-svc-7v77n [760.29496ms]
Apr 16 03:02:34.702: INFO: Created: latency-svc-d7rgn
Apr 16 03:02:34.716: INFO: Got endpoints: latency-svc-88wnt [753.762953ms]
Apr 16 03:02:34.740: INFO: Created: latency-svc-wpmvb
Apr 16 03:02:34.764: INFO: Got endpoints: latency-svc-jrctc [750.095893ms]
Apr 16 03:02:34.792: INFO: Created: latency-svc-9sjrl
Apr 16 03:02:34.817: INFO: Got endpoints: latency-svc-4z9pb [754.042228ms]
Apr 16 03:02:34.851: INFO: Created: latency-svc-5r6fz
Apr 16 03:02:34.863: INFO: Got endpoints: latency-svc-h88gl [744.218923ms]
Apr 16 03:02:34.889: INFO: Created: latency-svc-rpltk
Apr 16 03:02:34.913: INFO: Got endpoints: latency-svc-wm7p6 [752.487487ms]
Apr 16 03:02:34.945: INFO: Created: latency-svc-r99qh
Apr 16 03:02:34.963: INFO: Got endpoints: latency-svc-stsn5 [749.331008ms]
Apr 16 03:02:34.998: INFO: Created: latency-svc-tt8mb
Apr 16 03:02:35.014: INFO: Got endpoints: latency-svc-5kw4p [753.31839ms]
Apr 16 03:02:35.047: INFO: Created: latency-svc-h9t4z
Apr 16 03:02:35.061: INFO: Got endpoints: latency-svc-cbgkz [746.485051ms]
Apr 16 03:02:35.094: INFO: Created: latency-svc-dnkhd
Apr 16 03:02:35.115: INFO: Got endpoints: latency-svc-vpjft [745.008124ms]
Apr 16 03:02:35.144: INFO: Created: latency-svc-gjpvz
Apr 16 03:02:35.163: INFO: Got endpoints: latency-svc-m2fl6 [749.281476ms]
Apr 16 03:02:35.190: INFO: Created: latency-svc-rxs8l
Apr 16 03:02:35.216: INFO: Got endpoints: latency-svc-kscfd [752.934507ms]
Apr 16 03:02:35.245: INFO: Created: latency-svc-2zdrk
Apr 16 03:02:35.264: INFO: Got endpoints: latency-svc-lzgqm [754.068637ms]
Apr 16 03:02:35.298: INFO: Created: latency-svc-7c6ld
Apr 16 03:02:35.317: INFO: Got endpoints: latency-svc-4rbsr [753.881267ms]
Apr 16 03:02:35.345: INFO: Created: latency-svc-rgjvx
Apr 16 03:02:35.362: INFO: Got endpoints: latency-svc-qh7zt [742.501586ms]
Apr 16 03:02:35.394: INFO: Created: latency-svc-sk8ml
Apr 16 03:02:35.413: INFO: Got endpoints: latency-svc-d7rgn [739.03968ms]
Apr 16 03:02:35.445: INFO: Created: latency-svc-kdb8p
Apr 16 03:02:35.463: INFO: Got endpoints: latency-svc-wpmvb [746.497719ms]
Apr 16 03:02:35.492: INFO: Created: latency-svc-7zspc
Apr 16 03:02:35.511: INFO: Got endpoints: latency-svc-9sjrl [747.012767ms]
Apr 16 03:02:35.536: INFO: Created: latency-svc-srnfg
Apr 16 03:02:35.569: INFO: Got endpoints: latency-svc-5r6fz [752.41269ms]
Apr 16 03:02:35.604: INFO: Created: latency-svc-grns7
Apr 16 03:02:35.614: INFO: Got endpoints: latency-svc-rpltk [750.771927ms]
Apr 16 03:02:35.635: INFO: Created: latency-svc-gxk2c
Apr 16 03:02:35.661: INFO: Got endpoints: latency-svc-r99qh [747.921889ms]
Apr 16 03:02:35.698: INFO: Created: latency-svc-4pqhr
Apr 16 03:02:35.717: INFO: Got endpoints: latency-svc-tt8mb [753.532074ms]
Apr 16 03:02:35.749: INFO: Created: latency-svc-7qsp8
Apr 16 03:02:35.761: INFO: Got endpoints: latency-svc-h9t4z [746.473862ms]
Apr 16 03:02:35.786: INFO: Created: latency-svc-7d2q2
Apr 16 03:02:35.812: INFO: Got endpoints: latency-svc-dnkhd [750.905932ms]
Apr 16 03:02:35.841: INFO: Created: latency-svc-5xxk4
Apr 16 03:02:35.865: INFO: Got endpoints: latency-svc-gjpvz [750.291016ms]
Apr 16 03:02:35.889: INFO: Created: latency-svc-ldswf
Apr 16 03:02:35.914: INFO: Got endpoints: latency-svc-rxs8l [750.806931ms]
Apr 16 03:02:35.949: INFO: Created: latency-svc-8nrft
Apr 16 03:02:35.960: INFO: Got endpoints: latency-svc-2zdrk [744.212302ms]
Apr 16 03:02:35.988: INFO: Created: latency-svc-h55tt
Apr 16 03:02:36.016: INFO: Got endpoints: latency-svc-7c6ld [751.441983ms]
Apr 16 03:02:36.053: INFO: Created: latency-svc-zkvsb
Apr 16 03:02:36.069: INFO: Got endpoints: latency-svc-rgjvx [751.347404ms]
Apr 16 03:02:36.102: INFO: Created: latency-svc-csgbv
Apr 16 03:02:36.127: INFO: Got endpoints: latency-svc-sk8ml [764.002995ms]
Apr 16 03:02:36.153: INFO: Created: latency-svc-djq9j
Apr 16 03:02:36.162: INFO: Got endpoints: latency-svc-kdb8p [748.979757ms]
Apr 16 03:02:36.205: INFO: Created: latency-svc-ctgrb
Apr 16 03:02:36.230: INFO: Got endpoints: latency-svc-7zspc [766.434803ms]
Apr 16 03:02:36.281: INFO: Got endpoints: latency-svc-srnfg [769.172934ms]
Apr 16 03:02:36.291: INFO: Created: latency-svc-hdztf
Apr 16 03:02:36.310: INFO: Got endpoints: latency-svc-grns7 [740.856477ms]
Apr 16 03:02:36.317: INFO: Created: latency-svc-2jx5m
Apr 16 03:02:36.341: INFO: Created: latency-svc-79j2t
Apr 16 03:02:36.364: INFO: Got endpoints: latency-svc-gxk2c [750.464775ms]
Apr 16 03:02:36.393: INFO: Created: latency-svc-6dx2w
Apr 16 03:02:36.413: INFO: Got endpoints: latency-svc-4pqhr [751.357863ms]
Apr 16 03:02:36.466: INFO: Got endpoints: latency-svc-7qsp8 [749.284516ms]
Apr 16 03:02:36.514: INFO: Got endpoints: latency-svc-7d2q2 [753.048202ms]
Apr 16 03:02:36.571: INFO: Got endpoints: latency-svc-5xxk4 [758.451044ms]
Apr 16 03:02:36.615: INFO: Got endpoints: latency-svc-ldswf [749.299962ms]
Apr 16 03:02:36.665: INFO: Got endpoints: latency-svc-8nrft [750.943747ms]
Apr 16 03:02:36.712: INFO: Got endpoints: latency-svc-h55tt [752.474438ms]
Apr 16 03:02:36.763: INFO: Got endpoints: latency-svc-zkvsb [746.807991ms]
Apr 16 03:02:36.924: INFO: Got endpoints: latency-svc-csgbv [854.720865ms]
Apr 16 03:02:36.930: INFO: Got endpoints: latency-svc-ctgrb [767.724045ms]
Apr 16 03:02:36.930: INFO: Got endpoints: latency-svc-djq9j [803.617164ms]
Apr 16 03:02:36.977: INFO: Got endpoints: latency-svc-hdztf [746.988192ms]
Apr 16 03:02:37.015: INFO: Got endpoints: latency-svc-2jx5m [733.617051ms]
Apr 16 03:02:37.065: INFO: Got endpoints: latency-svc-79j2t [754.729391ms]
Apr 16 03:02:37.113: INFO: Got endpoints: latency-svc-6dx2w [748.874417ms]
Apr 16 03:02:37.114: INFO: Latencies: [46.089557ms 79.519871ms 113.938081ms 143.285917ms 167.765674ms 188.11763ms 236.545723ms 253.610389ms 273.473123ms 274.560326ms 295.328317ms 313.172746ms 317.150294ms 322.353954ms 324.303928ms 337.543148ms 344.506939ms 349.323342ms 351.916353ms 363.03581ms 366.500051ms 366.620547ms 372.789295ms 373.27714ms 390.304456ms 390.320515ms 390.803952ms 391.674997ms 392.136191ms 394.628684ms 396.738277ms 397.136505ms 397.240671ms 398.066732ms 398.669229ms 398.892018ms 403.046202ms 403.57663ms 404.701265ms 411.50791ms 413.765052ms 414.718761ms 417.412396ms 418.575211ms 418.603759ms 424.617387ms 429.040048ms 430.216804ms 436.333948ms 439.33564ms 440.358847ms 440.936833ms 447.264602ms 449.784574ms 453.396522ms 453.659255ms 453.834241ms 453.945073ms 458.066976ms 459.519001ms 460.672728ms 460.81898ms 461.252312ms 473.024954ms 483.144493ms 485.353585ms 488.866225ms 506.751893ms 526.119217ms 527.642737ms 551.075608ms 581.719652ms 622.913272ms 627.2999ms 627.94911ms 649.29679ms 685.145811ms 697.030053ms 721.442732ms 724.648742ms 731.737575ms 733.617051ms 733.784846ms 734.392652ms 736.57557ms 737.383339ms 738.965425ms 739.03968ms 739.075008ms 739.343553ms 739.654037ms 740.856477ms 741.461881ms 741.739654ms 742.074451ms 742.501586ms 742.528991ms 742.585059ms 742.798657ms 743.918239ms 744.064752ms 744.212302ms 744.218923ms 745.008124ms 745.271807ms 745.400779ms 745.620024ms 745.667035ms 746.433268ms 746.439256ms 746.473862ms 746.485051ms 746.497719ms 746.758396ms 746.807991ms 746.866949ms 746.988192ms 747.012767ms 747.119674ms 747.212368ms 747.226207ms 747.275629ms 747.370503ms 747.921889ms 748.147608ms 748.175394ms 748.346041ms 748.539222ms 748.571152ms 748.858152ms 748.874417ms 748.978798ms 748.979757ms 748.99209ms 749.17606ms 749.23451ms 749.281476ms 749.284516ms 749.294652ms 749.299962ms 749.331008ms 749.46688ms 749.951132ms 750.010621ms 750.095893ms 750.172205ms 750.27845ms 750.291016ms 750.432963ms 750.464775ms 750.606518ms 750.670788ms 750.771927ms 750.806931ms 750.905932ms 750.943747ms 751.187229ms 751.347404ms 751.357863ms 751.441983ms 751.984781ms 752.340438ms 752.41269ms 752.474438ms 752.487487ms 752.873348ms 752.913294ms 752.934507ms 753.033607ms 753.048202ms 753.31839ms 753.511931ms 753.532074ms 753.630487ms 753.762953ms 753.881267ms 754.042228ms 754.068637ms 754.416091ms 754.417624ms 754.729391ms 754.919467ms 755.3271ms 756.234754ms 757.125573ms 757.450669ms 757.564765ms 758.451044ms 758.9131ms 760.29496ms 762.495336ms 763.45903ms 764.002995ms 764.389493ms 766.434803ms 767.724045ms 769.172934ms 772.4643ms 803.617164ms 854.720865ms]
Apr 16 03:02:37.114: INFO: 50 %ile: 744.064752ms
Apr 16 03:02:37.114: INFO: 90 %ile: 754.729391ms
Apr 16 03:02:37.114: INFO: 99 %ile: 803.617164ms
Apr 16 03:02:37.114: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 03:02:37.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-779" for this suite.

• [SLOW TEST:11.121 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":301,"skipped":5162,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 03:02:37.155: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5405
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Apr 16 03:02:37.379: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 03:02:58.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5405" for this suite.

• [SLOW TEST:21.771 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":302,"skipped":5179,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 03:02:58.927: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5244
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5244
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-5244
Apr 16 03:02:59.260: INFO: Found 0 stateful pods, waiting for 1
Apr 16 03:03:09.289: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 16 03:03:09.350: INFO: Deleting all statefulset in ns statefulset-5244
Apr 16 03:03:09.364: INFO: Scaling statefulset ss to 0
Apr 16 03:03:39.438: INFO: Waiting for statefulset status.replicas updated to 0
Apr 16 03:03:39.447: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 03:03:39.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5244" for this suite.

• [SLOW TEST:40.596 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":303,"skipped":5219,"failed":0}
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 03:03:39.524: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1769
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-e99caca9-5273-4f27-ad1f-773c1c04280b
STEP: Creating a pod to test consume secrets
Apr 16 03:03:39.799: INFO: Waiting up to 5m0s for pod "pod-secrets-795848e0-e17c-4705-837f-ff5edf030ed7" in namespace "secrets-1769" to be "Succeeded or Failed"
Apr 16 03:03:39.818: INFO: Pod "pod-secrets-795848e0-e17c-4705-837f-ff5edf030ed7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.879228ms
Apr 16 03:03:41.837: INFO: Pod "pod-secrets-795848e0-e17c-4705-837f-ff5edf030ed7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.038535181s
STEP: Saw pod success
Apr 16 03:03:41.837: INFO: Pod "pod-secrets-795848e0-e17c-4705-837f-ff5edf030ed7" satisfied condition "Succeeded or Failed"
Apr 16 03:03:41.845: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod pod-secrets-795848e0-e17c-4705-837f-ff5edf030ed7 container secret-volume-test: <nil>
STEP: delete the pod
Apr 16 03:03:41.978: INFO: Waiting for pod pod-secrets-795848e0-e17c-4705-837f-ff5edf030ed7 to disappear
Apr 16 03:03:41.986: INFO: Pod pod-secrets-795848e0-e17c-4705-837f-ff5edf030ed7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 03:03:41.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1769" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":304,"skipped":5219,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 03:03:42.017: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-85
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 03:03:42.282: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1869392d-e0bf-4765-8937-592d4761fe0d" in namespace "downward-api-85" to be "Succeeded or Failed"
Apr 16 03:03:42.291: INFO: Pod "downwardapi-volume-1869392d-e0bf-4765-8937-592d4761fe0d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.481639ms
Apr 16 03:03:44.308: INFO: Pod "downwardapi-volume-1869392d-e0bf-4765-8937-592d4761fe0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025845849s
Apr 16 03:03:46.325: INFO: Pod "downwardapi-volume-1869392d-e0bf-4765-8937-592d4761fe0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043046981s
STEP: Saw pod success
Apr 16 03:03:46.325: INFO: Pod "downwardapi-volume-1869392d-e0bf-4765-8937-592d4761fe0d" satisfied condition "Succeeded or Failed"
Apr 16 03:03:46.333: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-1869392d-e0bf-4765-8937-592d4761fe0d container client-container: <nil>
STEP: delete the pod
Apr 16 03:03:46.403: INFO: Waiting for pod downwardapi-volume-1869392d-e0bf-4765-8937-592d4761fe0d to disappear
Apr 16 03:03:46.415: INFO: Pod downwardapi-volume-1869392d-e0bf-4765-8937-592d4761fe0d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 03:03:46.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-85" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":305,"skipped":5221,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 03:03:46.454: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1326
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-fe8a4971-d6f0-41f8-ba15-79350cf3d3e0
STEP: Creating configMap with name cm-test-opt-upd-069e4dfe-6aaa-46d2-a6aa-b1a4b0fa90a9
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-fe8a4971-d6f0-41f8-ba15-79350cf3d3e0
STEP: Updating configmap cm-test-opt-upd-069e4dfe-6aaa-46d2-a6aa-b1a4b0fa90a9
STEP: Creating configMap with name cm-test-opt-create-566402e2-13a4-4cf7-8aec-0ac16be031c4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 03:03:51.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1326" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":306,"skipped":5253,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 03:03:51.043: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9394
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 03:03:51.310: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5efabfdc-a0fa-4162-970a-639c05adb38c" in namespace "downward-api-9394" to be "Succeeded or Failed"
Apr 16 03:03:51.331: INFO: Pod "downwardapi-volume-5efabfdc-a0fa-4162-970a-639c05adb38c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.105422ms
Apr 16 03:03:53.347: INFO: Pod "downwardapi-volume-5efabfdc-a0fa-4162-970a-639c05adb38c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037697668s
Apr 16 03:03:55.368: INFO: Pod "downwardapi-volume-5efabfdc-a0fa-4162-970a-639c05adb38c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058496791s
STEP: Saw pod success
Apr 16 03:03:55.368: INFO: Pod "downwardapi-volume-5efabfdc-a0fa-4162-970a-639c05adb38c" satisfied condition "Succeeded or Failed"
Apr 16 03:03:55.375: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-5efabfdc-a0fa-4162-970a-639c05adb38c container client-container: <nil>
STEP: delete the pod
Apr 16 03:03:55.429: INFO: Waiting for pod downwardapi-volume-5efabfdc-a0fa-4162-970a-639c05adb38c to disappear
Apr 16 03:03:55.440: INFO: Pod downwardapi-volume-5efabfdc-a0fa-4162-970a-639c05adb38c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 03:03:55.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9394" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":307,"skipped":5286,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 03:03:55.481: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9778
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9778
STEP: creating service affinity-nodeport in namespace services-9778
STEP: creating replication controller affinity-nodeport in namespace services-9778
I0416 03:03:55.787513      25 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-9778, replica count: 3
I0416 03:03:58.838059      25 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 16 03:03:58.868: INFO: Creating new exec pod
Apr 16 03:04:01.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-9778 exec execpod-affinitydf4cg -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Apr 16 03:04:02.941: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Apr 16 03:04:02.941: INFO: stdout: ""
Apr 16 03:04:02.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-9778 exec execpod-affinitydf4cg -- /bin/sh -x -c nc -zv -t -w 2 10.254.190.166 80'
Apr 16 03:04:03.334: INFO: stderr: "+ nc -zv -t -w 2 10.254.190.166 80\nConnection to 10.254.190.166 80 port [tcp/http] succeeded!\n"
Apr 16 03:04:03.334: INFO: stdout: ""
Apr 16 03:04:03.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-9778 exec execpod-affinitydf4cg -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.13 32075'
Apr 16 03:04:03.666: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.13 32075\nConnection to 10.0.0.13 32075 port [tcp/32075] succeeded!\n"
Apr 16 03:04:03.666: INFO: stdout: ""
Apr 16 03:04:03.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-9778 exec execpod-affinitydf4cg -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.68 32075'
Apr 16 03:04:04.064: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.68 32075\nConnection to 10.0.0.68 32075 port [tcp/32075] succeeded!\n"
Apr 16 03:04:04.064: INFO: stdout: ""
Apr 16 03:04:04.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-565212062 --namespace=services-9778 exec execpod-affinitydf4cg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.13:32075/ ; done'
Apr 16 03:04:04.700: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.13:32075/\n"
Apr 16 03:04:04.700: INFO: stdout: "\naffinity-nodeport-p5hrc\naffinity-nodeport-p5hrc\naffinity-nodeport-p5hrc\naffinity-nodeport-p5hrc\naffinity-nodeport-p5hrc\naffinity-nodeport-p5hrc\naffinity-nodeport-p5hrc\naffinity-nodeport-p5hrc\naffinity-nodeport-p5hrc\naffinity-nodeport-p5hrc\naffinity-nodeport-p5hrc\naffinity-nodeport-p5hrc\naffinity-nodeport-p5hrc\naffinity-nodeport-p5hrc\naffinity-nodeport-p5hrc\naffinity-nodeport-p5hrc"
Apr 16 03:04:04.701: INFO: Received response from host: affinity-nodeport-p5hrc
Apr 16 03:04:04.701: INFO: Received response from host: affinity-nodeport-p5hrc
Apr 16 03:04:04.701: INFO: Received response from host: affinity-nodeport-p5hrc
Apr 16 03:04:04.701: INFO: Received response from host: affinity-nodeport-p5hrc
Apr 16 03:04:04.701: INFO: Received response from host: affinity-nodeport-p5hrc
Apr 16 03:04:04.701: INFO: Received response from host: affinity-nodeport-p5hrc
Apr 16 03:04:04.701: INFO: Received response from host: affinity-nodeport-p5hrc
Apr 16 03:04:04.701: INFO: Received response from host: affinity-nodeport-p5hrc
Apr 16 03:04:04.701: INFO: Received response from host: affinity-nodeport-p5hrc
Apr 16 03:04:04.701: INFO: Received response from host: affinity-nodeport-p5hrc
Apr 16 03:04:04.701: INFO: Received response from host: affinity-nodeport-p5hrc
Apr 16 03:04:04.701: INFO: Received response from host: affinity-nodeport-p5hrc
Apr 16 03:04:04.701: INFO: Received response from host: affinity-nodeport-p5hrc
Apr 16 03:04:04.701: INFO: Received response from host: affinity-nodeport-p5hrc
Apr 16 03:04:04.701: INFO: Received response from host: affinity-nodeport-p5hrc
Apr 16 03:04:04.701: INFO: Received response from host: affinity-nodeport-p5hrc
Apr 16 03:04:04.701: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9778, will wait for the garbage collector to delete the pods
Apr 16 03:04:04.869: INFO: Deleting ReplicationController affinity-nodeport took: 42.642423ms
Apr 16 03:04:05.969: INFO: Terminating ReplicationController affinity-nodeport pods took: 1.100300665s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 03:04:16.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9778" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:20.864 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":308,"skipped":5313,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 03:04:16.346: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-1551
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 16 03:04:16.734: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 16 03:05:16.850: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Apr 16 03:05:16.923: INFO: Created pod: pod0-sched-preemption-low-priority
Apr 16 03:05:16.984: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 03:05:27.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1551" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:71.011 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":309,"skipped":5333,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 03:05:27.359: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-947
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0416 03:05:37.752121      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0416 03:05:37.752196      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0416 03:05:37.752221      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 16 03:05:37.752: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 03:05:37.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-947" for this suite.

• [SLOW TEST:10.431 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":310,"skipped":5336,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 16 03:05:37.791: INFO: >>> kubeConfig: /tmp/kubeconfig-565212062
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8627
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 16 03:05:38.068: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1dca222-cb32-40e3-b6a8-2209bd6f8c6a" in namespace "downward-api-8627" to be "Succeeded or Failed"
Apr 16 03:05:38.080: INFO: Pod "downwardapi-volume-d1dca222-cb32-40e3-b6a8-2209bd6f8c6a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.844115ms
Apr 16 03:05:40.103: INFO: Pod "downwardapi-volume-d1dca222-cb32-40e3-b6a8-2209bd6f8c6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035654353s
STEP: Saw pod success
Apr 16 03:05:40.103: INFO: Pod "downwardapi-volume-d1dca222-cb32-40e3-b6a8-2209bd6f8c6a" satisfied condition "Succeeded or Failed"
Apr 16 03:05:40.121: INFO: Trying to get logs from node k8s-flwang-4oyd5hkwxglz-node-1 pod downwardapi-volume-d1dca222-cb32-40e3-b6a8-2209bd6f8c6a container client-container: <nil>
STEP: delete the pod
Apr 16 03:05:40.308: INFO: Waiting for pod downwardapi-volume-d1dca222-cb32-40e3-b6a8-2209bd6f8c6a to disappear
Apr 16 03:05:40.324: INFO: Pod downwardapi-volume-d1dca222-cb32-40e3-b6a8-2209bd6f8c6a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 16 03:05:40.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8627" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":311,"skipped":5353,"failed":0}
SSSApr 16 03:05:40.361: INFO: Running AfterSuite actions on all nodes
Apr 16 03:05:40.361: INFO: Running AfterSuite actions on node 1
Apr 16 03:05:40.361: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 5576.582 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
PASS

Ginkgo ran 1 suite in 1h32m58.087659878s
Test Suite Passed
