I0428 15:27:23.464734      24 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-091929713
I0428 15:27:23.464760      24 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0428 15:27:23.464841      24 e2e.go:129] Starting e2e run "eb4e9110-e70e-4e35-b39e-8a75e28ee659" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1619623642 - Will randomize all specs
Will run 311 of 5667 specs

Apr 28 15:27:23.482: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 15:27:23.484: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr 28 15:27:23.510: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr 28 15:27:23.537: INFO: 3 / 3 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr 28 15:27:23.537: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Apr 28 15:27:23.537: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr 28 15:27:23.545: INFO: e2e test version: v1.20.6
Apr 28 15:27:23.547: INFO: kube-apiserver version: v1.20.6
Apr 28 15:27:23.547: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 15:27:23.554: INFO: Cluster IP family: ipv4
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:27:23.554: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename statefulset
Apr 28 15:27:23.636: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Apr 28 15:27:23.652: INFO: PSP annotation exists on dry run pod: "privileged"; assuming PodSecurityPolicy is enabled
Apr 28 15:27:23.672: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9097
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9097
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-9097
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9097
Apr 28 15:27:23.824: INFO: Found 0 stateful pods, waiting for 1
Apr 28 15:27:33.835: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Apr 28 15:27:33.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 28 15:27:34.233: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 28 15:27:34.233: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 28 15:27:34.233: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 28 15:27:34.240: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 28 15:27:44.247: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 28 15:27:44.247: INFO: Waiting for statefulset status.replicas updated to 0
Apr 28 15:27:44.269: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Apr 28 15:27:44.269: INFO: ss-0  ip-172-31-13-33  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  }]
Apr 28 15:27:44.269: INFO: 
Apr 28 15:27:44.269: INFO: StatefulSet ss has not reached scale 3, at 1
Apr 28 15:27:45.277: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995143319s
Apr 28 15:27:46.285: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987640517s
Apr 28 15:27:47.293: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979653252s
Apr 28 15:27:48.300: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.971492767s
Apr 28 15:27:49.308: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.964204378s
Apr 28 15:27:50.315: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.956656559s
Apr 28 15:27:51.324: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.949693041s
Apr 28 15:27:52.331: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.940567846s
Apr 28 15:27:53.338: INFO: Verifying statefulset ss doesn't scale past 3 for another 933.261393ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9097
Apr 28 15:27:54.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:27:54.502: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 28 15:27:54.502: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 28 15:27:54.502: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 28 15:27:54.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:27:54.645: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 28 15:27:54.645: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 28 15:27:54.645: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 28 15:27:54.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:27:54.861: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 28 15:27:54.861: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 28 15:27:54.861: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 28 15:27:54.868: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Apr 28 15:28:04.877: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 28 15:28:04.877: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 28 15:28:04.877: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Apr 28 15:28:04.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 28 15:28:05.049: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 28 15:28:05.049: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 28 15:28:05.049: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 28 15:28:05.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 28 15:28:05.173: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 28 15:28:05.173: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 28 15:28:05.173: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 28 15:28:05.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 28 15:28:05.317: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 28 15:28:05.317: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 28 15:28:05.317: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 28 15:28:05.317: INFO: Waiting for statefulset status.replicas updated to 0
Apr 28 15:28:05.321: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Apr 28 15:28:15.337: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 28 15:28:15.337: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 28 15:28:15.337: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 28 15:28:15.356: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Apr 28 15:28:15.356: INFO: ss-0  ip-172-31-13-33  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  }]
Apr 28 15:28:15.356: INFO: ss-1  ip-172-31-13-33  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:15.356: INFO: ss-2  ip-172-31-76-85  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:15.356: INFO: 
Apr 28 15:28:15.356: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 28 15:28:16.363: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Apr 28 15:28:16.363: INFO: ss-0  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  }]
Apr 28 15:28:16.363: INFO: ss-1  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:16.363: INFO: ss-2  ip-172-31-76-85  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:16.363: INFO: 
Apr 28 15:28:16.363: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 28 15:28:17.371: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Apr 28 15:28:17.371: INFO: ss-0  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  }]
Apr 28 15:28:17.371: INFO: ss-1  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:17.371: INFO: ss-2  ip-172-31-76-85  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:17.371: INFO: 
Apr 28 15:28:17.371: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 28 15:28:18.378: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Apr 28 15:28:18.378: INFO: ss-0  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  }]
Apr 28 15:28:18.378: INFO: ss-1  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:18.378: INFO: ss-2  ip-172-31-76-85  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:18.378: INFO: 
Apr 28 15:28:18.378: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 28 15:28:19.385: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Apr 28 15:28:19.385: INFO: ss-0  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  }]
Apr 28 15:28:19.385: INFO: ss-1  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:19.385: INFO: ss-2  ip-172-31-76-85  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:19.385: INFO: 
Apr 28 15:28:19.385: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 28 15:28:20.399: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Apr 28 15:28:20.399: INFO: ss-0  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  }]
Apr 28 15:28:20.399: INFO: ss-1  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:20.399: INFO: ss-2  ip-172-31-76-85  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:20.399: INFO: 
Apr 28 15:28:20.399: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 28 15:28:21.407: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Apr 28 15:28:21.407: INFO: ss-0  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  }]
Apr 28 15:28:21.407: INFO: ss-1  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:21.407: INFO: ss-2  ip-172-31-76-85  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:21.407: INFO: 
Apr 28 15:28:21.407: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 28 15:28:22.413: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Apr 28 15:28:22.413: INFO: ss-0  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  }]
Apr 28 15:28:22.413: INFO: ss-1  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:22.413: INFO: ss-2  ip-172-31-76-85  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:22.413: INFO: 
Apr 28 15:28:22.413: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 28 15:28:23.426: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Apr 28 15:28:23.426: INFO: ss-0  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  }]
Apr 28 15:28:23.426: INFO: ss-1  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:23.426: INFO: ss-2  ip-172-31-76-85  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:23.426: INFO: 
Apr 28 15:28:23.426: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 28 15:28:24.435: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Apr 28 15:28:24.435: INFO: ss-0  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:23 +0000 UTC  }]
Apr 28 15:28:24.435: INFO: ss-1  ip-172-31-13-33  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:24.435: INFO: ss-2  ip-172-31-76-85  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:28:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 15:27:44 +0000 UTC  }]
Apr 28 15:28:24.435: INFO: 
Apr 28 15:28:24.435: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9097
Apr 28 15:28:25.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:28:25.525: INFO: rc: 1
Apr 28 15:28:25.525: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Apr 28 15:28:35.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:28:35.597: INFO: rc: 1
Apr 28 15:28:35.597: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:28:45.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:28:45.705: INFO: rc: 1
Apr 28 15:28:45.705: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:28:55.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:28:55.771: INFO: rc: 1
Apr 28 15:28:55.771: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:29:05.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:29:05.832: INFO: rc: 1
Apr 28 15:29:05.832: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:29:15.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:29:15.900: INFO: rc: 1
Apr 28 15:29:15.900: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:29:25.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:29:25.966: INFO: rc: 1
Apr 28 15:29:25.966: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:29:35.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:29:36.039: INFO: rc: 1
Apr 28 15:29:36.039: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:29:46.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:29:46.102: INFO: rc: 1
Apr 28 15:29:46.102: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:29:56.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:29:56.168: INFO: rc: 1
Apr 28 15:29:56.168: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:30:06.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:30:06.228: INFO: rc: 1
Apr 28 15:30:06.228: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:30:16.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:30:16.292: INFO: rc: 1
Apr 28 15:30:16.292: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:30:26.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:30:26.360: INFO: rc: 1
Apr 28 15:30:26.360: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:30:36.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:30:36.434: INFO: rc: 1
Apr 28 15:30:36.434: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:30:46.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:30:46.501: INFO: rc: 1
Apr 28 15:30:46.501: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:30:56.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:30:56.562: INFO: rc: 1
Apr 28 15:30:56.562: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:31:06.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:31:06.632: INFO: rc: 1
Apr 28 15:31:06.632: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:31:16.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:31:16.699: INFO: rc: 1
Apr 28 15:31:16.699: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:31:26.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:31:26.786: INFO: rc: 1
Apr 28 15:31:26.786: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:31:36.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:31:36.846: INFO: rc: 1
Apr 28 15:31:36.846: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:31:46.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:31:46.910: INFO: rc: 1
Apr 28 15:31:46.910: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:31:56.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:31:56.991: INFO: rc: 1
Apr 28 15:31:56.991: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:32:06.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:32:07.057: INFO: rc: 1
Apr 28 15:32:07.057: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:32:17.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:32:17.117: INFO: rc: 1
Apr 28 15:32:17.117: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:32:27.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:32:27.188: INFO: rc: 1
Apr 28 15:32:27.188: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:32:37.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:32:37.248: INFO: rc: 1
Apr 28 15:32:37.248: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:32:47.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:32:47.332: INFO: rc: 1
Apr 28 15:32:47.332: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:32:57.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:32:57.392: INFO: rc: 1
Apr 28 15:32:57.392: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:33:07.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:33:07.455: INFO: rc: 1
Apr 28 15:33:07.455: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:33:17.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:33:17.520: INFO: rc: 1
Apr 28 15:33:17.520: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Apr 28 15:33:27.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-9097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 15:33:27.591: INFO: rc: 1
Apr 28 15:33:27.591: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Apr 28 15:33:27.591: INFO: Scaling statefulset ss to 0
Apr 28 15:33:27.627: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 28 15:33:27.631: INFO: Deleting all statefulset in ns statefulset-9097
Apr 28 15:33:27.635: INFO: Scaling statefulset ss to 0
Apr 28 15:33:27.649: INFO: Waiting for statefulset status.replicas updated to 0
Apr 28 15:33:27.653: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:33:27.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9097" for this suite.

• [SLOW TEST:364.186 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":1,"skipped":3,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:33:27.740: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5885
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:33:32.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5885" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":2,"skipped":20,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:33:32.566: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2972
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:33:43.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2972" for this suite.

• [SLOW TEST:11.270 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":3,"skipped":40,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:33:43.837: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3024
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:33:46.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3024" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":4,"skipped":61,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:33:46.077: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6915
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-5eb8907e-5b72-45c6-ab6d-953d62492372
STEP: Creating a pod to test consume secrets
Apr 28 15:33:46.266: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4f2f5346-037c-425d-a577-8ad3824a5715" in namespace "projected-6915" to be "Succeeded or Failed"
Apr 28 15:33:46.274: INFO: Pod "pod-projected-secrets-4f2f5346-037c-425d-a577-8ad3824a5715": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094512ms
Apr 28 15:33:48.285: INFO: Pod "pod-projected-secrets-4f2f5346-037c-425d-a577-8ad3824a5715": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018769601s
Apr 28 15:33:50.295: INFO: Pod "pod-projected-secrets-4f2f5346-037c-425d-a577-8ad3824a5715": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02820811s
STEP: Saw pod success
Apr 28 15:33:50.295: INFO: Pod "pod-projected-secrets-4f2f5346-037c-425d-a577-8ad3824a5715" satisfied condition "Succeeded or Failed"
Apr 28 15:33:50.299: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-projected-secrets-4f2f5346-037c-425d-a577-8ad3824a5715 container secret-volume-test: <nil>
STEP: delete the pod
Apr 28 15:33:50.322: INFO: Waiting for pod pod-projected-secrets-4f2f5346-037c-425d-a577-8ad3824a5715 to disappear
Apr 28 15:33:50.326: INFO: Pod pod-projected-secrets-4f2f5346-037c-425d-a577-8ad3824a5715 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:33:50.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6915" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":5,"skipped":71,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:33:50.339: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9298
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9298
STEP: creating service affinity-clusterip in namespace services-9298
STEP: creating replication controller affinity-clusterip in namespace services-9298
I0428 15:33:50.561519      24 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-9298, replica count: 3
I0428 15:33:53.611777      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0428 15:33:56.611892      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 28 15:33:56.622: INFO: Creating new exec pod
Apr 28 15:33:59.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9298 exec execpod-affinityqchdl -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Apr 28 15:33:59.819: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Apr 28 15:33:59.819: INFO: stdout: ""
Apr 28 15:33:59.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9298 exec execpod-affinityqchdl -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.137 80'
Apr 28 15:33:59.944: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.137 80\nConnection to 10.152.183.137 80 port [tcp/http] succeeded!\n"
Apr 28 15:33:59.944: INFO: stdout: ""
Apr 28 15:33:59.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9298 exec execpod-affinityqchdl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.137:80/ ; done'
Apr 28 15:34:00.127: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.137:80/\n"
Apr 28 15:34:00.127: INFO: stdout: "\naffinity-clusterip-cxbn8\naffinity-clusterip-cxbn8\naffinity-clusterip-cxbn8\naffinity-clusterip-cxbn8\naffinity-clusterip-cxbn8\naffinity-clusterip-cxbn8\naffinity-clusterip-cxbn8\naffinity-clusterip-cxbn8\naffinity-clusterip-cxbn8\naffinity-clusterip-cxbn8\naffinity-clusterip-cxbn8\naffinity-clusterip-cxbn8\naffinity-clusterip-cxbn8\naffinity-clusterip-cxbn8\naffinity-clusterip-cxbn8\naffinity-clusterip-cxbn8"
Apr 28 15:34:00.127: INFO: Received response from host: affinity-clusterip-cxbn8
Apr 28 15:34:00.127: INFO: Received response from host: affinity-clusterip-cxbn8
Apr 28 15:34:00.127: INFO: Received response from host: affinity-clusterip-cxbn8
Apr 28 15:34:00.127: INFO: Received response from host: affinity-clusterip-cxbn8
Apr 28 15:34:00.127: INFO: Received response from host: affinity-clusterip-cxbn8
Apr 28 15:34:00.127: INFO: Received response from host: affinity-clusterip-cxbn8
Apr 28 15:34:00.127: INFO: Received response from host: affinity-clusterip-cxbn8
Apr 28 15:34:00.127: INFO: Received response from host: affinity-clusterip-cxbn8
Apr 28 15:34:00.127: INFO: Received response from host: affinity-clusterip-cxbn8
Apr 28 15:34:00.127: INFO: Received response from host: affinity-clusterip-cxbn8
Apr 28 15:34:00.127: INFO: Received response from host: affinity-clusterip-cxbn8
Apr 28 15:34:00.127: INFO: Received response from host: affinity-clusterip-cxbn8
Apr 28 15:34:00.127: INFO: Received response from host: affinity-clusterip-cxbn8
Apr 28 15:34:00.127: INFO: Received response from host: affinity-clusterip-cxbn8
Apr 28 15:34:00.127: INFO: Received response from host: affinity-clusterip-cxbn8
Apr 28 15:34:00.127: INFO: Received response from host: affinity-clusterip-cxbn8
Apr 28 15:34:00.127: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9298, will wait for the garbage collector to delete the pods
Apr 28 15:34:00.214: INFO: Deleting ReplicationController affinity-clusterip took: 11.978584ms
Apr 28 15:34:00.815: INFO: Terminating ReplicationController affinity-clusterip pods took: 600.147639ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:34:15.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9298" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:24.863 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":6,"skipped":76,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:34:15.202: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9186
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-dz59
STEP: Creating a pod to test atomic-volume-subpath
Apr 28 15:34:15.408: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-dz59" in namespace "subpath-9186" to be "Succeeded or Failed"
Apr 28 15:34:15.423: INFO: Pod "pod-subpath-test-secret-dz59": Phase="Pending", Reason="", readiness=false. Elapsed: 15.39394ms
Apr 28 15:34:17.433: INFO: Pod "pod-subpath-test-secret-dz59": Phase="Running", Reason="", readiness=true. Elapsed: 2.025447767s
Apr 28 15:34:19.442: INFO: Pod "pod-subpath-test-secret-dz59": Phase="Running", Reason="", readiness=true. Elapsed: 4.034086565s
Apr 28 15:34:21.452: INFO: Pod "pod-subpath-test-secret-dz59": Phase="Running", Reason="", readiness=true. Elapsed: 6.044084878s
Apr 28 15:34:23.458: INFO: Pod "pod-subpath-test-secret-dz59": Phase="Running", Reason="", readiness=true. Elapsed: 8.050107341s
Apr 28 15:34:25.465: INFO: Pod "pod-subpath-test-secret-dz59": Phase="Running", Reason="", readiness=true. Elapsed: 10.057589613s
Apr 28 15:34:27.472: INFO: Pod "pod-subpath-test-secret-dz59": Phase="Running", Reason="", readiness=true. Elapsed: 12.063777426s
Apr 28 15:34:29.493: INFO: Pod "pod-subpath-test-secret-dz59": Phase="Running", Reason="", readiness=true. Elapsed: 14.085106747s
Apr 28 15:34:31.514: INFO: Pod "pod-subpath-test-secret-dz59": Phase="Running", Reason="", readiness=true. Elapsed: 16.10605404s
Apr 28 15:34:33.526: INFO: Pod "pod-subpath-test-secret-dz59": Phase="Running", Reason="", readiness=true. Elapsed: 18.118151766s
Apr 28 15:34:35.534: INFO: Pod "pod-subpath-test-secret-dz59": Phase="Running", Reason="", readiness=true. Elapsed: 20.126562961s
Apr 28 15:34:37.542: INFO: Pod "pod-subpath-test-secret-dz59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.134428697s
STEP: Saw pod success
Apr 28 15:34:37.542: INFO: Pod "pod-subpath-test-secret-dz59" satisfied condition "Succeeded or Failed"
Apr 28 15:34:37.546: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-subpath-test-secret-dz59 container test-container-subpath-secret-dz59: <nil>
STEP: delete the pod
Apr 28 15:34:37.574: INFO: Waiting for pod pod-subpath-test-secret-dz59 to disappear
Apr 28 15:34:37.584: INFO: Pod pod-subpath-test-secret-dz59 no longer exists
STEP: Deleting pod pod-subpath-test-secret-dz59
Apr 28 15:34:37.584: INFO: Deleting pod "pod-subpath-test-secret-dz59" in namespace "subpath-9186"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:34:37.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9186" for this suite.

• [SLOW TEST:22.404 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":7,"skipped":101,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:34:37.606: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8330
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8330
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8330
STEP: Creating statefulset with conflicting port in namespace statefulset-8330
STEP: Waiting until pod test-pod will start running in namespace statefulset-8330
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8330
Apr 28 15:34:43.843: INFO: Observed stateful pod in namespace: statefulset-8330, name: ss-0, uid: fcc9978a-b105-402d-9009-b9d1be3398cf, status phase: Pending. Waiting for statefulset controller to delete.
Apr 28 15:34:44.025: INFO: Observed stateful pod in namespace: statefulset-8330, name: ss-0, uid: fcc9978a-b105-402d-9009-b9d1be3398cf, status phase: Failed. Waiting for statefulset controller to delete.
Apr 28 15:34:44.037: INFO: Observed stateful pod in namespace: statefulset-8330, name: ss-0, uid: fcc9978a-b105-402d-9009-b9d1be3398cf, status phase: Failed. Waiting for statefulset controller to delete.
Apr 28 15:34:44.046: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8330
STEP: Removing pod with conflicting port in namespace statefulset-8330
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8330 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 28 15:34:46.106: INFO: Deleting all statefulset in ns statefulset-8330
Apr 28 15:34:46.110: INFO: Scaling statefulset ss to 0
Apr 28 15:34:56.141: INFO: Waiting for statefulset status.replicas updated to 0
Apr 28 15:34:56.146: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:34:56.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8330" for this suite.

• [SLOW TEST:18.610 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":8,"skipped":119,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:34:56.216: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3043
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Apr 28 15:34:56.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-3043 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Apr 28 15:34:56.472: INFO: stderr: ""
Apr 28 15:34:56.472: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Apr 28 15:34:56.472: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Apr 28 15:34:56.472: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3043" to be "running and ready, or succeeded"
Apr 28 15:34:56.482: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.265071ms
Apr 28 15:34:58.493: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.02063118s
Apr 28 15:34:58.493: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Apr 28 15:34:58.493: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Apr 28 15:34:58.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-3043 logs logs-generator logs-generator'
Apr 28 15:34:58.577: INFO: stderr: ""
Apr 28 15:34:58.577: INFO: stdout: "I0428 15:34:57.039667       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/rfhw 336\nI0428 15:34:57.239790       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/8qcr 592\nI0428 15:34:57.439771       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/g7g 246\nI0428 15:34:57.639799       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/gb8 269\nI0428 15:34:57.839775       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/l7d8 242\nI0428 15:34:58.039770       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/vwg 311\nI0428 15:34:58.239704       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/7dh 521\nI0428 15:34:58.439777       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/zjmc 518\n"
STEP: limiting log lines
Apr 28 15:34:58.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-3043 logs logs-generator logs-generator --tail=1'
Apr 28 15:34:58.655: INFO: stderr: ""
Apr 28 15:34:58.655: INFO: stdout: "I0428 15:34:58.639767       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/w6fv 331\n"
Apr 28 15:34:58.655: INFO: got output "I0428 15:34:58.639767       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/w6fv 331\n"
STEP: limiting log bytes
Apr 28 15:34:58.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-3043 logs logs-generator logs-generator --limit-bytes=1'
Apr 28 15:34:58.725: INFO: stderr: ""
Apr 28 15:34:58.725: INFO: stdout: "I"
Apr 28 15:34:58.725: INFO: got output "I"
STEP: exposing timestamps
Apr 28 15:34:58.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-3043 logs logs-generator logs-generator --tail=1 --timestamps'
Apr 28 15:34:58.800: INFO: stderr: ""
Apr 28 15:34:58.800: INFO: stdout: "2021-04-28T15:34:58.639868531Z I0428 15:34:58.639767       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/w6fv 331\n"
Apr 28 15:34:58.800: INFO: got output "2021-04-28T15:34:58.639868531Z I0428 15:34:58.639767       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/w6fv 331\n"
STEP: restricting to a time range
Apr 28 15:35:01.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-3043 logs logs-generator logs-generator --since=1s'
Apr 28 15:35:01.377: INFO: stderr: ""
Apr 28 15:35:01.377: INFO: stdout: "I0428 15:35:00.439784       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/tlk 236\nI0428 15:35:00.639801       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/nlch 545\nI0428 15:35:00.839782       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/lfc 276\nI0428 15:35:01.039776       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/rwp 277\nI0428 15:35:01.239807       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/kv7 350\n"
Apr 28 15:35:01.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-3043 logs logs-generator logs-generator --since=24h'
Apr 28 15:35:01.458: INFO: stderr: ""
Apr 28 15:35:01.458: INFO: stdout: "I0428 15:34:57.039667       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/rfhw 336\nI0428 15:34:57.239790       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/8qcr 592\nI0428 15:34:57.439771       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/g7g 246\nI0428 15:34:57.639799       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/gb8 269\nI0428 15:34:57.839775       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/l7d8 242\nI0428 15:34:58.039770       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/vwg 311\nI0428 15:34:58.239704       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/7dh 521\nI0428 15:34:58.439777       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/zjmc 518\nI0428 15:34:58.639767       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/w6fv 331\nI0428 15:34:58.839760       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/8dz 236\nI0428 15:34:59.039763       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/7gt9 532\nI0428 15:34:59.239768       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/ds65 462\nI0428 15:34:59.439779       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/64ff 460\nI0428 15:34:59.639779       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/gqt 440\nI0428 15:34:59.839786       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/f6xb 370\nI0428 15:35:00.039781       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/p4dz 310\nI0428 15:35:00.239789       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/kmsl 295\nI0428 15:35:00.439784       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/tlk 236\nI0428 15:35:00.639801       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/nlch 545\nI0428 15:35:00.839782       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/lfc 276\nI0428 15:35:01.039776       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/rwp 277\nI0428 15:35:01.239807       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/kv7 350\nI0428 15:35:01.439785       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/5xd 335\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Apr 28 15:35:01.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-3043 delete pod logs-generator'
Apr 28 15:35:03.383: INFO: stderr: ""
Apr 28 15:35:03.383: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:35:03.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3043" for this suite.

• [SLOW TEST:7.190 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":9,"skipped":125,"failed":0}
S
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:35:03.406: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3635
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-3635
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3635 to expose endpoints map[]
Apr 28 15:35:03.625: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Apr 28 15:35:04.638: INFO: successfully validated that service multi-endpoint-test in namespace services-3635 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3635
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3635 to expose endpoints map[pod1:[100]]
Apr 28 15:35:05.719: INFO: successfully validated that service multi-endpoint-test in namespace services-3635 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3635
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3635 to expose endpoints map[pod1:[100] pod2:[101]]
Apr 28 15:35:07.769: INFO: successfully validated that service multi-endpoint-test in namespace services-3635 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-3635
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3635 to expose endpoints map[pod2:[101]]
Apr 28 15:35:07.828: INFO: successfully validated that service multi-endpoint-test in namespace services-3635 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3635
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3635 to expose endpoints map[]
Apr 28 15:35:07.875: INFO: successfully validated that service multi-endpoint-test in namespace services-3635 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:35:07.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3635" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":10,"skipped":126,"failed":0}
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:35:07.927: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7605
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-5def3eff-c05e-4e56-8eb2-3baa27f2380a
STEP: Creating a pod to test consume secrets
Apr 28 15:35:08.138: INFO: Waiting up to 5m0s for pod "pod-secrets-4a20b64a-b463-46c2-b279-bececfa6c462" in namespace "secrets-7605" to be "Succeeded or Failed"
Apr 28 15:35:08.141: INFO: Pod "pod-secrets-4a20b64a-b463-46c2-b279-bececfa6c462": Phase="Pending", Reason="", readiness=false. Elapsed: 3.568998ms
Apr 28 15:35:10.149: INFO: Pod "pod-secrets-4a20b64a-b463-46c2-b279-bececfa6c462": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0112003s
STEP: Saw pod success
Apr 28 15:35:10.149: INFO: Pod "pod-secrets-4a20b64a-b463-46c2-b279-bececfa6c462" satisfied condition "Succeeded or Failed"
Apr 28 15:35:10.154: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-secrets-4a20b64a-b463-46c2-b279-bececfa6c462 container secret-env-test: <nil>
STEP: delete the pod
Apr 28 15:35:10.176: INFO: Waiting for pod pod-secrets-4a20b64a-b463-46c2-b279-bececfa6c462 to disappear
Apr 28 15:35:10.180: INFO: Pod pod-secrets-4a20b64a-b463-46c2-b279-bececfa6c462 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:35:10.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7605" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":11,"skipped":128,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:35:10.191: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-866
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 15:35:10.375: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:35:11.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-866" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":12,"skipped":130,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:35:11.683: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1691
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-b8aa5cd0-8016-46ac-b5f4-d03fd3cc6845
STEP: Creating a pod to test consume configMaps
Apr 28 15:35:11.855: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6bd0b7ef-ea97-49a1-9b8e-6e6b7569d1d3" in namespace "projected-1691" to be "Succeeded or Failed"
Apr 28 15:35:11.859: INFO: Pod "pod-projected-configmaps-6bd0b7ef-ea97-49a1-9b8e-6e6b7569d1d3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.447922ms
Apr 28 15:35:13.865: INFO: Pod "pod-projected-configmaps-6bd0b7ef-ea97-49a1-9b8e-6e6b7569d1d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010516761s
STEP: Saw pod success
Apr 28 15:35:13.865: INFO: Pod "pod-projected-configmaps-6bd0b7ef-ea97-49a1-9b8e-6e6b7569d1d3" satisfied condition "Succeeded or Failed"
Apr 28 15:35:13.870: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-projected-configmaps-6bd0b7ef-ea97-49a1-9b8e-6e6b7569d1d3 container agnhost-container: <nil>
STEP: delete the pod
Apr 28 15:35:13.892: INFO: Waiting for pod pod-projected-configmaps-6bd0b7ef-ea97-49a1-9b8e-6e6b7569d1d3 to disappear
Apr 28 15:35:13.896: INFO: Pod pod-projected-configmaps-6bd0b7ef-ea97-49a1-9b8e-6e6b7569d1d3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:35:13.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1691" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":13,"skipped":150,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:35:13.911: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2893
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Apr 28 15:35:14.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-2893 create -f -'
Apr 28 15:35:14.515: INFO: stderr: ""
Apr 28 15:35:14.515: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Apr 28 15:35:14.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-2893 diff -f -'
Apr 28 15:35:14.781: INFO: rc: 1
Apr 28 15:35:14.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-2893 delete -f -'
Apr 28 15:35:14.845: INFO: stderr: ""
Apr 28 15:35:14.846: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:35:14.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2893" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":14,"skipped":157,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:35:14.865: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6627
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 28 15:35:15.379: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 28 15:35:17.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755220915, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755220915, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755220915, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755220915, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 15:35:20.437: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:35:30.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6627" for this suite.
STEP: Destroying namespace "webhook-6627-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:15.983 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":15,"skipped":160,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:35:30.847: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9460
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9460
Apr 28 15:35:33.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9460 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Apr 28 15:35:33.266: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Apr 28 15:35:33.266: INFO: stdout: "iptables"
Apr 28 15:35:33.266: INFO: proxyMode: iptables
Apr 28 15:35:33.314: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 28 15:35:33.320: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-9460
STEP: creating replication controller affinity-clusterip-timeout in namespace services-9460
I0428 15:35:33.372793      24 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9460, replica count: 3
I0428 15:35:36.423024      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0428 15:35:39.423148      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 28 15:35:39.451: INFO: Creating new exec pod
Apr 28 15:35:42.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9460 exec execpod-affinitysfq5f -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Apr 28 15:35:42.655: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Apr 28 15:35:42.655: INFO: stdout: ""
Apr 28 15:35:42.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9460 exec execpod-affinitysfq5f -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.58 80'
Apr 28 15:35:42.775: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.58 80\nConnection to 10.152.183.58 80 port [tcp/http] succeeded!\n"
Apr 28 15:35:42.775: INFO: stdout: ""
Apr 28 15:35:42.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9460 exec execpod-affinitysfq5f -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.58:80/ ; done'
Apr 28 15:35:42.966: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n"
Apr 28 15:35:42.966: INFO: stdout: "\naffinity-clusterip-timeout-xbdqd\naffinity-clusterip-timeout-xbdqd\naffinity-clusterip-timeout-xbdqd\naffinity-clusterip-timeout-xbdqd\naffinity-clusterip-timeout-xbdqd\naffinity-clusterip-timeout-xbdqd\naffinity-clusterip-timeout-xbdqd\naffinity-clusterip-timeout-xbdqd\naffinity-clusterip-timeout-xbdqd\naffinity-clusterip-timeout-xbdqd\naffinity-clusterip-timeout-xbdqd\naffinity-clusterip-timeout-xbdqd\naffinity-clusterip-timeout-xbdqd\naffinity-clusterip-timeout-xbdqd\naffinity-clusterip-timeout-xbdqd\naffinity-clusterip-timeout-xbdqd"
Apr 28 15:35:42.966: INFO: Received response from host: affinity-clusterip-timeout-xbdqd
Apr 28 15:35:42.966: INFO: Received response from host: affinity-clusterip-timeout-xbdqd
Apr 28 15:35:42.966: INFO: Received response from host: affinity-clusterip-timeout-xbdqd
Apr 28 15:35:42.966: INFO: Received response from host: affinity-clusterip-timeout-xbdqd
Apr 28 15:35:42.966: INFO: Received response from host: affinity-clusterip-timeout-xbdqd
Apr 28 15:35:42.966: INFO: Received response from host: affinity-clusterip-timeout-xbdqd
Apr 28 15:35:42.966: INFO: Received response from host: affinity-clusterip-timeout-xbdqd
Apr 28 15:35:42.966: INFO: Received response from host: affinity-clusterip-timeout-xbdqd
Apr 28 15:35:42.966: INFO: Received response from host: affinity-clusterip-timeout-xbdqd
Apr 28 15:35:42.966: INFO: Received response from host: affinity-clusterip-timeout-xbdqd
Apr 28 15:35:42.966: INFO: Received response from host: affinity-clusterip-timeout-xbdqd
Apr 28 15:35:42.966: INFO: Received response from host: affinity-clusterip-timeout-xbdqd
Apr 28 15:35:42.966: INFO: Received response from host: affinity-clusterip-timeout-xbdqd
Apr 28 15:35:42.966: INFO: Received response from host: affinity-clusterip-timeout-xbdqd
Apr 28 15:35:42.966: INFO: Received response from host: affinity-clusterip-timeout-xbdqd
Apr 28 15:35:42.966: INFO: Received response from host: affinity-clusterip-timeout-xbdqd
Apr 28 15:35:42.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9460 exec execpod-affinitysfq5f -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.152.183.58:80/'
Apr 28 15:35:43.097: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n"
Apr 28 15:35:43.097: INFO: stdout: "affinity-clusterip-timeout-xbdqd"
Apr 28 15:36:03.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9460 exec execpod-affinitysfq5f -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.152.183.58:80/'
Apr 28 15:36:03.256: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.152.183.58:80/\n"
Apr 28 15:36:03.256: INFO: stdout: "affinity-clusterip-timeout-mnspr"
Apr 28 15:36:03.256: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9460, will wait for the garbage collector to delete the pods
Apr 28 15:36:03.350: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 11.766922ms
Apr 28 15:36:03.950: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 600.155237ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:36:15.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9460" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:44.370 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":16,"skipped":165,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:36:15.218: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-292
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 15:36:15.382: INFO: Waiting up to 5m0s for pod "downwardapi-volume-467827a1-57ef-44e6-977b-2062726ef567" in namespace "projected-292" to be "Succeeded or Failed"
Apr 28 15:36:15.392: INFO: Pod "downwardapi-volume-467827a1-57ef-44e6-977b-2062726ef567": Phase="Pending", Reason="", readiness=false. Elapsed: 9.965323ms
Apr 28 15:36:17.399: INFO: Pod "downwardapi-volume-467827a1-57ef-44e6-977b-2062726ef567": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016684324s
STEP: Saw pod success
Apr 28 15:36:17.399: INFO: Pod "downwardapi-volume-467827a1-57ef-44e6-977b-2062726ef567" satisfied condition "Succeeded or Failed"
Apr 28 15:36:17.402: INFO: Trying to get logs from node ip-172-31-13-33 pod downwardapi-volume-467827a1-57ef-44e6-977b-2062726ef567 container client-container: <nil>
STEP: delete the pod
Apr 28 15:36:17.436: INFO: Waiting for pod downwardapi-volume-467827a1-57ef-44e6-977b-2062726ef567 to disappear
Apr 28 15:36:17.439: INFO: Pod downwardapi-volume-467827a1-57ef-44e6-977b-2062726ef567 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:36:17.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-292" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":17,"skipped":178,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:36:17.451: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9932
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 28 15:36:17.632: INFO: Waiting up to 5m0s for pod "downward-api-23148302-aa37-405e-a0b8-a976a8d896bb" in namespace "downward-api-9932" to be "Succeeded or Failed"
Apr 28 15:36:17.637: INFO: Pod "downward-api-23148302-aa37-405e-a0b8-a976a8d896bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.780967ms
Apr 28 15:36:19.647: INFO: Pod "downward-api-23148302-aa37-405e-a0b8-a976a8d896bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014555855s
STEP: Saw pod success
Apr 28 15:36:19.647: INFO: Pod "downward-api-23148302-aa37-405e-a0b8-a976a8d896bb" satisfied condition "Succeeded or Failed"
Apr 28 15:36:19.655: INFO: Trying to get logs from node ip-172-31-13-33 pod downward-api-23148302-aa37-405e-a0b8-a976a8d896bb container dapi-container: <nil>
STEP: delete the pod
Apr 28 15:36:19.688: INFO: Waiting for pod downward-api-23148302-aa37-405e-a0b8-a976a8d896bb to disappear
Apr 28 15:36:19.696: INFO: Pod downward-api-23148302-aa37-405e-a0b8-a976a8d896bb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:36:19.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9932" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":18,"skipped":179,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:36:19.726: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2044
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-z7tg
STEP: Creating a pod to test atomic-volume-subpath
Apr 28 15:36:20.008: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-z7tg" in namespace "subpath-2044" to be "Succeeded or Failed"
Apr 28 15:36:20.017: INFO: Pod "pod-subpath-test-configmap-z7tg": Phase="Pending", Reason="", readiness=false. Elapsed: 8.820277ms
Apr 28 15:36:22.025: INFO: Pod "pod-subpath-test-configmap-z7tg": Phase="Running", Reason="", readiness=true. Elapsed: 2.017032606s
Apr 28 15:36:24.031: INFO: Pod "pod-subpath-test-configmap-z7tg": Phase="Running", Reason="", readiness=true. Elapsed: 4.022760646s
Apr 28 15:36:26.044: INFO: Pod "pod-subpath-test-configmap-z7tg": Phase="Running", Reason="", readiness=true. Elapsed: 6.035997716s
Apr 28 15:36:28.055: INFO: Pod "pod-subpath-test-configmap-z7tg": Phase="Running", Reason="", readiness=true. Elapsed: 8.046200683s
Apr 28 15:36:30.076: INFO: Pod "pod-subpath-test-configmap-z7tg": Phase="Running", Reason="", readiness=true. Elapsed: 10.067721457s
Apr 28 15:36:32.084: INFO: Pod "pod-subpath-test-configmap-z7tg": Phase="Running", Reason="", readiness=true. Elapsed: 12.075446466s
Apr 28 15:36:34.091: INFO: Pod "pod-subpath-test-configmap-z7tg": Phase="Running", Reason="", readiness=true. Elapsed: 14.082991369s
Apr 28 15:36:36.103: INFO: Pod "pod-subpath-test-configmap-z7tg": Phase="Running", Reason="", readiness=true. Elapsed: 16.094638689s
Apr 28 15:36:38.113: INFO: Pod "pod-subpath-test-configmap-z7tg": Phase="Running", Reason="", readiness=true. Elapsed: 18.104492694s
Apr 28 15:36:40.128: INFO: Pod "pod-subpath-test-configmap-z7tg": Phase="Running", Reason="", readiness=true. Elapsed: 20.119474099s
Apr 28 15:36:42.136: INFO: Pod "pod-subpath-test-configmap-z7tg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.127539072s
STEP: Saw pod success
Apr 28 15:36:42.136: INFO: Pod "pod-subpath-test-configmap-z7tg" satisfied condition "Succeeded or Failed"
Apr 28 15:36:42.140: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-subpath-test-configmap-z7tg container test-container-subpath-configmap-z7tg: <nil>
STEP: delete the pod
Apr 28 15:36:42.166: INFO: Waiting for pod pod-subpath-test-configmap-z7tg to disappear
Apr 28 15:36:42.170: INFO: Pod pod-subpath-test-configmap-z7tg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-z7tg
Apr 28 15:36:42.170: INFO: Deleting pod "pod-subpath-test-configmap-z7tg" in namespace "subpath-2044"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:36:42.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2044" for this suite.

• [SLOW TEST:22.467 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":19,"skipped":183,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:36:42.194: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-193
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-mldq
STEP: Creating a pod to test atomic-volume-subpath
Apr 28 15:36:42.399: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mldq" in namespace "subpath-193" to be "Succeeded or Failed"
Apr 28 15:36:42.402: INFO: Pod "pod-subpath-test-configmap-mldq": Phase="Pending", Reason="", readiness=false. Elapsed: 3.639943ms
Apr 28 15:36:44.407: INFO: Pod "pod-subpath-test-configmap-mldq": Phase="Running", Reason="", readiness=true. Elapsed: 2.008880758s
Apr 28 15:36:46.422: INFO: Pod "pod-subpath-test-configmap-mldq": Phase="Running", Reason="", readiness=true. Elapsed: 4.023239509s
Apr 28 15:36:48.431: INFO: Pod "pod-subpath-test-configmap-mldq": Phase="Running", Reason="", readiness=true. Elapsed: 6.03231016s
Apr 28 15:36:50.441: INFO: Pod "pod-subpath-test-configmap-mldq": Phase="Running", Reason="", readiness=true. Elapsed: 8.041954965s
Apr 28 15:36:52.448: INFO: Pod "pod-subpath-test-configmap-mldq": Phase="Running", Reason="", readiness=true. Elapsed: 10.049855435s
Apr 28 15:36:54.455: INFO: Pod "pod-subpath-test-configmap-mldq": Phase="Running", Reason="", readiness=true. Elapsed: 12.056173364s
Apr 28 15:36:56.460: INFO: Pod "pod-subpath-test-configmap-mldq": Phase="Running", Reason="", readiness=true. Elapsed: 14.061198271s
Apr 28 15:36:58.469: INFO: Pod "pod-subpath-test-configmap-mldq": Phase="Running", Reason="", readiness=true. Elapsed: 16.070064092s
Apr 28 15:37:00.481: INFO: Pod "pod-subpath-test-configmap-mldq": Phase="Running", Reason="", readiness=true. Elapsed: 18.082507311s
Apr 28 15:37:02.490: INFO: Pod "pod-subpath-test-configmap-mldq": Phase="Running", Reason="", readiness=true. Elapsed: 20.091420425s
Apr 28 15:37:04.497: INFO: Pod "pod-subpath-test-configmap-mldq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.098162315s
STEP: Saw pod success
Apr 28 15:37:04.497: INFO: Pod "pod-subpath-test-configmap-mldq" satisfied condition "Succeeded or Failed"
Apr 28 15:37:04.502: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-subpath-test-configmap-mldq container test-container-subpath-configmap-mldq: <nil>
STEP: delete the pod
Apr 28 15:37:04.528: INFO: Waiting for pod pod-subpath-test-configmap-mldq to disappear
Apr 28 15:37:04.533: INFO: Pod pod-subpath-test-configmap-mldq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mldq
Apr 28 15:37:04.533: INFO: Deleting pod "pod-subpath-test-configmap-mldq" in namespace "subpath-193"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:37:04.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-193" for this suite.

• [SLOW TEST:22.363 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":20,"skipped":221,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:37:04.557: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4838
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 15:37:04.723: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:37:05.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4838" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":21,"skipped":228,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:37:05.769: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8839
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-77915c8a-a7af-40b3-827d-7eb60f1c8093
STEP: Creating configMap with name cm-test-opt-upd-31b9670c-64c8-48c8-b783-b3a5c80b37ee
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-77915c8a-a7af-40b3-827d-7eb60f1c8093
STEP: Updating configmap cm-test-opt-upd-31b9670c-64c8-48c8-b783-b3a5c80b37ee
STEP: Creating configMap with name cm-test-opt-create-74bf6969-dc15-4cd5-a00b-9853ea955870
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:37:12.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8839" for this suite.

• [SLOW TEST:6.326 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":22,"skipped":260,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:37:12.096: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Apr 28 15:37:12.282: INFO: Waiting up to 5m0s for pod "var-expansion-6db60d3d-2d83-4583-9041-cee617d7d0eb" in namespace "var-expansion-5539" to be "Succeeded or Failed"
Apr 28 15:37:12.308: INFO: Pod "var-expansion-6db60d3d-2d83-4583-9041-cee617d7d0eb": Phase="Pending", Reason="", readiness=false. Elapsed: 25.787958ms
Apr 28 15:37:14.317: INFO: Pod "var-expansion-6db60d3d-2d83-4583-9041-cee617d7d0eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035386147s
STEP: Saw pod success
Apr 28 15:37:14.317: INFO: Pod "var-expansion-6db60d3d-2d83-4583-9041-cee617d7d0eb" satisfied condition "Succeeded or Failed"
Apr 28 15:37:14.321: INFO: Trying to get logs from node ip-172-31-30-112 pod var-expansion-6db60d3d-2d83-4583-9041-cee617d7d0eb container dapi-container: <nil>
STEP: delete the pod
Apr 28 15:37:14.358: INFO: Waiting for pod var-expansion-6db60d3d-2d83-4583-9041-cee617d7d0eb to disappear
Apr 28 15:37:14.361: INFO: Pod var-expansion-6db60d3d-2d83-4583-9041-cee617d7d0eb no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:37:14.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5539" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":23,"skipped":265,"failed":0}

------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:37:14.376: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-1134
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Apr 28 15:37:18.581: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1134 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:37:18.581: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 15:37:18.664: INFO: Exec stderr: ""
Apr 28 15:37:18.664: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1134 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:37:18.664: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 15:37:18.735: INFO: Exec stderr: ""
Apr 28 15:37:18.735: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1134 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:37:18.735: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 15:37:18.803: INFO: Exec stderr: ""
Apr 28 15:37:18.803: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1134 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:37:18.803: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 15:37:18.866: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Apr 28 15:37:18.866: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1134 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:37:18.866: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 15:37:18.907: INFO: Exec stderr: ""
Apr 28 15:37:18.907: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1134 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:37:18.907: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 15:37:18.988: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Apr 28 15:37:18.988: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1134 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:37:18.988: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 15:37:19.088: INFO: Exec stderr: ""
Apr 28 15:37:19.088: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1134 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:37:19.088: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 15:37:19.147: INFO: Exec stderr: ""
Apr 28 15:37:19.147: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1134 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:37:19.147: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 15:37:19.194: INFO: Exec stderr: ""
Apr 28 15:37:19.194: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1134 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:37:19.194: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 15:37:19.260: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:37:19.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1134" for this suite.
•{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":24,"skipped":265,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:37:19.274: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7948
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Apr 28 15:37:19.984: INFO: created pod pod-service-account-defaultsa
Apr 28 15:37:19.985: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr 28 15:37:19.994: INFO: created pod pod-service-account-mountsa
Apr 28 15:37:19.994: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr 28 15:37:20.001: INFO: created pod pod-service-account-nomountsa
Apr 28 15:37:20.001: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr 28 15:37:20.022: INFO: created pod pod-service-account-defaultsa-mountspec
Apr 28 15:37:20.022: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr 28 15:37:20.039: INFO: created pod pod-service-account-mountsa-mountspec
Apr 28 15:37:20.039: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr 28 15:37:20.046: INFO: created pod pod-service-account-nomountsa-mountspec
Apr 28 15:37:20.046: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr 28 15:37:20.054: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr 28 15:37:20.055: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr 28 15:37:20.063: INFO: created pod pod-service-account-mountsa-nomountspec
Apr 28 15:37:20.063: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr 28 15:37:20.070: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr 28 15:37:20.070: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:37:20.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7948" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":25,"skipped":271,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:37:20.087: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6665
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:37:23.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6665" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":26,"skipped":294,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:37:23.791: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9348
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Apr 28 15:37:24.063: INFO: Waiting up to 5m0s for pod "pod-d3ad4cb1-1d62-49d8-b690-ad42023f062d" in namespace "emptydir-9348" to be "Succeeded or Failed"
Apr 28 15:37:24.070: INFO: Pod "pod-d3ad4cb1-1d62-49d8-b690-ad42023f062d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.779329ms
Apr 28 15:37:26.078: INFO: Pod "pod-d3ad4cb1-1d62-49d8-b690-ad42023f062d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014446296s
STEP: Saw pod success
Apr 28 15:37:26.078: INFO: Pod "pod-d3ad4cb1-1d62-49d8-b690-ad42023f062d" satisfied condition "Succeeded or Failed"
Apr 28 15:37:26.082: INFO: Trying to get logs from node ip-172-31-30-112 pod pod-d3ad4cb1-1d62-49d8-b690-ad42023f062d container test-container: <nil>
STEP: delete the pod
Apr 28 15:37:26.104: INFO: Waiting for pod pod-d3ad4cb1-1d62-49d8-b690-ad42023f062d to disappear
Apr 28 15:37:26.109: INFO: Pod pod-d3ad4cb1-1d62-49d8-b690-ad42023f062d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:37:26.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9348" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":27,"skipped":305,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:37:26.126: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1478
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Apr 28 15:39:26.852: INFO: Successfully updated pod "var-expansion-0b05c5c7-aa8c-4942-914f-9a928665ac4b"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Apr 28 15:39:28.870: INFO: Deleting pod "var-expansion-0b05c5c7-aa8c-4942-914f-9a928665ac4b" in namespace "var-expansion-1478"
Apr 28 15:39:28.886: INFO: Wait up to 5m0s for pod "var-expansion-0b05c5c7-aa8c-4942-914f-9a928665ac4b" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:40:12.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1478" for this suite.

• [SLOW TEST:166.796 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":28,"skipped":309,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:40:12.922: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3922
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 15:40:13.125: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:40:13.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3922" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":29,"skipped":322,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:40:13.722: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6059
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-7cd7bb79-4db1-42e9-8d5a-8301ed89c25d
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:40:15.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6059" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":30,"skipped":322,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:40:15.996: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9248
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Apr 28 15:40:16.145: INFO: namespace kubectl-9248
Apr 28 15:40:16.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9248 create -f -'
Apr 28 15:40:16.766: INFO: stderr: ""
Apr 28 15:40:16.766: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 28 15:40:17.774: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 28 15:40:17.774: INFO: Found 0 / 1
Apr 28 15:40:18.776: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 28 15:40:18.776: INFO: Found 1 / 1
Apr 28 15:40:18.776: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 28 15:40:18.783: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 28 15:40:18.783: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 28 15:40:18.783: INFO: wait on agnhost-primary startup in kubectl-9248 
Apr 28 15:40:18.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9248 logs agnhost-primary-f4t9r agnhost-primary'
Apr 28 15:40:18.872: INFO: stderr: ""
Apr 28 15:40:18.872: INFO: stdout: "Paused\n"
STEP: exposing RC
Apr 28 15:40:18.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9248 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Apr 28 15:40:18.966: INFO: stderr: ""
Apr 28 15:40:18.966: INFO: stdout: "service/rm2 exposed\n"
Apr 28 15:40:18.973: INFO: Service rm2 in namespace kubectl-9248 found.
STEP: exposing service
Apr 28 15:40:20.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9248 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Apr 28 15:40:21.090: INFO: stderr: ""
Apr 28 15:40:21.090: INFO: stdout: "service/rm3 exposed\n"
Apr 28 15:40:21.136: INFO: Service rm3 in namespace kubectl-9248 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:40:23.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9248" for this suite.

• [SLOW TEST:7.171 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":31,"skipped":333,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:40:23.167: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-2105
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Apr 28 15:40:23.344: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Apr 28 15:40:23.620: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Apr 28 15:40:25.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755221223, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755221223, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755221223, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755221223, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 28 15:40:27.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755221223, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755221223, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755221223, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755221223, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 28 15:40:29.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755221223, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755221223, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755221223, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755221223, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 28 15:40:31.813: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755221223, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755221223, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755221223, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755221223, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 28 15:40:34.948: INFO: Waited 1.125547392s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:40:35.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2105" for this suite.

• [SLOW TEST:12.595 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":32,"skipped":367,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:40:35.762: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-hmjh
STEP: Creating a pod to test atomic-volume-subpath
Apr 28 15:40:35.951: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hmjh" in namespace "subpath-4112" to be "Succeeded or Failed"
Apr 28 15:40:35.968: INFO: Pod "pod-subpath-test-projected-hmjh": Phase="Pending", Reason="", readiness=false. Elapsed: 16.970156ms
Apr 28 15:40:37.980: INFO: Pod "pod-subpath-test-projected-hmjh": Phase="Running", Reason="", readiness=true. Elapsed: 2.02902823s
Apr 28 15:40:39.990: INFO: Pod "pod-subpath-test-projected-hmjh": Phase="Running", Reason="", readiness=true. Elapsed: 4.039214884s
Apr 28 15:40:42.002: INFO: Pod "pod-subpath-test-projected-hmjh": Phase="Running", Reason="", readiness=true. Elapsed: 6.051626583s
Apr 28 15:40:44.035: INFO: Pod "pod-subpath-test-projected-hmjh": Phase="Running", Reason="", readiness=true. Elapsed: 8.084422927s
Apr 28 15:40:46.042: INFO: Pod "pod-subpath-test-projected-hmjh": Phase="Running", Reason="", readiness=true. Elapsed: 10.090974722s
Apr 28 15:40:48.049: INFO: Pod "pod-subpath-test-projected-hmjh": Phase="Running", Reason="", readiness=true. Elapsed: 12.098734262s
Apr 28 15:40:50.058: INFO: Pod "pod-subpath-test-projected-hmjh": Phase="Running", Reason="", readiness=true. Elapsed: 14.107177848s
Apr 28 15:40:52.066: INFO: Pod "pod-subpath-test-projected-hmjh": Phase="Running", Reason="", readiness=true. Elapsed: 16.115626626s
Apr 28 15:40:54.090: INFO: Pod "pod-subpath-test-projected-hmjh": Phase="Running", Reason="", readiness=true. Elapsed: 18.139744182s
Apr 28 15:40:56.097: INFO: Pod "pod-subpath-test-projected-hmjh": Phase="Running", Reason="", readiness=true. Elapsed: 20.146578896s
Apr 28 15:40:58.103: INFO: Pod "pod-subpath-test-projected-hmjh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.152312257s
STEP: Saw pod success
Apr 28 15:40:58.103: INFO: Pod "pod-subpath-test-projected-hmjh" satisfied condition "Succeeded or Failed"
Apr 28 15:40:58.107: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-subpath-test-projected-hmjh container test-container-subpath-projected-hmjh: <nil>
STEP: delete the pod
Apr 28 15:40:58.131: INFO: Waiting for pod pod-subpath-test-projected-hmjh to disappear
Apr 28 15:40:58.135: INFO: Pod pod-subpath-test-projected-hmjh no longer exists
STEP: Deleting pod pod-subpath-test-projected-hmjh
Apr 28 15:40:58.135: INFO: Deleting pod "pod-subpath-test-projected-hmjh" in namespace "subpath-4112"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:40:58.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4112" for this suite.

• [SLOW TEST:22.388 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":33,"skipped":388,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:40:58.150: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3976
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3976.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3976.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3976.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3976.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 28 15:41:04.368: INFO: DNS probes using dns-test-cdac67c1-291c-4c08-906d-8c11b614ad48 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3976.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3976.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3976.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3976.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 28 15:41:06.435: INFO: File wheezy_udp@dns-test-service-3.dns-3976.svc.cluster.local from pod  dns-3976/dns-test-dadd04d4-f308-4715-94a1-233a720347af contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 28 15:41:06.439: INFO: File jessie_udp@dns-test-service-3.dns-3976.svc.cluster.local from pod  dns-3976/dns-test-dadd04d4-f308-4715-94a1-233a720347af contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 28 15:41:06.439: INFO: Lookups using dns-3976/dns-test-dadd04d4-f308-4715-94a1-233a720347af failed for: [wheezy_udp@dns-test-service-3.dns-3976.svc.cluster.local jessie_udp@dns-test-service-3.dns-3976.svc.cluster.local]

Apr 28 15:41:11.450: INFO: DNS probes using dns-test-dadd04d4-f308-4715-94a1-233a720347af succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3976.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3976.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3976.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3976.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 28 15:41:13.566: INFO: DNS probes using dns-test-3bea1227-1718-4ab1-9a69-0081c5c8648d succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:41:13.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3976" for this suite.

• [SLOW TEST:15.590 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":34,"skipped":389,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:41:13.740: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1702
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Apr 28 15:41:13.977: INFO: Waiting up to 5m0s for pod "client-containers-5a9ad711-0be0-43b8-839f-dcfaaebc9d7b" in namespace "containers-1702" to be "Succeeded or Failed"
Apr 28 15:41:13.981: INFO: Pod "client-containers-5a9ad711-0be0-43b8-839f-dcfaaebc9d7b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040075ms
Apr 28 15:41:15.988: INFO: Pod "client-containers-5a9ad711-0be0-43b8-839f-dcfaaebc9d7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010872431s
STEP: Saw pod success
Apr 28 15:41:15.988: INFO: Pod "client-containers-5a9ad711-0be0-43b8-839f-dcfaaebc9d7b" satisfied condition "Succeeded or Failed"
Apr 28 15:41:15.992: INFO: Trying to get logs from node ip-172-31-13-33 pod client-containers-5a9ad711-0be0-43b8-839f-dcfaaebc9d7b container agnhost-container: <nil>
STEP: delete the pod
Apr 28 15:41:16.020: INFO: Waiting for pod client-containers-5a9ad711-0be0-43b8-839f-dcfaaebc9d7b to disappear
Apr 28 15:41:16.024: INFO: Pod client-containers-5a9ad711-0be0-43b8-839f-dcfaaebc9d7b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:41:16.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1702" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":35,"skipped":412,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:41:16.037: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1119
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-0b0fe5cb-9c87-4381-af5e-7f43e683f9f5 in namespace container-probe-1119
Apr 28 15:41:18.213: INFO: Started pod liveness-0b0fe5cb-9c87-4381-af5e-7f43e683f9f5 in namespace container-probe-1119
STEP: checking the pod's current state and verifying that restartCount is present
Apr 28 15:41:18.217: INFO: Initial restart count of pod liveness-0b0fe5cb-9c87-4381-af5e-7f43e683f9f5 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:45:19.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1119" for this suite.

• [SLOW TEST:243.329 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":36,"skipped":418,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:45:19.367: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3342
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 28 15:45:19.973: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 15:45:23.003: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:45:23.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3342" for this suite.
STEP: Destroying namespace "webhook-3342-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":37,"skipped":431,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:45:23.280: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-838
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 15:45:23.496: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5818fe75-fe76-45e2-b90a-694ac3ff94db" in namespace "downward-api-838" to be "Succeeded or Failed"
Apr 28 15:45:23.500: INFO: Pod "downwardapi-volume-5818fe75-fe76-45e2-b90a-694ac3ff94db": Phase="Pending", Reason="", readiness=false. Elapsed: 3.869927ms
Apr 28 15:45:25.508: INFO: Pod "downwardapi-volume-5818fe75-fe76-45e2-b90a-694ac3ff94db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012395028s
STEP: Saw pod success
Apr 28 15:45:25.508: INFO: Pod "downwardapi-volume-5818fe75-fe76-45e2-b90a-694ac3ff94db" satisfied condition "Succeeded or Failed"
Apr 28 15:45:25.516: INFO: Trying to get logs from node ip-172-31-13-33 pod downwardapi-volume-5818fe75-fe76-45e2-b90a-694ac3ff94db container client-container: <nil>
STEP: delete the pod
Apr 28 15:45:25.561: INFO: Waiting for pod downwardapi-volume-5818fe75-fe76-45e2-b90a-694ac3ff94db to disappear
Apr 28 15:45:25.565: INFO: Pod downwardapi-volume-5818fe75-fe76-45e2-b90a-694ac3ff94db no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:45:25.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-838" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":38,"skipped":434,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:45:25.577: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-6216
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 28 15:45:25.803: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 28 15:46:25.825: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Apr 28 15:46:25.862: INFO: Created pod: pod0-sched-preemption-low-priority
Apr 28 15:46:25.890: INFO: Created pod: pod1-sched-preemption-medium-priority
Apr 28 15:46:25.912: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:46:41.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6216" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:76.479 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":39,"skipped":462,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:46:42.056: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6119
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Apr 28 15:46:42.225: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:46:56.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6119" for this suite.

• [SLOW TEST:14.182 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":40,"skipped":483,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:46:56.238: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4427
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 15:46:56.443: INFO: The status of Pod test-webserver-29cebd9f-b6cb-4c82-8618-d9a582784317 is Pending, waiting for it to be Running (with Ready = true)
Apr 28 15:46:58.449: INFO: The status of Pod test-webserver-29cebd9f-b6cb-4c82-8618-d9a582784317 is Running (Ready = false)
Apr 28 15:47:00.451: INFO: The status of Pod test-webserver-29cebd9f-b6cb-4c82-8618-d9a582784317 is Running (Ready = false)
Apr 28 15:47:02.456: INFO: The status of Pod test-webserver-29cebd9f-b6cb-4c82-8618-d9a582784317 is Running (Ready = false)
Apr 28 15:47:04.453: INFO: The status of Pod test-webserver-29cebd9f-b6cb-4c82-8618-d9a582784317 is Running (Ready = false)
Apr 28 15:47:06.451: INFO: The status of Pod test-webserver-29cebd9f-b6cb-4c82-8618-d9a582784317 is Running (Ready = false)
Apr 28 15:47:08.449: INFO: The status of Pod test-webserver-29cebd9f-b6cb-4c82-8618-d9a582784317 is Running (Ready = false)
Apr 28 15:47:10.452: INFO: The status of Pod test-webserver-29cebd9f-b6cb-4c82-8618-d9a582784317 is Running (Ready = false)
Apr 28 15:47:12.453: INFO: The status of Pod test-webserver-29cebd9f-b6cb-4c82-8618-d9a582784317 is Running (Ready = false)
Apr 28 15:47:14.452: INFO: The status of Pod test-webserver-29cebd9f-b6cb-4c82-8618-d9a582784317 is Running (Ready = false)
Apr 28 15:47:16.455: INFO: The status of Pod test-webserver-29cebd9f-b6cb-4c82-8618-d9a582784317 is Running (Ready = true)
Apr 28 15:47:16.459: INFO: Container started at 2021-04-28 15:46:57 +0000 UTC, pod became ready at 2021-04-28 15:47:15 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:47:16.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4427" for this suite.

• [SLOW TEST:20.236 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":41,"skipped":521,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:47:16.474: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8491
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-8ed96582-9b8c-4dd2-9d4b-c2f9d408a66e
STEP: Creating a pod to test consume secrets
Apr 28 15:47:16.694: INFO: Waiting up to 5m0s for pod "pod-secrets-ef148930-8fde-4788-a04c-74e7042ba3a9" in namespace "secrets-8491" to be "Succeeded or Failed"
Apr 28 15:47:16.699: INFO: Pod "pod-secrets-ef148930-8fde-4788-a04c-74e7042ba3a9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.046941ms
Apr 28 15:47:18.707: INFO: Pod "pod-secrets-ef148930-8fde-4788-a04c-74e7042ba3a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012990578s
STEP: Saw pod success
Apr 28 15:47:18.707: INFO: Pod "pod-secrets-ef148930-8fde-4788-a04c-74e7042ba3a9" satisfied condition "Succeeded or Failed"
Apr 28 15:47:18.712: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-secrets-ef148930-8fde-4788-a04c-74e7042ba3a9 container secret-volume-test: <nil>
STEP: delete the pod
Apr 28 15:47:18.747: INFO: Waiting for pod pod-secrets-ef148930-8fde-4788-a04c-74e7042ba3a9 to disappear
Apr 28 15:47:18.751: INFO: Pod pod-secrets-ef148930-8fde-4788-a04c-74e7042ba3a9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:47:18.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8491" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":42,"skipped":541,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:47:18.771: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8840
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 15:47:18.946: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4dcbb861-6b05-4cf6-b239-9afbf64fd369" in namespace "downward-api-8840" to be "Succeeded or Failed"
Apr 28 15:47:18.953: INFO: Pod "downwardapi-volume-4dcbb861-6b05-4cf6-b239-9afbf64fd369": Phase="Pending", Reason="", readiness=false. Elapsed: 6.913505ms
Apr 28 15:47:20.969: INFO: Pod "downwardapi-volume-4dcbb861-6b05-4cf6-b239-9afbf64fd369": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022766s
STEP: Saw pod success
Apr 28 15:47:20.969: INFO: Pod "downwardapi-volume-4dcbb861-6b05-4cf6-b239-9afbf64fd369" satisfied condition "Succeeded or Failed"
Apr 28 15:47:20.973: INFO: Trying to get logs from node ip-172-31-13-33 pod downwardapi-volume-4dcbb861-6b05-4cf6-b239-9afbf64fd369 container client-container: <nil>
STEP: delete the pod
Apr 28 15:47:21.015: INFO: Waiting for pod downwardapi-volume-4dcbb861-6b05-4cf6-b239-9afbf64fd369 to disappear
Apr 28 15:47:21.028: INFO: Pod downwardapi-volume-4dcbb861-6b05-4cf6-b239-9afbf64fd369 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:47:21.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8840" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":43,"skipped":556,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:47:21.055: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3690
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0428 15:47:21.855844      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0428 15:47:21.855864      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0428 15:47:21.855869      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 28 15:47:21.855: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:47:21.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3690" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":44,"skipped":582,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:47:21.870: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8277
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-8277/secret-test-bd6fb0ef-27dd-4b47-a23e-c93b484fcbba
STEP: Creating a pod to test consume secrets
Apr 28 15:47:22.154: INFO: Waiting up to 5m0s for pod "pod-configmaps-2536b9c5-3536-4e29-8cd5-2d52c87c7930" in namespace "secrets-8277" to be "Succeeded or Failed"
Apr 28 15:47:22.161: INFO: Pod "pod-configmaps-2536b9c5-3536-4e29-8cd5-2d52c87c7930": Phase="Pending", Reason="", readiness=false. Elapsed: 7.297398ms
Apr 28 15:47:24.174: INFO: Pod "pod-configmaps-2536b9c5-3536-4e29-8cd5-2d52c87c7930": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02092578s
STEP: Saw pod success
Apr 28 15:47:24.174: INFO: Pod "pod-configmaps-2536b9c5-3536-4e29-8cd5-2d52c87c7930" satisfied condition "Succeeded or Failed"
Apr 28 15:47:24.179: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-configmaps-2536b9c5-3536-4e29-8cd5-2d52c87c7930 container env-test: <nil>
STEP: delete the pod
Apr 28 15:47:24.211: INFO: Waiting for pod pod-configmaps-2536b9c5-3536-4e29-8cd5-2d52c87c7930 to disappear
Apr 28 15:47:24.215: INFO: Pod pod-configmaps-2536b9c5-3536-4e29-8cd5-2d52c87c7930 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:47:24.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8277" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":45,"skipped":605,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:47:24.232: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2589
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-4a62a064-49ba-4dbb-b3e8-1e2d88ec8663
STEP: Creating a pod to test consume configMaps
Apr 28 15:47:24.454: INFO: Waiting up to 5m0s for pod "pod-configmaps-bdb3288f-1cd2-410c-8cf0-02e3ea6a9770" in namespace "configmap-2589" to be "Succeeded or Failed"
Apr 28 15:47:24.460: INFO: Pod "pod-configmaps-bdb3288f-1cd2-410c-8cf0-02e3ea6a9770": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026924ms
Apr 28 15:47:26.476: INFO: Pod "pod-configmaps-bdb3288f-1cd2-410c-8cf0-02e3ea6a9770": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021769668s
STEP: Saw pod success
Apr 28 15:47:26.476: INFO: Pod "pod-configmaps-bdb3288f-1cd2-410c-8cf0-02e3ea6a9770" satisfied condition "Succeeded or Failed"
Apr 28 15:47:26.482: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-configmaps-bdb3288f-1cd2-410c-8cf0-02e3ea6a9770 container agnhost-container: <nil>
STEP: delete the pod
Apr 28 15:47:26.522: INFO: Waiting for pod pod-configmaps-bdb3288f-1cd2-410c-8cf0-02e3ea6a9770 to disappear
Apr 28 15:47:26.526: INFO: Pod pod-configmaps-bdb3288f-1cd2-410c-8cf0-02e3ea6a9770 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:47:26.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2589" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":46,"skipped":649,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:47:26.556: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9860
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Apr 28 15:47:26.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 create -f -'
Apr 28 15:47:27.126: INFO: stderr: ""
Apr 28 15:47:27.126: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 28 15:47:27.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 28 15:47:27.202: INFO: stderr: ""
Apr 28 15:47:27.202: INFO: stdout: "update-demo-nautilus-jmwj6 update-demo-nautilus-xmt75 "
Apr 28 15:47:27.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods update-demo-nautilus-jmwj6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 28 15:47:27.263: INFO: stderr: ""
Apr 28 15:47:27.263: INFO: stdout: ""
Apr 28 15:47:27.263: INFO: update-demo-nautilus-jmwj6 is created but not running
Apr 28 15:47:32.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 28 15:47:32.339: INFO: stderr: ""
Apr 28 15:47:32.339: INFO: stdout: "update-demo-nautilus-jmwj6 update-demo-nautilus-xmt75 "
Apr 28 15:47:32.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods update-demo-nautilus-jmwj6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 28 15:47:32.399: INFO: stderr: ""
Apr 28 15:47:32.399: INFO: stdout: "true"
Apr 28 15:47:32.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods update-demo-nautilus-jmwj6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 28 15:47:32.459: INFO: stderr: ""
Apr 28 15:47:32.459: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 28 15:47:32.459: INFO: validating pod update-demo-nautilus-jmwj6
Apr 28 15:47:32.467: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 28 15:47:32.467: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 28 15:47:32.467: INFO: update-demo-nautilus-jmwj6 is verified up and running
Apr 28 15:47:32.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods update-demo-nautilus-xmt75 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 28 15:47:32.522: INFO: stderr: ""
Apr 28 15:47:32.522: INFO: stdout: "true"
Apr 28 15:47:32.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods update-demo-nautilus-xmt75 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 28 15:47:32.580: INFO: stderr: ""
Apr 28 15:47:32.580: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 28 15:47:32.580: INFO: validating pod update-demo-nautilus-xmt75
Apr 28 15:47:32.591: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 28 15:47:32.591: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 28 15:47:32.591: INFO: update-demo-nautilus-xmt75 is verified up and running
STEP: scaling down the replication controller
Apr 28 15:47:32.592: INFO: scanned /root for discovery docs: <nil>
Apr 28 15:47:32.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Apr 28 15:47:33.689: INFO: stderr: ""
Apr 28 15:47:33.689: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 28 15:47:33.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 28 15:47:33.753: INFO: stderr: ""
Apr 28 15:47:33.753: INFO: stdout: "update-demo-nautilus-jmwj6 update-demo-nautilus-xmt75 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 28 15:47:38.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 28 15:47:38.824: INFO: stderr: ""
Apr 28 15:47:38.824: INFO: stdout: "update-demo-nautilus-jmwj6 update-demo-nautilus-xmt75 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 28 15:47:43.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 28 15:47:43.888: INFO: stderr: ""
Apr 28 15:47:43.888: INFO: stdout: "update-demo-nautilus-jmwj6 update-demo-nautilus-xmt75 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 28 15:47:48.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 28 15:47:48.959: INFO: stderr: ""
Apr 28 15:47:48.959: INFO: stdout: "update-demo-nautilus-xmt75 "
Apr 28 15:47:48.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods update-demo-nautilus-xmt75 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 28 15:47:49.018: INFO: stderr: ""
Apr 28 15:47:49.018: INFO: stdout: "true"
Apr 28 15:47:49.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods update-demo-nautilus-xmt75 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 28 15:47:49.078: INFO: stderr: ""
Apr 28 15:47:49.078: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 28 15:47:49.078: INFO: validating pod update-demo-nautilus-xmt75
Apr 28 15:47:49.084: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 28 15:47:49.084: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 28 15:47:49.084: INFO: update-demo-nautilus-xmt75 is verified up and running
STEP: scaling up the replication controller
Apr 28 15:47:49.085: INFO: scanned /root for discovery docs: <nil>
Apr 28 15:47:49.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Apr 28 15:47:50.170: INFO: stderr: ""
Apr 28 15:47:50.170: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 28 15:47:50.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 28 15:47:50.233: INFO: stderr: ""
Apr 28 15:47:50.233: INFO: stdout: "update-demo-nautilus-hlqll update-demo-nautilus-xmt75 "
Apr 28 15:47:50.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods update-demo-nautilus-hlqll -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 28 15:47:50.292: INFO: stderr: ""
Apr 28 15:47:50.292: INFO: stdout: "true"
Apr 28 15:47:50.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods update-demo-nautilus-hlqll -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 28 15:47:50.351: INFO: stderr: ""
Apr 28 15:47:50.351: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 28 15:47:50.351: INFO: validating pod update-demo-nautilus-hlqll
Apr 28 15:47:50.358: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 28 15:47:50.358: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 28 15:47:50.358: INFO: update-demo-nautilus-hlqll is verified up and running
Apr 28 15:47:50.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods update-demo-nautilus-xmt75 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 28 15:47:50.416: INFO: stderr: ""
Apr 28 15:47:50.416: INFO: stdout: "true"
Apr 28 15:47:50.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods update-demo-nautilus-xmt75 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 28 15:47:50.472: INFO: stderr: ""
Apr 28 15:47:50.472: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 28 15:47:50.472: INFO: validating pod update-demo-nautilus-xmt75
Apr 28 15:47:50.477: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 28 15:47:50.477: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 28 15:47:50.477: INFO: update-demo-nautilus-xmt75 is verified up and running
STEP: using delete to clean up resources
Apr 28 15:47:50.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 delete --grace-period=0 --force -f -'
Apr 28 15:47:50.548: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 28 15:47:50.548: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 28 15:47:50.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get rc,svc -l name=update-demo --no-headers'
Apr 28 15:47:50.613: INFO: stderr: "No resources found in kubectl-9860 namespace.\n"
Apr 28 15:47:50.613: INFO: stdout: ""
Apr 28 15:47:50.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 28 15:47:50.671: INFO: stderr: ""
Apr 28 15:47:50.671: INFO: stdout: "update-demo-nautilus-hlqll\nupdate-demo-nautilus-xmt75\n"
Apr 28 15:47:51.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get rc,svc -l name=update-demo --no-headers'
Apr 28 15:47:51.235: INFO: stderr: "No resources found in kubectl-9860 namespace.\n"
Apr 28 15:47:51.235: INFO: stdout: ""
Apr 28 15:47:51.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-9860 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 28 15:47:51.299: INFO: stderr: ""
Apr 28 15:47:51.299: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:47:51.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9860" for this suite.

• [SLOW TEST:24.756 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":47,"skipped":684,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:47:51.312: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3592
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 15:47:51.481: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7106d353-0d05-4512-8fee-2074af48831f" in namespace "downward-api-3592" to be "Succeeded or Failed"
Apr 28 15:47:51.486: INFO: Pod "downwardapi-volume-7106d353-0d05-4512-8fee-2074af48831f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.345221ms
Apr 28 15:47:53.500: INFO: Pod "downwardapi-volume-7106d353-0d05-4512-8fee-2074af48831f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018470847s
STEP: Saw pod success
Apr 28 15:47:53.500: INFO: Pod "downwardapi-volume-7106d353-0d05-4512-8fee-2074af48831f" satisfied condition "Succeeded or Failed"
Apr 28 15:47:53.504: INFO: Trying to get logs from node ip-172-31-76-85 pod downwardapi-volume-7106d353-0d05-4512-8fee-2074af48831f container client-container: <nil>
STEP: delete the pod
Apr 28 15:47:53.567: INFO: Waiting for pod downwardapi-volume-7106d353-0d05-4512-8fee-2074af48831f to disappear
Apr 28 15:47:53.575: INFO: Pod downwardapi-volume-7106d353-0d05-4512-8fee-2074af48831f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:47:53.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3592" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":48,"skipped":697,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:47:53.604: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-cbc78498-ee11-4f8c-b3f5-43d3d720f475
STEP: Creating a pod to test consume configMaps
Apr 28 15:47:53.810: INFO: Waiting up to 5m0s for pod "pod-configmaps-82b96113-8969-4e7e-ab02-f0ef02c6b025" in namespace "configmap-6909" to be "Succeeded or Failed"
Apr 28 15:47:53.816: INFO: Pod "pod-configmaps-82b96113-8969-4e7e-ab02-f0ef02c6b025": Phase="Pending", Reason="", readiness=false. Elapsed: 5.529926ms
Apr 28 15:47:55.827: INFO: Pod "pod-configmaps-82b96113-8969-4e7e-ab02-f0ef02c6b025": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016120806s
STEP: Saw pod success
Apr 28 15:47:55.827: INFO: Pod "pod-configmaps-82b96113-8969-4e7e-ab02-f0ef02c6b025" satisfied condition "Succeeded or Failed"
Apr 28 15:47:55.831: INFO: Trying to get logs from node ip-172-31-76-85 pod pod-configmaps-82b96113-8969-4e7e-ab02-f0ef02c6b025 container agnhost-container: <nil>
STEP: delete the pod
Apr 28 15:47:55.874: INFO: Waiting for pod pod-configmaps-82b96113-8969-4e7e-ab02-f0ef02c6b025 to disappear
Apr 28 15:47:55.883: INFO: Pod pod-configmaps-82b96113-8969-4e7e-ab02-f0ef02c6b025 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:47:55.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6909" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":49,"skipped":697,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:47:55.898: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 15:47:56.077: INFO: Waiting up to 5m0s for pod "downwardapi-volume-602157f9-5208-4a55-9460-3ba39889631f" in namespace "projected-6373" to be "Succeeded or Failed"
Apr 28 15:47:56.089: INFO: Pod "downwardapi-volume-602157f9-5208-4a55-9460-3ba39889631f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.669942ms
Apr 28 15:47:58.096: INFO: Pod "downwardapi-volume-602157f9-5208-4a55-9460-3ba39889631f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019476033s
STEP: Saw pod success
Apr 28 15:47:58.097: INFO: Pod "downwardapi-volume-602157f9-5208-4a55-9460-3ba39889631f" satisfied condition "Succeeded or Failed"
Apr 28 15:47:58.101: INFO: Trying to get logs from node ip-172-31-13-33 pod downwardapi-volume-602157f9-5208-4a55-9460-3ba39889631f container client-container: <nil>
STEP: delete the pod
Apr 28 15:47:58.128: INFO: Waiting for pod downwardapi-volume-602157f9-5208-4a55-9460-3ba39889631f to disappear
Apr 28 15:47:58.132: INFO: Pod downwardapi-volume-602157f9-5208-4a55-9460-3ba39889631f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:47:58.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6373" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":50,"skipped":726,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:47:58.144: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7730
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Apr 28 15:48:08.495: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0428 15:48:08.495848      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0428 15:48:08.495863      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0428 15:48:08.495867      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:48:08.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7730" for this suite.

• [SLOW TEST:10.375 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":51,"skipped":742,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:48:08.519: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9109
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 28 15:48:08.709: INFO: Waiting up to 5m0s for pod "downward-api-fb3b2b4b-e38d-4321-a8e1-24bdc5ad271d" in namespace "downward-api-9109" to be "Succeeded or Failed"
Apr 28 15:48:08.714: INFO: Pod "downward-api-fb3b2b4b-e38d-4321-a8e1-24bdc5ad271d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.146483ms
Apr 28 15:48:10.727: INFO: Pod "downward-api-fb3b2b4b-e38d-4321-a8e1-24bdc5ad271d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017794695s
STEP: Saw pod success
Apr 28 15:48:10.727: INFO: Pod "downward-api-fb3b2b4b-e38d-4321-a8e1-24bdc5ad271d" satisfied condition "Succeeded or Failed"
Apr 28 15:48:10.733: INFO: Trying to get logs from node ip-172-31-13-33 pod downward-api-fb3b2b4b-e38d-4321-a8e1-24bdc5ad271d container dapi-container: <nil>
STEP: delete the pod
Apr 28 15:48:10.764: INFO: Waiting for pod downward-api-fb3b2b4b-e38d-4321-a8e1-24bdc5ad271d to disappear
Apr 28 15:48:10.770: INFO: Pod downward-api-fb3b2b4b-e38d-4321-a8e1-24bdc5ad271d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:48:10.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9109" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":52,"skipped":747,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:48:10.786: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-546
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 15:48:10.971: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6021ae65-737d-4f11-9667-ee190b13efa4" in namespace "downward-api-546" to be "Succeeded or Failed"
Apr 28 15:48:10.977: INFO: Pod "downwardapi-volume-6021ae65-737d-4f11-9667-ee190b13efa4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.706059ms
Apr 28 15:48:12.982: INFO: Pod "downwardapi-volume-6021ae65-737d-4f11-9667-ee190b13efa4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011850434s
STEP: Saw pod success
Apr 28 15:48:12.983: INFO: Pod "downwardapi-volume-6021ae65-737d-4f11-9667-ee190b13efa4" satisfied condition "Succeeded or Failed"
Apr 28 15:48:12.987: INFO: Trying to get logs from node ip-172-31-13-33 pod downwardapi-volume-6021ae65-737d-4f11-9667-ee190b13efa4 container client-container: <nil>
STEP: delete the pod
Apr 28 15:48:13.013: INFO: Waiting for pod downwardapi-volume-6021ae65-737d-4f11-9667-ee190b13efa4 to disappear
Apr 28 15:48:13.019: INFO: Pod downwardapi-volume-6021ae65-737d-4f11-9667-ee190b13efa4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:48:13.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-546" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":53,"skipped":749,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:48:13.032: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3356
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-25c5a5b9-1d61-4fa6-8a56-00cb0c8091d7
STEP: Creating a pod to test consume configMaps
Apr 28 15:48:13.227: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cf20b8fc-5d36-48bf-ac07-155cb1ba008c" in namespace "projected-3356" to be "Succeeded or Failed"
Apr 28 15:48:13.234: INFO: Pod "pod-projected-configmaps-cf20b8fc-5d36-48bf-ac07-155cb1ba008c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.477165ms
Apr 28 15:48:15.244: INFO: Pod "pod-projected-configmaps-cf20b8fc-5d36-48bf-ac07-155cb1ba008c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017024878s
STEP: Saw pod success
Apr 28 15:48:15.244: INFO: Pod "pod-projected-configmaps-cf20b8fc-5d36-48bf-ac07-155cb1ba008c" satisfied condition "Succeeded or Failed"
Apr 28 15:48:15.251: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-projected-configmaps-cf20b8fc-5d36-48bf-ac07-155cb1ba008c container agnhost-container: <nil>
STEP: delete the pod
Apr 28 15:48:15.282: INFO: Waiting for pod pod-projected-configmaps-cf20b8fc-5d36-48bf-ac07-155cb1ba008c to disappear
Apr 28 15:48:15.286: INFO: Pod pod-projected-configmaps-cf20b8fc-5d36-48bf-ac07-155cb1ba008c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:48:15.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3356" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":54,"skipped":780,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:48:15.297: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9037
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:48:26.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9037" for this suite.

• [SLOW TEST:11.270 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":55,"skipped":796,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:48:26.568: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7870
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 28 15:48:26.771: INFO: Number of nodes with available pods: 0
Apr 28 15:48:26.771: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 15:48:27.787: INFO: Number of nodes with available pods: 2
Apr 28 15:48:27.787: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 15:48:28.784: INFO: Number of nodes with available pods: 3
Apr 28 15:48:28.784: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Apr 28 15:48:28.828: INFO: Number of nodes with available pods: 2
Apr 28 15:48:28.828: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 15:48:29.846: INFO: Number of nodes with available pods: 2
Apr 28 15:48:29.846: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 15:48:30.841: INFO: Number of nodes with available pods: 3
Apr 28 15:48:30.841: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7870, will wait for the garbage collector to delete the pods
Apr 28 15:48:30.920: INFO: Deleting DaemonSet.extensions daemon-set took: 13.427488ms
Apr 28 15:48:31.520: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.165605ms
Apr 28 15:48:45.144: INFO: Number of nodes with available pods: 0
Apr 28 15:48:45.144: INFO: Number of running nodes: 0, number of available pods: 0
Apr 28 15:48:45.152: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11947"},"items":null}

Apr 28 15:48:45.156: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11947"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:48:45.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7870" for this suite.

• [SLOW TEST:18.628 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":56,"skipped":807,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:48:45.196: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7557
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:48:45.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7557" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":57,"skipped":817,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:48:45.502: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1115
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Apr 28 15:48:47.707: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1115 PodName:pod-sharedvolume-5a214e62-1ad6-48b3-8c2a-2ab42deabb5e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:48:47.708: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 15:48:47.797: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:48:47.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1115" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":58,"skipped":827,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:48:47.812: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-530
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 15:48:48.013: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85d60efb-86a7-4471-887c-143d4e60d8cb" in namespace "downward-api-530" to be "Succeeded or Failed"
Apr 28 15:48:48.028: INFO: Pod "downwardapi-volume-85d60efb-86a7-4471-887c-143d4e60d8cb": Phase="Pending", Reason="", readiness=false. Elapsed: 15.909207ms
Apr 28 15:48:50.041: INFO: Pod "downwardapi-volume-85d60efb-86a7-4471-887c-143d4e60d8cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028120985s
STEP: Saw pod success
Apr 28 15:48:50.041: INFO: Pod "downwardapi-volume-85d60efb-86a7-4471-887c-143d4e60d8cb" satisfied condition "Succeeded or Failed"
Apr 28 15:48:50.046: INFO: Trying to get logs from node ip-172-31-76-85 pod downwardapi-volume-85d60efb-86a7-4471-887c-143d4e60d8cb container client-container: <nil>
STEP: delete the pod
Apr 28 15:48:50.081: INFO: Waiting for pod downwardapi-volume-85d60efb-86a7-4471-887c-143d4e60d8cb to disappear
Apr 28 15:48:50.087: INFO: Pod downwardapi-volume-85d60efb-86a7-4471-887c-143d4e60d8cb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:48:50.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-530" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":59,"skipped":827,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:48:50.109: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3939
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-238ffb54-65b5-4e95-87c6-0928349dd24f
STEP: Creating a pod to test consume secrets
Apr 28 15:48:50.533: INFO: Waiting up to 5m0s for pod "pod-secrets-b7cd0a8b-f7ee-46b9-b4fd-da03a1e47098" in namespace "secrets-3939" to be "Succeeded or Failed"
Apr 28 15:48:50.593: INFO: Pod "pod-secrets-b7cd0a8b-f7ee-46b9-b4fd-da03a1e47098": Phase="Pending", Reason="", readiness=false. Elapsed: 59.873029ms
Apr 28 15:48:52.600: INFO: Pod "pod-secrets-b7cd0a8b-f7ee-46b9-b4fd-da03a1e47098": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.067300237s
STEP: Saw pod success
Apr 28 15:48:52.600: INFO: Pod "pod-secrets-b7cd0a8b-f7ee-46b9-b4fd-da03a1e47098" satisfied condition "Succeeded or Failed"
Apr 28 15:48:52.604: INFO: Trying to get logs from node ip-172-31-76-85 pod pod-secrets-b7cd0a8b-f7ee-46b9-b4fd-da03a1e47098 container secret-volume-test: <nil>
STEP: delete the pod
Apr 28 15:48:52.627: INFO: Waiting for pod pod-secrets-b7cd0a8b-f7ee-46b9-b4fd-da03a1e47098 to disappear
Apr 28 15:48:52.631: INFO: Pod pod-secrets-b7cd0a8b-f7ee-46b9-b4fd-da03a1e47098 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:48:52.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3939" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":60,"skipped":860,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:48:52.644: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-d65c9b94-8b27-42d4-8cbd-bfeafc572437
STEP: Creating a pod to test consume secrets
Apr 28 15:48:52.814: INFO: Waiting up to 5m0s for pod "pod-secrets-211d372e-389d-41b6-a0a7-5b003c8f8ee9" in namespace "secrets-8584" to be "Succeeded or Failed"
Apr 28 15:48:52.820: INFO: Pod "pod-secrets-211d372e-389d-41b6-a0a7-5b003c8f8ee9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.566765ms
Apr 28 15:48:54.830: INFO: Pod "pod-secrets-211d372e-389d-41b6-a0a7-5b003c8f8ee9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015900857s
Apr 28 15:48:56.836: INFO: Pod "pod-secrets-211d372e-389d-41b6-a0a7-5b003c8f8ee9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02195831s
STEP: Saw pod success
Apr 28 15:48:56.836: INFO: Pod "pod-secrets-211d372e-389d-41b6-a0a7-5b003c8f8ee9" satisfied condition "Succeeded or Failed"
Apr 28 15:48:56.839: INFO: Trying to get logs from node ip-172-31-30-112 pod pod-secrets-211d372e-389d-41b6-a0a7-5b003c8f8ee9 container secret-volume-test: <nil>
STEP: delete the pod
Apr 28 15:48:56.875: INFO: Waiting for pod pod-secrets-211d372e-389d-41b6-a0a7-5b003c8f8ee9 to disappear
Apr 28 15:48:56.878: INFO: Pod pod-secrets-211d372e-389d-41b6-a0a7-5b003c8f8ee9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:48:56.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8584" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":61,"skipped":865,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:48:56.891: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7220
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-71ee0748-c29e-4444-9ea6-b9d262d12a7d
STEP: Creating secret with name s-test-opt-upd-f06eeffe-91ae-456f-bc3a-49c69d7aa831
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-71ee0748-c29e-4444-9ea6-b9d262d12a7d
STEP: Updating secret s-test-opt-upd-f06eeffe-91ae-456f-bc3a-49c69d7aa831
STEP: Creating secret with name s-test-opt-create-fcc7cd56-2133-4445-8bb3-3e6290a23b1f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:49:01.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7220" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":62,"skipped":888,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:49:01.227: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7519
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7519 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7519;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7519 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7519;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7519.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7519.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7519.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7519.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7519.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7519.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7519.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7519.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7519.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7519.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7519.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7519.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7519.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 172.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.172_udp@PTR;check="$$(dig +tcp +noall +answer +search 172.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.172_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7519 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7519;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7519 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7519;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7519.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7519.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7519.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7519.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7519.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7519.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7519.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7519.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7519.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7519.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7519.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7519.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7519.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 172.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.172_udp@PTR;check="$$(dig +tcp +noall +answer +search 172.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.172_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 28 15:49:09.469: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7519/dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f: the server could not find the requested resource (get pods dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f)
Apr 28 15:49:09.474: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7519/dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f: the server could not find the requested resource (get pods dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f)
Apr 28 15:49:09.478: INFO: Unable to read wheezy_udp@dns-test-service.dns-7519 from pod dns-7519/dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f: the server could not find the requested resource (get pods dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f)
Apr 28 15:49:09.483: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7519 from pod dns-7519/dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f: the server could not find the requested resource (get pods dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f)
Apr 28 15:49:09.495: INFO: Unable to read wheezy_udp@dns-test-service.dns-7519.svc from pod dns-7519/dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f: the server could not find the requested resource (get pods dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f)
Apr 28 15:49:09.499: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7519.svc from pod dns-7519/dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f: the server could not find the requested resource (get pods dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f)
Apr 28 15:49:09.553: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7519/dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f: the server could not find the requested resource (get pods dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f)
Apr 28 15:49:09.558: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7519/dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f: the server could not find the requested resource (get pods dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f)
Apr 28 15:49:09.566: INFO: Unable to read jessie_udp@dns-test-service.dns-7519 from pod dns-7519/dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f: the server could not find the requested resource (get pods dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f)
Apr 28 15:49:09.570: INFO: Unable to read jessie_tcp@dns-test-service.dns-7519 from pod dns-7519/dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f: the server could not find the requested resource (get pods dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f)
Apr 28 15:49:09.575: INFO: Unable to read jessie_udp@dns-test-service.dns-7519.svc from pod dns-7519/dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f: the server could not find the requested resource (get pods dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f)
Apr 28 15:49:09.580: INFO: Unable to read jessie_tcp@dns-test-service.dns-7519.svc from pod dns-7519/dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f: the server could not find the requested resource (get pods dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f)
Apr 28 15:49:09.623: INFO: Lookups using dns-7519/dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7519 wheezy_tcp@dns-test-service.dns-7519 wheezy_udp@dns-test-service.dns-7519.svc wheezy_tcp@dns-test-service.dns-7519.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7519 jessie_tcp@dns-test-service.dns-7519 jessie_udp@dns-test-service.dns-7519.svc jessie_tcp@dns-test-service.dns-7519.svc]

Apr 28 15:49:14.750: INFO: DNS probes using dns-7519/dns-test-0d80bc02-3e77-4601-a9f7-76de3366ae0f succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:49:14.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7519" for this suite.

• [SLOW TEST:13.655 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":63,"skipped":891,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:49:14.881: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-967
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-967
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-967
Apr 28 15:49:15.080: INFO: Found 0 stateful pods, waiting for 1
Apr 28 15:49:25.096: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 28 15:49:25.131: INFO: Deleting all statefulset in ns statefulset-967
Apr 28 15:49:25.140: INFO: Scaling statefulset ss to 0
Apr 28 15:49:45.197: INFO: Waiting for statefulset status.replicas updated to 0
Apr 28 15:49:45.201: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:49:45.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-967" for this suite.

• [SLOW TEST:30.390 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":64,"skipped":901,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:49:45.271: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7493
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:49:47.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7493" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":65,"skipped":911,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:49:47.468: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3692
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Apr 28 15:49:50.184: INFO: Successfully updated pod "labelsupdatef261792e-d969-46fb-b131-7d9a71dedf4c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:49:54.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3692" for this suite.

• [SLOW TEST:6.771 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":66,"skipped":914,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:49:54.239: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7637
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Apr 28 15:49:54.437: INFO: Waiting up to 5m0s for pod "var-expansion-8316cadb-2add-4fbb-81c6-8709c3e7313f" in namespace "var-expansion-7637" to be "Succeeded or Failed"
Apr 28 15:49:54.442: INFO: Pod "var-expansion-8316cadb-2add-4fbb-81c6-8709c3e7313f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.257262ms
Apr 28 15:49:56.449: INFO: Pod "var-expansion-8316cadb-2add-4fbb-81c6-8709c3e7313f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011936872s
STEP: Saw pod success
Apr 28 15:49:56.449: INFO: Pod "var-expansion-8316cadb-2add-4fbb-81c6-8709c3e7313f" satisfied condition "Succeeded or Failed"
Apr 28 15:49:56.452: INFO: Trying to get logs from node ip-172-31-30-112 pod var-expansion-8316cadb-2add-4fbb-81c6-8709c3e7313f container dapi-container: <nil>
STEP: delete the pod
Apr 28 15:49:56.486: INFO: Waiting for pod var-expansion-8316cadb-2add-4fbb-81c6-8709c3e7313f to disappear
Apr 28 15:49:56.489: INFO: Pod var-expansion-8316cadb-2add-4fbb-81c6-8709c3e7313f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:49:56.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7637" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":67,"skipped":932,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:49:56.503: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1360
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Apr 28 15:49:58.695: INFO: &Pod{ObjectMeta:{send-events-6482cf68-6480-436f-8602-af5e92214700  events-1360  6da5891d-1e34-4b43-b7e2-018c9dc03c6d 12555 0 2021-04-28 15:49:56 +0000 UTC <nil> <nil> map[name:foo time:662422702] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-04-28 15:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 15:49:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.39.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h9rxf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h9rxf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h9rxf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-76-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 15:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 15:49:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 15:49:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 15:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.76.85,PodIP:10.1.39.17,StartTime:2021-04-28 15:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-28 15:49:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://8c2cca2b22d15095641d52264889acf677c4895d4d9e249498587370ccaa33fe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.39.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Apr 28 15:50:00.706: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Apr 28 15:50:02.716: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:50:02.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1360" for this suite.

• [SLOW TEST:6.244 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":68,"skipped":946,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:50:02.748: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2740
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-c2f22f61-4e2e-4663-a802-5f3d2ba9d682
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-c2f22f61-4e2e-4663-a802-5f3d2ba9d682
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:50:07.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2740" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":69,"skipped":948,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:50:07.055: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7688
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 28 15:50:07.695: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 15:50:10.732: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:50:11.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7688" for this suite.
STEP: Destroying namespace "webhook-7688-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":70,"skipped":959,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:50:11.115: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1343
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 28 15:50:11.269: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 28 15:50:11.278: INFO: Waiting for terminating namespaces to be deleted...
Apr 28 15:50:11.282: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-13-33 before test
Apr 28 15:50:11.287: INFO: nginx-ingress-controller-kubernetes-worker-b68vn from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 15:50:11.287: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Apr 28 15:50:11.287: INFO: coredns-7bb4d77796-bxk7f from kube-system started at 2021-04-28 14:28:57 +0000 UTC (1 container statuses recorded)
Apr 28 15:50:11.287: INFO: 	Container coredns ready: true, restart count 0
Apr 28 15:50:11.287: INFO: metrics-server-v0.3.6-f6cf867b4-sgq8s from kube-system started at 2021-04-28 14:32:04 +0000 UTC (2 container statuses recorded)
Apr 28 15:50:11.287: INFO: 	Container metrics-server ready: true, restart count 0
Apr 28 15:50:11.287: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Apr 28 15:50:11.287: INFO: busybox-host-aliases24ffdb5c-4522-4cea-acc7-239feebb3329 from kubelet-test-7493 started at 2021-04-28 15:49:45 +0000 UTC (1 container statuses recorded)
Apr 28 15:50:11.287: INFO: 	Container busybox-host-aliases24ffdb5c-4522-4cea-acc7-239feebb3329 ready: true, restart count 0
Apr 28 15:50:11.287: INFO: pod-projected-configmaps-56d8c99e-8743-4526-a572-b8ea97d3c97b from projected-2740 started at 2021-04-28 15:50:02 +0000 UTC (1 container statuses recorded)
Apr 28 15:50:11.287: INFO: 	Container agnhost-container ready: true, restart count 0
Apr 28 15:50:11.287: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-30-112 before test
Apr 28 15:50:11.292: INFO: nginx-ingress-controller-kubernetes-worker-9plpg from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:47 +0000 UTC (1 container statuses recorded)
Apr 28 15:50:11.292: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Apr 28 15:50:11.292: INFO: kube-state-metrics-7f55b9fcd7-jjs4x from kube-system started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 15:50:11.292: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 28 15:50:11.292: INFO: dashboard-metrics-scraper-74757fb5b7-x9lrp from kubernetes-dashboard started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 15:50:11.292: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr 28 15:50:11.292: INFO: kubernetes-dashboard-64f87676d4-mshn4 from kubernetes-dashboard started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 15:50:11.292: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr 28 15:50:11.292: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-76-85 before test
Apr 28 15:50:11.297: INFO: send-events-6482cf68-6480-436f-8602-af5e92214700 from events-1360 started at 2021-04-28 15:49:56 +0000 UTC (1 container statuses recorded)
Apr 28 15:50:11.297: INFO: 	Container p ready: true, restart count 0
Apr 28 15:50:11.297: INFO: default-http-backend-kubernetes-worker-6494cbc7fd-74bdv from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:58 +0000 UTC (1 container statuses recorded)
Apr 28 15:50:11.297: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
Apr 28 15:50:11.297: INFO: nginx-ingress-controller-kubernetes-worker-bjnb5 from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:49 +0000 UTC (1 container statuses recorded)
Apr 28 15:50:11.297: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Apr 28 15:50:11.297: INFO: sonobuoy from sonobuoy started at 2021-04-28 15:27:11 +0000 UTC (1 container statuses recorded)
Apr 28 15:50:11.297: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 28 15:50:11.297: INFO: sonobuoy-e2e-job-c0dbab055ba045c5 from sonobuoy started at 2021-04-28 15:27:15 +0000 UTC (2 container statuses recorded)
Apr 28 15:50:11.297: INFO: 	Container e2e ready: true, restart count 0
Apr 28 15:50:11.297: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-985899fd-47be-4f27-be8d-48a9e91cfc61 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.30.112 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-985899fd-47be-4f27-be8d-48a9e91cfc61 off the node ip-172-31-30-112
STEP: verifying the node doesn't have the label kubernetes.io/e2e-985899fd-47be-4f27-be8d-48a9e91cfc61
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:55:15.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1343" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:304.371 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":71,"skipped":970,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:55:15.487: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4167
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4167
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Apr 28 15:55:15.675: INFO: Found 0 stateful pods, waiting for 3
Apr 28 15:55:25.695: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 28 15:55:25.695: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 28 15:55:25.695: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Apr 28 15:55:25.743: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Apr 28 15:55:35.813: INFO: Updating stateful set ss2
Apr 28 15:55:35.829: INFO: Waiting for Pod statefulset-4167/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 28 15:55:45.843: INFO: Waiting for Pod statefulset-4167/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Apr 28 15:55:55.915: INFO: Found 2 stateful pods, waiting for 3
Apr 28 15:56:05.930: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 28 15:56:05.930: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 28 15:56:05.930: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Apr 28 15:56:05.964: INFO: Updating stateful set ss2
Apr 28 15:56:05.992: INFO: Waiting for Pod statefulset-4167/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 28 15:56:16.047: INFO: Updating stateful set ss2
Apr 28 15:56:16.065: INFO: Waiting for StatefulSet statefulset-4167/ss2 to complete update
Apr 28 15:56:16.065: INFO: Waiting for Pod statefulset-4167/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 28 15:56:26.073: INFO: Deleting all statefulset in ns statefulset-4167
Apr 28 15:56:26.077: INFO: Scaling statefulset ss2 to 0
Apr 28 15:56:36.113: INFO: Waiting for statefulset status.replicas updated to 0
Apr 28 15:56:36.117: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:56:36.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4167" for this suite.

• [SLOW TEST:80.673 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":72,"skipped":1001,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:56:36.160: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2851
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 15:56:36.329: INFO: Waiting up to 5m0s for pod "downwardapi-volume-537a5fcf-1745-4184-b58c-98568e68c03f" in namespace "projected-2851" to be "Succeeded or Failed"
Apr 28 15:56:36.333: INFO: Pod "downwardapi-volume-537a5fcf-1745-4184-b58c-98568e68c03f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.979042ms
Apr 28 15:56:38.343: INFO: Pod "downwardapi-volume-537a5fcf-1745-4184-b58c-98568e68c03f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014058333s
STEP: Saw pod success
Apr 28 15:56:38.343: INFO: Pod "downwardapi-volume-537a5fcf-1745-4184-b58c-98568e68c03f" satisfied condition "Succeeded or Failed"
Apr 28 15:56:38.347: INFO: Trying to get logs from node ip-172-31-13-33 pod downwardapi-volume-537a5fcf-1745-4184-b58c-98568e68c03f container client-container: <nil>
STEP: delete the pod
Apr 28 15:56:38.393: INFO: Waiting for pod downwardapi-volume-537a5fcf-1745-4184-b58c-98568e68c03f to disappear
Apr 28 15:56:38.398: INFO: Pod downwardapi-volume-537a5fcf-1745-4184-b58c-98568e68c03f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:56:38.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2851" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":73,"skipped":1016,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:56:38.411: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6678
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Apr 28 15:56:38.591: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:56:41.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6678" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":74,"skipped":1050,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:56:41.835: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1365
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:56:46.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1365" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":75,"skipped":1102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:56:46.026: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 15:56:46.191: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4123e119-f362-4329-b8cf-2491e4cea338" in namespace "projected-9373" to be "Succeeded or Failed"
Apr 28 15:56:46.195: INFO: Pod "downwardapi-volume-4123e119-f362-4329-b8cf-2491e4cea338": Phase="Pending", Reason="", readiness=false. Elapsed: 4.587299ms
Apr 28 15:56:48.202: INFO: Pod "downwardapi-volume-4123e119-f362-4329-b8cf-2491e4cea338": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011764764s
STEP: Saw pod success
Apr 28 15:56:48.203: INFO: Pod "downwardapi-volume-4123e119-f362-4329-b8cf-2491e4cea338" satisfied condition "Succeeded or Failed"
Apr 28 15:56:48.209: INFO: Trying to get logs from node ip-172-31-13-33 pod downwardapi-volume-4123e119-f362-4329-b8cf-2491e4cea338 container client-container: <nil>
STEP: delete the pod
Apr 28 15:56:48.235: INFO: Waiting for pod downwardapi-volume-4123e119-f362-4329-b8cf-2491e4cea338 to disappear
Apr 28 15:56:48.243: INFO: Pod downwardapi-volume-4123e119-f362-4329-b8cf-2491e4cea338 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:56:48.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9373" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":76,"skipped":1149,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:56:48.256: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-564
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:56:54.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-564" for this suite.

• [SLOW TEST:6.191 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":77,"skipped":1152,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:56:54.447: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-314
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 28 15:56:54.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-314 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Apr 28 15:56:54.862: INFO: stderr: ""
Apr 28 15:56:54.862: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Apr 28 15:56:54.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-314 delete pods e2e-test-httpd-pod'
Apr 28 15:57:05.068: INFO: stderr: ""
Apr 28 15:57:05.068: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:57:05.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-314" for this suite.

• [SLOW TEST:10.638 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":78,"skipped":1163,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:57:05.086: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-8768
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr 28 15:57:05.921: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 15:57:08.947: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 15:57:08.955: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:57:10.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8768" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:5.091 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":79,"skipped":1184,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:57:10.177: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1099
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Apr 28 15:57:10.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-1099 create -f -'
Apr 28 15:57:10.540: INFO: stderr: ""
Apr 28 15:57:10.540: INFO: stdout: "pod/pause created\n"
Apr 28 15:57:10.540: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr 28 15:57:10.540: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1099" to be "running and ready"
Apr 28 15:57:10.543: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.338915ms
Apr 28 15:57:12.551: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.011709717s
Apr 28 15:57:12.551: INFO: Pod "pause" satisfied condition "running and ready"
Apr 28 15:57:12.551: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Apr 28 15:57:12.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-1099 label pods pause testing-label=testing-label-value'
Apr 28 15:57:12.652: INFO: stderr: ""
Apr 28 15:57:12.652: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Apr 28 15:57:12.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-1099 get pod pause -L testing-label'
Apr 28 15:57:12.712: INFO: stderr: ""
Apr 28 15:57:12.712: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Apr 28 15:57:12.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-1099 label pods pause testing-label-'
Apr 28 15:57:12.790: INFO: stderr: ""
Apr 28 15:57:12.790: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Apr 28 15:57:12.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-1099 get pod pause -L testing-label'
Apr 28 15:57:12.845: INFO: stderr: ""
Apr 28 15:57:12.845: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Apr 28 15:57:12.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-1099 delete --grace-period=0 --force -f -'
Apr 28 15:57:12.921: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 28 15:57:12.921: INFO: stdout: "pod \"pause\" force deleted\n"
Apr 28 15:57:12.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-1099 get rc,svc -l name=pause --no-headers'
Apr 28 15:57:12.988: INFO: stderr: "No resources found in kubectl-1099 namespace.\n"
Apr 28 15:57:12.988: INFO: stdout: ""
Apr 28 15:57:12.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-1099 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 28 15:57:13.043: INFO: stderr: ""
Apr 28 15:57:13.043: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:57:13.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1099" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":80,"skipped":1199,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:57:13.057: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8521
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8521
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8521
STEP: creating replication controller externalsvc in namespace services-8521
I0428 15:57:13.281953      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8521, replica count: 2
I0428 15:57:16.332211      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Apr 28 15:57:16.374: INFO: Creating new exec pod
Apr 28 15:57:18.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-8521 exec execpod4m8jm -- /bin/sh -x -c nslookup nodeport-service.services-8521.svc.cluster.local'
Apr 28 15:57:18.616: INFO: stderr: "+ nslookup nodeport-service.services-8521.svc.cluster.local\n"
Apr 28 15:57:18.616: INFO: stdout: "Server:\t\t10.152.183.87\nAddress:\t10.152.183.87#53\n\nnodeport-service.services-8521.svc.cluster.local\tcanonical name = externalsvc.services-8521.svc.cluster.local.\nName:\texternalsvc.services-8521.svc.cluster.local\nAddress: 10.152.183.131\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8521, will wait for the garbage collector to delete the pods
Apr 28 15:57:18.683: INFO: Deleting ReplicationController externalsvc took: 11.182557ms
Apr 28 15:57:19.283: INFO: Terminating ReplicationController externalsvc pods took: 600.155597ms
Apr 28 15:57:35.129: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:57:35.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8521" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:22.125 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":81,"skipped":1278,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:57:35.183: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7008
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 28 15:57:35.413: INFO: Number of nodes with available pods: 0
Apr 28 15:57:35.413: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 15:57:36.426: INFO: Number of nodes with available pods: 2
Apr 28 15:57:36.426: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 15:57:37.424: INFO: Number of nodes with available pods: 3
Apr 28 15:57:37.424: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Apr 28 15:57:37.457: INFO: Number of nodes with available pods: 2
Apr 28 15:57:37.457: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 15:57:38.470: INFO: Number of nodes with available pods: 2
Apr 28 15:57:38.470: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 15:57:39.470: INFO: Number of nodes with available pods: 2
Apr 28 15:57:39.470: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 15:57:40.473: INFO: Number of nodes with available pods: 2
Apr 28 15:57:40.473: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 15:57:41.469: INFO: Number of nodes with available pods: 2
Apr 28 15:57:41.469: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 15:57:42.469: INFO: Number of nodes with available pods: 2
Apr 28 15:57:42.469: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 15:57:43.471: INFO: Number of nodes with available pods: 2
Apr 28 15:57:43.471: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 15:57:44.470: INFO: Number of nodes with available pods: 2
Apr 28 15:57:44.470: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 15:57:45.471: INFO: Number of nodes with available pods: 2
Apr 28 15:57:45.471: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 15:57:46.469: INFO: Number of nodes with available pods: 3
Apr 28 15:57:46.469: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7008, will wait for the garbage collector to delete the pods
Apr 28 15:57:46.538: INFO: Deleting DaemonSet.extensions daemon-set took: 11.120403ms
Apr 28 15:57:47.138: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.155127ms
Apr 28 15:57:57.046: INFO: Number of nodes with available pods: 0
Apr 28 15:57:57.046: INFO: Number of running nodes: 0, number of available pods: 0
Apr 28 15:57:57.052: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14467"},"items":null}

Apr 28 15:57:57.056: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14467"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:57:57.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7008" for this suite.

• [SLOW TEST:21.908 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":82,"skipped":1286,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:57:57.090: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5218
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 15:57:57.275: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:57:59.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5218" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":83,"skipped":1306,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:57:59.353: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9842
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9842.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9842.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 28 15:58:01.735: INFO: DNS probes using dns-9842/dns-test-f559209d-92c2-4d12-a8b0-b2f49d285044 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:58:01.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9842" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":84,"skipped":1322,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:58:01.846: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2440
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Apr 28 15:58:02.070: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:58:15.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2440" for this suite.

• [SLOW TEST:13.242 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":85,"skipped":1336,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:58:15.088: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6569
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Apr 28 15:58:15.269: INFO: created test-pod-1
Apr 28 15:58:15.282: INFO: created test-pod-2
Apr 28 15:58:15.301: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:58:15.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6569" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":86,"skipped":1344,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:58:15.411: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7630
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 15:58:15.575: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Apr 28 15:58:18.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-7630 --namespace=crd-publish-openapi-7630 create -f -'
Apr 28 15:58:19.006: INFO: stderr: ""
Apr 28 15:58:19.006: INFO: stdout: "e2e-test-crd-publish-openapi-6521-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 28 15:58:19.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-7630 --namespace=crd-publish-openapi-7630 delete e2e-test-crd-publish-openapi-6521-crds test-foo'
Apr 28 15:58:19.088: INFO: stderr: ""
Apr 28 15:58:19.088: INFO: stdout: "e2e-test-crd-publish-openapi-6521-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Apr 28 15:58:19.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-7630 --namespace=crd-publish-openapi-7630 apply -f -'
Apr 28 15:58:19.385: INFO: stderr: ""
Apr 28 15:58:19.385: INFO: stdout: "e2e-test-crd-publish-openapi-6521-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 28 15:58:19.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-7630 --namespace=crd-publish-openapi-7630 delete e2e-test-crd-publish-openapi-6521-crds test-foo'
Apr 28 15:58:19.473: INFO: stderr: ""
Apr 28 15:58:19.473: INFO: stdout: "e2e-test-crd-publish-openapi-6521-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Apr 28 15:58:19.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-7630 --namespace=crd-publish-openapi-7630 create -f -'
Apr 28 15:58:19.672: INFO: rc: 1
Apr 28 15:58:19.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-7630 --namespace=crd-publish-openapi-7630 apply -f -'
Apr 28 15:58:19.920: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Apr 28 15:58:19.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-7630 --namespace=crd-publish-openapi-7630 create -f -'
Apr 28 15:58:20.146: INFO: rc: 1
Apr 28 15:58:20.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-7630 --namespace=crd-publish-openapi-7630 apply -f -'
Apr 28 15:58:20.372: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Apr 28 15:58:20.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-7630 explain e2e-test-crd-publish-openapi-6521-crds'
Apr 28 15:58:20.521: INFO: stderr: ""
Apr 28 15:58:20.521: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6521-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Apr 28 15:58:20.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-7630 explain e2e-test-crd-publish-openapi-6521-crds.metadata'
Apr 28 15:58:20.745: INFO: stderr: ""
Apr 28 15:58:20.745: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6521-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Apr 28 15:58:20.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-7630 explain e2e-test-crd-publish-openapi-6521-crds.spec'
Apr 28 15:58:20.957: INFO: stderr: ""
Apr 28 15:58:20.957: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6521-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Apr 28 15:58:20.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-7630 explain e2e-test-crd-publish-openapi-6521-crds.spec.bars'
Apr 28 15:58:21.106: INFO: stderr: ""
Apr 28 15:58:21.106: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6521-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Apr 28 15:58:21.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-7630 explain e2e-test-crd-publish-openapi-6521-crds.spec.bars2'
Apr 28 15:58:21.327: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:58:24.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7630" for this suite.

• [SLOW TEST:8.664 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":87,"skipped":1379,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:58:24.076: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-2132
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:58:24.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2132" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":88,"skipped":1432,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:58:24.319: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3287
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:58:37.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3287" for this suite.

• [SLOW TEST:13.290 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":89,"skipped":1441,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:58:37.610: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8772
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:58:48.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8772" for this suite.

• [SLOW TEST:11.257 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":90,"skipped":1459,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:58:48.866: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8307
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 28 15:58:49.528: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 28 15:58:51.551: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755222329, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755222329, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755222329, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755222329, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 15:58:54.585: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:58:54.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8307" for this suite.
STEP: Destroying namespace "webhook-8307-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.833 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":91,"skipped":1460,"failed":0}
SSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:58:54.700: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7392
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Apr 28 15:58:54.932: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:58:54.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7392" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":92,"skipped":1465,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:58:54.978: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1781
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:58:55.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1781" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":93,"skipped":1491,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:58:55.245: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Apr 28 15:58:58.000: INFO: Successfully updated pod "labelsupdate3fc42cb7-a1d8-4b8a-be27-1cec377f86f4"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:59:02.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9115" for this suite.

• [SLOW TEST:6.806 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":94,"skipped":1517,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:59:02.052: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-6396
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:59:02.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6396" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":95,"skipped":1530,"failed":0}
S
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:59:02.290: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1089
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:59:02.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1089" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":96,"skipped":1531,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:59:02.477: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1064
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-22727aaf-5b3e-48e4-8d00-677a4b193f2d
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:59:02.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1064" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":97,"skipped":1556,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:59:02.816: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7084
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Apr 28 15:59:05.567: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7084 pod-service-account-915c12ef-9515-4db0-85a0-fc26c2356afe -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Apr 28 15:59:05.724: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7084 pod-service-account-915c12ef-9515-4db0-85a0-fc26c2356afe -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Apr 28 15:59:05.860: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7084 pod-service-account-915c12ef-9515-4db0-85a0-fc26c2356afe -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:59:06.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7084" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":98,"skipped":1582,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:59:06.029: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9913
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-d19f17ba-7524-44d9-a9d6-2042a856ecc7
STEP: Creating a pod to test consume secrets
Apr 28 15:59:06.302: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f6ed0358-10f7-490f-8090-2709eee5c8dd" in namespace "projected-9913" to be "Succeeded or Failed"
Apr 28 15:59:06.310: INFO: Pod "pod-projected-secrets-f6ed0358-10f7-490f-8090-2709eee5c8dd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.638889ms
Apr 28 15:59:08.316: INFO: Pod "pod-projected-secrets-f6ed0358-10f7-490f-8090-2709eee5c8dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014509978s
STEP: Saw pod success
Apr 28 15:59:08.316: INFO: Pod "pod-projected-secrets-f6ed0358-10f7-490f-8090-2709eee5c8dd" satisfied condition "Succeeded or Failed"
Apr 28 15:59:08.321: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-projected-secrets-f6ed0358-10f7-490f-8090-2709eee5c8dd container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 28 15:59:08.352: INFO: Waiting for pod pod-projected-secrets-f6ed0358-10f7-490f-8090-2709eee5c8dd to disappear
Apr 28 15:59:08.358: INFO: Pod pod-projected-secrets-f6ed0358-10f7-490f-8090-2709eee5c8dd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:59:08.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9913" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":99,"skipped":1585,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:59:08.370: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 28 15:59:08.530: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 28 15:59:08.540: INFO: Waiting for terminating namespaces to be deleted...
Apr 28 15:59:08.544: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-13-33 before test
Apr 28 15:59:08.549: INFO: nginx-ingress-controller-kubernetes-worker-b68vn from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 15:59:08.549: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Apr 28 15:59:08.549: INFO: coredns-7bb4d77796-bxk7f from kube-system started at 2021-04-28 14:28:57 +0000 UTC (1 container statuses recorded)
Apr 28 15:59:08.549: INFO: 	Container coredns ready: true, restart count 0
Apr 28 15:59:08.549: INFO: metrics-server-v0.3.6-f6cf867b4-sgq8s from kube-system started at 2021-04-28 14:32:04 +0000 UTC (2 container statuses recorded)
Apr 28 15:59:08.549: INFO: 	Container metrics-server ready: true, restart count 0
Apr 28 15:59:08.549: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Apr 28 15:59:08.549: INFO: labelsupdate3fc42cb7-a1d8-4b8a-be27-1cec377f86f4 from projected-9115 started at 2021-04-28 15:58:55 +0000 UTC (1 container statuses recorded)
Apr 28 15:59:08.549: INFO: 	Container client-container ready: false, restart count 0
Apr 28 15:59:08.549: INFO: pod-service-account-915c12ef-9515-4db0-85a0-fc26c2356afe from svcaccounts-7084 started at 2021-04-28 15:59:03 +0000 UTC (1 container statuses recorded)
Apr 28 15:59:08.549: INFO: 	Container test ready: true, restart count 0
Apr 28 15:59:08.549: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-30-112 before test
Apr 28 15:59:08.555: INFO: nginx-ingress-controller-kubernetes-worker-9plpg from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:47 +0000 UTC (1 container statuses recorded)
Apr 28 15:59:08.555: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Apr 28 15:59:08.555: INFO: kube-state-metrics-7f55b9fcd7-jjs4x from kube-system started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 15:59:08.555: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 28 15:59:08.555: INFO: dashboard-metrics-scraper-74757fb5b7-x9lrp from kubernetes-dashboard started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 15:59:08.555: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr 28 15:59:08.555: INFO: kubernetes-dashboard-64f87676d4-mshn4 from kubernetes-dashboard started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 15:59:08.555: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr 28 15:59:08.555: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-76-85 before test
Apr 28 15:59:08.561: INFO: default-http-backend-kubernetes-worker-6494cbc7fd-74bdv from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:58 +0000 UTC (1 container statuses recorded)
Apr 28 15:59:08.561: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
Apr 28 15:59:08.561: INFO: nginx-ingress-controller-kubernetes-worker-bjnb5 from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:49 +0000 UTC (1 container statuses recorded)
Apr 28 15:59:08.561: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Apr 28 15:59:08.561: INFO: sonobuoy from sonobuoy started at 2021-04-28 15:27:11 +0000 UTC (1 container statuses recorded)
Apr 28 15:59:08.561: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 28 15:59:08.561: INFO: sonobuoy-e2e-job-c0dbab055ba045c5 from sonobuoy started at 2021-04-28 15:27:15 +0000 UTC (2 container statuses recorded)
Apr 28 15:59:08.561: INFO: 	Container e2e ready: true, restart count 0
Apr 28 15:59:08.561: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-51415476-747f-4648-9af2-cbdbd4a503b0 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 172.31.13.33 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 172.31.13.33 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 28 15:59:18.770: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.13.33 http://127.0.0.1:54321/hostname] Namespace:sched-pred-2869 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:59:18.770: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.13.33, port: 54321
Apr 28 15:59:18.854: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.13.33:54321/hostname] Namespace:sched-pred-2869 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:59:18.854: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.13.33, port: 54321 UDP
Apr 28 15:59:18.933: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.13.33 54321] Namespace:sched-pred-2869 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:59:18.933: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 28 15:59:23.975: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.13.33 http://127.0.0.1:54321/hostname] Namespace:sched-pred-2869 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:59:23.975: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.13.33, port: 54321
Apr 28 15:59:24.069: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.13.33:54321/hostname] Namespace:sched-pred-2869 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:59:24.069: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.13.33, port: 54321 UDP
Apr 28 15:59:24.141: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.13.33 54321] Namespace:sched-pred-2869 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:59:24.141: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 28 15:59:29.209: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.13.33 http://127.0.0.1:54321/hostname] Namespace:sched-pred-2869 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:59:29.209: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.13.33, port: 54321
Apr 28 15:59:29.309: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.13.33:54321/hostname] Namespace:sched-pred-2869 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:59:29.309: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.13.33, port: 54321 UDP
Apr 28 15:59:29.366: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.13.33 54321] Namespace:sched-pred-2869 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:59:29.366: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 28 15:59:34.446: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.13.33 http://127.0.0.1:54321/hostname] Namespace:sched-pred-2869 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:59:34.446: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.13.33, port: 54321
Apr 28 15:59:34.533: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.13.33:54321/hostname] Namespace:sched-pred-2869 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:59:34.533: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.13.33, port: 54321 UDP
Apr 28 15:59:34.603: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.13.33 54321] Namespace:sched-pred-2869 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:59:34.603: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 28 15:59:39.702: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.13.33 http://127.0.0.1:54321/hostname] Namespace:sched-pred-2869 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:59:39.702: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.13.33, port: 54321
Apr 28 15:59:39.792: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.13.33:54321/hostname] Namespace:sched-pred-2869 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:59:39.792: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.13.33, port: 54321 UDP
Apr 28 15:59:39.839: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.13.33 54321] Namespace:sched-pred-2869 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 15:59:39.839: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: removing the label kubernetes.io/e2e-51415476-747f-4648-9af2-cbdbd4a503b0 off the node ip-172-31-13-33
STEP: verifying the node doesn't have the label kubernetes.io/e2e-51415476-747f-4648-9af2-cbdbd4a503b0
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:59:44.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2869" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:36.594 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":100,"skipped":1596,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:59:44.963: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-549
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 28 15:59:49.262: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 28 15:59:49.266: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 28 15:59:51.266: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 28 15:59:51.275: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 28 15:59:53.266: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 28 15:59:53.277: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:59:53.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-549" for this suite.

• [SLOW TEST:8.328 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":101,"skipped":1601,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:59:53.292: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2765
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2765
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2765
I0428 15:59:53.505999      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2765, replica count: 2
I0428 15:59:56.556325      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 28 15:59:56.556: INFO: Creating new exec pod
Apr 28 15:59:59.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-2765 exec execpodkbb2t -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Apr 28 15:59:59.714: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 28 15:59:59.714: INFO: stdout: ""
Apr 28 15:59:59.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-2765 exec execpodkbb2t -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.243 80'
Apr 28 15:59:59.837: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.243 80\nConnection to 10.152.183.243 80 port [tcp/http] succeeded!\n"
Apr 28 15:59:59.837: INFO: stdout: ""
Apr 28 15:59:59.837: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 15:59:59.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2765" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.584 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":102,"skipped":1610,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 15:59:59.876: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9140
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:00:07.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9140" for this suite.

• [SLOW TEST:7.296 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":103,"skipped":1656,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:00:07.172: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4703
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-5de3b01c-31bc-4622-94a7-a03faca12a76
STEP: Creating a pod to test consume configMaps
Apr 28 16:00:07.381: INFO: Waiting up to 5m0s for pod "pod-configmaps-2ae500f1-fed3-47a1-9e0b-0a95cf53b4e7" in namespace "configmap-4703" to be "Succeeded or Failed"
Apr 28 16:00:07.397: INFO: Pod "pod-configmaps-2ae500f1-fed3-47a1-9e0b-0a95cf53b4e7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.750928ms
Apr 28 16:00:09.405: INFO: Pod "pod-configmaps-2ae500f1-fed3-47a1-9e0b-0a95cf53b4e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024379835s
STEP: Saw pod success
Apr 28 16:00:09.405: INFO: Pod "pod-configmaps-2ae500f1-fed3-47a1-9e0b-0a95cf53b4e7" satisfied condition "Succeeded or Failed"
Apr 28 16:00:09.410: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-configmaps-2ae500f1-fed3-47a1-9e0b-0a95cf53b4e7 container agnhost-container: <nil>
STEP: delete the pod
Apr 28 16:00:09.434: INFO: Waiting for pod pod-configmaps-2ae500f1-fed3-47a1-9e0b-0a95cf53b4e7 to disappear
Apr 28 16:00:09.437: INFO: Pod pod-configmaps-2ae500f1-fed3-47a1-9e0b-0a95cf53b4e7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:00:09.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4703" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":104,"skipped":1680,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:00:09.451: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5456
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:00:09.629: INFO: Creating ReplicaSet my-hostname-basic-5df8c344-887d-4cc4-b4f0-14d5ed428e07
Apr 28 16:00:09.641: INFO: Pod name my-hostname-basic-5df8c344-887d-4cc4-b4f0-14d5ed428e07: Found 0 pods out of 1
Apr 28 16:00:14.652: INFO: Pod name my-hostname-basic-5df8c344-887d-4cc4-b4f0-14d5ed428e07: Found 1 pods out of 1
Apr 28 16:00:14.652: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-5df8c344-887d-4cc4-b4f0-14d5ed428e07" is running
Apr 28 16:00:14.657: INFO: Pod "my-hostname-basic-5df8c344-887d-4cc4-b4f0-14d5ed428e07-2p642" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-28 16:00:09 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-28 16:00:11 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-28 16:00:11 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-28 16:00:09 +0000 UTC Reason: Message:}])
Apr 28 16:00:14.657: INFO: Trying to dial the pod
Apr 28 16:00:19.673: INFO: Controller my-hostname-basic-5df8c344-887d-4cc4-b4f0-14d5ed428e07: Got expected result from replica 1 [my-hostname-basic-5df8c344-887d-4cc4-b4f0-14d5ed428e07-2p642]: "my-hostname-basic-5df8c344-887d-4cc4-b4f0-14d5ed428e07-2p642", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:00:19.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5456" for this suite.

• [SLOW TEST:10.235 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":105,"skipped":1714,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:00:19.686: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7917
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 28 16:00:19.868: INFO: Waiting up to 5m0s for pod "downward-api-8b0f953f-6783-4d78-98b1-0f4a4f57c5e8" in namespace "downward-api-7917" to be "Succeeded or Failed"
Apr 28 16:00:19.880: INFO: Pod "downward-api-8b0f953f-6783-4d78-98b1-0f4a4f57c5e8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007205ms
Apr 28 16:00:21.889: INFO: Pod "downward-api-8b0f953f-6783-4d78-98b1-0f4a4f57c5e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020908528s
STEP: Saw pod success
Apr 28 16:00:21.889: INFO: Pod "downward-api-8b0f953f-6783-4d78-98b1-0f4a4f57c5e8" satisfied condition "Succeeded or Failed"
Apr 28 16:00:21.893: INFO: Trying to get logs from node ip-172-31-13-33 pod downward-api-8b0f953f-6783-4d78-98b1-0f4a4f57c5e8 container dapi-container: <nil>
STEP: delete the pod
Apr 28 16:00:21.917: INFO: Waiting for pod downward-api-8b0f953f-6783-4d78-98b1-0f4a4f57c5e8 to disappear
Apr 28 16:00:21.921: INFO: Pod downward-api-8b0f953f-6783-4d78-98b1-0f4a4f57c5e8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:00:21.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7917" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":106,"skipped":1760,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:00:21.947: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3445
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-08bea606-2cd3-40a0-be4b-c633ac6a9156
STEP: Creating secret with name s-test-opt-upd-3a690e00-5e50-4fc8-953a-8335376abf09
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-08bea606-2cd3-40a0-be4b-c633ac6a9156
STEP: Updating secret s-test-opt-upd-3a690e00-5e50-4fc8-953a-8335376abf09
STEP: Creating secret with name s-test-opt-create-9a9678fa-e9f7-411a-8fba-cd4ec63668f6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:01:38.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3445" for this suite.

• [SLOW TEST:76.703 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":107,"skipped":1796,"failed":0}
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:01:38.650: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9345
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Apr 28 16:01:38.855: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9345  2d23f425-780e-41b6-a067-86f3469e408b 15733 0 2021-04-28 16:01:38 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-04-28 16:01:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 28 16:01:38.856: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9345  2d23f425-780e-41b6-a067-86f3469e408b 15734 0 2021-04-28 16:01:38 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-04-28 16:01:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:01:38.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9345" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":108,"skipped":1796,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:01:38.870: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 16:01:39.062: INFO: Waiting up to 5m0s for pod "downwardapi-volume-45f8cb14-5cb5-4434-8828-33406d60a235" in namespace "projected-6620" to be "Succeeded or Failed"
Apr 28 16:01:39.067: INFO: Pod "downwardapi-volume-45f8cb14-5cb5-4434-8828-33406d60a235": Phase="Pending", Reason="", readiness=false. Elapsed: 5.171262ms
Apr 28 16:01:41.077: INFO: Pod "downwardapi-volume-45f8cb14-5cb5-4434-8828-33406d60a235": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014890443s
STEP: Saw pod success
Apr 28 16:01:41.077: INFO: Pod "downwardapi-volume-45f8cb14-5cb5-4434-8828-33406d60a235" satisfied condition "Succeeded or Failed"
Apr 28 16:01:41.083: INFO: Trying to get logs from node ip-172-31-76-85 pod downwardapi-volume-45f8cb14-5cb5-4434-8828-33406d60a235 container client-container: <nil>
STEP: delete the pod
Apr 28 16:01:41.117: INFO: Waiting for pod downwardapi-volume-45f8cb14-5cb5-4434-8828-33406d60a235 to disappear
Apr 28 16:01:41.122: INFO: Pod downwardapi-volume-45f8cb14-5cb5-4434-8828-33406d60a235 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:01:41.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6620" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":109,"skipped":1817,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:01:41.139: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2885
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 28 16:01:41.332: INFO: Waiting up to 5m0s for pod "pod-97b581b9-1a15-49f4-a9a7-0eb8adf0837e" in namespace "emptydir-2885" to be "Succeeded or Failed"
Apr 28 16:01:41.344: INFO: Pod "pod-97b581b9-1a15-49f4-a9a7-0eb8adf0837e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.599682ms
Apr 28 16:01:43.355: INFO: Pod "pod-97b581b9-1a15-49f4-a9a7-0eb8adf0837e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022973002s
STEP: Saw pod success
Apr 28 16:01:43.355: INFO: Pod "pod-97b581b9-1a15-49f4-a9a7-0eb8adf0837e" satisfied condition "Succeeded or Failed"
Apr 28 16:01:43.359: INFO: Trying to get logs from node ip-172-31-76-85 pod pod-97b581b9-1a15-49f4-a9a7-0eb8adf0837e container test-container: <nil>
STEP: delete the pod
Apr 28 16:01:43.400: INFO: Waiting for pod pod-97b581b9-1a15-49f4-a9a7-0eb8adf0837e to disappear
Apr 28 16:01:43.404: INFO: Pod pod-97b581b9-1a15-49f4-a9a7-0eb8adf0837e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:01:43.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2885" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":110,"skipped":1829,"failed":0}
S
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:01:43.421: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1198
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Apr 28 16:01:50.128: INFO: Successfully updated pod "adopt-release-7jlss"
STEP: Checking that the Job readopts the Pod
Apr 28 16:01:50.128: INFO: Waiting up to 15m0s for pod "adopt-release-7jlss" in namespace "job-1198" to be "adopted"
Apr 28 16:01:50.134: INFO: Pod "adopt-release-7jlss": Phase="Running", Reason="", readiness=true. Elapsed: 5.384576ms
Apr 28 16:01:52.146: INFO: Pod "adopt-release-7jlss": Phase="Running", Reason="", readiness=true. Elapsed: 2.018055148s
Apr 28 16:01:52.146: INFO: Pod "adopt-release-7jlss" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Apr 28 16:01:52.675: INFO: Successfully updated pod "adopt-release-7jlss"
STEP: Checking that the Job releases the Pod
Apr 28 16:01:52.675: INFO: Waiting up to 15m0s for pod "adopt-release-7jlss" in namespace "job-1198" to be "released"
Apr 28 16:01:52.682: INFO: Pod "adopt-release-7jlss": Phase="Running", Reason="", readiness=true. Elapsed: 6.675002ms
Apr 28 16:01:54.690: INFO: Pod "adopt-release-7jlss": Phase="Running", Reason="", readiness=true. Elapsed: 2.014780372s
Apr 28 16:01:54.690: INFO: Pod "adopt-release-7jlss" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:01:54.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1198" for this suite.

• [SLOW TEST:11.283 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":111,"skipped":1830,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:01:54.705: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4136
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Apr 28 16:01:56.913: INFO: Pod pod-hostip-c3ea8efd-a631-4514-8df1-ab9c1934e4a9 has hostIP: 172.31.13.33
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:01:56.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4136" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":112,"skipped":1839,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:01:56.930: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7803
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 16:01:57.107: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e78f9fee-f753-441f-8d37-965f3d7d2ded" in namespace "downward-api-7803" to be "Succeeded or Failed"
Apr 28 16:01:57.112: INFO: Pod "downwardapi-volume-e78f9fee-f753-441f-8d37-965f3d7d2ded": Phase="Pending", Reason="", readiness=false. Elapsed: 5.366261ms
Apr 28 16:01:59.118: INFO: Pod "downwardapi-volume-e78f9fee-f753-441f-8d37-965f3d7d2ded": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010868967s
STEP: Saw pod success
Apr 28 16:01:59.118: INFO: Pod "downwardapi-volume-e78f9fee-f753-441f-8d37-965f3d7d2ded" satisfied condition "Succeeded or Failed"
Apr 28 16:01:59.123: INFO: Trying to get logs from node ip-172-31-13-33 pod downwardapi-volume-e78f9fee-f753-441f-8d37-965f3d7d2ded container client-container: <nil>
STEP: delete the pod
Apr 28 16:01:59.158: INFO: Waiting for pod downwardapi-volume-e78f9fee-f753-441f-8d37-965f3d7d2ded to disappear
Apr 28 16:01:59.163: INFO: Pod downwardapi-volume-e78f9fee-f753-441f-8d37-965f3d7d2ded no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:01:59.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7803" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":113,"skipped":1851,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:01:59.178: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3233
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:01:59.361: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:02:01.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3233" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":114,"skipped":1853,"failed":0}

------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:02:01.509: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4169
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-c3841fab-7190-4a16-8019-520518b79de1
Apr 28 16:02:01.698: INFO: Pod name my-hostname-basic-c3841fab-7190-4a16-8019-520518b79de1: Found 0 pods out of 1
Apr 28 16:02:06.707: INFO: Pod name my-hostname-basic-c3841fab-7190-4a16-8019-520518b79de1: Found 1 pods out of 1
Apr 28 16:02:06.707: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c3841fab-7190-4a16-8019-520518b79de1" are running
Apr 28 16:02:06.713: INFO: Pod "my-hostname-basic-c3841fab-7190-4a16-8019-520518b79de1-8xccp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-28 16:02:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-28 16:02:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-28 16:02:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-28 16:02:01 +0000 UTC Reason: Message:}])
Apr 28 16:02:06.713: INFO: Trying to dial the pod
Apr 28 16:02:11.735: INFO: Controller my-hostname-basic-c3841fab-7190-4a16-8019-520518b79de1: Got expected result from replica 1 [my-hostname-basic-c3841fab-7190-4a16-8019-520518b79de1-8xccp]: "my-hostname-basic-c3841fab-7190-4a16-8019-520518b79de1-8xccp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:02:11.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4169" for this suite.

• [SLOW TEST:10.250 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":115,"skipped":1853,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:02:11.759: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 28 16:02:14.508: INFO: Successfully updated pod "pod-update-875f6040-f297-4752-a3b4-9724125491ec"
STEP: verifying the updated pod is in kubernetes
Apr 28 16:02:14.517: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:02:14.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9643" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":116,"skipped":1879,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:02:14.531: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-02a7f2a1-4cf8-4b9f-9656-18fe9840b44d
STEP: Creating a pod to test consume configMaps
Apr 28 16:02:14.729: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-07f92b1a-2e56-41ed-afb2-967baafeaa43" in namespace "projected-6470" to be "Succeeded or Failed"
Apr 28 16:02:14.738: INFO: Pod "pod-projected-configmaps-07f92b1a-2e56-41ed-afb2-967baafeaa43": Phase="Pending", Reason="", readiness=false. Elapsed: 8.673314ms
Apr 28 16:02:16.746: INFO: Pod "pod-projected-configmaps-07f92b1a-2e56-41ed-afb2-967baafeaa43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016436099s
STEP: Saw pod success
Apr 28 16:02:16.746: INFO: Pod "pod-projected-configmaps-07f92b1a-2e56-41ed-afb2-967baafeaa43" satisfied condition "Succeeded or Failed"
Apr 28 16:02:16.750: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-projected-configmaps-07f92b1a-2e56-41ed-afb2-967baafeaa43 container agnhost-container: <nil>
STEP: delete the pod
Apr 28 16:02:16.785: INFO: Waiting for pod pod-projected-configmaps-07f92b1a-2e56-41ed-afb2-967baafeaa43 to disappear
Apr 28 16:02:16.790: INFO: Pod pod-projected-configmaps-07f92b1a-2e56-41ed-afb2-967baafeaa43 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:02:16.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6470" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":117,"skipped":1931,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:02:16.804: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4955
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 28 16:02:17.448: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 16:02:20.477: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:02:20.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4955" for this suite.
STEP: Destroying namespace "webhook-4955-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":118,"skipped":1933,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:02:20.654: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5069
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:02:20.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5069" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":119,"skipped":1950,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:02:20.854: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6568
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:02:21.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6568" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":120,"skipped":1965,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:02:21.137: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 28 16:02:25.384: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 28 16:02:25.392: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 28 16:02:27.393: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 28 16:02:27.406: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 28 16:02:29.392: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 28 16:02:29.399: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 28 16:02:31.393: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 28 16:02:31.406: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 28 16:02:33.393: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 28 16:02:33.402: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 28 16:02:35.393: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 28 16:02:35.399: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:02:35.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1109" for this suite.

• [SLOW TEST:14.285 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":121,"skipped":1992,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:02:35.423: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8413
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0428 16:02:36.664488      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0428 16:02:36.664506      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0428 16:02:36.664510      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 28 16:02:36.664: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:02:36.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8413" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":122,"skipped":2016,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:02:36.679: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9187
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9187
STEP: creating service affinity-clusterip-transition in namespace services-9187
STEP: creating replication controller affinity-clusterip-transition in namespace services-9187
I0428 16:02:36.903953      24 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-9187, replica count: 3
I0428 16:02:39.954225      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 28 16:02:39.976: INFO: Creating new exec pod
Apr 28 16:02:43.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9187 exec execpod-affinityckssq -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Apr 28 16:02:43.184: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Apr 28 16:02:43.184: INFO: stdout: ""
Apr 28 16:02:43.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9187 exec execpod-affinityckssq -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.43 80'
Apr 28 16:02:43.313: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.43 80\nConnection to 10.152.183.43 80 port [tcp/http] succeeded!\n"
Apr 28 16:02:43.313: INFO: stdout: ""
Apr 28 16:02:43.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9187 exec execpod-affinityckssq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.43:80/ ; done'
Apr 28 16:02:43.518: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n"
Apr 28 16:02:43.518: INFO: stdout: "\naffinity-clusterip-transition-76msk\naffinity-clusterip-transition-h6zg5\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-76msk\naffinity-clusterip-transition-h6zg5\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-h6zg5\naffinity-clusterip-transition-76msk\naffinity-clusterip-transition-h6zg5\naffinity-clusterip-transition-76msk\naffinity-clusterip-transition-76msk\naffinity-clusterip-transition-h6zg5\naffinity-clusterip-transition-76msk\naffinity-clusterip-transition-76msk\naffinity-clusterip-transition-ll62c"
Apr 28 16:02:43.518: INFO: Received response from host: affinity-clusterip-transition-76msk
Apr 28 16:02:43.518: INFO: Received response from host: affinity-clusterip-transition-h6zg5
Apr 28 16:02:43.518: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.518: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.518: INFO: Received response from host: affinity-clusterip-transition-76msk
Apr 28 16:02:43.518: INFO: Received response from host: affinity-clusterip-transition-h6zg5
Apr 28 16:02:43.518: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.518: INFO: Received response from host: affinity-clusterip-transition-h6zg5
Apr 28 16:02:43.518: INFO: Received response from host: affinity-clusterip-transition-76msk
Apr 28 16:02:43.518: INFO: Received response from host: affinity-clusterip-transition-h6zg5
Apr 28 16:02:43.518: INFO: Received response from host: affinity-clusterip-transition-76msk
Apr 28 16:02:43.518: INFO: Received response from host: affinity-clusterip-transition-76msk
Apr 28 16:02:43.518: INFO: Received response from host: affinity-clusterip-transition-h6zg5
Apr 28 16:02:43.518: INFO: Received response from host: affinity-clusterip-transition-76msk
Apr 28 16:02:43.518: INFO: Received response from host: affinity-clusterip-transition-76msk
Apr 28 16:02:43.518: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9187 exec execpod-affinityckssq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.43:80/ ; done'
Apr 28 16:02:43.731: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.43:80/\n"
Apr 28 16:02:43.731: INFO: stdout: "\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-ll62c\naffinity-clusterip-transition-ll62c"
Apr 28 16:02:43.731: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.731: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.731: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.731: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.731: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.731: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.731: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.731: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.731: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.731: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.731: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.731: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.731: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.731: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.731: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.731: INFO: Received response from host: affinity-clusterip-transition-ll62c
Apr 28 16:02:43.731: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9187, will wait for the garbage collector to delete the pods
Apr 28 16:02:43.820: INFO: Deleting ReplicationController affinity-clusterip-transition took: 10.711508ms
Apr 28 16:02:44.420: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 600.147661ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:02:57.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9187" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:20.397 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":123,"skipped":2037,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:02:57.076: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-876
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 28 16:02:57.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-876 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Apr 28 16:02:57.316: INFO: stderr: ""
Apr 28 16:02:57.317: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Apr 28 16:02:57.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-876 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Apr 28 16:02:57.556: INFO: stderr: ""
Apr 28 16:02:57.556: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Apr 28 16:02:57.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-876 delete pods e2e-test-httpd-pod'
Apr 28 16:02:59.357: INFO: stderr: ""
Apr 28 16:02:59.357: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:02:59.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-876" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":124,"skipped":2056,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:02:59.372: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 28 16:03:00.612: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:03:00.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5002" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":125,"skipped":2087,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:03:00.646: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1082
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1082.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1082.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1082.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1082.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1082.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1082.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 28 16:03:02.905: INFO: DNS probes using dns-1082/dns-test-d9b30fcb-a081-4fb2-be47-12b3dc7cb9a4 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:03:02.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1082" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":126,"skipped":2115,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:03:02.971: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6853
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6853.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6853.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6853.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6853.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6853.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6853.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 28 16:03:05.204: INFO: DNS probes using dns-6853/dns-test-b5c5053a-5bdf-4c18-8045-11280443c935 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:03:05.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6853" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":127,"skipped":2116,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:03:05.248: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9842
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Apr 28 16:03:05.428: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 28 16:03:05.428: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 28 16:03:05.436: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 28 16:03:05.436: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 28 16:03:05.452: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 28 16:03:05.452: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 28 16:03:05.488: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 28 16:03:05.488: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 28 16:03:06.397: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr 28 16:03:06.397: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr 28 16:03:06.426: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Apr 28 16:03:06.442: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Apr 28 16:03:06.446: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 0
Apr 28 16:03:06.446: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 0
Apr 28 16:03:06.446: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 0
Apr 28 16:03:06.446: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 0
Apr 28 16:03:06.446: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 0
Apr 28 16:03:06.446: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 0
Apr 28 16:03:06.446: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 0
Apr 28 16:03:06.446: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 0
Apr 28 16:03:06.447: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1
Apr 28 16:03:06.447: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1
Apr 28 16:03:06.447: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 2
Apr 28 16:03:06.447: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 2
Apr 28 16:03:06.447: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 2
Apr 28 16:03:06.447: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 2
Apr 28 16:03:06.459: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 2
Apr 28 16:03:06.459: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 2
Apr 28 16:03:06.478: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 2
Apr 28 16:03:06.478: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 2
Apr 28 16:03:06.508: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1
STEP: listing Deployments
Apr 28 16:03:06.513: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Apr 28 16:03:06.550: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Apr 28 16:03:06.558: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1 and labels map[test-deployment:patched test-deployment-static:true]
Apr 28 16:03:06.558: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 28 16:03:06.568: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 28 16:03:06.591: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 28 16:03:06.629: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 28 16:03:06.650: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 28 16:03:06.669: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 28 16:03:06.683: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 28 16:03:06.697: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Apr 28 16:03:07.437: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1
Apr 28 16:03:07.437: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1
Apr 28 16:03:07.437: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1
Apr 28 16:03:07.437: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1
Apr 28 16:03:07.437: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1
Apr 28 16:03:07.437: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1
Apr 28 16:03:07.437: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1
Apr 28 16:03:07.437: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1
Apr 28 16:03:07.437: INFO: observed Deployment test-deployment in namespace deployment-9842 with ReadyReplicas 1
STEP: deleting the Deployment
Apr 28 16:03:07.470: INFO: observed event type MODIFIED
Apr 28 16:03:07.470: INFO: observed event type MODIFIED
Apr 28 16:03:07.471: INFO: observed event type MODIFIED
Apr 28 16:03:07.471: INFO: observed event type MODIFIED
Apr 28 16:03:07.471: INFO: observed event type MODIFIED
Apr 28 16:03:07.471: INFO: observed event type MODIFIED
Apr 28 16:03:07.471: INFO: observed event type MODIFIED
Apr 28 16:03:07.471: INFO: observed event type MODIFIED
Apr 28 16:03:07.471: INFO: observed event type MODIFIED
Apr 28 16:03:07.471: INFO: observed event type MODIFIED
Apr 28 16:03:07.471: INFO: observed event type MODIFIED
Apr 28 16:03:07.471: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 28 16:03:07.476: INFO: Log out all the ReplicaSets if there is no deployment created
Apr 28 16:03:07.484: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-9842  6be68b58-78ba-423c-b729-4befb4d12cc4 16827 3 2021-04-28 16:03:06 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 0aa566fa-1c5a-408b-999e-6845e4956565 0xc0044c3c97 0xc0044c3c98}] []  [{kube-controller-manager Update apps/v1 2021-04-28 16:03:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0aa566fa-1c5a-408b-999e-6845e4956565\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0044c3d00 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Apr 28 16:03:07.496: INFO: pod: "test-deployment-768947d6f5-9tgd4":
&Pod{ObjectMeta:{test-deployment-768947d6f5-9tgd4 test-deployment-768947d6f5- deployment-9842  e4b795ac-8da9-4651-8863-cf23622c3c9a 16808 0 2021-04-28 16:03:06 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-768947d6f5 6be68b58-78ba-423c-b729-4befb4d12cc4 0xc0046eb537 0xc0046eb538}] []  [{kube-controller-manager Update v1 2021-04-28 16:03:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6be68b58-78ba-423c-b729-4befb4d12cc4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:03:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.62.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bx4wb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bx4wb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bx4wb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:03:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:03:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:03:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:03:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.33,PodIP:10.1.62.121,StartTime:2021-04-28 16:03:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-28 16:03:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://0fe6f500207794674147c70898f1e725f20adadf1bbc1fd46a99c24d88bb0470,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.62.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 28 16:03:07.496: INFO: pod: "test-deployment-768947d6f5-k2ngp":
&Pod{ObjectMeta:{test-deployment-768947d6f5-k2ngp test-deployment-768947d6f5- deployment-9842  78af4b31-7961-4118-b8b6-63622a25046a 16820 0 2021-04-28 16:03:07 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-768947d6f5 6be68b58-78ba-423c-b729-4befb4d12cc4 0xc0046eb6e7 0xc0046eb6e8}] []  [{kube-controller-manager Update v1 2021-04-28 16:03:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6be68b58-78ba-423c-b729-4befb4d12cc4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bx4wb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bx4wb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bx4wb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-30-112,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:03:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 28 16:03:07.496: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-9842  a2661f87-1bbe-4172-862b-65ed5b6e9349 16823 4 2021-04-28 16:03:06 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 0aa566fa-1c5a-408b-999e-6845e4956565 0xc0044c3d67 0xc0044c3d68}] []  [{kube-controller-manager Update apps/v1 2021-04-28 16:03:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0aa566fa-1c5a-408b-999e-6845e4956565\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0044c3de8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Apr 28 16:03:07.504: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-9842  edae5185-91b7-4527-8ed0-f2c5f3024ec8 16766 2 2021-04-28 16:03:05 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 0aa566fa-1c5a-408b-999e-6845e4956565 0xc0044c3e47 0xc0044c3e48}] []  [{kube-controller-manager Update apps/v1 2021-04-28 16:03:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0aa566fa-1c5a-408b-999e-6845e4956565\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0044c3eb0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Apr 28 16:03:07.513: INFO: pod: "test-deployment-8b6954bfb-wk62r":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-wk62r test-deployment-8b6954bfb- deployment-9842  6f6213d9-43c5-41f9-a3ce-f1a1f1064c20 16737 0 2021-04-28 16:03:05 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-8b6954bfb edae5185-91b7-4527-8ed0-f2c5f3024ec8 0xc004754777 0xc004754778}] []  [{kube-controller-manager Update v1 2021-04-28 16:03:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"edae5185-91b7-4527-8ed0-f2c5f3024ec8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:03:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.62.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bx4wb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bx4wb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bx4wb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:03:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:03:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:03:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:03:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.33,PodIP:10.1.62.119,StartTime:2021-04-28 16:03:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-28 16:03:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://5ffbdaa441f1f1407d0be5fdb32be541d26d6758b32ba9bcff7a5e29ac5d8dca,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.62.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:03:07.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9842" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":128,"skipped":2122,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:03:07.530: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:03:07.717: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr 28 16:03:12.723: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 28 16:03:12.723: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 28 16:03:12.756: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7109  a973e874-1311-4530-b0f1-3b1241967e9d 16920 1 2021-04-28 16:03:12 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-04-28 16:03:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00449e438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Apr 28 16:03:12.761: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-7109  9f68a67e-7edf-44e4-abc5-c880e8b5a0cb 16924 1 2021-04-28 16:03:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment a973e874-1311-4530-b0f1-3b1241967e9d 0xc004754a77 0xc004754a78}] []  [{kube-controller-manager Update apps/v1 2021-04-28 16:03:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a973e874-1311-4530-b0f1-3b1241967e9d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004754b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 28 16:03:12.761: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Apr 28 16:03:12.761: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7109  c2a0aea3-c90f-4d91-8094-9c4e0b5a4584 16922 1 2021-04-28 16:03:07 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment a973e874-1311-4530-b0f1-3b1241967e9d 0xc004754967 0xc004754968}] []  [{e2e.test Update apps/v1 2021-04-28 16:03:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-28 16:03:12 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"a973e874-1311-4530-b0f1-3b1241967e9d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004754a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 28 16:03:12.766: INFO: Pod "test-cleanup-controller-lx8g4" is available:
&Pod{ObjectMeta:{test-cleanup-controller-lx8g4 test-cleanup-controller- deployment-7109  4ecdd091-0823-4791-89dc-90cd8b7e0143 16874 0 2021-04-28 16:03:07 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller c2a0aea3-c90f-4d91-8094-9c4e0b5a4584 0xc004754f17 0xc004754f18}] []  [{kube-controller-manager Update v1 2021-04-28 16:03:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2a0aea3-c90f-4d91-8094-9c4e0b5a4584\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:03:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.62.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-46rpf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-46rpf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-46rpf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:03:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:03:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:03:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:03:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.33,PodIP:10.1.62.122,StartTime:2021-04-28 16:03:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-28 16:03:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://19ee4ed0337b47135b14bc1a571f20b5e63a2418a9e75994746eaeafbdc5f40c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.62.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:03:12.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7109" for this suite.

• [SLOW TEST:5.270 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":129,"skipped":2130,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:03:12.800: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5914
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 16:03:12.990: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a600d67-ff1f-4097-b136-ae21a347f63d" in namespace "projected-5914" to be "Succeeded or Failed"
Apr 28 16:03:12.997: INFO: Pod "downwardapi-volume-5a600d67-ff1f-4097-b136-ae21a347f63d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.766673ms
Apr 28 16:03:15.013: INFO: Pod "downwardapi-volume-5a600d67-ff1f-4097-b136-ae21a347f63d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023084084s
STEP: Saw pod success
Apr 28 16:03:15.013: INFO: Pod "downwardapi-volume-5a600d67-ff1f-4097-b136-ae21a347f63d" satisfied condition "Succeeded or Failed"
Apr 28 16:03:15.025: INFO: Trying to get logs from node ip-172-31-30-112 pod downwardapi-volume-5a600d67-ff1f-4097-b136-ae21a347f63d container client-container: <nil>
STEP: delete the pod
Apr 28 16:03:15.127: INFO: Waiting for pod downwardapi-volume-5a600d67-ff1f-4097-b136-ae21a347f63d to disappear
Apr 28 16:03:15.149: INFO: Pod downwardapi-volume-5a600d67-ff1f-4097-b136-ae21a347f63d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:03:15.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5914" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":130,"skipped":2137,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:03:15.184: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1433
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Apr 28 16:03:15.375: INFO: Waiting up to 5m0s for pod "var-expansion-448f01db-5832-4248-a984-e53f7c400b69" in namespace "var-expansion-1433" to be "Succeeded or Failed"
Apr 28 16:03:15.392: INFO: Pod "var-expansion-448f01db-5832-4248-a984-e53f7c400b69": Phase="Pending", Reason="", readiness=false. Elapsed: 17.143413ms
Apr 28 16:03:17.404: INFO: Pod "var-expansion-448f01db-5832-4248-a984-e53f7c400b69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028976392s
STEP: Saw pod success
Apr 28 16:03:17.404: INFO: Pod "var-expansion-448f01db-5832-4248-a984-e53f7c400b69" satisfied condition "Succeeded or Failed"
Apr 28 16:03:17.409: INFO: Trying to get logs from node ip-172-31-13-33 pod var-expansion-448f01db-5832-4248-a984-e53f7c400b69 container dapi-container: <nil>
STEP: delete the pod
Apr 28 16:03:17.438: INFO: Waiting for pod var-expansion-448f01db-5832-4248-a984-e53f7c400b69 to disappear
Apr 28 16:03:17.443: INFO: Pod var-expansion-448f01db-5832-4248-a984-e53f7c400b69 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:03:17.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1433" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":131,"skipped":2139,"failed":0}
S
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:03:17.457: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-8219
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Apr 28 16:03:17.639: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Apr 28 16:03:17.652: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 28 16:03:17.652: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Apr 28 16:03:17.674: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 28 16:03:17.674: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Apr 28 16:03:17.696: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Apr 28 16:03:17.696: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Apr 28 16:03:24.775: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:03:24.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-8219" for this suite.

• [SLOW TEST:7.352 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":132,"skipped":2140,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:03:24.809: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3037
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 28 16:03:24.992: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 28 16:03:25.007: INFO: Waiting for terminating namespaces to be deleted...
Apr 28 16:03:25.013: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-13-33 before test
Apr 28 16:03:25.020: INFO: nginx-ingress-controller-kubernetes-worker-b68vn from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 16:03:25.020: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Apr 28 16:03:25.020: INFO: coredns-7bb4d77796-bxk7f from kube-system started at 2021-04-28 14:28:57 +0000 UTC (1 container statuses recorded)
Apr 28 16:03:25.020: INFO: 	Container coredns ready: true, restart count 0
Apr 28 16:03:25.020: INFO: metrics-server-v0.3.6-f6cf867b4-sgq8s from kube-system started at 2021-04-28 14:32:04 +0000 UTC (2 container statuses recorded)
Apr 28 16:03:25.020: INFO: 	Container metrics-server ready: true, restart count 0
Apr 28 16:03:25.020: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Apr 28 16:03:25.020: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-30-112 before test
Apr 28 16:03:25.028: INFO: nginx-ingress-controller-kubernetes-worker-9plpg from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:47 +0000 UTC (1 container statuses recorded)
Apr 28 16:03:25.028: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Apr 28 16:03:25.028: INFO: kube-state-metrics-7f55b9fcd7-jjs4x from kube-system started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 16:03:25.028: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 28 16:03:25.028: INFO: dashboard-metrics-scraper-74757fb5b7-x9lrp from kubernetes-dashboard started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 16:03:25.028: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr 28 16:03:25.028: INFO: kubernetes-dashboard-64f87676d4-mshn4 from kubernetes-dashboard started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 16:03:25.028: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr 28 16:03:25.028: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-76-85 before test
Apr 28 16:03:25.035: INFO: default-http-backend-kubernetes-worker-6494cbc7fd-74bdv from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:58 +0000 UTC (1 container statuses recorded)
Apr 28 16:03:25.035: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
Apr 28 16:03:25.035: INFO: nginx-ingress-controller-kubernetes-worker-bjnb5 from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:49 +0000 UTC (1 container statuses recorded)
Apr 28 16:03:25.035: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Apr 28 16:03:25.035: INFO: sonobuoy from sonobuoy started at 2021-04-28 15:27:11 +0000 UTC (1 container statuses recorded)
Apr 28 16:03:25.035: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 28 16:03:25.035: INFO: sonobuoy-e2e-job-c0dbab055ba045c5 from sonobuoy started at 2021-04-28 15:27:15 +0000 UTC (2 container statuses recorded)
Apr 28 16:03:25.035: INFO: 	Container e2e ready: true, restart count 0
Apr 28 16:03:25.035: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b070d48e-76ee-4ded-8c6a-acfd80a5a85e 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-b070d48e-76ee-4ded-8c6a-acfd80a5a85e off the node ip-172-31-13-33
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b070d48e-76ee-4ded-8c6a-acfd80a5a85e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:03:29.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3037" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":133,"skipped":2185,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:03:29.217: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5698
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Apr 28 16:03:29.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-5698 create -f -'
Apr 28 16:03:29.649: INFO: stderr: ""
Apr 28 16:03:29.650: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 28 16:03:29.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-5698 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 28 16:03:29.736: INFO: stderr: ""
Apr 28 16:03:29.736: INFO: stdout: "update-demo-nautilus-98gnt update-demo-nautilus-9vnfs "
Apr 28 16:03:29.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-5698 get pods update-demo-nautilus-98gnt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 28 16:03:29.790: INFO: stderr: ""
Apr 28 16:03:29.790: INFO: stdout: ""
Apr 28 16:03:29.790: INFO: update-demo-nautilus-98gnt is created but not running
Apr 28 16:03:34.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-5698 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 28 16:03:34.849: INFO: stderr: ""
Apr 28 16:03:34.849: INFO: stdout: "update-demo-nautilus-98gnt update-demo-nautilus-9vnfs "
Apr 28 16:03:34.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-5698 get pods update-demo-nautilus-98gnt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 28 16:03:34.904: INFO: stderr: ""
Apr 28 16:03:34.904: INFO: stdout: "true"
Apr 28 16:03:34.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-5698 get pods update-demo-nautilus-98gnt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 28 16:03:34.968: INFO: stderr: ""
Apr 28 16:03:34.968: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 28 16:03:34.968: INFO: validating pod update-demo-nautilus-98gnt
Apr 28 16:03:34.975: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 28 16:03:34.975: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 28 16:03:34.975: INFO: update-demo-nautilus-98gnt is verified up and running
Apr 28 16:03:34.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-5698 get pods update-demo-nautilus-9vnfs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 28 16:03:35.039: INFO: stderr: ""
Apr 28 16:03:35.039: INFO: stdout: "true"
Apr 28 16:03:35.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-5698 get pods update-demo-nautilus-9vnfs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 28 16:03:35.136: INFO: stderr: ""
Apr 28 16:03:35.136: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 28 16:03:35.136: INFO: validating pod update-demo-nautilus-9vnfs
Apr 28 16:03:35.141: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 28 16:03:35.141: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 28 16:03:35.141: INFO: update-demo-nautilus-9vnfs is verified up and running
STEP: using delete to clean up resources
Apr 28 16:03:35.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-5698 delete --grace-period=0 --force -f -'
Apr 28 16:03:35.205: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 28 16:03:35.205: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 28 16:03:35.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-5698 get rc,svc -l name=update-demo --no-headers'
Apr 28 16:03:35.266: INFO: stderr: "No resources found in kubectl-5698 namespace.\n"
Apr 28 16:03:35.266: INFO: stdout: ""
Apr 28 16:03:35.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-5698 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 28 16:03:35.326: INFO: stderr: ""
Apr 28 16:03:35.326: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:03:35.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5698" for this suite.

• [SLOW TEST:6.122 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":134,"skipped":2196,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:03:35.339: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-561
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Apr 28 16:03:35.495: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 28 16:04:35.513: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:04:35.518: INFO: Starting informer...
STEP: Starting pod...
Apr 28 16:04:35.747: INFO: Pod is running on ip-172-31-13-33. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Apr 28 16:04:35.770: INFO: Pod wasn't evicted. Proceeding
Apr 28 16:04:35.770: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Apr 28 16:05:50.789: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:05:50.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-561" for this suite.

• [SLOW TEST:135.478 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":135,"skipped":2199,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:05:50.817: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1368
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1368
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1368
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1368
Apr 28 16:05:51.035: INFO: Found 0 stateful pods, waiting for 1
Apr 28 16:06:01.044: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Apr 28 16:06:01.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-1368 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 28 16:06:01.247: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 28 16:06:01.247: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 28 16:06:01.247: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 28 16:06:01.253: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 28 16:06:11.261: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 28 16:06:11.261: INFO: Waiting for statefulset status.replicas updated to 0
Apr 28 16:06:11.281: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999983s
Apr 28 16:06:12.288: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995737162s
Apr 28 16:06:13.294: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987915175s
Apr 28 16:06:14.302: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.982216656s
Apr 28 16:06:15.311: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.974297883s
Apr 28 16:06:16.318: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.965623255s
Apr 28 16:06:17.323: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.958487941s
Apr 28 16:06:18.331: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.953833641s
Apr 28 16:06:19.337: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.945643447s
Apr 28 16:06:20.372: INFO: Verifying statefulset ss doesn't scale past 1 for another 939.030671ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1368
Apr 28 16:06:21.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-1368 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 16:06:21.550: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 28 16:06:21.550: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 28 16:06:21.550: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 28 16:06:21.555: INFO: Found 1 stateful pods, waiting for 3
Apr 28 16:06:31.563: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 28 16:06:31.563: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 28 16:06:31.563: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Apr 28 16:06:31.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-1368 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 28 16:06:31.727: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 28 16:06:31.727: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 28 16:06:31.727: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 28 16:06:31.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-1368 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 28 16:06:31.857: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 28 16:06:31.857: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 28 16:06:31.857: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 28 16:06:31.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-1368 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 28 16:06:32.025: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 28 16:06:32.025: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 28 16:06:32.025: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 28 16:06:32.025: INFO: Waiting for statefulset status.replicas updated to 0
Apr 28 16:06:32.029: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Apr 28 16:06:42.039: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 28 16:06:42.039: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 28 16:06:42.039: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 28 16:06:42.063: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999672s
Apr 28 16:06:43.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993653173s
Apr 28 16:06:44.079: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983603668s
Apr 28 16:06:45.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977372808s
Apr 28 16:06:46.095: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969187108s
Apr 28 16:06:47.103: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.962229942s
Apr 28 16:06:48.110: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.953737941s
Apr 28 16:06:49.117: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.9464022s
Apr 28 16:06:50.125: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.940075381s
Apr 28 16:06:51.133: INFO: Verifying statefulset ss doesn't scale past 3 for another 931.90859ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1368
Apr 28 16:06:52.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-1368 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 16:06:52.314: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 28 16:06:52.314: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 28 16:06:52.314: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 28 16:06:52.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-1368 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 16:06:52.438: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 28 16:06:52.438: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 28 16:06:52.438: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 28 16:06:52.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-1368 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 16:06:52.591: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 28 16:06:52.591: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 28 16:06:52.591: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 28 16:06:52.591: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 28 16:07:12.617: INFO: Deleting all statefulset in ns statefulset-1368
Apr 28 16:07:12.623: INFO: Scaling statefulset ss to 0
Apr 28 16:07:12.641: INFO: Waiting for statefulset status.replicas updated to 0
Apr 28 16:07:12.646: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:07:12.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1368" for this suite.

• [SLOW TEST:81.864 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":136,"skipped":2204,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:07:12.681: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9441
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-qw89
STEP: Creating a pod to test atomic-volume-subpath
Apr 28 16:07:12.885: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-qw89" in namespace "subpath-9441" to be "Succeeded or Failed"
Apr 28 16:07:12.898: INFO: Pod "pod-subpath-test-downwardapi-qw89": Phase="Pending", Reason="", readiness=false. Elapsed: 12.615257ms
Apr 28 16:07:14.907: INFO: Pod "pod-subpath-test-downwardapi-qw89": Phase="Running", Reason="", readiness=true. Elapsed: 2.021661808s
Apr 28 16:07:16.923: INFO: Pod "pod-subpath-test-downwardapi-qw89": Phase="Running", Reason="", readiness=true. Elapsed: 4.03800966s
Apr 28 16:07:18.929: INFO: Pod "pod-subpath-test-downwardapi-qw89": Phase="Running", Reason="", readiness=true. Elapsed: 6.043911667s
Apr 28 16:07:20.957: INFO: Pod "pod-subpath-test-downwardapi-qw89": Phase="Running", Reason="", readiness=true. Elapsed: 8.071582069s
Apr 28 16:07:22.966: INFO: Pod "pod-subpath-test-downwardapi-qw89": Phase="Running", Reason="", readiness=true. Elapsed: 10.080401596s
Apr 28 16:07:24.971: INFO: Pod "pod-subpath-test-downwardapi-qw89": Phase="Running", Reason="", readiness=true. Elapsed: 12.085823294s
Apr 28 16:07:26.976: INFO: Pod "pod-subpath-test-downwardapi-qw89": Phase="Running", Reason="", readiness=true. Elapsed: 14.091176657s
Apr 28 16:07:28.986: INFO: Pod "pod-subpath-test-downwardapi-qw89": Phase="Running", Reason="", readiness=true. Elapsed: 16.100733736s
Apr 28 16:07:30.993: INFO: Pod "pod-subpath-test-downwardapi-qw89": Phase="Running", Reason="", readiness=true. Elapsed: 18.108147145s
Apr 28 16:07:33.001: INFO: Pod "pod-subpath-test-downwardapi-qw89": Phase="Running", Reason="", readiness=true. Elapsed: 20.115384205s
Apr 28 16:07:35.009: INFO: Pod "pod-subpath-test-downwardapi-qw89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.124049107s
STEP: Saw pod success
Apr 28 16:07:35.009: INFO: Pod "pod-subpath-test-downwardapi-qw89" satisfied condition "Succeeded or Failed"
Apr 28 16:07:35.015: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-subpath-test-downwardapi-qw89 container test-container-subpath-downwardapi-qw89: <nil>
STEP: delete the pod
Apr 28 16:07:35.058: INFO: Waiting for pod pod-subpath-test-downwardapi-qw89 to disappear
Apr 28 16:07:35.064: INFO: Pod pod-subpath-test-downwardapi-qw89 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-qw89
Apr 28 16:07:35.064: INFO: Deleting pod "pod-subpath-test-downwardapi-qw89" in namespace "subpath-9441"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:07:35.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9441" for this suite.

• [SLOW TEST:22.400 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":137,"skipped":2209,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:07:35.081: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1514
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Apr 28 16:07:35.281: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1514  d054b39d-078f-4f42-be16-6f15bc2f118e 18052 0 2021-04-28 16:07:35 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-28 16:07:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 28 16:07:35.281: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1514  d054b39d-078f-4f42-be16-6f15bc2f118e 18053 0 2021-04-28 16:07:35 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-28 16:07:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Apr 28 16:07:35.302: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1514  d054b39d-078f-4f42-be16-6f15bc2f118e 18054 0 2021-04-28 16:07:35 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-28 16:07:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 28 16:07:35.302: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1514  d054b39d-078f-4f42-be16-6f15bc2f118e 18055 0 2021-04-28 16:07:35 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-28 16:07:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:07:35.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1514" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":138,"skipped":2212,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:07:35.316: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3601
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Apr 28 16:07:37.529: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3601 PodName:var-expansion-d44a08c4-4d32-43ac-ac40-46b5fab912e0 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 16:07:37.529: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: test for file in mounted path
Apr 28 16:07:37.627: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3601 PodName:var-expansion-d44a08c4-4d32-43ac-ac40-46b5fab912e0 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 16:07:37.627: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: updating the annotation value
Apr 28 16:07:38.214: INFO: Successfully updated pod "var-expansion-d44a08c4-4d32-43ac-ac40-46b5fab912e0"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Apr 28 16:07:38.218: INFO: Deleting pod "var-expansion-d44a08c4-4d32-43ac-ac40-46b5fab912e0" in namespace "var-expansion-3601"
Apr 28 16:07:38.230: INFO: Wait up to 5m0s for pod "var-expansion-d44a08c4-4d32-43ac-ac40-46b5fab912e0" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:08:12.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3601" for this suite.

• [SLOW TEST:36.942 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":139,"skipped":2213,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:08:12.259: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2688
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Apr 28 16:08:15.541: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:08:16.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2688" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":140,"skipped":2231,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:08:16.579: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3148
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1710
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5454
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:08:47.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3148" for this suite.
STEP: Destroying namespace "nsdeletetest-1710" for this suite.
Apr 28 16:08:47.123: INFO: Namespace nsdeletetest-1710 was already deleted
STEP: Destroying namespace "nsdeletetest-5454" for this suite.

• [SLOW TEST:30.552 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":141,"skipped":2236,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:08:47.131: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3898
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 28 16:08:47.301: INFO: Waiting up to 5m0s for pod "downward-api-9b87a63c-5a60-4d37-984d-2e478b92264c" in namespace "downward-api-3898" to be "Succeeded or Failed"
Apr 28 16:08:47.314: INFO: Pod "downward-api-9b87a63c-5a60-4d37-984d-2e478b92264c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.546796ms
Apr 28 16:08:49.323: INFO: Pod "downward-api-9b87a63c-5a60-4d37-984d-2e478b92264c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02118501s
STEP: Saw pod success
Apr 28 16:08:49.323: INFO: Pod "downward-api-9b87a63c-5a60-4d37-984d-2e478b92264c" satisfied condition "Succeeded or Failed"
Apr 28 16:08:49.329: INFO: Trying to get logs from node ip-172-31-13-33 pod downward-api-9b87a63c-5a60-4d37-984d-2e478b92264c container dapi-container: <nil>
STEP: delete the pod
Apr 28 16:08:49.354: INFO: Waiting for pod downward-api-9b87a63c-5a60-4d37-984d-2e478b92264c to disappear
Apr 28 16:08:49.359: INFO: Pod downward-api-9b87a63c-5a60-4d37-984d-2e478b92264c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:08:49.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3898" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":142,"skipped":2237,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:08:49.374: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3012
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 28 16:08:49.789: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 16:08:52.816: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Apr 28 16:08:54.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=webhook-3012 attach --namespace=webhook-3012 to-be-attached-pod -i -c=container1'
Apr 28 16:08:55.135: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:08:55.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3012" for this suite.
STEP: Destroying namespace "webhook-3012-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.859 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":143,"skipped":2254,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:08:55.234: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9421
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 28 16:08:55.383: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 28 16:08:55.393: INFO: Waiting for terminating namespaces to be deleted...
Apr 28 16:08:55.398: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-13-33 before test
Apr 28 16:08:55.402: INFO: nginx-ingress-controller-kubernetes-worker-9knnc from ingress-nginx-kubernetes-worker started at 2021-04-28 16:04:51 +0000 UTC (1 container statuses recorded)
Apr 28 16:08:55.402: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Apr 28 16:08:55.402: INFO: metrics-server-v0.3.6-f6cf867b4-kjjbl from kube-system started at 2021-04-28 16:04:35 +0000 UTC (2 container statuses recorded)
Apr 28 16:08:55.402: INFO: 	Container metrics-server ready: true, restart count 0
Apr 28 16:08:55.402: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Apr 28 16:08:55.402: INFO: sample-webhook-deployment-6bd9446d55-mcgg9 from webhook-3012 started at 2021-04-28 16:08:49 +0000 UTC (1 container statuses recorded)
Apr 28 16:08:55.402: INFO: 	Container sample-webhook ready: true, restart count 0
Apr 28 16:08:55.402: INFO: to-be-attached-pod from webhook-3012 started at 2021-04-28 16:08:52 +0000 UTC (1 container statuses recorded)
Apr 28 16:08:55.402: INFO: 	Container container1 ready: true, restart count 0
Apr 28 16:08:55.402: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-30-112 before test
Apr 28 16:08:55.407: INFO: nginx-ingress-controller-kubernetes-worker-9plpg from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:47 +0000 UTC (1 container statuses recorded)
Apr 28 16:08:55.407: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Apr 28 16:08:55.407: INFO: kube-state-metrics-7f55b9fcd7-jjs4x from kube-system started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 16:08:55.407: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 28 16:08:55.407: INFO: dashboard-metrics-scraper-74757fb5b7-x9lrp from kubernetes-dashboard started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 16:08:55.407: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr 28 16:08:55.407: INFO: kubernetes-dashboard-64f87676d4-mshn4 from kubernetes-dashboard started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 16:08:55.407: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr 28 16:08:55.407: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-76-85 before test
Apr 28 16:08:55.414: INFO: default-http-backend-kubernetes-worker-6494cbc7fd-74bdv from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:58 +0000 UTC (1 container statuses recorded)
Apr 28 16:08:55.414: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
Apr 28 16:08:55.414: INFO: nginx-ingress-controller-kubernetes-worker-bjnb5 from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:49 +0000 UTC (1 container statuses recorded)
Apr 28 16:08:55.414: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Apr 28 16:08:55.414: INFO: coredns-7bb4d77796-mgrfg from kube-system started at 2021-04-28 16:04:35 +0000 UTC (1 container statuses recorded)
Apr 28 16:08:55.414: INFO: 	Container coredns ready: true, restart count 0
Apr 28 16:08:55.414: INFO: sonobuoy from sonobuoy started at 2021-04-28 15:27:11 +0000 UTC (1 container statuses recorded)
Apr 28 16:08:55.414: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 28 16:08:55.414: INFO: sonobuoy-e2e-job-c0dbab055ba045c5 from sonobuoy started at 2021-04-28 15:27:15 +0000 UTC (2 container statuses recorded)
Apr 28 16:08:55.414: INFO: 	Container e2e ready: true, restart count 0
Apr 28 16:08:55.414: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.167a116397abbb11], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.167a11639861b876], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:08:56.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9421" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":144,"skipped":2298,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:08:56.458: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7551
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Apr 28 16:08:56.631: INFO: Waiting up to 5m0s for pod "test-pod-28f7f427-ac3e-4a46-a299-79019a6dcd26" in namespace "svcaccounts-7551" to be "Succeeded or Failed"
Apr 28 16:08:56.637: INFO: Pod "test-pod-28f7f427-ac3e-4a46-a299-79019a6dcd26": Phase="Pending", Reason="", readiness=false. Elapsed: 5.551767ms
Apr 28 16:08:58.646: INFO: Pod "test-pod-28f7f427-ac3e-4a46-a299-79019a6dcd26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01462795s
STEP: Saw pod success
Apr 28 16:08:58.646: INFO: Pod "test-pod-28f7f427-ac3e-4a46-a299-79019a6dcd26" satisfied condition "Succeeded or Failed"
Apr 28 16:08:58.650: INFO: Trying to get logs from node ip-172-31-13-33 pod test-pod-28f7f427-ac3e-4a46-a299-79019a6dcd26 container agnhost-container: <nil>
STEP: delete the pod
Apr 28 16:08:58.676: INFO: Waiting for pod test-pod-28f7f427-ac3e-4a46-a299-79019a6dcd26 to disappear
Apr 28 16:08:58.682: INFO: Pod test-pod-28f7f427-ac3e-4a46-a299-79019a6dcd26 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:08:58.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7551" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":145,"skipped":2321,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:08:58.701: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2169
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:08:58.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2169" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":146,"skipped":2339,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:08:58.914: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2262
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-514ebd2a-43df-40db-a546-a199a90a345f
STEP: Creating a pod to test consume secrets
Apr 28 16:08:59.098: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-26485917-589c-42c2-ad3f-6fe994c1c10f" in namespace "projected-2262" to be "Succeeded or Failed"
Apr 28 16:08:59.129: INFO: Pod "pod-projected-secrets-26485917-589c-42c2-ad3f-6fe994c1c10f": Phase="Pending", Reason="", readiness=false. Elapsed: 30.854824ms
Apr 28 16:09:01.144: INFO: Pod "pod-projected-secrets-26485917-589c-42c2-ad3f-6fe994c1c10f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046374582s
STEP: Saw pod success
Apr 28 16:09:01.144: INFO: Pod "pod-projected-secrets-26485917-589c-42c2-ad3f-6fe994c1c10f" satisfied condition "Succeeded or Failed"
Apr 28 16:09:01.149: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-projected-secrets-26485917-589c-42c2-ad3f-6fe994c1c10f container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 28 16:09:01.179: INFO: Waiting for pod pod-projected-secrets-26485917-589c-42c2-ad3f-6fe994c1c10f to disappear
Apr 28 16:09:01.183: INFO: Pod pod-projected-secrets-26485917-589c-42c2-ad3f-6fe994c1c10f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:09:01.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2262" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":147,"skipped":2340,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:09:01.196: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9188
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-bc825593-be4f-4d35-90e3-4447a6dd306d
STEP: Creating a pod to test consume configMaps
Apr 28 16:09:01.411: INFO: Waiting up to 5m0s for pod "pod-configmaps-4cf64e76-f95b-4dc7-aed6-8b5a2096a62e" in namespace "configmap-9188" to be "Succeeded or Failed"
Apr 28 16:09:01.420: INFO: Pod "pod-configmaps-4cf64e76-f95b-4dc7-aed6-8b5a2096a62e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.816588ms
Apr 28 16:09:03.429: INFO: Pod "pod-configmaps-4cf64e76-f95b-4dc7-aed6-8b5a2096a62e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017643767s
STEP: Saw pod success
Apr 28 16:09:03.429: INFO: Pod "pod-configmaps-4cf64e76-f95b-4dc7-aed6-8b5a2096a62e" satisfied condition "Succeeded or Failed"
Apr 28 16:09:03.438: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-configmaps-4cf64e76-f95b-4dc7-aed6-8b5a2096a62e container agnhost-container: <nil>
STEP: delete the pod
Apr 28 16:09:03.462: INFO: Waiting for pod pod-configmaps-4cf64e76-f95b-4dc7-aed6-8b5a2096a62e to disappear
Apr 28 16:09:03.466: INFO: Pod pod-configmaps-4cf64e76-f95b-4dc7-aed6-8b5a2096a62e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:09:03.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9188" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":148,"skipped":2379,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:09:03.478: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2985
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2985.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2985.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2985.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2985.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2985.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2985.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2985.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2985.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2985.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2985.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2985.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2985.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2985.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2985.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2985.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2985.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2985.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2985.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 28 16:09:05.741: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2985.svc.cluster.local from pod dns-2985/dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15: the server could not find the requested resource (get pods dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15)
Apr 28 16:09:05.747: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2985.svc.cluster.local from pod dns-2985/dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15: the server could not find the requested resource (get pods dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15)
Apr 28 16:09:05.753: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2985.svc.cluster.local from pod dns-2985/dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15: the server could not find the requested resource (get pods dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15)
Apr 28 16:09:05.756: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2985.svc.cluster.local from pod dns-2985/dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15: the server could not find the requested resource (get pods dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15)
Apr 28 16:09:05.769: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2985.svc.cluster.local from pod dns-2985/dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15: the server could not find the requested resource (get pods dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15)
Apr 28 16:09:05.774: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2985.svc.cluster.local from pod dns-2985/dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15: the server could not find the requested resource (get pods dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15)
Apr 28 16:09:05.779: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2985.svc.cluster.local from pod dns-2985/dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15: the server could not find the requested resource (get pods dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15)
Apr 28 16:09:05.783: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2985.svc.cluster.local from pod dns-2985/dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15: the server could not find the requested resource (get pods dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15)
Apr 28 16:09:05.793: INFO: Lookups using dns-2985/dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2985.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2985.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2985.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2985.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2985.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2985.svc.cluster.local jessie_udp@dns-test-service-2.dns-2985.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2985.svc.cluster.local]

Apr 28 16:09:10.853: INFO: DNS probes using dns-2985/dns-test-d05a7fe5-d103-499e-a53e-d99a3ab0bc15 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:09:10.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2985" for this suite.

• [SLOW TEST:7.425 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":149,"skipped":2391,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:09:10.904: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-5599
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 28 16:09:11.104: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 28 16:10:11.124: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:10:11.127: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-1105
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Apr 28 16:10:13.368: INFO: found a healthy node: ip-172-31-13-33
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:10:21.487: INFO: pods created so far: [1 1 1]
Apr 28 16:10:21.487: INFO: length of pods created so far: 3
Apr 28 16:10:41.508: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:10:48.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1105" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:10:48.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5599" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:97.758 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":150,"skipped":2393,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:10:48.661: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6622
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-d1fd9b15-e62a-4eaf-b58d-4cecc7e31fee-6285
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:10:49.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6622" for this suite.
STEP: Destroying namespace "nspatchtest-d1fd9b15-e62a-4eaf-b58d-4cecc7e31fee-6285" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":151,"skipped":2404,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:10:49.043: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3706
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Apr 28 16:10:51.809: INFO: Successfully updated pod "annotationupdatebcf486f8-3cc4-4b1f-b590-7d1ab6d995bd"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:10:55.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3706" for this suite.

• [SLOW TEST:6.806 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":152,"skipped":2409,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:10:55.849: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8733
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-f529415f-fba5-4aac-8427-5afaf5c6836a
STEP: Creating a pod to test consume secrets
Apr 28 16:10:56.039: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f35fa2a9-f178-4977-989c-114f795ff690" in namespace "projected-8733" to be "Succeeded or Failed"
Apr 28 16:10:56.045: INFO: Pod "pod-projected-secrets-f35fa2a9-f178-4977-989c-114f795ff690": Phase="Pending", Reason="", readiness=false. Elapsed: 5.790103ms
Apr 28 16:10:58.054: INFO: Pod "pod-projected-secrets-f35fa2a9-f178-4977-989c-114f795ff690": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014370953s
STEP: Saw pod success
Apr 28 16:10:58.054: INFO: Pod "pod-projected-secrets-f35fa2a9-f178-4977-989c-114f795ff690" satisfied condition "Succeeded or Failed"
Apr 28 16:10:58.059: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-projected-secrets-f35fa2a9-f178-4977-989c-114f795ff690 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 28 16:10:58.092: INFO: Waiting for pod pod-projected-secrets-f35fa2a9-f178-4977-989c-114f795ff690 to disappear
Apr 28 16:10:58.097: INFO: Pod pod-projected-secrets-f35fa2a9-f178-4977-989c-114f795ff690 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:10:58.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8733" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":153,"skipped":2440,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:10:58.111: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-1414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 28 16:10:58.356: INFO: starting watch
STEP: patching
STEP: updating
Apr 28 16:10:58.371: INFO: waiting for watch events with expected annotations
Apr 28 16:10:58.371: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:10:58.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-1414" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":154,"skipped":2453,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:10:58.434: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-365
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:11:58.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-365" for this suite.

• [SLOW TEST:60.193 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":155,"skipped":2455,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:11:58.626: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1890
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Apr 28 16:11:58.823: INFO: Pod name pod-release: Found 0 pods out of 1
Apr 28 16:12:03.832: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:12:04.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1890" for this suite.

• [SLOW TEST:6.261 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":156,"skipped":2478,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:12:04.888: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2456
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-a5c8d9cd-792b-41ce-8f1e-09cf9fb98fb0 in namespace container-probe-2456
Apr 28 16:12:07.138: INFO: Started pod test-webserver-a5c8d9cd-792b-41ce-8f1e-09cf9fb98fb0 in namespace container-probe-2456
STEP: checking the pod's current state and verifying that restartCount is present
Apr 28 16:12:07.144: INFO: Initial restart count of pod test-webserver-a5c8d9cd-792b-41ce-8f1e-09cf9fb98fb0 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:16:08.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2456" for this suite.

• [SLOW TEST:243.378 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":157,"skipped":2481,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:16:08.266: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-791
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-791
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-791
STEP: Deleting pre-stop pod
Apr 28 16:16:17.576: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:16:17.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-791" for this suite.

• [SLOW TEST:9.395 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":158,"skipped":2483,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:16:17.660: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6693
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-54389a16-587b-44d4-8f8e-3b65e466479b
STEP: Creating a pod to test consume configMaps
Apr 28 16:16:17.857: INFO: Waiting up to 5m0s for pod "pod-configmaps-7833231b-3895-488e-b2be-3ad68f68c2ff" in namespace "configmap-6693" to be "Succeeded or Failed"
Apr 28 16:16:17.864: INFO: Pod "pod-configmaps-7833231b-3895-488e-b2be-3ad68f68c2ff": Phase="Pending", Reason="", readiness=false. Elapsed: 7.732494ms
Apr 28 16:16:19.871: INFO: Pod "pod-configmaps-7833231b-3895-488e-b2be-3ad68f68c2ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014608537s
STEP: Saw pod success
Apr 28 16:16:19.871: INFO: Pod "pod-configmaps-7833231b-3895-488e-b2be-3ad68f68c2ff" satisfied condition "Succeeded or Failed"
Apr 28 16:16:19.876: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-configmaps-7833231b-3895-488e-b2be-3ad68f68c2ff container agnhost-container: <nil>
STEP: delete the pod
Apr 28 16:16:19.919: INFO: Waiting for pod pod-configmaps-7833231b-3895-488e-b2be-3ad68f68c2ff to disappear
Apr 28 16:16:19.924: INFO: Pod pod-configmaps-7833231b-3895-488e-b2be-3ad68f68c2ff no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:16:19.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6693" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":159,"skipped":2486,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:16:19.944: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9286
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-3ae6020a-182c-4e65-a761-aad11b70e234 in namespace container-probe-9286
Apr 28 16:16:24.151: INFO: Started pod busybox-3ae6020a-182c-4e65-a761-aad11b70e234 in namespace container-probe-9286
STEP: checking the pod's current state and verifying that restartCount is present
Apr 28 16:16:24.157: INFO: Initial restart count of pod busybox-3ae6020a-182c-4e65-a761-aad11b70e234 is 0
Apr 28 16:17:08.354: INFO: Restart count of pod container-probe-9286/busybox-3ae6020a-182c-4e65-a761-aad11b70e234 is now 1 (44.197031662s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:17:08.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9286" for this suite.

• [SLOW TEST:48.444 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":160,"skipped":2498,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:17:08.389: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1940
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:17:08.551: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr 28 16:17:08.566: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 28 16:17:13.574: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 28 16:17:13.574: INFO: Creating deployment "test-rolling-update-deployment"
Apr 28 16:17:13.585: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr 28 16:17:13.601: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr 28 16:17:15.614: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr 28 16:17:15.626: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 28 16:17:15.645: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1940  965d397b-c328-45d3-969a-98009c41c1ef 20096 1 2021-04-28 16:17:13 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-04-28 16:17:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-28 16:17:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004754d78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-04-28 16:17:13 +0000 UTC,LastTransitionTime:2021-04-28 16:17:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-04-28 16:17:14 +0000 UTC,LastTransitionTime:2021-04-28 16:17:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 28 16:17:15.654: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-1940  d861dace-cc06-4009-820f-e885a5b5870c 20085 1 2021-04-28 16:17:13 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 965d397b-c328-45d3-969a-98009c41c1ef 0xc00349f487 0xc00349f488}] []  [{kube-controller-manager Update apps/v1 2021-04-28 16:17:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"965d397b-c328-45d3-969a-98009c41c1ef\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00349f518 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 28 16:17:15.654: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr 28 16:17:15.654: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1940  98d98156-7cec-40ee-8873-f6e25c999719 20095 2 2021-04-28 16:17:08 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 965d397b-c328-45d3-969a-98009c41c1ef 0xc00349f377 0xc00349f378}] []  [{e2e.test Update apps/v1 2021-04-28 16:17:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-28 16:17:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"965d397b-c328-45d3-969a-98009c41c1ef\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00349f418 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 28 16:17:15.666: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-dswzq" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-dswzq test-rolling-update-deployment-6b6bf9df46- deployment-1940  a334d68c-e43b-4a86-947f-6dac61dfce60 20084 0 2021-04-28 16:17:13 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 d861dace-cc06-4009-820f-e885a5b5870c 0xc004755127 0xc004755128}] []  [{kube-controller-manager Update v1 2021-04-28 16:17:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d861dace-cc06-4009-820f-e885a5b5870c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:17:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.62.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-b9dw2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-b9dw2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-b9dw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:17:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:17:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:17:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:17:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.33,PodIP:10.1.62.162,StartTime:2021-04-28 16:17:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-28 16:17:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://038c8ce08e891da79c45812f542167682a2d0f86dd316c1814446533d3b09cd5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.62.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:17:15.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1940" for this suite.

• [SLOW TEST:7.298 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":161,"skipped":2508,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:17:15.687: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-365
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:17:17.868: INFO: Deleting pod "var-expansion-a5a4286d-8b3f-4bb6-b669-ef1c51fd1f3a" in namespace "var-expansion-365"
Apr 28 16:17:17.878: INFO: Wait up to 5m0s for pod "var-expansion-a5a4286d-8b3f-4bb6-b669-ef1c51fd1f3a" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:17:19.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-365" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":162,"skipped":2516,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:17:19.905: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7294
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 28 16:17:20.548: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 16:17:23.583: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:17:23.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7294" for this suite.
STEP: Destroying namespace "webhook-7294-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":163,"skipped":2523,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:17:23.720: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-133
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:17:23.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-133" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":164,"skipped":2534,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:17:23.951: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8329
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 28 16:17:24.128: INFO: Waiting up to 5m0s for pod "pod-e0af794c-a7c1-41b2-8b0f-f1a9b40364ae" in namespace "emptydir-8329" to be "Succeeded or Failed"
Apr 28 16:17:24.132: INFO: Pod "pod-e0af794c-a7c1-41b2-8b0f-f1a9b40364ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.755764ms
Apr 28 16:17:26.138: INFO: Pod "pod-e0af794c-a7c1-41b2-8b0f-f1a9b40364ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010157211s
STEP: Saw pod success
Apr 28 16:17:26.138: INFO: Pod "pod-e0af794c-a7c1-41b2-8b0f-f1a9b40364ae" satisfied condition "Succeeded or Failed"
Apr 28 16:17:26.143: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-e0af794c-a7c1-41b2-8b0f-f1a9b40364ae container test-container: <nil>
STEP: delete the pod
Apr 28 16:17:26.170: INFO: Waiting for pod pod-e0af794c-a7c1-41b2-8b0f-f1a9b40364ae to disappear
Apr 28 16:17:26.175: INFO: Pod pod-e0af794c-a7c1-41b2-8b0f-f1a9b40364ae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:17:26.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8329" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":165,"skipped":2580,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:17:26.188: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7799
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Apr 28 16:17:26.366: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7799  75842302-d47d-4690-96e4-4de5a1f5eae0 20259 0 2021-04-28 16:17:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-28 16:17:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 28 16:17:26.366: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7799  75842302-d47d-4690-96e4-4de5a1f5eae0 20259 0 2021-04-28 16:17:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-28 16:17:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Apr 28 16:17:36.382: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7799  75842302-d47d-4690-96e4-4de5a1f5eae0 20312 0 2021-04-28 16:17:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-28 16:17:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 28 16:17:36.382: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7799  75842302-d47d-4690-96e4-4de5a1f5eae0 20312 0 2021-04-28 16:17:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-28 16:17:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Apr 28 16:17:46.402: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7799  75842302-d47d-4690-96e4-4de5a1f5eae0 20329 0 2021-04-28 16:17:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-28 16:17:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 28 16:17:46.402: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7799  75842302-d47d-4690-96e4-4de5a1f5eae0 20329 0 2021-04-28 16:17:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-28 16:17:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Apr 28 16:17:56.414: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7799  75842302-d47d-4690-96e4-4de5a1f5eae0 20347 0 2021-04-28 16:17:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-28 16:17:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 28 16:17:56.414: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7799  75842302-d47d-4690-96e4-4de5a1f5eae0 20347 0 2021-04-28 16:17:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-28 16:17:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Apr 28 16:18:06.450: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7799  fdeedaa4-c78d-4202-a3bd-0ca39c4452ed 20365 0 2021-04-28 16:18:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-28 16:18:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 28 16:18:06.450: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7799  fdeedaa4-c78d-4202-a3bd-0ca39c4452ed 20365 0 2021-04-28 16:18:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-28 16:18:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Apr 28 16:18:16.476: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7799  fdeedaa4-c78d-4202-a3bd-0ca39c4452ed 20384 0 2021-04-28 16:18:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-28 16:18:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 28 16:18:16.476: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7799  fdeedaa4-c78d-4202-a3bd-0ca39c4452ed 20384 0 2021-04-28 16:18:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-28 16:18:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:18:26.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7799" for this suite.

• [SLOW TEST:60.304 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":166,"skipped":2595,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:18:26.492: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-7120
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:18:28.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7120" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":167,"skipped":2598,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:18:28.785: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3249
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:18:28.942: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 28 16:18:32.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-3249 --namespace=crd-publish-openapi-3249 create -f -'
Apr 28 16:18:32.713: INFO: stderr: ""
Apr 28 16:18:32.713: INFO: stdout: "e2e-test-crd-publish-openapi-5921-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 28 16:18:32.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-3249 --namespace=crd-publish-openapi-3249 delete e2e-test-crd-publish-openapi-5921-crds test-cr'
Apr 28 16:18:32.780: INFO: stderr: ""
Apr 28 16:18:32.780: INFO: stdout: "e2e-test-crd-publish-openapi-5921-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Apr 28 16:18:32.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-3249 --namespace=crd-publish-openapi-3249 apply -f -'
Apr 28 16:18:33.036: INFO: stderr: ""
Apr 28 16:18:33.036: INFO: stdout: "e2e-test-crd-publish-openapi-5921-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 28 16:18:33.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-3249 --namespace=crd-publish-openapi-3249 delete e2e-test-crd-publish-openapi-5921-crds test-cr'
Apr 28 16:18:33.104: INFO: stderr: ""
Apr 28 16:18:33.104: INFO: stdout: "e2e-test-crd-publish-openapi-5921-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Apr 28 16:18:33.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-3249 explain e2e-test-crd-publish-openapi-5921-crds'
Apr 28 16:18:33.243: INFO: stderr: ""
Apr 28 16:18:33.243: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5921-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:18:35.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3249" for this suite.

• [SLOW TEST:7.185 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":168,"skipped":2608,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:18:35.971: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1203
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-1203
STEP: creating replication controller nodeport-test in namespace services-1203
I0428 16:18:36.167921      24 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-1203, replica count: 2
I0428 16:18:39.218179      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 28 16:18:39.218: INFO: Creating new exec pod
Apr 28 16:18:42.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-1203 exec execpodf4qs8 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Apr 28 16:18:42.428: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Apr 28 16:18:42.428: INFO: stdout: ""
Apr 28 16:18:42.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-1203 exec execpodf4qs8 -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.98 80'
Apr 28 16:18:42.559: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.98 80\nConnection to 10.152.183.98 80 port [tcp/http] succeeded!\n"
Apr 28 16:18:42.559: INFO: stdout: ""
Apr 28 16:18:42.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-1203 exec execpodf4qs8 -- /bin/sh -x -c nc -zv -t -w 2 172.31.30.112 32678'
Apr 28 16:18:42.684: INFO: stderr: "+ nc -zv -t -w 2 172.31.30.112 32678\nConnection to 172.31.30.112 32678 port [tcp/32678] succeeded!\n"
Apr 28 16:18:42.684: INFO: stdout: ""
Apr 28 16:18:42.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-1203 exec execpodf4qs8 -- /bin/sh -x -c nc -zv -t -w 2 172.31.13.33 32678'
Apr 28 16:18:42.809: INFO: stderr: "+ nc -zv -t -w 2 172.31.13.33 32678\nConnection to 172.31.13.33 32678 port [tcp/32678] succeeded!\n"
Apr 28 16:18:42.809: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:18:42.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1203" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.851 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":169,"skipped":2665,"failed":0}
SSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:18:42.822: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-8316
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Apr 28 16:18:42.985: INFO: created test-podtemplate-1
Apr 28 16:18:42.997: INFO: created test-podtemplate-2
Apr 28 16:18:43.006: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Apr 28 16:18:43.010: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Apr 28 16:18:43.045: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:18:43.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8316" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":170,"skipped":2670,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:18:43.062: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2261
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 28 16:18:43.582: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 16:18:46.623: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Apr 28 16:18:46.647: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:18:46.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2261" for this suite.
STEP: Destroying namespace "webhook-2261-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":171,"skipped":2700,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:18:46.784: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2426
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:18:46.963: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-83febff4-b303-4a5e-b03b-05bc7d5492d2" in namespace "security-context-test-2426" to be "Succeeded or Failed"
Apr 28 16:18:46.975: INFO: Pod "busybox-readonly-false-83febff4-b303-4a5e-b03b-05bc7d5492d2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.381442ms
Apr 28 16:18:48.980: INFO: Pod "busybox-readonly-false-83febff4-b303-4a5e-b03b-05bc7d5492d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016860249s
Apr 28 16:18:48.980: INFO: Pod "busybox-readonly-false-83febff4-b303-4a5e-b03b-05bc7d5492d2" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:18:48.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2426" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":172,"skipped":2743,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:18:48.998: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6800
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 28 16:18:49.174: INFO: Waiting up to 5m0s for pod "pod-236007e1-2ea0-42a0-8f61-f79072a31ead" in namespace "emptydir-6800" to be "Succeeded or Failed"
Apr 28 16:18:49.196: INFO: Pod "pod-236007e1-2ea0-42a0-8f61-f79072a31ead": Phase="Pending", Reason="", readiness=false. Elapsed: 22.206019ms
Apr 28 16:18:51.211: INFO: Pod "pod-236007e1-2ea0-42a0-8f61-f79072a31ead": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036661409s
STEP: Saw pod success
Apr 28 16:18:51.211: INFO: Pod "pod-236007e1-2ea0-42a0-8f61-f79072a31ead" satisfied condition "Succeeded or Failed"
Apr 28 16:18:51.216: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-236007e1-2ea0-42a0-8f61-f79072a31ead container test-container: <nil>
STEP: delete the pod
Apr 28 16:18:51.246: INFO: Waiting for pod pod-236007e1-2ea0-42a0-8f61-f79072a31ead to disappear
Apr 28 16:18:51.252: INFO: Pod pod-236007e1-2ea0-42a0-8f61-f79072a31ead no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:18:51.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6800" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":173,"skipped":2753,"failed":0}
SSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:18:51.268: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3454
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:18:51.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3454" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":174,"skipped":2756,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:18:51.491: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-5728
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Apr 28 16:18:51.658: INFO: Major version: 1
STEP: Confirm minor version
Apr 28 16:18:51.658: INFO: cleanMinorVersion: 20
Apr 28 16:18:51.658: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:18:51.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-5728" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":175,"skipped":2825,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:18:51.673: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4907
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 28 16:18:53.965: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:18:54.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4907" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":176,"skipped":2829,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:18:54.072: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2300
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-nfm85 in namespace proxy-2300
I0428 16:18:54.308272      24 runners.go:190] Created replication controller with name: proxy-service-nfm85, namespace: proxy-2300, replica count: 1
I0428 16:18:55.358511      24 runners.go:190] proxy-service-nfm85 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0428 16:18:56.358642      24 runners.go:190] proxy-service-nfm85 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0428 16:18:57.358785      24 runners.go:190] proxy-service-nfm85 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0428 16:18:58.358931      24 runners.go:190] proxy-service-nfm85 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0428 16:18:59.359072      24 runners.go:190] proxy-service-nfm85 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0428 16:19:00.359208      24 runners.go:190] proxy-service-nfm85 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0428 16:19:01.359330      24 runners.go:190] proxy-service-nfm85 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0428 16:19:02.359452      24 runners.go:190] proxy-service-nfm85 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0428 16:19:03.359582      24 runners.go:190] proxy-service-nfm85 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0428 16:19:04.359718      24 runners.go:190] proxy-service-nfm85 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0428 16:19:05.359860      24 runners.go:190] proxy-service-nfm85 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 28 16:19:05.376: INFO: setup took 11.113190931s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Apr 28 16:19:05.416: INFO: (0) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 39.543615ms)
Apr 28 16:19:05.416: INFO: (0) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 39.857658ms)
Apr 28 16:19:05.416: INFO: (0) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 39.850905ms)
Apr 28 16:19:05.421: INFO: (0) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 45.483088ms)
Apr 28 16:19:05.427: INFO: (0) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 50.763905ms)
Apr 28 16:19:05.427: INFO: (0) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 51.288231ms)
Apr 28 16:19:05.427: INFO: (0) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 51.598817ms)
Apr 28 16:19:05.439: INFO: (0) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 63.3591ms)
Apr 28 16:19:05.440: INFO: (0) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 63.576929ms)
Apr 28 16:19:05.440: INFO: (0) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 63.717844ms)
Apr 28 16:19:05.440: INFO: (0) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 63.812856ms)
Apr 28 16:19:05.441: INFO: (0) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 65.528353ms)
Apr 28 16:19:05.441: INFO: (0) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 65.418055ms)
Apr 28 16:19:05.443: INFO: (0) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 67.156836ms)
Apr 28 16:19:05.444: INFO: (0) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 67.869696ms)
Apr 28 16:19:05.444: INFO: (0) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 67.795331ms)
Apr 28 16:19:05.471: INFO: (1) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 27.02819ms)
Apr 28 16:19:05.474: INFO: (1) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 30.35477ms)
Apr 28 16:19:05.474: INFO: (1) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 30.593601ms)
Apr 28 16:19:05.475: INFO: (1) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 31.151737ms)
Apr 28 16:19:05.475: INFO: (1) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 31.065565ms)
Apr 28 16:19:05.483: INFO: (1) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 39.136423ms)
Apr 28 16:19:05.483: INFO: (1) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 39.324906ms)
Apr 28 16:19:05.486: INFO: (1) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 42.359846ms)
Apr 28 16:19:05.487: INFO: (1) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 42.579556ms)
Apr 28 16:19:05.487: INFO: (1) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 42.720797ms)
Apr 28 16:19:05.487: INFO: (1) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 42.649619ms)
Apr 28 16:19:05.493: INFO: (1) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 49.33589ms)
Apr 28 16:19:05.494: INFO: (1) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 50.036304ms)
Apr 28 16:19:05.494: INFO: (1) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 50.052689ms)
Apr 28 16:19:05.494: INFO: (1) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 50.131163ms)
Apr 28 16:19:05.494: INFO: (1) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 50.071672ms)
Apr 28 16:19:05.516: INFO: (2) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 21.856533ms)
Apr 28 16:19:05.516: INFO: (2) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 21.883683ms)
Apr 28 16:19:05.525: INFO: (2) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 31.082912ms)
Apr 28 16:19:05.525: INFO: (2) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 30.85425ms)
Apr 28 16:19:05.526: INFO: (2) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 31.210044ms)
Apr 28 16:19:05.526: INFO: (2) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 31.306232ms)
Apr 28 16:19:05.526: INFO: (2) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 31.266344ms)
Apr 28 16:19:05.526: INFO: (2) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 31.551208ms)
Apr 28 16:19:05.526: INFO: (2) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 31.37199ms)
Apr 28 16:19:05.526: INFO: (2) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 31.494892ms)
Apr 28 16:19:05.526: INFO: (2) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 31.243087ms)
Apr 28 16:19:05.526: INFO: (2) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 31.537379ms)
Apr 28 16:19:05.526: INFO: (2) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 31.329007ms)
Apr 28 16:19:05.526: INFO: (2) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 31.897911ms)
Apr 28 16:19:05.530: INFO: (2) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 35.819131ms)
Apr 28 16:19:05.531: INFO: (2) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 36.165226ms)
Apr 28 16:19:05.547: INFO: (3) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 15.789133ms)
Apr 28 16:19:05.550: INFO: (3) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 18.798563ms)
Apr 28 16:19:05.550: INFO: (3) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 18.868777ms)
Apr 28 16:19:05.551: INFO: (3) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 19.910292ms)
Apr 28 16:19:05.551: INFO: (3) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 20.335353ms)
Apr 28 16:19:05.553: INFO: (3) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 21.799195ms)
Apr 28 16:19:05.553: INFO: (3) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 22.301138ms)
Apr 28 16:19:05.556: INFO: (3) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 25.231794ms)
Apr 28 16:19:05.556: INFO: (3) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 25.452875ms)
Apr 28 16:19:05.556: INFO: (3) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 25.53854ms)
Apr 28 16:19:05.557: INFO: (3) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 25.824214ms)
Apr 28 16:19:05.557: INFO: (3) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 26.158308ms)
Apr 28 16:19:05.557: INFO: (3) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 26.050331ms)
Apr 28 16:19:05.557: INFO: (3) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 26.204853ms)
Apr 28 16:19:05.558: INFO: (3) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 26.839251ms)
Apr 28 16:19:05.558: INFO: (3) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 27.319982ms)
Apr 28 16:19:05.565: INFO: (4) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 7.069353ms)
Apr 28 16:19:05.566: INFO: (4) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 7.5665ms)
Apr 28 16:19:05.567: INFO: (4) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 8.196086ms)
Apr 28 16:19:05.567: INFO: (4) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 8.152635ms)
Apr 28 16:19:05.567: INFO: (4) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 8.94273ms)
Apr 28 16:19:05.567: INFO: (4) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 9.01368ms)
Apr 28 16:19:05.568: INFO: (4) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 9.639741ms)
Apr 28 16:19:05.569: INFO: (4) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 10.20283ms)
Apr 28 16:19:05.569: INFO: (4) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 10.302122ms)
Apr 28 16:19:05.570: INFO: (4) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 11.97323ms)
Apr 28 16:19:05.571: INFO: (4) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 12.060941ms)
Apr 28 16:19:05.571: INFO: (4) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 12.17022ms)
Apr 28 16:19:05.571: INFO: (4) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 12.443416ms)
Apr 28 16:19:05.571: INFO: (4) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 12.594622ms)
Apr 28 16:19:05.573: INFO: (4) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 14.268743ms)
Apr 28 16:19:05.573: INFO: (4) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 14.666786ms)
Apr 28 16:19:05.579: INFO: (5) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 6.089004ms)
Apr 28 16:19:05.580: INFO: (5) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 7.015224ms)
Apr 28 16:19:05.580: INFO: (5) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 7.113244ms)
Apr 28 16:19:05.581: INFO: (5) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 8.102123ms)
Apr 28 16:19:05.582: INFO: (5) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 8.608957ms)
Apr 28 16:19:05.585: INFO: (5) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 11.364995ms)
Apr 28 16:19:05.588: INFO: (5) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 14.503919ms)
Apr 28 16:19:05.588: INFO: (5) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 14.882964ms)
Apr 28 16:19:05.588: INFO: (5) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 15.084363ms)
Apr 28 16:19:05.589: INFO: (5) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 15.283069ms)
Apr 28 16:19:05.589: INFO: (5) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 15.552939ms)
Apr 28 16:19:05.589: INFO: (5) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 15.834174ms)
Apr 28 16:19:05.589: INFO: (5) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 16.176694ms)
Apr 28 16:19:05.590: INFO: (5) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 16.434688ms)
Apr 28 16:19:05.590: INFO: (5) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 16.44156ms)
Apr 28 16:19:05.590: INFO: (5) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 17.026266ms)
Apr 28 16:19:05.608: INFO: (6) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 18.01366ms)
Apr 28 16:19:05.609: INFO: (6) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 17.979524ms)
Apr 28 16:19:05.609: INFO: (6) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 18.477382ms)
Apr 28 16:19:05.609: INFO: (6) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 18.615254ms)
Apr 28 16:19:05.609: INFO: (6) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 18.905593ms)
Apr 28 16:19:05.610: INFO: (6) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 18.906112ms)
Apr 28 16:19:05.610: INFO: (6) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 19.18636ms)
Apr 28 16:19:05.610: INFO: (6) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 19.662964ms)
Apr 28 16:19:05.610: INFO: (6) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 19.68007ms)
Apr 28 16:19:05.610: INFO: (6) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 19.732619ms)
Apr 28 16:19:05.611: INFO: (6) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 20.282554ms)
Apr 28 16:19:05.611: INFO: (6) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 20.361182ms)
Apr 28 16:19:05.611: INFO: (6) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 20.436566ms)
Apr 28 16:19:05.611: INFO: (6) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 20.661487ms)
Apr 28 16:19:05.612: INFO: (6) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 21.13261ms)
Apr 28 16:19:05.612: INFO: (6) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 20.971699ms)
Apr 28 16:19:05.621: INFO: (7) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 9.859109ms)
Apr 28 16:19:05.621: INFO: (7) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 9.925248ms)
Apr 28 16:19:05.626: INFO: (7) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 14.786661ms)
Apr 28 16:19:05.627: INFO: (7) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 14.964307ms)
Apr 28 16:19:05.627: INFO: (7) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 15.21363ms)
Apr 28 16:19:05.627: INFO: (7) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 15.405989ms)
Apr 28 16:19:05.627: INFO: (7) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 15.768619ms)
Apr 28 16:19:05.628: INFO: (7) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 15.942242ms)
Apr 28 16:19:05.628: INFO: (7) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 16.031421ms)
Apr 28 16:19:05.628: INFO: (7) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 16.388171ms)
Apr 28 16:19:05.628: INFO: (7) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 16.501017ms)
Apr 28 16:19:05.634: INFO: (7) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 21.99694ms)
Apr 28 16:19:05.636: INFO: (7) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 24.350983ms)
Apr 28 16:19:05.636: INFO: (7) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 24.303055ms)
Apr 28 16:19:05.636: INFO: (7) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 24.393593ms)
Apr 28 16:19:05.645: INFO: (7) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 33.354322ms)
Apr 28 16:19:05.654: INFO: (8) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 8.404636ms)
Apr 28 16:19:05.654: INFO: (8) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 8.476708ms)
Apr 28 16:19:05.654: INFO: (8) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 8.587142ms)
Apr 28 16:19:05.656: INFO: (8) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 10.089418ms)
Apr 28 16:19:05.656: INFO: (8) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 10.356288ms)
Apr 28 16:19:05.657: INFO: (8) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 11.083938ms)
Apr 28 16:19:05.657: INFO: (8) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 11.335811ms)
Apr 28 16:19:05.657: INFO: (8) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 11.72855ms)
Apr 28 16:19:05.657: INFO: (8) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 11.822373ms)
Apr 28 16:19:05.657: INFO: (8) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 11.81633ms)
Apr 28 16:19:05.658: INFO: (8) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 12.695691ms)
Apr 28 16:19:05.658: INFO: (8) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 12.90392ms)
Apr 28 16:19:05.659: INFO: (8) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 13.219028ms)
Apr 28 16:19:05.659: INFO: (8) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 13.501248ms)
Apr 28 16:19:05.660: INFO: (8) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 14.912969ms)
Apr 28 16:19:05.660: INFO: (8) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 14.932859ms)
Apr 28 16:19:05.670: INFO: (9) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 9.118705ms)
Apr 28 16:19:05.671: INFO: (9) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 10.039189ms)
Apr 28 16:19:05.672: INFO: (9) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 11.38576ms)
Apr 28 16:19:05.675: INFO: (9) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 14.557841ms)
Apr 28 16:19:05.675: INFO: (9) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 14.682214ms)
Apr 28 16:19:05.677: INFO: (9) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 16.73197ms)
Apr 28 16:19:05.677: INFO: (9) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 16.950338ms)
Apr 28 16:19:05.677: INFO: (9) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 16.80561ms)
Apr 28 16:19:05.677: INFO: (9) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 16.870706ms)
Apr 28 16:19:05.678: INFO: (9) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 17.67532ms)
Apr 28 16:19:05.678: INFO: (9) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 17.906625ms)
Apr 28 16:19:05.679: INFO: (9) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 18.339704ms)
Apr 28 16:19:05.679: INFO: (9) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 18.743961ms)
Apr 28 16:19:05.681: INFO: (9) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 20.204827ms)
Apr 28 16:19:05.681: INFO: (9) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 20.257593ms)
Apr 28 16:19:05.681: INFO: (9) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 20.653535ms)
Apr 28 16:19:05.689: INFO: (10) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 7.179643ms)
Apr 28 16:19:05.689: INFO: (10) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 7.991655ms)
Apr 28 16:19:05.691: INFO: (10) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 9.198327ms)
Apr 28 16:19:05.691: INFO: (10) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 9.414487ms)
Apr 28 16:19:05.693: INFO: (10) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 11.43198ms)
Apr 28 16:19:05.693: INFO: (10) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 11.27581ms)
Apr 28 16:19:05.694: INFO: (10) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 12.40187ms)
Apr 28 16:19:05.694: INFO: (10) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 12.339753ms)
Apr 28 16:19:05.695: INFO: (10) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 13.390739ms)
Apr 28 16:19:05.695: INFO: (10) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 13.362559ms)
Apr 28 16:19:05.696: INFO: (10) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 14.644289ms)
Apr 28 16:19:05.697: INFO: (10) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 15.635816ms)
Apr 28 16:19:05.697: INFO: (10) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 15.686752ms)
Apr 28 16:19:05.697: INFO: (10) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 15.706521ms)
Apr 28 16:19:05.698: INFO: (10) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 16.224523ms)
Apr 28 16:19:05.698: INFO: (10) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 16.399508ms)
Apr 28 16:19:05.708: INFO: (11) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 9.785499ms)
Apr 28 16:19:05.709: INFO: (11) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 10.682086ms)
Apr 28 16:19:05.709: INFO: (11) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 11.30396ms)
Apr 28 16:19:05.710: INFO: (11) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 11.825493ms)
Apr 28 16:19:05.711: INFO: (11) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 12.438914ms)
Apr 28 16:19:05.712: INFO: (11) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 14.01971ms)
Apr 28 16:19:05.712: INFO: (11) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 13.996287ms)
Apr 28 16:19:05.712: INFO: (11) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 14.316774ms)
Apr 28 16:19:05.713: INFO: (11) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 14.786149ms)
Apr 28 16:19:05.713: INFO: (11) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 15.323977ms)
Apr 28 16:19:05.713: INFO: (11) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 15.463731ms)
Apr 28 16:19:05.713: INFO: (11) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 15.290348ms)
Apr 28 16:19:05.714: INFO: (11) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 15.938435ms)
Apr 28 16:19:05.714: INFO: (11) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 16.322575ms)
Apr 28 16:19:05.715: INFO: (11) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 16.523401ms)
Apr 28 16:19:05.717: INFO: (11) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 19.452478ms)
Apr 28 16:19:05.729: INFO: (12) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 11.235409ms)
Apr 28 16:19:05.729: INFO: (12) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 11.134882ms)
Apr 28 16:19:05.729: INFO: (12) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 11.073866ms)
Apr 28 16:19:05.729: INFO: (12) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 11.437332ms)
Apr 28 16:19:05.736: INFO: (12) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 18.713394ms)
Apr 28 16:19:05.737: INFO: (12) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 19.020929ms)
Apr 28 16:19:05.737: INFO: (12) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 19.05953ms)
Apr 28 16:19:05.737: INFO: (12) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 19.473305ms)
Apr 28 16:19:05.737: INFO: (12) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 19.630949ms)
Apr 28 16:19:05.738: INFO: (12) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 19.985663ms)
Apr 28 16:19:05.738: INFO: (12) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 20.174477ms)
Apr 28 16:19:05.738: INFO: (12) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 20.423615ms)
Apr 28 16:19:05.739: INFO: (12) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 21.449624ms)
Apr 28 16:19:05.740: INFO: (12) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 22.143862ms)
Apr 28 16:19:05.740: INFO: (12) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 22.374937ms)
Apr 28 16:19:05.740: INFO: (12) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 22.824858ms)
Apr 28 16:19:05.756: INFO: (13) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 15.559387ms)
Apr 28 16:19:05.756: INFO: (13) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 15.697044ms)
Apr 28 16:19:05.758: INFO: (13) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 17.327528ms)
Apr 28 16:19:05.758: INFO: (13) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 17.399105ms)
Apr 28 16:19:05.764: INFO: (13) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 23.668072ms)
Apr 28 16:19:05.764: INFO: (13) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 23.686412ms)
Apr 28 16:19:05.764: INFO: (13) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 24.013728ms)
Apr 28 16:19:05.765: INFO: (13) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 24.277513ms)
Apr 28 16:19:05.765: INFO: (13) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 24.549063ms)
Apr 28 16:19:05.765: INFO: (13) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 24.776089ms)
Apr 28 16:19:05.766: INFO: (13) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 25.049145ms)
Apr 28 16:19:05.766: INFO: (13) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 25.253162ms)
Apr 28 16:19:05.766: INFO: (13) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 25.429002ms)
Apr 28 16:19:05.766: INFO: (13) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 25.991343ms)
Apr 28 16:19:05.767: INFO: (13) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 26.265193ms)
Apr 28 16:19:05.767: INFO: (13) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 26.367543ms)
Apr 28 16:19:05.774: INFO: (14) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 7.32614ms)
Apr 28 16:19:05.775: INFO: (14) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 8.454005ms)
Apr 28 16:19:05.776: INFO: (14) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 8.745623ms)
Apr 28 16:19:05.777: INFO: (14) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 9.643147ms)
Apr 28 16:19:05.778: INFO: (14) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 11.328553ms)
Apr 28 16:19:05.778: INFO: (14) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 11.504808ms)
Apr 28 16:19:05.778: INFO: (14) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 11.355889ms)
Apr 28 16:19:05.778: INFO: (14) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 11.375824ms)
Apr 28 16:19:05.778: INFO: (14) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 11.446934ms)
Apr 28 16:19:05.779: INFO: (14) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 12.148074ms)
Apr 28 16:19:05.780: INFO: (14) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 12.791038ms)
Apr 28 16:19:05.782: INFO: (14) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 15.194757ms)
Apr 28 16:19:05.782: INFO: (14) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 15.31574ms)
Apr 28 16:19:05.784: INFO: (14) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 16.530121ms)
Apr 28 16:19:05.784: INFO: (14) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 17.037163ms)
Apr 28 16:19:05.784: INFO: (14) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 17.147217ms)
Apr 28 16:19:05.791: INFO: (15) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 6.780851ms)
Apr 28 16:19:05.798: INFO: (15) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 13.470069ms)
Apr 28 16:19:05.798: INFO: (15) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 13.545403ms)
Apr 28 16:19:05.798: INFO: (15) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 13.917167ms)
Apr 28 16:19:05.798: INFO: (15) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 13.922734ms)
Apr 28 16:19:05.799: INFO: (15) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 14.581676ms)
Apr 28 16:19:05.799: INFO: (15) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 14.955442ms)
Apr 28 16:19:05.800: INFO: (15) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 15.786563ms)
Apr 28 16:19:05.802: INFO: (15) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 18.028648ms)
Apr 28 16:19:05.816: INFO: (15) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 31.734419ms)
Apr 28 16:19:05.816: INFO: (15) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 31.891482ms)
Apr 28 16:19:05.817: INFO: (15) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 32.57683ms)
Apr 28 16:19:05.818: INFO: (15) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 33.595208ms)
Apr 28 16:19:05.818: INFO: (15) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 33.674927ms)
Apr 28 16:19:05.818: INFO: (15) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 33.644419ms)
Apr 28 16:19:05.818: INFO: (15) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 33.841705ms)
Apr 28 16:19:05.825: INFO: (16) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 6.418061ms)
Apr 28 16:19:05.825: INFO: (16) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 6.564207ms)
Apr 28 16:19:05.829: INFO: (16) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 11.098554ms)
Apr 28 16:19:05.830: INFO: (16) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 11.365079ms)
Apr 28 16:19:05.830: INFO: (16) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 11.47814ms)
Apr 28 16:19:05.830: INFO: (16) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 11.917623ms)
Apr 28 16:19:05.830: INFO: (16) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 12.085944ms)
Apr 28 16:19:05.830: INFO: (16) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 12.222767ms)
Apr 28 16:19:05.832: INFO: (16) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 13.470469ms)
Apr 28 16:19:05.832: INFO: (16) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 13.963996ms)
Apr 28 16:19:05.832: INFO: (16) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 13.833855ms)
Apr 28 16:19:05.832: INFO: (16) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 14.021634ms)
Apr 28 16:19:05.833: INFO: (16) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 14.250128ms)
Apr 28 16:19:05.833: INFO: (16) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 14.934746ms)
Apr 28 16:19:05.834: INFO: (16) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 15.223897ms)
Apr 28 16:19:05.834: INFO: (16) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 15.364513ms)
Apr 28 16:19:05.843: INFO: (17) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 9.086972ms)
Apr 28 16:19:05.843: INFO: (17) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 9.149155ms)
Apr 28 16:19:05.843: INFO: (17) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 9.257015ms)
Apr 28 16:19:05.844: INFO: (17) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 10.656183ms)
Apr 28 16:19:05.844: INFO: (17) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 10.538924ms)
Apr 28 16:19:05.844: INFO: (17) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 10.570348ms)
Apr 28 16:19:05.846: INFO: (17) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 12.275427ms)
Apr 28 16:19:05.846: INFO: (17) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 12.436619ms)
Apr 28 16:19:05.847: INFO: (17) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 12.760495ms)
Apr 28 16:19:05.848: INFO: (17) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 13.786701ms)
Apr 28 16:19:05.848: INFO: (17) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 13.843392ms)
Apr 28 16:19:05.848: INFO: (17) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 13.775481ms)
Apr 28 16:19:05.848: INFO: (17) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 13.876692ms)
Apr 28 16:19:05.848: INFO: (17) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 13.960465ms)
Apr 28 16:19:05.848: INFO: (17) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 14.001508ms)
Apr 28 16:19:05.849: INFO: (17) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 14.853091ms)
Apr 28 16:19:05.854: INFO: (18) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 5.047267ms)
Apr 28 16:19:05.856: INFO: (18) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 7.193795ms)
Apr 28 16:19:05.857: INFO: (18) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 7.77262ms)
Apr 28 16:19:05.858: INFO: (18) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 8.737067ms)
Apr 28 16:19:05.859: INFO: (18) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 10.143026ms)
Apr 28 16:19:05.861: INFO: (18) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 12.168028ms)
Apr 28 16:19:05.861: INFO: (18) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 12.499485ms)
Apr 28 16:19:05.861: INFO: (18) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 12.391732ms)
Apr 28 16:19:05.861: INFO: (18) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 12.418191ms)
Apr 28 16:19:05.861: INFO: (18) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 12.506222ms)
Apr 28 16:19:05.861: INFO: (18) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 12.587056ms)
Apr 28 16:19:05.861: INFO: (18) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 12.507842ms)
Apr 28 16:19:05.862: INFO: (18) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 13.386981ms)
Apr 28 16:19:05.863: INFO: (18) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 13.760324ms)
Apr 28 16:19:05.863: INFO: (18) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 14.096853ms)
Apr 28 16:19:05.863: INFO: (18) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 14.176915ms)
Apr 28 16:19:05.873: INFO: (19) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">test<... (200; 9.538991ms)
Apr 28 16:19:05.873: INFO: (19) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:1080/proxy/rewriteme">... (200; 9.877692ms)
Apr 28 16:19:05.878: INFO: (19) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:460/proxy/: tls baz (200; 14.809805ms)
Apr 28 16:19:05.878: INFO: (19) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 14.79641ms)
Apr 28 16:19:05.879: INFO: (19) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:462/proxy/: tls qux (200; 15.63139ms)
Apr 28 16:19:05.879: INFO: (19) /api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/https:proxy-service-nfm85-jdtnt:443/proxy/tlsrewritem... (200; 16.303421ms)
Apr 28 16:19:05.880: INFO: (19) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 16.530431ms)
Apr 28 16:19:05.880: INFO: (19) /api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/: <a href="/api/v1/namespaces/proxy-2300/pods/proxy-service-nfm85-jdtnt/proxy/rewriteme">test</a> (200; 16.493275ms)
Apr 28 16:19:05.880: INFO: (19) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:162/proxy/: bar (200; 16.756538ms)
Apr 28 16:19:05.880: INFO: (19) /api/v1/namespaces/proxy-2300/pods/http:proxy-service-nfm85-jdtnt:160/proxy/: foo (200; 17.187875ms)
Apr 28 16:19:05.882: INFO: (19) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname1/proxy/: foo (200; 18.57461ms)
Apr 28 16:19:05.882: INFO: (19) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname2/proxy/: tls qux (200; 18.484735ms)
Apr 28 16:19:05.882: INFO: (19) /api/v1/namespaces/proxy-2300/services/https:proxy-service-nfm85:tlsportname1/proxy/: tls baz (200; 19.133439ms)
Apr 28 16:19:05.882: INFO: (19) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname1/proxy/: foo (200; 19.285692ms)
Apr 28 16:19:05.882: INFO: (19) /api/v1/namespaces/proxy-2300/services/http:proxy-service-nfm85:portname2/proxy/: bar (200; 19.23234ms)
Apr 28 16:19:05.883: INFO: (19) /api/v1/namespaces/proxy-2300/services/proxy-service-nfm85:portname2/proxy/: bar (200; 20.123707ms)
STEP: deleting ReplicationController proxy-service-nfm85 in namespace proxy-2300, will wait for the garbage collector to delete the pods
Apr 28 16:19:05.954: INFO: Deleting ReplicationController proxy-service-nfm85 took: 12.142439ms
Apr 28 16:19:06.054: INFO: Terminating ReplicationController proxy-service-nfm85 pods took: 100.114602ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:19:07.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2300" for this suite.

• [SLOW TEST:13.503 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":177,"skipped":2842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:19:07.575: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-8362
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Apr 28 16:19:07.774: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Apr 28 16:19:07.840: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:19:07.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8362" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":178,"skipped":2875,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:19:07.917: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5172
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 28 16:19:08.479: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 16:19:11.510: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:19:11.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5172" for this suite.
STEP: Destroying namespace "webhook-5172-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":179,"skipped":2911,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:19:11.744: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1504
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-8627
STEP: Creating secret with name secret-test-727dcc12-caba-4116-9f15-137fb5a48fc7
STEP: Creating a pod to test consume secrets
Apr 28 16:19:12.106: INFO: Waiting up to 5m0s for pod "pod-secrets-e29b7265-6c38-4a5a-a2c3-388287bfa14e" in namespace "secrets-1504" to be "Succeeded or Failed"
Apr 28 16:19:12.119: INFO: Pod "pod-secrets-e29b7265-6c38-4a5a-a2c3-388287bfa14e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.14516ms
Apr 28 16:19:14.124: INFO: Pod "pod-secrets-e29b7265-6c38-4a5a-a2c3-388287bfa14e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017646616s
STEP: Saw pod success
Apr 28 16:19:14.124: INFO: Pod "pod-secrets-e29b7265-6c38-4a5a-a2c3-388287bfa14e" satisfied condition "Succeeded or Failed"
Apr 28 16:19:14.128: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-secrets-e29b7265-6c38-4a5a-a2c3-388287bfa14e container secret-volume-test: <nil>
STEP: delete the pod
Apr 28 16:19:14.152: INFO: Waiting for pod pod-secrets-e29b7265-6c38-4a5a-a2c3-388287bfa14e to disappear
Apr 28 16:19:14.155: INFO: Pod pod-secrets-e29b7265-6c38-4a5a-a2c3-388287bfa14e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:19:14.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1504" for this suite.
STEP: Destroying namespace "secret-namespace-8627" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":180,"skipped":2951,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:19:14.177: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5932
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Apr 28 16:19:14.361: INFO: observed Pod pod-test in namespace pods-5932 in phase Pending conditions []
Apr 28 16:19:14.364: INFO: observed Pod pod-test in namespace pods-5932 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 16:19:14 +0000 UTC  }]
Apr 28 16:19:14.381: INFO: observed Pod pod-test in namespace pods-5932 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 16:19:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 16:19:14 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-28 16:19:14 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-28 16:19:14 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Apr 28 16:19:15.509: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Apr 28 16:19:15.551: INFO: observed event type ADDED
Apr 28 16:19:15.551: INFO: observed event type MODIFIED
Apr 28 16:19:15.551: INFO: observed event type MODIFIED
Apr 28 16:19:15.551: INFO: observed event type MODIFIED
Apr 28 16:19:15.551: INFO: observed event type MODIFIED
Apr 28 16:19:15.551: INFO: observed event type MODIFIED
Apr 28 16:19:15.551: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:19:15.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5932" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":181,"skipped":2954,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:19:15.567: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-142
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-142
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-142 to expose endpoints map[]
Apr 28 16:19:15.750: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Apr 28 16:19:16.759: INFO: successfully validated that service endpoint-test2 in namespace services-142 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-142
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-142 to expose endpoints map[pod1:[80]]
Apr 28 16:19:17.813: INFO: successfully validated that service endpoint-test2 in namespace services-142 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-142
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-142 to expose endpoints map[pod1:[80] pod2:[80]]
Apr 28 16:19:18.857: INFO: successfully validated that service endpoint-test2 in namespace services-142 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-142
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-142 to expose endpoints map[pod2:[80]]
Apr 28 16:19:18.890: INFO: successfully validated that service endpoint-test2 in namespace services-142 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-142
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-142 to expose endpoints map[]
Apr 28 16:19:18.919: INFO: successfully validated that service endpoint-test2 in namespace services-142 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:19:18.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-142" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":182,"skipped":2976,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:19:18.969: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1885
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:19:19.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-1885 create -f -'
Apr 28 16:19:19.403: INFO: stderr: ""
Apr 28 16:19:19.403: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Apr 28 16:19:19.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-1885 create -f -'
Apr 28 16:19:19.613: INFO: stderr: ""
Apr 28 16:19:19.613: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 28 16:19:20.619: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 28 16:19:20.619: INFO: Found 1 / 1
Apr 28 16:19:20.619: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 28 16:19:20.625: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 28 16:19:20.625: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 28 16:19:20.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-1885 describe pod agnhost-primary-966gg'
Apr 28 16:19:20.697: INFO: stderr: ""
Apr 28 16:19:20.697: INFO: stdout: "Name:         agnhost-primary-966gg\nNamespace:    kubectl-1885\nPriority:     0\nNode:         ip-172-31-13-33/172.31.13.33\nStart Time:   Wed, 28 Apr 2021 16:19:19 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           10.1.62.179\nIPs:\n  IP:           10.1.62.179\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://7f4d1ed3cae4b73ec348642f2da369abcb92b8d9549c29436f300bd55b056305\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 28 Apr 2021 16:19:20 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lzcgv (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-lzcgv:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-lzcgv\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-1885/agnhost-primary-966gg to ip-172-31-13-33\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Apr 28 16:19:20.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-1885 describe rc agnhost-primary'
Apr 28 16:19:20.771: INFO: stderr: ""
Apr 28 16:19:20.771: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1885\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-966gg\n"
Apr 28 16:19:20.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-1885 describe service agnhost-primary'
Apr 28 16:19:20.840: INFO: stderr: ""
Apr 28 16:19:20.840: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1885\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.152.183.180\nIPs:               10.152.183.180\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.1.62.179:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr 28 16:19:20.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-1885 describe node ip-172-31-13-33'
Apr 28 16:19:20.924: INFO: stderr: ""
Apr 28 16:19:20.924: INFO: stdout: "Name:               ip-172-31-13-33\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-worker\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-13-33\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 28 Apr 2021 14:28:38 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-13-33\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 28 Apr 2021 16:19:12 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 28 Apr 2021 16:15:55 +0000   Wed, 28 Apr 2021 14:28:38 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 28 Apr 2021 16:15:55 +0000   Wed, 28 Apr 2021 14:28:38 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 28 Apr 2021 16:15:55 +0000   Wed, 28 Apr 2021 14:28:38 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 28 Apr 2021 16:15:55 +0000   Wed, 28 Apr 2021 14:28:48 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.13.33\n  Hostname:    ip-172-31-13-33\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    16197480Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               7792680Ki\n  pods:                 110\nAllocatable:\n  cpu:                  4\n  ephemeral-storage:    14927597544\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               7690280Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                      ec262eee5738151fbeae5f14606c6abf\n  System UUID:                     ec262eee-5738-151f-beae-5f14606c6abf\n  Boot ID:                         1d86910e-38ac-4bb5-9b8d-da07c1d283e0\n  Kernel Version:                  5.4.0-1045-aws\n  OS Image:                        Ubuntu 20.04.2 LTS\n  Operating System:                linux\n  Architecture:                    amd64\n  Container Runtime Version:       containerd://1.3.3-0ubuntu2.3\n  Kubelet Version:                 v1.20.6\n  Kube-Proxy Version:              v1.20.6\nNon-terminated Pods:               (3 in total)\n  Namespace                        Name                                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                        ----                                                ------------  ----------  ---------------  -------------  ---\n  ingress-nginx-kubernetes-worker  nginx-ingress-controller-kubernetes-worker-9knnc    0 (0%)        0 (0%)      0 (0%)           0 (0%)         14m\n  kube-system                      metrics-server-v0.3.6-f6cf867b4-kjjbl               53m (1%)      148m (3%)   154Mi (2%)       404Mi (5%)     14m\n  kubectl-1885                     agnhost-primary-966gg                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         1s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests    Limits\n  --------             --------    ------\n  cpu                  53m (1%)    148m (3%)\n  memory               154Mi (2%)  404Mi (5%)\n  ephemeral-storage    0 (0%)      0 (0%)\n  hugepages-1Gi        0 (0%)      0 (0%)\n  hugepages-2Mi        0 (0%)      0 (0%)\n  example.com/fakecpu  0           0\nEvents:                <none>\n"
Apr 28 16:19:20.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-1885 describe namespace kubectl-1885'
Apr 28 16:19:20.997: INFO: stderr: ""
Apr 28 16:19:20.997: INFO: stdout: "Name:         kubectl-1885\nLabels:       e2e-framework=kubectl\n              e2e-run=eb4e9110-e70e-4e35-b39e-8a75e28ee659\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:19:20.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1885" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":183,"skipped":2992,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:19:21.011: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6806
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:19:21.170: INFO: Creating deployment "test-recreate-deployment"
Apr 28 16:19:21.179: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr 28 16:19:21.192: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Apr 28 16:19:23.225: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr 28 16:19:23.238: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr 28 16:19:23.262: INFO: Updating deployment test-recreate-deployment
Apr 28 16:19:23.262: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 28 16:19:23.407: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6806  c97c4a8e-402b-4842-8285-8f2a0ce9dab6 21211 2 2021-04-28 16:19:21 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-04-28 16:19:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-28 16:19:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00393aba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-04-28 16:19:23 +0000 UTC,LastTransitionTime:2021-04-28 16:19:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-04-28 16:19:23 +0000 UTC,LastTransitionTime:2021-04-28 16:19:21 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Apr 28 16:19:23.414: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-6806  9b59b4d2-a18f-41c1-874a-a87739c823ff 21209 1 2021-04-28 16:19:23 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment c97c4a8e-402b-4842-8285-8f2a0ce9dab6 0xc00393b020 0xc00393b021}] []  [{kube-controller-manager Update apps/v1 2021-04-28 16:19:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c97c4a8e-402b-4842-8285-8f2a0ce9dab6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00393b098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 28 16:19:23.414: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr 28 16:19:23.414: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-6806  8315f1c4-6e9f-408c-9e70-ddb29c3138f8 21198 2 2021-04-28 16:19:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment c97c4a8e-402b-4842-8285-8f2a0ce9dab6 0xc00393af27 0xc00393af28}] []  [{kube-controller-manager Update apps/v1 2021-04-28 16:19:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c97c4a8e-402b-4842-8285-8f2a0ce9dab6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00393afb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 28 16:19:23.419: INFO: Pod "test-recreate-deployment-f79dd4667-rvbnv" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-rvbnv test-recreate-deployment-f79dd4667- deployment-6806  bc3a0b28-ad65-4733-b49b-30f2e770bcbb 21210 0 2021-04-28 16:19:23 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 9b59b4d2-a18f-41c1-874a-a87739c823ff 0xc00393b4c0 0xc00393b4c1}] []  [{kube-controller-manager Update v1 2021-04-28 16:19:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9b59b4d2-a18f-41c1-874a-a87739c823ff\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:19:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-csckt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-csckt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-csckt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:19:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:19:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:19:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:19:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.33,PodIP:,StartTime:2021-04-28 16:19:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:19:23.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6806" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":184,"skipped":3002,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:19:23.436: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4946
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-4946
STEP: creating service affinity-nodeport in namespace services-4946
STEP: creating replication controller affinity-nodeport in namespace services-4946
I0428 16:19:23.646150      24 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-4946, replica count: 3
I0428 16:19:26.696424      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 28 16:19:26.711: INFO: Creating new exec pod
Apr 28 16:19:29.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-4946 exec execpod-affinitygzgcn -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Apr 28 16:19:29.910: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Apr 28 16:19:29.910: INFO: stdout: ""
Apr 28 16:19:29.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-4946 exec execpod-affinitygzgcn -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.254 80'
Apr 28 16:19:30.031: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.254 80\nConnection to 10.152.183.254 80 port [tcp/http] succeeded!\n"
Apr 28 16:19:30.031: INFO: stdout: ""
Apr 28 16:19:30.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-4946 exec execpod-affinitygzgcn -- /bin/sh -x -c nc -zv -t -w 2 172.31.30.112 31080'
Apr 28 16:19:30.165: INFO: stderr: "+ nc -zv -t -w 2 172.31.30.112 31080\nConnection to 172.31.30.112 31080 port [tcp/31080] succeeded!\n"
Apr 28 16:19:30.165: INFO: stdout: ""
Apr 28 16:19:30.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-4946 exec execpod-affinitygzgcn -- /bin/sh -x -c nc -zv -t -w 2 172.31.13.33 31080'
Apr 28 16:19:30.282: INFO: stderr: "+ nc -zv -t -w 2 172.31.13.33 31080\nConnection to 172.31.13.33 31080 port [tcp/31080] succeeded!\n"
Apr 28 16:19:30.283: INFO: stdout: ""
Apr 28 16:19:30.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-4946 exec execpod-affinitygzgcn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.13.33:31080/ ; done'
Apr 28 16:19:30.504: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31080/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31080/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31080/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31080/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31080/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31080/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31080/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31080/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31080/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31080/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31080/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31080/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31080/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31080/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31080/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31080/\n"
Apr 28 16:19:30.504: INFO: stdout: "\naffinity-nodeport-kslbp\naffinity-nodeport-kslbp\naffinity-nodeport-kslbp\naffinity-nodeport-kslbp\naffinity-nodeport-kslbp\naffinity-nodeport-kslbp\naffinity-nodeport-kslbp\naffinity-nodeport-kslbp\naffinity-nodeport-kslbp\naffinity-nodeport-kslbp\naffinity-nodeport-kslbp\naffinity-nodeport-kslbp\naffinity-nodeport-kslbp\naffinity-nodeport-kslbp\naffinity-nodeport-kslbp\naffinity-nodeport-kslbp"
Apr 28 16:19:30.504: INFO: Received response from host: affinity-nodeport-kslbp
Apr 28 16:19:30.504: INFO: Received response from host: affinity-nodeport-kslbp
Apr 28 16:19:30.504: INFO: Received response from host: affinity-nodeport-kslbp
Apr 28 16:19:30.504: INFO: Received response from host: affinity-nodeport-kslbp
Apr 28 16:19:30.504: INFO: Received response from host: affinity-nodeport-kslbp
Apr 28 16:19:30.504: INFO: Received response from host: affinity-nodeport-kslbp
Apr 28 16:19:30.504: INFO: Received response from host: affinity-nodeport-kslbp
Apr 28 16:19:30.504: INFO: Received response from host: affinity-nodeport-kslbp
Apr 28 16:19:30.504: INFO: Received response from host: affinity-nodeport-kslbp
Apr 28 16:19:30.504: INFO: Received response from host: affinity-nodeport-kslbp
Apr 28 16:19:30.504: INFO: Received response from host: affinity-nodeport-kslbp
Apr 28 16:19:30.504: INFO: Received response from host: affinity-nodeport-kslbp
Apr 28 16:19:30.504: INFO: Received response from host: affinity-nodeport-kslbp
Apr 28 16:19:30.504: INFO: Received response from host: affinity-nodeport-kslbp
Apr 28 16:19:30.504: INFO: Received response from host: affinity-nodeport-kslbp
Apr 28 16:19:30.504: INFO: Received response from host: affinity-nodeport-kslbp
Apr 28 16:19:30.504: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4946, will wait for the garbage collector to delete the pods
Apr 28 16:19:30.592: INFO: Deleting ReplicationController affinity-nodeport took: 13.255101ms
Apr 28 16:19:30.692: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.160538ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:19:45.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4946" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:21.725 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":185,"skipped":3030,"failed":0}
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:19:45.160: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-327
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:19:45.353: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Creating first CR 
Apr 28 16:19:45.966: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-28T16:19:45Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-28T16:19:45Z]] name:name1 resourceVersion:21425 uid:31e474cc-0211-4ffe-920c-b95011878736] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Apr 28 16:19:55.979: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-28T16:19:55Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-28T16:19:55Z]] name:name2 resourceVersion:21475 uid:dceaad2d-9821-415a-925c-93e604194017] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Apr 28 16:20:05.991: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-28T16:19:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-28T16:20:05Z]] name:name1 resourceVersion:21493 uid:31e474cc-0211-4ffe-920c-b95011878736] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Apr 28 16:20:16.002: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-28T16:19:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-28T16:20:15Z]] name:name2 resourceVersion:21509 uid:dceaad2d-9821-415a-925c-93e604194017] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Apr 28 16:20:26.017: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-28T16:19:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-28T16:20:05Z]] name:name1 resourceVersion:21526 uid:31e474cc-0211-4ffe-920c-b95011878736] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Apr 28 16:20:36.033: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-28T16:19:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-28T16:20:15Z]] name:name2 resourceVersion:21544 uid:dceaad2d-9821-415a-925c-93e604194017] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:20:46.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-327" for this suite.

• [SLOW TEST:61.422 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":186,"skipped":3030,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:20:46.583: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-490
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-21d2fa20-f6b3-4446-9a7a-d4a8cd2bc11d
STEP: Creating a pod to test consume configMaps
Apr 28 16:20:46.827: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c62340a5-d0a9-45bb-9064-84b3ac984843" in namespace "projected-490" to be "Succeeded or Failed"
Apr 28 16:20:46.844: INFO: Pod "pod-projected-configmaps-c62340a5-d0a9-45bb-9064-84b3ac984843": Phase="Pending", Reason="", readiness=false. Elapsed: 17.09384ms
Apr 28 16:20:48.854: INFO: Pod "pod-projected-configmaps-c62340a5-d0a9-45bb-9064-84b3ac984843": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026467864s
STEP: Saw pod success
Apr 28 16:20:48.854: INFO: Pod "pod-projected-configmaps-c62340a5-d0a9-45bb-9064-84b3ac984843" satisfied condition "Succeeded or Failed"
Apr 28 16:20:48.858: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-projected-configmaps-c62340a5-d0a9-45bb-9064-84b3ac984843 container agnhost-container: <nil>
STEP: delete the pod
Apr 28 16:20:48.889: INFO: Waiting for pod pod-projected-configmaps-c62340a5-d0a9-45bb-9064-84b3ac984843 to disappear
Apr 28 16:20:48.892: INFO: Pod pod-projected-configmaps-c62340a5-d0a9-45bb-9064-84b3ac984843 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:20:48.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-490" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":187,"skipped":3047,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:20:48.903: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3544
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 28 16:20:49.479: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 16:20:52.511: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:20:52.519: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3880-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:20:53.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3544" for this suite.
STEP: Destroying namespace "webhook-3544-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":188,"skipped":3060,"failed":0}
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:20:53.764: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5276
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Apr 28 16:20:53.924: INFO: PodSpec: initContainers in spec.initContainers
Apr 28 16:21:39.680: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7a5f4d83-5ec3-44e7-95e0-ff021dd5d75b", GenerateName:"", Namespace:"init-container-5276", SelfLink:"", UID:"b17fede3-342f-4f3a-aa11-44f1ae9085d0", ResourceVersion:"21806", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63755223653, loc:(*time.Location)(0x7975ee0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"924087355"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0057f9560), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0057f9580)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0057f95a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0057f95c0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-x7qrw", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001346280), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-x7qrw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-x7qrw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-x7qrw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0073848f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-13-33", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002522a80), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc007384990)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0073849b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0073849b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0073849bc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0040b9f00), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755223653, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755223653, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755223653, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755223653, loc:(*time.Location)(0x7975ee0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.13.33", PodIP:"10.1.62.188", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.1.62.188"}}, StartTime:(*v1.Time)(0xc0057f95e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002522b60)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002522bd0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://6be0e2f211ae59eb263bc1969133ccd9796108beb49190da36db1f2a0b184062", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0057f9620), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0057f9600), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc007384a3f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:21:39.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5276" for this suite.

• [SLOW TEST:45.933 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":189,"skipped":3064,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:21:39.697: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3816
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 16:21:39.873: INFO: Waiting up to 5m0s for pod "downwardapi-volume-976a4edb-6f30-40e1-8b57-76ba703ae88a" in namespace "projected-3816" to be "Succeeded or Failed"
Apr 28 16:21:39.879: INFO: Pod "downwardapi-volume-976a4edb-6f30-40e1-8b57-76ba703ae88a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.310745ms
Apr 28 16:21:41.884: INFO: Pod "downwardapi-volume-976a4edb-6f30-40e1-8b57-76ba703ae88a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011286171s
STEP: Saw pod success
Apr 28 16:21:41.884: INFO: Pod "downwardapi-volume-976a4edb-6f30-40e1-8b57-76ba703ae88a" satisfied condition "Succeeded or Failed"
Apr 28 16:21:41.890: INFO: Trying to get logs from node ip-172-31-13-33 pod downwardapi-volume-976a4edb-6f30-40e1-8b57-76ba703ae88a container client-container: <nil>
STEP: delete the pod
Apr 28 16:21:41.924: INFO: Waiting for pod downwardapi-volume-976a4edb-6f30-40e1-8b57-76ba703ae88a to disappear
Apr 28 16:21:41.928: INFO: Pod downwardapi-volume-976a4edb-6f30-40e1-8b57-76ba703ae88a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:21:41.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3816" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":190,"skipped":3065,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:21:41.942: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8472
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-8472/configmap-test-4c0069b7-c3b7-41e5-b266-a04d20fb0c64
STEP: Creating a pod to test consume configMaps
Apr 28 16:21:42.138: INFO: Waiting up to 5m0s for pod "pod-configmaps-3e36dabc-3aa9-419b-8434-fca713d2f260" in namespace "configmap-8472" to be "Succeeded or Failed"
Apr 28 16:21:42.145: INFO: Pod "pod-configmaps-3e36dabc-3aa9-419b-8434-fca713d2f260": Phase="Pending", Reason="", readiness=false. Elapsed: 7.220947ms
Apr 28 16:21:44.154: INFO: Pod "pod-configmaps-3e36dabc-3aa9-419b-8434-fca713d2f260": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016119664s
STEP: Saw pod success
Apr 28 16:21:44.154: INFO: Pod "pod-configmaps-3e36dabc-3aa9-419b-8434-fca713d2f260" satisfied condition "Succeeded or Failed"
Apr 28 16:21:44.158: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-configmaps-3e36dabc-3aa9-419b-8434-fca713d2f260 container env-test: <nil>
STEP: delete the pod
Apr 28 16:21:44.184: INFO: Waiting for pod pod-configmaps-3e36dabc-3aa9-419b-8434-fca713d2f260 to disappear
Apr 28 16:21:44.189: INFO: Pod pod-configmaps-3e36dabc-3aa9-419b-8434-fca713d2f260 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:21:44.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8472" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":191,"skipped":3076,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:21:44.200: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4756
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-c12afe99-2416-4752-bfa4-f41ac3bd4323 in namespace container-probe-4756
Apr 28 16:21:46.409: INFO: Started pod liveness-c12afe99-2416-4752-bfa4-f41ac3bd4323 in namespace container-probe-4756
STEP: checking the pod's current state and verifying that restartCount is present
Apr 28 16:21:46.414: INFO: Initial restart count of pod liveness-c12afe99-2416-4752-bfa4-f41ac3bd4323 is 0
Apr 28 16:22:04.512: INFO: Restart count of pod container-probe-4756/liveness-c12afe99-2416-4752-bfa4-f41ac3bd4323 is now 1 (18.098163874s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:22:04.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4756" for this suite.

• [SLOW TEST:20.343 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":192,"skipped":3081,"failed":0}
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:22:04.543: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5379
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4145
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1799
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:22:11.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5379" for this suite.
STEP: Destroying namespace "nsdeletetest-4145" for this suite.
Apr 28 16:22:11.071: INFO: Namespace nsdeletetest-4145 was already deleted
STEP: Destroying namespace "nsdeletetest-1799" for this suite.

• [SLOW TEST:6.537 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":193,"skipped":3083,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:22:11.081: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5965
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Apr 28 16:22:11.253: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 16:22:12.979: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:22:23.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5965" for this suite.

• [SLOW TEST:12.582 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":194,"skipped":3105,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:22:23.663: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5357
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 16:22:23.840: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee5f62c5-7337-4b87-bb18-245981966c42" in namespace "downward-api-5357" to be "Succeeded or Failed"
Apr 28 16:22:23.856: INFO: Pod "downwardapi-volume-ee5f62c5-7337-4b87-bb18-245981966c42": Phase="Pending", Reason="", readiness=false. Elapsed: 16.017739ms
Apr 28 16:22:25.863: INFO: Pod "downwardapi-volume-ee5f62c5-7337-4b87-bb18-245981966c42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023158231s
STEP: Saw pod success
Apr 28 16:22:25.864: INFO: Pod "downwardapi-volume-ee5f62c5-7337-4b87-bb18-245981966c42" satisfied condition "Succeeded or Failed"
Apr 28 16:22:25.868: INFO: Trying to get logs from node ip-172-31-13-33 pod downwardapi-volume-ee5f62c5-7337-4b87-bb18-245981966c42 container client-container: <nil>
STEP: delete the pod
Apr 28 16:22:25.893: INFO: Waiting for pod downwardapi-volume-ee5f62c5-7337-4b87-bb18-245981966c42 to disappear
Apr 28 16:22:25.898: INFO: Pod downwardapi-volume-ee5f62c5-7337-4b87-bb18-245981966c42 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:22:25.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5357" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":195,"skipped":3178,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:22:25.915: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4102
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Apr 28 16:22:26.082: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:22:28.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4102" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":196,"skipped":3189,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:22:28.767: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-932
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 28 16:22:29.221: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 28 16:22:31.240: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755223749, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755223749, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755223749, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755223749, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 16:22:34.260: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:22:46.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-932" for this suite.
STEP: Destroying namespace "webhook-932-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:17.767 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":197,"skipped":3229,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:22:46.535: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6844
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 28 16:22:46.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-6844 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Apr 28 16:22:46.779: INFO: stderr: ""
Apr 28 16:22:46.779: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Apr 28 16:22:51.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-6844 get pod e2e-test-httpd-pod -o json'
Apr 28 16:22:51.889: INFO: stderr: ""
Apr 28 16:22:51.889: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2021-04-28T16:22:46Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-04-28T16:22:46Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.1.62.196\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-04-28T16:22:47Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6844\",\n        \"resourceVersion\": \"22251\",\n        \"uid\": \"2d5e3d1b-e166-4a0e-a1e2-e04865e80ebc\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-d667j\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-13-33\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-d667j\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-d667j\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-28T16:22:46Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-28T16:22:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-28T16:22:47Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-28T16:22:46Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://b8eea9f5ee661bc631ebac15d0a2e38bb5c65f6436d57be2781abdfdd94c591c\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-04-28T16:22:47Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.13.33\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.1.62.196\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.1.62.196\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-04-28T16:22:46Z\"\n    }\n}\n"
STEP: replace the image in the pod
Apr 28 16:22:51.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-6844 replace -f -'
Apr 28 16:22:52.127: INFO: stderr: ""
Apr 28 16:22:52.127: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Apr 28 16:22:52.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-6844 delete pods e2e-test-httpd-pod'
Apr 28 16:22:53.798: INFO: stderr: ""
Apr 28 16:22:53.798: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:22:53.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6844" for this suite.

• [SLOW TEST:7.282 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":198,"skipped":3250,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:22:53.818: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6232
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Apr 28 16:22:53.982: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Apr 28 16:23:04.523: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 16:23:07.257: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:23:17.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6232" for this suite.

• [SLOW TEST:24.115 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":199,"skipped":3256,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:23:17.933: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2643
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-2643/configmap-test-8a5c297b-025a-4e1d-a1bd-19439fb00092
STEP: Creating a pod to test consume configMaps
Apr 28 16:23:18.100: INFO: Waiting up to 5m0s for pod "pod-configmaps-b10c92f8-0019-460e-abb9-aeb6973d4470" in namespace "configmap-2643" to be "Succeeded or Failed"
Apr 28 16:23:18.106: INFO: Pod "pod-configmaps-b10c92f8-0019-460e-abb9-aeb6973d4470": Phase="Pending", Reason="", readiness=false. Elapsed: 5.363706ms
Apr 28 16:23:20.111: INFO: Pod "pod-configmaps-b10c92f8-0019-460e-abb9-aeb6973d4470": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010971309s
STEP: Saw pod success
Apr 28 16:23:20.111: INFO: Pod "pod-configmaps-b10c92f8-0019-460e-abb9-aeb6973d4470" satisfied condition "Succeeded or Failed"
Apr 28 16:23:20.116: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-configmaps-b10c92f8-0019-460e-abb9-aeb6973d4470 container env-test: <nil>
STEP: delete the pod
Apr 28 16:23:20.138: INFO: Waiting for pod pod-configmaps-b10c92f8-0019-460e-abb9-aeb6973d4470 to disappear
Apr 28 16:23:20.143: INFO: Pod pod-configmaps-b10c92f8-0019-460e-abb9-aeb6973d4470 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:23:20.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2643" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":200,"skipped":3267,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:23:20.156: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7713
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7713, will wait for the garbage collector to delete the pods
Apr 28 16:23:22.411: INFO: Deleting Job.batch foo took: 11.800343ms
Apr 28 16:23:23.011: INFO: Terminating Job.batch foo pods took: 600.14004ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:24:05.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7713" for this suite.

• [SLOW TEST:44.985 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":201,"skipped":3279,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:24:05.141: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3002
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:24:05.319: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 28 16:24:08.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-3002 --namespace=crd-publish-openapi-3002 create -f -'
Apr 28 16:24:08.467: INFO: stderr: ""
Apr 28 16:24:08.467: INFO: stdout: "e2e-test-crd-publish-openapi-4813-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 28 16:24:08.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-3002 --namespace=crd-publish-openapi-3002 delete e2e-test-crd-publish-openapi-4813-crds test-cr'
Apr 28 16:24:08.574: INFO: stderr: ""
Apr 28 16:24:08.574: INFO: stdout: "e2e-test-crd-publish-openapi-4813-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Apr 28 16:24:08.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-3002 --namespace=crd-publish-openapi-3002 apply -f -'
Apr 28 16:24:08.723: INFO: stderr: ""
Apr 28 16:24:08.723: INFO: stdout: "e2e-test-crd-publish-openapi-4813-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 28 16:24:08.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-3002 --namespace=crd-publish-openapi-3002 delete e2e-test-crd-publish-openapi-4813-crds test-cr'
Apr 28 16:24:08.790: INFO: stderr: ""
Apr 28 16:24:08.790: INFO: stdout: "e2e-test-crd-publish-openapi-4813-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr 28 16:24:08.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-3002 explain e2e-test-crd-publish-openapi-4813-crds'
Apr 28 16:24:08.928: INFO: stderr: ""
Apr 28 16:24:08.928: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4813-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:24:11.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3002" for this suite.

• [SLOW TEST:6.528 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":202,"skipped":3299,"failed":0}
S
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:24:11.669: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1817
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:24:11.850: INFO: Waiting up to 5m0s for pod "busybox-user-65534-570b875c-bd01-4efa-8861-5b7c60a665fe" in namespace "security-context-test-1817" to be "Succeeded or Failed"
Apr 28 16:24:11.862: INFO: Pod "busybox-user-65534-570b875c-bd01-4efa-8861-5b7c60a665fe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.656898ms
Apr 28 16:24:13.876: INFO: Pod "busybox-user-65534-570b875c-bd01-4efa-8861-5b7c60a665fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025738695s
Apr 28 16:24:13.876: INFO: Pod "busybox-user-65534-570b875c-bd01-4efa-8861-5b7c60a665fe" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:24:13.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1817" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":203,"skipped":3300,"failed":0}
SSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:24:13.891: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-1217
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 28 16:24:14.715: INFO: starting watch
STEP: patching
STEP: updating
Apr 28 16:24:14.749: INFO: waiting for watch events with expected annotations
Apr 28 16:24:14.749: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:24:14.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-1217" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":204,"skipped":3305,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:24:14.886: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-1563
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Apr 28 16:24:15.058: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 28 16:25:15.106: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:25:15.110: INFO: Starting informer...
STEP: Starting pods...
Apr 28 16:25:15.354: INFO: Pod1 is running on ip-172-31-13-33. Tainting Node
Apr 28 16:25:17.593: INFO: Pod2 is running on ip-172-31-13-33. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Apr 28 16:25:35.052: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Apr 28 16:25:55.049: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:25:55.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-1563" for this suite.

• [SLOW TEST:100.214 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":205,"skipped":3351,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:25:55.100: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2937
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:25:55.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-2937 version'
Apr 28 16:25:55.418: INFO: stderr: ""
Apr 28 16:25:55.419: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"8a62859e515889f07e3e3be6a1080413f17cf2c3\", GitTreeState:\"clean\", BuildDate:\"2021-04-15T03:28:42Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"8a62859e515889f07e3e3be6a1080413f17cf2c3\", GitTreeState:\"clean\", BuildDate:\"2021-04-16T07:46:21Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:25:55.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2937" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":206,"skipped":3371,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:25:55.437: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6744
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 16:25:55.618: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d072bbc2-cfac-44d5-9d7d-38986fd63fd7" in namespace "downward-api-6744" to be "Succeeded or Failed"
Apr 28 16:25:55.625: INFO: Pod "downwardapi-volume-d072bbc2-cfac-44d5-9d7d-38986fd63fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.809789ms
Apr 28 16:25:57.635: INFO: Pod "downwardapi-volume-d072bbc2-cfac-44d5-9d7d-38986fd63fd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016641321s
STEP: Saw pod success
Apr 28 16:25:57.635: INFO: Pod "downwardapi-volume-d072bbc2-cfac-44d5-9d7d-38986fd63fd7" satisfied condition "Succeeded or Failed"
Apr 28 16:25:57.639: INFO: Trying to get logs from node ip-172-31-13-33 pod downwardapi-volume-d072bbc2-cfac-44d5-9d7d-38986fd63fd7 container client-container: <nil>
STEP: delete the pod
Apr 28 16:25:57.671: INFO: Waiting for pod downwardapi-volume-d072bbc2-cfac-44d5-9d7d-38986fd63fd7 to disappear
Apr 28 16:25:57.678: INFO: Pod downwardapi-volume-d072bbc2-cfac-44d5-9d7d-38986fd63fd7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:25:57.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6744" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":207,"skipped":3443,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:25:57.694: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7005
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Apr 28 16:25:57.853: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Apr 28 16:25:57.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-7005 create -f -'
Apr 28 16:25:58.070: INFO: stderr: ""
Apr 28 16:25:58.070: INFO: stdout: "service/agnhost-replica created\n"
Apr 28 16:25:58.071: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Apr 28 16:25:58.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-7005 create -f -'
Apr 28 16:25:58.240: INFO: stderr: ""
Apr 28 16:25:58.240: INFO: stdout: "service/agnhost-primary created\n"
Apr 28 16:25:58.240: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr 28 16:25:58.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-7005 create -f -'
Apr 28 16:25:58.392: INFO: stderr: ""
Apr 28 16:25:58.392: INFO: stdout: "service/frontend created\n"
Apr 28 16:25:58.392: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Apr 28 16:25:58.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-7005 create -f -'
Apr 28 16:25:58.565: INFO: stderr: ""
Apr 28 16:25:58.565: INFO: stdout: "deployment.apps/frontend created\n"
Apr 28 16:25:58.565: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 28 16:25:58.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-7005 create -f -'
Apr 28 16:25:58.729: INFO: stderr: ""
Apr 28 16:25:58.729: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Apr 28 16:25:58.729: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 28 16:25:58.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-7005 create -f -'
Apr 28 16:25:58.926: INFO: stderr: ""
Apr 28 16:25:58.926: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Apr 28 16:25:58.926: INFO: Waiting for all frontend pods to be Running.
Apr 28 16:26:03.976: INFO: Waiting for frontend to serve content.
Apr 28 16:26:03.991: INFO: Trying to add a new entry to the guestbook.
Apr 28 16:26:04.004: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Apr 28 16:26:04.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-7005 delete --grace-period=0 --force -f -'
Apr 28 16:26:04.130: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 28 16:26:04.130: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Apr 28 16:26:04.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-7005 delete --grace-period=0 --force -f -'
Apr 28 16:26:04.224: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 28 16:26:04.224: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Apr 28 16:26:04.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-7005 delete --grace-period=0 --force -f -'
Apr 28 16:26:04.311: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 28 16:26:04.311: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 28 16:26:04.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-7005 delete --grace-period=0 --force -f -'
Apr 28 16:26:04.380: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 28 16:26:04.380: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 28 16:26:04.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-7005 delete --grace-period=0 --force -f -'
Apr 28 16:26:04.452: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 28 16:26:04.452: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Apr 28 16:26:04.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-7005 delete --grace-period=0 --force -f -'
Apr 28 16:26:04.518: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 28 16:26:04.518: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:26:04.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7005" for this suite.

• [SLOW TEST:6.838 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":208,"skipped":3490,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:26:04.532: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3084
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Apr 28 16:26:04.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-3084 create -f -'
Apr 28 16:26:04.877: INFO: stderr: ""
Apr 28 16:26:04.877: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 28 16:26:05.883: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 28 16:26:05.883: INFO: Found 1 / 1
Apr 28 16:26:05.883: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Apr 28 16:26:05.887: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 28 16:26:05.887: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 28 16:26:05.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-3084 patch pod agnhost-primary-64d5s -p {"metadata":{"annotations":{"x":"y"}}}'
Apr 28 16:26:05.979: INFO: stderr: ""
Apr 28 16:26:05.979: INFO: stdout: "pod/agnhost-primary-64d5s patched\n"
STEP: checking annotations
Apr 28 16:26:05.983: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 28 16:26:05.983: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:26:05.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3084" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":209,"skipped":3521,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:26:05.998: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-699
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:26:23.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-699" for this suite.

• [SLOW TEST:17.268 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":210,"skipped":3546,"failed":0}
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:26:23.265: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-5325
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 28 16:26:23.477: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Apr 28 16:26:23.484: INFO: starting watch
STEP: patching
STEP: updating
Apr 28 16:26:23.507: INFO: waiting for watch events with expected annotations
Apr 28 16:26:23.507: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:26:23.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-5325" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":211,"skipped":3546,"failed":0}
SSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:26:23.615: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-8705
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:26:23.776: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8705
I0428 16:26:23.787077      24 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8705, replica count: 1
I0428 16:26:24.837338      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0428 16:26:25.837495      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 28 16:26:25.951: INFO: Created: latency-svc-pgjhh
Apr 28 16:26:25.970: INFO: Got endpoints: latency-svc-pgjhh [33.175996ms]
Apr 28 16:26:26.002: INFO: Created: latency-svc-js7nd
Apr 28 16:26:26.008: INFO: Created: latency-svc-9m2l8
Apr 28 16:26:26.011: INFO: Got endpoints: latency-svc-js7nd [40.553645ms]
Apr 28 16:26:26.021: INFO: Created: latency-svc-q6qvh
Apr 28 16:26:26.024: INFO: Got endpoints: latency-svc-9m2l8 [52.97917ms]
Apr 28 16:26:26.035: INFO: Got endpoints: latency-svc-q6qvh [63.6211ms]
Apr 28 16:26:26.037: INFO: Created: latency-svc-qrf52
Apr 28 16:26:26.042: INFO: Got endpoints: latency-svc-qrf52 [71.128434ms]
Apr 28 16:26:26.052: INFO: Created: latency-svc-4cb9b
Apr 28 16:26:26.059: INFO: Created: latency-svc-pssvt
Apr 28 16:26:26.059: INFO: Got endpoints: latency-svc-4cb9b [88.540064ms]
Apr 28 16:26:26.069: INFO: Got endpoints: latency-svc-pssvt [98.154865ms]
Apr 28 16:26:26.078: INFO: Created: latency-svc-2tb7s
Apr 28 16:26:26.080: INFO: Got endpoints: latency-svc-2tb7s [109.569183ms]
Apr 28 16:26:26.092: INFO: Created: latency-svc-h4zdf
Apr 28 16:26:26.096: INFO: Got endpoints: latency-svc-h4zdf [125.474512ms]
Apr 28 16:26:26.100: INFO: Created: latency-svc-fkkxr
Apr 28 16:26:26.107: INFO: Created: latency-svc-9q862
Apr 28 16:26:26.117: INFO: Created: latency-svc-dbwp7
Apr 28 16:26:26.120: INFO: Created: latency-svc-46fk6
Apr 28 16:26:26.124: INFO: Got endpoints: latency-svc-fkkxr [153.115558ms]
Apr 28 16:26:26.133: INFO: Got endpoints: latency-svc-9q862 [162.517714ms]
Apr 28 16:26:26.137: INFO: Got endpoints: latency-svc-dbwp7 [166.825815ms]
Apr 28 16:26:26.140: INFO: Got endpoints: latency-svc-46fk6 [169.074135ms]
Apr 28 16:26:26.143: INFO: Created: latency-svc-bdlxw
Apr 28 16:26:26.158: INFO: Got endpoints: latency-svc-bdlxw [186.985989ms]
Apr 28 16:26:26.170: INFO: Created: latency-svc-fkxsx
Apr 28 16:26:26.173: INFO: Created: latency-svc-pzhbn
Apr 28 16:26:26.181: INFO: Got endpoints: latency-svc-fkxsx [210.213844ms]
Apr 28 16:26:26.191: INFO: Created: latency-svc-hhdrb
Apr 28 16:26:26.191: INFO: Got endpoints: latency-svc-pzhbn [220.369708ms]
Apr 28 16:26:26.211: INFO: Got endpoints: latency-svc-hhdrb [199.453069ms]
Apr 28 16:26:26.216: INFO: Created: latency-svc-xzkdb
Apr 28 16:26:26.225: INFO: Got endpoints: latency-svc-xzkdb [165.340394ms]
Apr 28 16:26:26.230: INFO: Created: latency-svc-hnr5c
Apr 28 16:26:26.235: INFO: Got endpoints: latency-svc-hnr5c [210.793251ms]
Apr 28 16:26:26.241: INFO: Created: latency-svc-x5ld2
Apr 28 16:26:26.251: INFO: Got endpoints: latency-svc-x5ld2 [182.022563ms]
Apr 28 16:26:26.255: INFO: Created: latency-svc-g2zxk
Apr 28 16:26:26.258: INFO: Created: latency-svc-6g2dt
Apr 28 16:26:26.265: INFO: Got endpoints: latency-svc-g2zxk [230.389686ms]
Apr 28 16:26:26.267: INFO: Got endpoints: latency-svc-6g2dt [225.224436ms]
Apr 28 16:26:26.270: INFO: Created: latency-svc-q4b2h
Apr 28 16:26:26.279: INFO: Got endpoints: latency-svc-q4b2h [199.029862ms]
Apr 28 16:26:26.281: INFO: Created: latency-svc-2qz7p
Apr 28 16:26:26.288: INFO: Got endpoints: latency-svc-2qz7p [191.805934ms]
Apr 28 16:26:26.290: INFO: Created: latency-svc-smv44
Apr 28 16:26:26.296: INFO: Created: latency-svc-k5sbt
Apr 28 16:26:26.297: INFO: Got endpoints: latency-svc-smv44 [172.698024ms]
Apr 28 16:26:26.302: INFO: Got endpoints: latency-svc-k5sbt [169.176275ms]
Apr 28 16:26:26.306: INFO: Created: latency-svc-dcgks
Apr 28 16:26:26.311: INFO: Created: latency-svc-4dqs5
Apr 28 16:26:26.313: INFO: Got endpoints: latency-svc-dcgks [175.786079ms]
Apr 28 16:26:26.321: INFO: Got endpoints: latency-svc-4dqs5 [181.358239ms]
Apr 28 16:26:26.326: INFO: Created: latency-svc-l2rdg
Apr 28 16:26:26.331: INFO: Created: latency-svc-86vwt
Apr 28 16:26:26.334: INFO: Got endpoints: latency-svc-l2rdg [176.367708ms]
Apr 28 16:26:26.337: INFO: Got endpoints: latency-svc-86vwt [156.370141ms]
Apr 28 16:26:26.343: INFO: Created: latency-svc-z5kdq
Apr 28 16:26:26.354: INFO: Created: latency-svc-hs9xr
Apr 28 16:26:26.364: INFO: Got endpoints: latency-svc-z5kdq [172.549951ms]
Apr 28 16:26:26.367: INFO: Got endpoints: latency-svc-hs9xr [32.844343ms]
Apr 28 16:26:26.372: INFO: Created: latency-svc-49w22
Apr 28 16:26:26.378: INFO: Got endpoints: latency-svc-49w22 [166.898258ms]
Apr 28 16:26:26.409: INFO: Created: latency-svc-88n59
Apr 28 16:26:26.420: INFO: Got endpoints: latency-svc-88n59 [194.889264ms]
Apr 28 16:26:26.424: INFO: Created: latency-svc-skp9s
Apr 28 16:26:26.429: INFO: Created: latency-svc-szcjh
Apr 28 16:26:26.430: INFO: Got endpoints: latency-svc-skp9s [195.502856ms]
Apr 28 16:26:26.441: INFO: Got endpoints: latency-svc-szcjh [190.171774ms]
Apr 28 16:26:26.445: INFO: Created: latency-svc-mhkkl
Apr 28 16:26:26.454: INFO: Created: latency-svc-l2pvj
Apr 28 16:26:26.463: INFO: Got endpoints: latency-svc-mhkkl [197.762802ms]
Apr 28 16:26:26.476: INFO: Got endpoints: latency-svc-l2pvj [208.46871ms]
Apr 28 16:26:26.479: INFO: Created: latency-svc-zqbdk
Apr 28 16:26:26.486: INFO: Got endpoints: latency-svc-zqbdk [206.506059ms]
Apr 28 16:26:26.486: INFO: Created: latency-svc-zw9nh
Apr 28 16:26:26.495: INFO: Created: latency-svc-9gf6f
Apr 28 16:26:26.499: INFO: Got endpoints: latency-svc-zw9nh [211.281195ms]
Apr 28 16:26:26.504: INFO: Created: latency-svc-trttz
Apr 28 16:26:26.514: INFO: Got endpoints: latency-svc-9gf6f [216.835206ms]
Apr 28 16:26:26.516: INFO: Created: latency-svc-h277p
Apr 28 16:26:26.524: INFO: Created: latency-svc-g2wtt
Apr 28 16:26:26.533: INFO: Created: latency-svc-dr2hw
Apr 28 16:26:26.539: INFO: Created: latency-svc-wnl8t
Apr 28 16:26:26.551: INFO: Created: latency-svc-l4dbt
Apr 28 16:26:26.557: INFO: Got endpoints: latency-svc-trttz [255.019756ms]
Apr 28 16:26:26.561: INFO: Created: latency-svc-8b7gd
Apr 28 16:26:26.568: INFO: Created: latency-svc-z4kbf
Apr 28 16:26:26.576: INFO: Created: latency-svc-8nv69
Apr 28 16:26:26.581: INFO: Created: latency-svc-tjw6w
Apr 28 16:26:26.585: INFO: Created: latency-svc-rcgn8
Apr 28 16:26:26.595: INFO: Created: latency-svc-kkw92
Apr 28 16:26:26.604: INFO: Created: latency-svc-24s2w
Apr 28 16:26:26.610: INFO: Got endpoints: latency-svc-h277p [296.704607ms]
Apr 28 16:26:26.613: INFO: Created: latency-svc-bdnxt
Apr 28 16:26:26.622: INFO: Created: latency-svc-clkhq
Apr 28 16:26:26.627: INFO: Created: latency-svc-9jwhm
Apr 28 16:26:26.633: INFO: Created: latency-svc-b4vvx
Apr 28 16:26:26.659: INFO: Got endpoints: latency-svc-g2wtt [337.928354ms]
Apr 28 16:26:26.682: INFO: Created: latency-svc-hkhmn
Apr 28 16:26:26.708: INFO: Got endpoints: latency-svc-dr2hw [370.592385ms]
Apr 28 16:26:26.721: INFO: Created: latency-svc-bpr9g
Apr 28 16:26:26.761: INFO: Got endpoints: latency-svc-wnl8t [397.423987ms]
Apr 28 16:26:26.774: INFO: Created: latency-svc-zr8zq
Apr 28 16:26:26.810: INFO: Got endpoints: latency-svc-l4dbt [442.839965ms]
Apr 28 16:26:26.822: INFO: Created: latency-svc-4nmkf
Apr 28 16:26:26.859: INFO: Got endpoints: latency-svc-8b7gd [481.265507ms]
Apr 28 16:26:26.873: INFO: Created: latency-svc-7grfw
Apr 28 16:26:26.909: INFO: Got endpoints: latency-svc-z4kbf [489.449559ms]
Apr 28 16:26:26.921: INFO: Created: latency-svc-gq6nl
Apr 28 16:26:26.960: INFO: Got endpoints: latency-svc-8nv69 [529.823571ms]
Apr 28 16:26:26.976: INFO: Created: latency-svc-v67w8
Apr 28 16:26:27.010: INFO: Got endpoints: latency-svc-tjw6w [568.413241ms]
Apr 28 16:26:27.022: INFO: Created: latency-svc-qcqfs
Apr 28 16:26:27.060: INFO: Got endpoints: latency-svc-rcgn8 [596.796422ms]
Apr 28 16:26:27.071: INFO: Created: latency-svc-p8gt4
Apr 28 16:26:27.111: INFO: Got endpoints: latency-svc-kkw92 [635.311056ms]
Apr 28 16:26:27.127: INFO: Created: latency-svc-6g69g
Apr 28 16:26:27.158: INFO: Got endpoints: latency-svc-24s2w [672.492868ms]
Apr 28 16:26:27.197: INFO: Created: latency-svc-mn99n
Apr 28 16:26:27.210: INFO: Got endpoints: latency-svc-bdnxt [710.828043ms]
Apr 28 16:26:27.228: INFO: Created: latency-svc-bvlqb
Apr 28 16:26:27.260: INFO: Got endpoints: latency-svc-clkhq [746.701569ms]
Apr 28 16:26:27.273: INFO: Created: latency-svc-d5ltm
Apr 28 16:26:27.308: INFO: Got endpoints: latency-svc-9jwhm [750.9244ms]
Apr 28 16:26:27.321: INFO: Created: latency-svc-6bmdn
Apr 28 16:26:27.360: INFO: Got endpoints: latency-svc-b4vvx [749.559687ms]
Apr 28 16:26:27.372: INFO: Created: latency-svc-9hkrd
Apr 28 16:26:27.409: INFO: Got endpoints: latency-svc-hkhmn [750.166276ms]
Apr 28 16:26:27.422: INFO: Created: latency-svc-vnkp6
Apr 28 16:26:27.458: INFO: Got endpoints: latency-svc-bpr9g [749.914472ms]
Apr 28 16:26:27.470: INFO: Created: latency-svc-xzfrf
Apr 28 16:26:27.509: INFO: Got endpoints: latency-svc-zr8zq [747.579406ms]
Apr 28 16:26:27.521: INFO: Created: latency-svc-zgczp
Apr 28 16:26:27.560: INFO: Got endpoints: latency-svc-4nmkf [749.957954ms]
Apr 28 16:26:27.573: INFO: Created: latency-svc-rnggl
Apr 28 16:26:27.611: INFO: Got endpoints: latency-svc-7grfw [752.076145ms]
Apr 28 16:26:27.624: INFO: Created: latency-svc-8x4nd
Apr 28 16:26:27.661: INFO: Got endpoints: latency-svc-gq6nl [751.31304ms]
Apr 28 16:26:27.675: INFO: Created: latency-svc-pszzl
Apr 28 16:26:27.714: INFO: Got endpoints: latency-svc-v67w8 [754.337527ms]
Apr 28 16:26:27.735: INFO: Created: latency-svc-brj66
Apr 28 16:26:27.775: INFO: Got endpoints: latency-svc-qcqfs [765.283765ms]
Apr 28 16:26:27.806: INFO: Created: latency-svc-587fv
Apr 28 16:26:27.827: INFO: Got endpoints: latency-svc-p8gt4 [767.344747ms]
Apr 28 16:26:27.842: INFO: Created: latency-svc-ttpcz
Apr 28 16:26:27.882: INFO: Got endpoints: latency-svc-6g69g [771.157694ms]
Apr 28 16:26:27.908: INFO: Created: latency-svc-pbk2n
Apr 28 16:26:27.914: INFO: Got endpoints: latency-svc-mn99n [756.033716ms]
Apr 28 16:26:27.960: INFO: Created: latency-svc-4hmzf
Apr 28 16:26:27.971: INFO: Got endpoints: latency-svc-bvlqb [761.235515ms]
Apr 28 16:26:27.994: INFO: Created: latency-svc-xx8wp
Apr 28 16:26:28.021: INFO: Got endpoints: latency-svc-d5ltm [760.484995ms]
Apr 28 16:26:28.035: INFO: Created: latency-svc-dsp6l
Apr 28 16:26:28.071: INFO: Got endpoints: latency-svc-6bmdn [762.415392ms]
Apr 28 16:26:28.082: INFO: Created: latency-svc-rwslw
Apr 28 16:26:28.111: INFO: Got endpoints: latency-svc-9hkrd [751.848253ms]
Apr 28 16:26:28.125: INFO: Created: latency-svc-9jtb8
Apr 28 16:26:28.160: INFO: Got endpoints: latency-svc-vnkp6 [750.555455ms]
Apr 28 16:26:28.174: INFO: Created: latency-svc-pp7r9
Apr 28 16:26:28.209: INFO: Got endpoints: latency-svc-xzfrf [750.652448ms]
Apr 28 16:26:28.222: INFO: Created: latency-svc-x78x2
Apr 28 16:26:28.264: INFO: Got endpoints: latency-svc-zgczp [754.890503ms]
Apr 28 16:26:28.278: INFO: Created: latency-svc-x2bjm
Apr 28 16:26:28.312: INFO: Got endpoints: latency-svc-rnggl [751.82279ms]
Apr 28 16:26:28.325: INFO: Created: latency-svc-gzqkl
Apr 28 16:26:28.360: INFO: Got endpoints: latency-svc-8x4nd [749.227652ms]
Apr 28 16:26:28.371: INFO: Created: latency-svc-5jqn4
Apr 28 16:26:28.415: INFO: Got endpoints: latency-svc-pszzl [753.997722ms]
Apr 28 16:26:28.427: INFO: Created: latency-svc-zc7jm
Apr 28 16:26:28.461: INFO: Got endpoints: latency-svc-brj66 [746.298893ms]
Apr 28 16:26:28.474: INFO: Created: latency-svc-jkl2p
Apr 28 16:26:28.515: INFO: Got endpoints: latency-svc-587fv [739.5391ms]
Apr 28 16:26:28.529: INFO: Created: latency-svc-t5lbw
Apr 28 16:26:28.559: INFO: Got endpoints: latency-svc-ttpcz [731.528867ms]
Apr 28 16:26:28.572: INFO: Created: latency-svc-9h878
Apr 28 16:26:28.617: INFO: Got endpoints: latency-svc-pbk2n [734.798549ms]
Apr 28 16:26:28.631: INFO: Created: latency-svc-k8smq
Apr 28 16:26:28.668: INFO: Got endpoints: latency-svc-4hmzf [754.095924ms]
Apr 28 16:26:28.680: INFO: Created: latency-svc-mn8pl
Apr 28 16:26:28.711: INFO: Got endpoints: latency-svc-xx8wp [739.286091ms]
Apr 28 16:26:28.723: INFO: Created: latency-svc-76dzk
Apr 28 16:26:28.759: INFO: Got endpoints: latency-svc-dsp6l [738.349931ms]
Apr 28 16:26:28.772: INFO: Created: latency-svc-99tmp
Apr 28 16:26:28.811: INFO: Got endpoints: latency-svc-rwslw [740.486692ms]
Apr 28 16:26:28.823: INFO: Created: latency-svc-mx2wk
Apr 28 16:26:28.861: INFO: Got endpoints: latency-svc-9jtb8 [749.647084ms]
Apr 28 16:26:28.877: INFO: Created: latency-svc-db28p
Apr 28 16:26:28.910: INFO: Got endpoints: latency-svc-pp7r9 [749.974408ms]
Apr 28 16:26:28.924: INFO: Created: latency-svc-n8ngk
Apr 28 16:26:28.960: INFO: Got endpoints: latency-svc-x78x2 [750.96607ms]
Apr 28 16:26:28.977: INFO: Created: latency-svc-lbwpf
Apr 28 16:26:29.016: INFO: Got endpoints: latency-svc-x2bjm [752.371049ms]
Apr 28 16:26:29.033: INFO: Created: latency-svc-q2fm8
Apr 28 16:26:29.064: INFO: Got endpoints: latency-svc-gzqkl [751.867498ms]
Apr 28 16:26:29.076: INFO: Created: latency-svc-q8lrs
Apr 28 16:26:29.112: INFO: Got endpoints: latency-svc-5jqn4 [751.27496ms]
Apr 28 16:26:29.125: INFO: Created: latency-svc-57wfb
Apr 28 16:26:29.162: INFO: Got endpoints: latency-svc-zc7jm [747.555275ms]
Apr 28 16:26:29.174: INFO: Created: latency-svc-g68vz
Apr 28 16:26:29.210: INFO: Got endpoints: latency-svc-jkl2p [749.165549ms]
Apr 28 16:26:29.223: INFO: Created: latency-svc-7sfxq
Apr 28 16:26:29.261: INFO: Got endpoints: latency-svc-t5lbw [746.003293ms]
Apr 28 16:26:29.274: INFO: Created: latency-svc-cxrd5
Apr 28 16:26:29.309: INFO: Got endpoints: latency-svc-9h878 [750.160395ms]
Apr 28 16:26:29.322: INFO: Created: latency-svc-wk7d9
Apr 28 16:26:29.358: INFO: Got endpoints: latency-svc-k8smq [740.969399ms]
Apr 28 16:26:29.374: INFO: Created: latency-svc-dk57p
Apr 28 16:26:29.409: INFO: Got endpoints: latency-svc-mn8pl [740.696064ms]
Apr 28 16:26:29.420: INFO: Created: latency-svc-mn6kv
Apr 28 16:26:29.460: INFO: Got endpoints: latency-svc-76dzk [749.231943ms]
Apr 28 16:26:29.471: INFO: Created: latency-svc-wbz4d
Apr 28 16:26:29.509: INFO: Got endpoints: latency-svc-99tmp [750.013987ms]
Apr 28 16:26:29.522: INFO: Created: latency-svc-fw9bt
Apr 28 16:26:29.560: INFO: Got endpoints: latency-svc-mx2wk [748.541859ms]
Apr 28 16:26:29.572: INFO: Created: latency-svc-5wkjb
Apr 28 16:26:29.610: INFO: Got endpoints: latency-svc-db28p [748.527092ms]
Apr 28 16:26:29.623: INFO: Created: latency-svc-frjfz
Apr 28 16:26:29.658: INFO: Got endpoints: latency-svc-n8ngk [747.763165ms]
Apr 28 16:26:29.670: INFO: Created: latency-svc-d49pn
Apr 28 16:26:29.712: INFO: Got endpoints: latency-svc-lbwpf [752.354999ms]
Apr 28 16:26:29.725: INFO: Created: latency-svc-gmrnw
Apr 28 16:26:29.761: INFO: Got endpoints: latency-svc-q2fm8 [744.788346ms]
Apr 28 16:26:29.773: INFO: Created: latency-svc-mdcpk
Apr 28 16:26:29.811: INFO: Got endpoints: latency-svc-q8lrs [747.318994ms]
Apr 28 16:26:29.822: INFO: Created: latency-svc-mdvmf
Apr 28 16:26:29.859: INFO: Got endpoints: latency-svc-57wfb [747.31886ms]
Apr 28 16:26:29.872: INFO: Created: latency-svc-h9hnq
Apr 28 16:26:29.910: INFO: Got endpoints: latency-svc-g68vz [747.347666ms]
Apr 28 16:26:29.922: INFO: Created: latency-svc-zf5sj
Apr 28 16:26:29.959: INFO: Got endpoints: latency-svc-7sfxq [749.074448ms]
Apr 28 16:26:29.979: INFO: Created: latency-svc-x795z
Apr 28 16:26:30.012: INFO: Got endpoints: latency-svc-cxrd5 [751.209439ms]
Apr 28 16:26:30.025: INFO: Created: latency-svc-2dlh4
Apr 28 16:26:30.062: INFO: Got endpoints: latency-svc-wk7d9 [753.331391ms]
Apr 28 16:26:30.074: INFO: Created: latency-svc-b89gw
Apr 28 16:26:30.112: INFO: Got endpoints: latency-svc-dk57p [753.266151ms]
Apr 28 16:26:30.130: INFO: Created: latency-svc-2rqmg
Apr 28 16:26:30.162: INFO: Got endpoints: latency-svc-mn6kv [753.305385ms]
Apr 28 16:26:30.177: INFO: Created: latency-svc-xrxlp
Apr 28 16:26:30.210: INFO: Got endpoints: latency-svc-wbz4d [749.988409ms]
Apr 28 16:26:30.223: INFO: Created: latency-svc-qpjkm
Apr 28 16:26:30.259: INFO: Got endpoints: latency-svc-fw9bt [749.30773ms]
Apr 28 16:26:30.270: INFO: Created: latency-svc-7vlhk
Apr 28 16:26:30.311: INFO: Got endpoints: latency-svc-5wkjb [751.14301ms]
Apr 28 16:26:30.323: INFO: Created: latency-svc-t6szb
Apr 28 16:26:30.360: INFO: Got endpoints: latency-svc-frjfz [749.850682ms]
Apr 28 16:26:30.371: INFO: Created: latency-svc-bl2qh
Apr 28 16:26:30.408: INFO: Got endpoints: latency-svc-d49pn [749.952321ms]
Apr 28 16:26:30.422: INFO: Created: latency-svc-zfnvq
Apr 28 16:26:30.468: INFO: Got endpoints: latency-svc-gmrnw [755.573637ms]
Apr 28 16:26:30.480: INFO: Created: latency-svc-w8wdf
Apr 28 16:26:30.510: INFO: Got endpoints: latency-svc-mdcpk [749.307383ms]
Apr 28 16:26:30.522: INFO: Created: latency-svc-nhv8q
Apr 28 16:26:30.560: INFO: Got endpoints: latency-svc-mdvmf [748.817407ms]
Apr 28 16:26:30.572: INFO: Created: latency-svc-mjm64
Apr 28 16:26:30.610: INFO: Got endpoints: latency-svc-h9hnq [750.810444ms]
Apr 28 16:26:30.622: INFO: Created: latency-svc-hq88c
Apr 28 16:26:30.659: INFO: Got endpoints: latency-svc-zf5sj [749.724076ms]
Apr 28 16:26:30.673: INFO: Created: latency-svc-rpcsd
Apr 28 16:26:30.708: INFO: Got endpoints: latency-svc-x795z [749.021675ms]
Apr 28 16:26:30.722: INFO: Created: latency-svc-j6zpc
Apr 28 16:26:30.761: INFO: Got endpoints: latency-svc-2dlh4 [748.865801ms]
Apr 28 16:26:30.773: INFO: Created: latency-svc-jg7vz
Apr 28 16:26:30.809: INFO: Got endpoints: latency-svc-b89gw [747.04348ms]
Apr 28 16:26:30.821: INFO: Created: latency-svc-qvfqb
Apr 28 16:26:30.860: INFO: Got endpoints: latency-svc-2rqmg [748.150924ms]
Apr 28 16:26:30.874: INFO: Created: latency-svc-rv869
Apr 28 16:26:30.910: INFO: Got endpoints: latency-svc-xrxlp [747.51418ms]
Apr 28 16:26:30.924: INFO: Created: latency-svc-kjgjf
Apr 28 16:26:30.958: INFO: Got endpoints: latency-svc-qpjkm [748.354481ms]
Apr 28 16:26:30.979: INFO: Created: latency-svc-c6cn9
Apr 28 16:26:31.019: INFO: Got endpoints: latency-svc-7vlhk [759.880904ms]
Apr 28 16:26:31.035: INFO: Created: latency-svc-9fvtb
Apr 28 16:26:31.060: INFO: Got endpoints: latency-svc-t6szb [749.015576ms]
Apr 28 16:26:31.072: INFO: Created: latency-svc-nsq6w
Apr 28 16:26:31.110: INFO: Got endpoints: latency-svc-bl2qh [750.745815ms]
Apr 28 16:26:31.128: INFO: Created: latency-svc-qvlp8
Apr 28 16:26:31.160: INFO: Got endpoints: latency-svc-zfnvq [751.70412ms]
Apr 28 16:26:31.175: INFO: Created: latency-svc-scvfh
Apr 28 16:26:31.208: INFO: Got endpoints: latency-svc-w8wdf [740.162384ms]
Apr 28 16:26:31.219: INFO: Created: latency-svc-cmd8c
Apr 28 16:26:31.259: INFO: Got endpoints: latency-svc-nhv8q [748.084037ms]
Apr 28 16:26:31.272: INFO: Created: latency-svc-wz22g
Apr 28 16:26:31.314: INFO: Got endpoints: latency-svc-mjm64 [754.263247ms]
Apr 28 16:26:31.326: INFO: Created: latency-svc-82m4w
Apr 28 16:26:31.361: INFO: Got endpoints: latency-svc-hq88c [750.920551ms]
Apr 28 16:26:31.372: INFO: Created: latency-svc-kd98h
Apr 28 16:26:31.408: INFO: Got endpoints: latency-svc-rpcsd [748.945963ms]
Apr 28 16:26:31.425: INFO: Created: latency-svc-rwf2f
Apr 28 16:26:31.460: INFO: Got endpoints: latency-svc-j6zpc [751.830328ms]
Apr 28 16:26:31.473: INFO: Created: latency-svc-5zlj4
Apr 28 16:26:31.514: INFO: Got endpoints: latency-svc-jg7vz [752.845764ms]
Apr 28 16:26:31.526: INFO: Created: latency-svc-c2mdp
Apr 28 16:26:31.574: INFO: Got endpoints: latency-svc-qvfqb [764.984511ms]
Apr 28 16:26:31.588: INFO: Created: latency-svc-pnx2d
Apr 28 16:26:31.612: INFO: Got endpoints: latency-svc-rv869 [752.471281ms]
Apr 28 16:26:31.626: INFO: Created: latency-svc-bq6pv
Apr 28 16:26:31.661: INFO: Got endpoints: latency-svc-kjgjf [751.539066ms]
Apr 28 16:26:31.672: INFO: Created: latency-svc-vvgqk
Apr 28 16:26:31.712: INFO: Got endpoints: latency-svc-c6cn9 [753.846164ms]
Apr 28 16:26:31.728: INFO: Created: latency-svc-fs26b
Apr 28 16:26:31.773: INFO: Got endpoints: latency-svc-9fvtb [754.078236ms]
Apr 28 16:26:31.786: INFO: Created: latency-svc-mkfjr
Apr 28 16:26:31.809: INFO: Got endpoints: latency-svc-nsq6w [749.073596ms]
Apr 28 16:26:31.821: INFO: Created: latency-svc-zjw94
Apr 28 16:26:31.858: INFO: Got endpoints: latency-svc-qvlp8 [747.987186ms]
Apr 28 16:26:31.889: INFO: Created: latency-svc-kjt7t
Apr 28 16:26:31.910: INFO: Got endpoints: latency-svc-scvfh [750.790051ms]
Apr 28 16:26:31.925: INFO: Created: latency-svc-x4r99
Apr 28 16:26:31.960: INFO: Got endpoints: latency-svc-cmd8c [752.240174ms]
Apr 28 16:26:31.973: INFO: Created: latency-svc-79flh
Apr 28 16:26:32.013: INFO: Got endpoints: latency-svc-wz22g [754.353192ms]
Apr 28 16:26:32.028: INFO: Created: latency-svc-4mgcv
Apr 28 16:26:32.062: INFO: Got endpoints: latency-svc-82m4w [747.705823ms]
Apr 28 16:26:32.077: INFO: Created: latency-svc-9cx5s
Apr 28 16:26:32.109: INFO: Got endpoints: latency-svc-kd98h [748.229076ms]
Apr 28 16:26:32.120: INFO: Created: latency-svc-hbh6x
Apr 28 16:26:32.159: INFO: Got endpoints: latency-svc-rwf2f [750.240539ms]
Apr 28 16:26:32.176: INFO: Created: latency-svc-gz727
Apr 28 16:26:32.210: INFO: Got endpoints: latency-svc-5zlj4 [749.796995ms]
Apr 28 16:26:32.222: INFO: Created: latency-svc-kjdpl
Apr 28 16:26:32.259: INFO: Got endpoints: latency-svc-c2mdp [745.669844ms]
Apr 28 16:26:32.277: INFO: Created: latency-svc-4hkvw
Apr 28 16:26:32.307: INFO: Got endpoints: latency-svc-pnx2d [733.027997ms]
Apr 28 16:26:32.323: INFO: Created: latency-svc-bxzh6
Apr 28 16:26:32.359: INFO: Got endpoints: latency-svc-bq6pv [746.545126ms]
Apr 28 16:26:32.372: INFO: Created: latency-svc-ks7l9
Apr 28 16:26:32.408: INFO: Got endpoints: latency-svc-vvgqk [746.876308ms]
Apr 28 16:26:32.420: INFO: Created: latency-svc-7j777
Apr 28 16:26:32.460: INFO: Got endpoints: latency-svc-fs26b [748.070835ms]
Apr 28 16:26:32.476: INFO: Created: latency-svc-2c2fz
Apr 28 16:26:32.509: INFO: Got endpoints: latency-svc-mkfjr [736.421806ms]
Apr 28 16:26:32.523: INFO: Created: latency-svc-skdvf
Apr 28 16:26:32.561: INFO: Got endpoints: latency-svc-zjw94 [751.47791ms]
Apr 28 16:26:32.574: INFO: Created: latency-svc-hppmj
Apr 28 16:26:32.608: INFO: Got endpoints: latency-svc-kjt7t [749.281979ms]
Apr 28 16:26:32.622: INFO: Created: latency-svc-q6tnz
Apr 28 16:26:32.659: INFO: Got endpoints: latency-svc-x4r99 [747.999618ms]
Apr 28 16:26:32.674: INFO: Created: latency-svc-6spmt
Apr 28 16:26:32.710: INFO: Got endpoints: latency-svc-79flh [749.805816ms]
Apr 28 16:26:32.721: INFO: Created: latency-svc-7wq5s
Apr 28 16:26:32.761: INFO: Got endpoints: latency-svc-4mgcv [747.560405ms]
Apr 28 16:26:32.775: INFO: Created: latency-svc-xqv6v
Apr 28 16:26:32.811: INFO: Got endpoints: latency-svc-9cx5s [748.694631ms]
Apr 28 16:26:32.824: INFO: Created: latency-svc-4qk82
Apr 28 16:26:32.862: INFO: Got endpoints: latency-svc-hbh6x [752.476303ms]
Apr 28 16:26:32.874: INFO: Created: latency-svc-pp9ph
Apr 28 16:26:32.909: INFO: Got endpoints: latency-svc-gz727 [750.615545ms]
Apr 28 16:26:32.926: INFO: Created: latency-svc-rflvt
Apr 28 16:26:32.958: INFO: Got endpoints: latency-svc-kjdpl [748.151914ms]
Apr 28 16:26:32.972: INFO: Created: latency-svc-7pwwl
Apr 28 16:26:33.010: INFO: Got endpoints: latency-svc-4hkvw [750.392649ms]
Apr 28 16:26:33.025: INFO: Created: latency-svc-7x4pq
Apr 28 16:26:33.061: INFO: Got endpoints: latency-svc-bxzh6 [753.861578ms]
Apr 28 16:26:33.082: INFO: Created: latency-svc-6l9sv
Apr 28 16:26:33.108: INFO: Got endpoints: latency-svc-ks7l9 [748.966442ms]
Apr 28 16:26:33.121: INFO: Created: latency-svc-25t84
Apr 28 16:26:33.160: INFO: Got endpoints: latency-svc-7j777 [751.619917ms]
Apr 28 16:26:33.175: INFO: Created: latency-svc-s5k99
Apr 28 16:26:33.212: INFO: Got endpoints: latency-svc-2c2fz [752.119266ms]
Apr 28 16:26:33.228: INFO: Created: latency-svc-2bvbp
Apr 28 16:26:33.258: INFO: Got endpoints: latency-svc-skdvf [749.22027ms]
Apr 28 16:26:33.275: INFO: Created: latency-svc-x8656
Apr 28 16:26:33.350: INFO: Got endpoints: latency-svc-hppmj [789.462061ms]
Apr 28 16:26:33.363: INFO: Got endpoints: latency-svc-q6tnz [755.022441ms]
Apr 28 16:26:33.364: INFO: Created: latency-svc-f26vm
Apr 28 16:26:33.383: INFO: Created: latency-svc-lzlq5
Apr 28 16:26:33.410: INFO: Got endpoints: latency-svc-6spmt [751.000037ms]
Apr 28 16:26:33.423: INFO: Created: latency-svc-2r576
Apr 28 16:26:33.460: INFO: Got endpoints: latency-svc-7wq5s [750.248336ms]
Apr 28 16:26:33.471: INFO: Created: latency-svc-kn6ct
Apr 28 16:26:33.511: INFO: Got endpoints: latency-svc-xqv6v [750.394259ms]
Apr 28 16:26:33.525: INFO: Created: latency-svc-f6pk5
Apr 28 16:26:33.559: INFO: Got endpoints: latency-svc-4qk82 [748.544911ms]
Apr 28 16:26:33.575: INFO: Created: latency-svc-xzkfd
Apr 28 16:26:33.610: INFO: Got endpoints: latency-svc-pp9ph [747.996456ms]
Apr 28 16:26:33.625: INFO: Created: latency-svc-9fgdm
Apr 28 16:26:33.662: INFO: Got endpoints: latency-svc-rflvt [752.401513ms]
Apr 28 16:26:33.677: INFO: Created: latency-svc-zsxm9
Apr 28 16:26:33.710: INFO: Got endpoints: latency-svc-7pwwl [751.651198ms]
Apr 28 16:26:33.726: INFO: Created: latency-svc-57dq2
Apr 28 16:26:33.761: INFO: Got endpoints: latency-svc-7x4pq [750.687842ms]
Apr 28 16:26:33.772: INFO: Created: latency-svc-htzbd
Apr 28 16:26:33.811: INFO: Got endpoints: latency-svc-6l9sv [749.130991ms]
Apr 28 16:26:33.859: INFO: Got endpoints: latency-svc-25t84 [751.420425ms]
Apr 28 16:26:33.909: INFO: Got endpoints: latency-svc-s5k99 [748.347874ms]
Apr 28 16:26:33.959: INFO: Got endpoints: latency-svc-2bvbp [746.440336ms]
Apr 28 16:26:34.011: INFO: Got endpoints: latency-svc-x8656 [752.345729ms]
Apr 28 16:26:34.060: INFO: Got endpoints: latency-svc-f26vm [709.735625ms]
Apr 28 16:26:34.113: INFO: Got endpoints: latency-svc-lzlq5 [749.730627ms]
Apr 28 16:26:34.158: INFO: Got endpoints: latency-svc-2r576 [748.514825ms]
Apr 28 16:26:34.211: INFO: Got endpoints: latency-svc-kn6ct [750.230919ms]
Apr 28 16:26:34.261: INFO: Got endpoints: latency-svc-f6pk5 [749.68558ms]
Apr 28 16:26:34.311: INFO: Got endpoints: latency-svc-xzkfd [751.730289ms]
Apr 28 16:26:34.359: INFO: Got endpoints: latency-svc-9fgdm [749.871916ms]
Apr 28 16:26:34.409: INFO: Got endpoints: latency-svc-zsxm9 [747.143107ms]
Apr 28 16:26:34.459: INFO: Got endpoints: latency-svc-57dq2 [749.309276ms]
Apr 28 16:26:34.509: INFO: Got endpoints: latency-svc-htzbd [748.177473ms]
Apr 28 16:26:34.509: INFO: Latencies: [32.844343ms 40.553645ms 52.97917ms 63.6211ms 71.128434ms 88.540064ms 98.154865ms 109.569183ms 125.474512ms 153.115558ms 156.370141ms 162.517714ms 165.340394ms 166.825815ms 166.898258ms 169.074135ms 169.176275ms 172.549951ms 172.698024ms 175.786079ms 176.367708ms 181.358239ms 182.022563ms 186.985989ms 190.171774ms 191.805934ms 194.889264ms 195.502856ms 197.762802ms 199.029862ms 199.453069ms 206.506059ms 208.46871ms 210.213844ms 210.793251ms 211.281195ms 216.835206ms 220.369708ms 225.224436ms 230.389686ms 255.019756ms 296.704607ms 337.928354ms 370.592385ms 397.423987ms 442.839965ms 481.265507ms 489.449559ms 529.823571ms 568.413241ms 596.796422ms 635.311056ms 672.492868ms 709.735625ms 710.828043ms 731.528867ms 733.027997ms 734.798549ms 736.421806ms 738.349931ms 739.286091ms 739.5391ms 740.162384ms 740.486692ms 740.696064ms 740.969399ms 744.788346ms 745.669844ms 746.003293ms 746.298893ms 746.440336ms 746.545126ms 746.701569ms 746.876308ms 747.04348ms 747.143107ms 747.31886ms 747.318994ms 747.347666ms 747.51418ms 747.555275ms 747.560405ms 747.579406ms 747.705823ms 747.763165ms 747.987186ms 747.996456ms 747.999618ms 748.070835ms 748.084037ms 748.150924ms 748.151914ms 748.177473ms 748.229076ms 748.347874ms 748.354481ms 748.514825ms 748.527092ms 748.541859ms 748.544911ms 748.694631ms 748.817407ms 748.865801ms 748.945963ms 748.966442ms 749.015576ms 749.021675ms 749.073596ms 749.074448ms 749.130991ms 749.165549ms 749.22027ms 749.227652ms 749.231943ms 749.281979ms 749.307383ms 749.30773ms 749.309276ms 749.559687ms 749.647084ms 749.68558ms 749.724076ms 749.730627ms 749.796995ms 749.805816ms 749.850682ms 749.871916ms 749.914472ms 749.952321ms 749.957954ms 749.974408ms 749.988409ms 750.013987ms 750.160395ms 750.166276ms 750.230919ms 750.240539ms 750.248336ms 750.392649ms 750.394259ms 750.555455ms 750.615545ms 750.652448ms 750.687842ms 750.745815ms 750.790051ms 750.810444ms 750.920551ms 750.9244ms 750.96607ms 751.000037ms 751.14301ms 751.209439ms 751.27496ms 751.31304ms 751.420425ms 751.47791ms 751.539066ms 751.619917ms 751.651198ms 751.70412ms 751.730289ms 751.82279ms 751.830328ms 751.848253ms 751.867498ms 752.076145ms 752.119266ms 752.240174ms 752.345729ms 752.354999ms 752.371049ms 752.401513ms 752.471281ms 752.476303ms 752.845764ms 753.266151ms 753.305385ms 753.331391ms 753.846164ms 753.861578ms 753.997722ms 754.078236ms 754.095924ms 754.263247ms 754.337527ms 754.353192ms 754.890503ms 755.022441ms 755.573637ms 756.033716ms 759.880904ms 760.484995ms 761.235515ms 762.415392ms 764.984511ms 765.283765ms 767.344747ms 771.157694ms 789.462061ms]
Apr 28 16:26:34.509: INFO: 50 %ile: 748.694631ms
Apr 28 16:26:34.509: INFO: 90 %ile: 753.861578ms
Apr 28 16:26:34.509: INFO: 99 %ile: 771.157694ms
Apr 28 16:26:34.509: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:26:34.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8705" for this suite.

• [SLOW TEST:10.912 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":212,"skipped":3553,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:26:34.528: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9873
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:26:36.749: INFO: Waiting up to 5m0s for pod "client-envvars-68eb16bb-3b3f-4b81-9d62-ed482df543ea" in namespace "pods-9873" to be "Succeeded or Failed"
Apr 28 16:26:36.759: INFO: Pod "client-envvars-68eb16bb-3b3f-4b81-9d62-ed482df543ea": Phase="Pending", Reason="", readiness=false. Elapsed: 10.531046ms
Apr 28 16:26:38.768: INFO: Pod "client-envvars-68eb16bb-3b3f-4b81-9d62-ed482df543ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018984025s
STEP: Saw pod success
Apr 28 16:26:38.768: INFO: Pod "client-envvars-68eb16bb-3b3f-4b81-9d62-ed482df543ea" satisfied condition "Succeeded or Failed"
Apr 28 16:26:38.771: INFO: Trying to get logs from node ip-172-31-13-33 pod client-envvars-68eb16bb-3b3f-4b81-9d62-ed482df543ea container env3cont: <nil>
STEP: delete the pod
Apr 28 16:26:38.797: INFO: Waiting for pod client-envvars-68eb16bb-3b3f-4b81-9d62-ed482df543ea to disappear
Apr 28 16:26:38.802: INFO: Pod client-envvars-68eb16bb-3b3f-4b81-9d62-ed482df543ea no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:26:38.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9873" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":213,"skipped":3563,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:26:38.819: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2686
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 16:26:38.998: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82708e30-7960-445c-8447-3352bbafdad7" in namespace "projected-2686" to be "Succeeded or Failed"
Apr 28 16:26:39.004: INFO: Pod "downwardapi-volume-82708e30-7960-445c-8447-3352bbafdad7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.96681ms
Apr 28 16:26:41.009: INFO: Pod "downwardapi-volume-82708e30-7960-445c-8447-3352bbafdad7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01130105s
STEP: Saw pod success
Apr 28 16:26:41.009: INFO: Pod "downwardapi-volume-82708e30-7960-445c-8447-3352bbafdad7" satisfied condition "Succeeded or Failed"
Apr 28 16:26:41.015: INFO: Trying to get logs from node ip-172-31-13-33 pod downwardapi-volume-82708e30-7960-445c-8447-3352bbafdad7 container client-container: <nil>
STEP: delete the pod
Apr 28 16:26:41.041: INFO: Waiting for pod downwardapi-volume-82708e30-7960-445c-8447-3352bbafdad7 to disappear
Apr 28 16:26:41.048: INFO: Pod downwardapi-volume-82708e30-7960-445c-8447-3352bbafdad7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:26:41.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2686" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":214,"skipped":3639,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:26:41.060: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2135
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-e4b1b9b6-bf0c-4fd0-8cb8-5de2941733de
STEP: Creating configMap with name cm-test-opt-upd-0ea566ef-a7b1-41d5-9bc5-3907ff83b626
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e4b1b9b6-bf0c-4fd0-8cb8-5de2941733de
STEP: Updating configmap cm-test-opt-upd-0ea566ef-a7b1-41d5-9bc5-3907ff83b626
STEP: Creating configMap with name cm-test-opt-create-45fdd5fb-7c64-42a5-8e97-a5759e9b0557
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:26:47.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2135" for this suite.

• [SLOW TEST:6.314 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":215,"skipped":3657,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:26:47.373: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5986
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-8e60c1cc-7cdc-4e39-99db-cfda9cd7361c
STEP: Creating a pod to test consume secrets
Apr 28 16:26:47.559: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a9a1e6d7-3f19-4e24-91ea-2521686e4535" in namespace "projected-5986" to be "Succeeded or Failed"
Apr 28 16:26:47.564: INFO: Pod "pod-projected-secrets-a9a1e6d7-3f19-4e24-91ea-2521686e4535": Phase="Pending", Reason="", readiness=false. Elapsed: 4.563037ms
Apr 28 16:26:49.571: INFO: Pod "pod-projected-secrets-a9a1e6d7-3f19-4e24-91ea-2521686e4535": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012211868s
STEP: Saw pod success
Apr 28 16:26:49.571: INFO: Pod "pod-projected-secrets-a9a1e6d7-3f19-4e24-91ea-2521686e4535" satisfied condition "Succeeded or Failed"
Apr 28 16:26:49.576: INFO: Trying to get logs from node ip-172-31-76-85 pod pod-projected-secrets-a9a1e6d7-3f19-4e24-91ea-2521686e4535 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 28 16:26:49.624: INFO: Waiting for pod pod-projected-secrets-a9a1e6d7-3f19-4e24-91ea-2521686e4535 to disappear
Apr 28 16:26:49.630: INFO: Pod pod-projected-secrets-a9a1e6d7-3f19-4e24-91ea-2521686e4535 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:26:49.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5986" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":216,"skipped":3665,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:26:49.644: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7630
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 28 16:26:49.859: INFO: Waiting up to 5m0s for pod "pod-93ae2a96-e8de-4a42-b0b4-3fe295439b04" in namespace "emptydir-7630" to be "Succeeded or Failed"
Apr 28 16:26:49.863: INFO: Pod "pod-93ae2a96-e8de-4a42-b0b4-3fe295439b04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.24711ms
Apr 28 16:26:51.891: INFO: Pod "pod-93ae2a96-e8de-4a42-b0b4-3fe295439b04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032339953s
STEP: Saw pod success
Apr 28 16:26:51.891: INFO: Pod "pod-93ae2a96-e8de-4a42-b0b4-3fe295439b04" satisfied condition "Succeeded or Failed"
Apr 28 16:26:51.897: INFO: Trying to get logs from node ip-172-31-76-85 pod pod-93ae2a96-e8de-4a42-b0b4-3fe295439b04 container test-container: <nil>
STEP: delete the pod
Apr 28 16:26:51.937: INFO: Waiting for pod pod-93ae2a96-e8de-4a42-b0b4-3fe295439b04 to disappear
Apr 28 16:26:51.945: INFO: Pod pod-93ae2a96-e8de-4a42-b0b4-3fe295439b04 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:26:51.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7630" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":217,"skipped":3665,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:26:51.965: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 28 16:26:52.660: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 28 16:26:54.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224012, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224012, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224012, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224012, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 16:26:57.705: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Apr 28 16:26:58.705: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:26:58.710: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:26:59.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2736" for this suite.
STEP: Destroying namespace "webhook-2736-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:8.051 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":218,"skipped":3684,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:27:00.017: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 28 16:27:00.612: INFO: Waiting up to 5m0s for pod "pod-e084f4d9-2741-4afe-9413-05ddd0fcbb99" in namespace "emptydir-2539" to be "Succeeded or Failed"
Apr 28 16:27:00.628: INFO: Pod "pod-e084f4d9-2741-4afe-9413-05ddd0fcbb99": Phase="Pending", Reason="", readiness=false. Elapsed: 16.11814ms
Apr 28 16:27:02.639: INFO: Pod "pod-e084f4d9-2741-4afe-9413-05ddd0fcbb99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026490667s
STEP: Saw pod success
Apr 28 16:27:02.639: INFO: Pod "pod-e084f4d9-2741-4afe-9413-05ddd0fcbb99" satisfied condition "Succeeded or Failed"
Apr 28 16:27:02.644: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-e084f4d9-2741-4afe-9413-05ddd0fcbb99 container test-container: <nil>
STEP: delete the pod
Apr 28 16:27:02.672: INFO: Waiting for pod pod-e084f4d9-2741-4afe-9413-05ddd0fcbb99 to disappear
Apr 28 16:27:02.682: INFO: Pod pod-e084f4d9-2741-4afe-9413-05ddd0fcbb99 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:27:02.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2539" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":219,"skipped":3686,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:27:02.711: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1937
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 28 16:27:03.350: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 28 16:27:05.366: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224023, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224023, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224023, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224023, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 16:27:08.388: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:27:08.400: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1178-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:27:09.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1937" for this suite.
STEP: Destroying namespace "webhook-1937-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.142 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":220,"skipped":3694,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:27:09.853: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-127
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Apr 28 16:27:12.627: INFO: Successfully updated pod "annotationupdatee8629a02-0a5d-495e-966f-6860193c1893"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:27:14.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-127" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":221,"skipped":3713,"failed":0}

------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:27:14.679: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6552
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:27:16.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6552" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":222,"skipped":3713,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:27:16.893: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1761
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:27:17.066: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr 28 16:27:22.075: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 28 16:27:22.075: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr 28 16:27:24.086: INFO: Creating deployment "test-rollover-deployment"
Apr 28 16:27:24.103: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr 28 16:27:26.114: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr 28 16:27:26.124: INFO: Ensure that both replica sets have 1 created replica
Apr 28 16:27:26.133: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr 28 16:27:26.147: INFO: Updating deployment test-rollover-deployment
Apr 28 16:27:26.147: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr 28 16:27:28.160: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr 28 16:27:28.168: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr 28 16:27:28.176: INFO: all replica sets need to contain the pod-template-hash label
Apr 28 16:27:28.176: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224044, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224044, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224047, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224044, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 28 16:27:30.192: INFO: all replica sets need to contain the pod-template-hash label
Apr 28 16:27:30.192: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224044, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224044, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224047, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224044, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 28 16:27:32.188: INFO: all replica sets need to contain the pod-template-hash label
Apr 28 16:27:32.189: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224044, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224044, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224047, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224044, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 28 16:27:34.189: INFO: all replica sets need to contain the pod-template-hash label
Apr 28 16:27:34.190: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224044, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224044, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224047, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224044, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 28 16:27:36.191: INFO: all replica sets need to contain the pod-template-hash label
Apr 28 16:27:36.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224044, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224044, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224047, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224044, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 28 16:27:38.188: INFO: 
Apr 28 16:27:38.188: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 28 16:27:38.201: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-1761  3e860cce-c03a-4f73-a4e7-1b30e15e449d 25684 2 2021-04-28 16:27:24 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-04-28 16:27:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-28 16:27:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d8cd78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-04-28 16:27:24 +0000 UTC,LastTransitionTime:2021-04-28 16:27:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-04-28 16:27:37 +0000 UTC,LastTransitionTime:2021-04-28 16:27:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 28 16:27:38.211: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-1761  c5dfe6c9-a7c0-4027-a9be-8ba07fa82588 25673 2 2021-04-28 16:27:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 3e860cce-c03a-4f73-a4e7-1b30e15e449d 0xc005fd7997 0xc005fd7998}] []  [{kube-controller-manager Update apps/v1 2021-04-28 16:27:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3e860cce-c03a-4f73-a4e7-1b30e15e449d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005fd7a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 28 16:27:38.211: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr 28 16:27:38.211: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1761  4117daba-b50b-42fb-a161-20a025e7d827 25683 2 2021-04-28 16:27:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 3e860cce-c03a-4f73-a4e7-1b30e15e449d 0xc005fd7887 0xc005fd7888}] []  [{e2e.test Update apps/v1 2021-04-28 16:27:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-28 16:27:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3e860cce-c03a-4f73-a4e7-1b30e15e449d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005fd7928 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 28 16:27:38.211: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-1761  f76667a2-a2dc-4852-b78d-150d1310bc97 25639 2 2021-04-28 16:27:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 3e860cce-c03a-4f73-a4e7-1b30e15e449d 0xc005fd7aa7 0xc005fd7aa8}] []  [{kube-controller-manager Update apps/v1 2021-04-28 16:27:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3e860cce-c03a-4f73-a4e7-1b30e15e449d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005fd7b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 28 16:27:38.216: INFO: Pod "test-rollover-deployment-668db69979-t8p7x" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-t8p7x test-rollover-deployment-668db69979- deployment-1761  4d7b0dbe-c5dc-4eee-90b2-263bd1e49df5 25654 0 2021-04-28 16:27:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 c5dfe6c9-a7c0-4027-a9be-8ba07fa82588 0xc004d28067 0xc004d28068}] []  [{kube-controller-manager Update v1 2021-04-28 16:27:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5dfe6c9-a7c0-4027-a9be-8ba07fa82588\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:27:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.62.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-s7bxd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-s7bxd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-s7bxd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:27:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:27:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:27:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:27:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.33,PodIP:10.1.62.219,StartTime:2021-04-28 16:27:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-28 16:27:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://f0c7e8e115a0406117543303bdaf2d09e577b31f8b7b01896140c4629ea7580c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.62.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:27:38.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1761" for this suite.

• [SLOW TEST:21.340 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":223,"skipped":3731,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:27:38.234: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8254
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 28 16:27:38.409: INFO: Waiting up to 5m0s for pod "pod-553e48d4-117c-4671-aa89-ab2bbe152eb7" in namespace "emptydir-8254" to be "Succeeded or Failed"
Apr 28 16:27:38.421: INFO: Pod "pod-553e48d4-117c-4671-aa89-ab2bbe152eb7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.872128ms
Apr 28 16:27:40.430: INFO: Pod "pod-553e48d4-117c-4671-aa89-ab2bbe152eb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020775624s
STEP: Saw pod success
Apr 28 16:27:40.430: INFO: Pod "pod-553e48d4-117c-4671-aa89-ab2bbe152eb7" satisfied condition "Succeeded or Failed"
Apr 28 16:27:40.435: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-553e48d4-117c-4671-aa89-ab2bbe152eb7 container test-container: <nil>
STEP: delete the pod
Apr 28 16:27:40.462: INFO: Waiting for pod pod-553e48d4-117c-4671-aa89-ab2bbe152eb7 to disappear
Apr 28 16:27:40.467: INFO: Pod pod-553e48d4-117c-4671-aa89-ab2bbe152eb7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:27:40.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8254" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":224,"skipped":3734,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:27:40.487: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-2564
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr 28 16:27:40.990: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Apr 28 16:27:43.016: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224061, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224061, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224061, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224060, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 16:27:46.036: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:27:46.043: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:27:47.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2564" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.175 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":225,"skipped":3775,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:27:47.662: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9830
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 28 16:27:48.252: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 16:27:51.285: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:27:51.294: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5189-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:27:52.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9830" for this suite.
STEP: Destroying namespace "webhook-9830-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":226,"skipped":3783,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:27:52.526: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5722
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-5722
Apr 28 16:27:54.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-5722 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Apr 28 16:27:54.897: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Apr 28 16:27:54.897: INFO: stdout: "iptables"
Apr 28 16:27:54.897: INFO: proxyMode: iptables
Apr 28 16:27:54.913: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 28 16:27:54.919: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-5722
STEP: creating replication controller affinity-nodeport-timeout in namespace services-5722
I0428 16:27:54.952176      24 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5722, replica count: 3
I0428 16:27:58.002427      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 28 16:27:58.022: INFO: Creating new exec pod
Apr 28 16:28:01.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-5722 exec execpod-affinity866c2 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Apr 28 16:28:01.220: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Apr 28 16:28:01.220: INFO: stdout: ""
Apr 28 16:28:01.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-5722 exec execpod-affinity866c2 -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.133 80'
Apr 28 16:28:01.362: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.133 80\nConnection to 10.152.183.133 80 port [tcp/http] succeeded!\n"
Apr 28 16:28:01.362: INFO: stdout: ""
Apr 28 16:28:01.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-5722 exec execpod-affinity866c2 -- /bin/sh -x -c nc -zv -t -w 2 172.31.76.85 32022'
Apr 28 16:28:01.535: INFO: stderr: "+ nc -zv -t -w 2 172.31.76.85 32022\nConnection to 172.31.76.85 32022 port [tcp/32022] succeeded!\n"
Apr 28 16:28:01.535: INFO: stdout: ""
Apr 28 16:28:01.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-5722 exec execpod-affinity866c2 -- /bin/sh -x -c nc -zv -t -w 2 172.31.13.33 32022'
Apr 28 16:28:01.662: INFO: stderr: "+ nc -zv -t -w 2 172.31.13.33 32022\nConnection to 172.31.13.33 32022 port [tcp/32022] succeeded!\n"
Apr 28 16:28:01.662: INFO: stdout: ""
Apr 28 16:28:01.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-5722 exec execpod-affinity866c2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.13.33:32022/ ; done'
Apr 28 16:28:01.842: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n"
Apr 28 16:28:01.842: INFO: stdout: "\naffinity-nodeport-timeout-xjmnb\naffinity-nodeport-timeout-xjmnb\naffinity-nodeport-timeout-xjmnb\naffinity-nodeport-timeout-xjmnb\naffinity-nodeport-timeout-xjmnb\naffinity-nodeport-timeout-xjmnb\naffinity-nodeport-timeout-xjmnb\naffinity-nodeport-timeout-xjmnb\naffinity-nodeport-timeout-xjmnb\naffinity-nodeport-timeout-xjmnb\naffinity-nodeport-timeout-xjmnb\naffinity-nodeport-timeout-xjmnb\naffinity-nodeport-timeout-xjmnb\naffinity-nodeport-timeout-xjmnb\naffinity-nodeport-timeout-xjmnb\naffinity-nodeport-timeout-xjmnb"
Apr 28 16:28:01.842: INFO: Received response from host: affinity-nodeport-timeout-xjmnb
Apr 28 16:28:01.842: INFO: Received response from host: affinity-nodeport-timeout-xjmnb
Apr 28 16:28:01.842: INFO: Received response from host: affinity-nodeport-timeout-xjmnb
Apr 28 16:28:01.842: INFO: Received response from host: affinity-nodeport-timeout-xjmnb
Apr 28 16:28:01.842: INFO: Received response from host: affinity-nodeport-timeout-xjmnb
Apr 28 16:28:01.842: INFO: Received response from host: affinity-nodeport-timeout-xjmnb
Apr 28 16:28:01.842: INFO: Received response from host: affinity-nodeport-timeout-xjmnb
Apr 28 16:28:01.842: INFO: Received response from host: affinity-nodeport-timeout-xjmnb
Apr 28 16:28:01.842: INFO: Received response from host: affinity-nodeport-timeout-xjmnb
Apr 28 16:28:01.842: INFO: Received response from host: affinity-nodeport-timeout-xjmnb
Apr 28 16:28:01.842: INFO: Received response from host: affinity-nodeport-timeout-xjmnb
Apr 28 16:28:01.842: INFO: Received response from host: affinity-nodeport-timeout-xjmnb
Apr 28 16:28:01.842: INFO: Received response from host: affinity-nodeport-timeout-xjmnb
Apr 28 16:28:01.842: INFO: Received response from host: affinity-nodeport-timeout-xjmnb
Apr 28 16:28:01.842: INFO: Received response from host: affinity-nodeport-timeout-xjmnb
Apr 28 16:28:01.842: INFO: Received response from host: affinity-nodeport-timeout-xjmnb
Apr 28 16:28:01.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-5722 exec execpod-affinity866c2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.13.33:32022/'
Apr 28 16:28:01.980: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n"
Apr 28 16:28:01.980: INFO: stdout: "affinity-nodeport-timeout-xjmnb"
Apr 28 16:28:21.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-5722 exec execpod-affinity866c2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.13.33:32022/'
Apr 28 16:28:22.148: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.13.33:32022/\n"
Apr 28 16:28:22.148: INFO: stdout: "affinity-nodeport-timeout-9wmdg"
Apr 28 16:28:22.148: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5722, will wait for the garbage collector to delete the pods
Apr 28 16:28:22.237: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 11.75459ms
Apr 28 16:28:22.837: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 600.149904ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:28:35.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5722" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:42.663 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":227,"skipped":3798,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:28:35.190: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7109
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 28 16:28:36.400: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:28:36.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7109" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":228,"skipped":3804,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:28:36.435: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3711
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:28:36.633: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Apr 28 16:28:36.651: INFO: Number of nodes with available pods: 0
Apr 28 16:28:36.651: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 16:28:37.665: INFO: Number of nodes with available pods: 0
Apr 28 16:28:37.665: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 16:28:38.663: INFO: Number of nodes with available pods: 3
Apr 28 16:28:38.663: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Apr 28 16:28:38.701: INFO: Wrong image for pod: daemon-set-7s2t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:38.701: INFO: Wrong image for pod: daemon-set-bbshg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:38.701: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:39.718: INFO: Wrong image for pod: daemon-set-7s2t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:39.718: INFO: Wrong image for pod: daemon-set-bbshg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:39.718: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:40.713: INFO: Wrong image for pod: daemon-set-7s2t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:40.713: INFO: Wrong image for pod: daemon-set-bbshg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:40.713: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:41.716: INFO: Wrong image for pod: daemon-set-7s2t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:41.716: INFO: Wrong image for pod: daemon-set-bbshg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:41.716: INFO: Pod daemon-set-bbshg is not available
Apr 28 16:28:41.716: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:42.716: INFO: Wrong image for pod: daemon-set-7s2t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:42.716: INFO: Wrong image for pod: daemon-set-bbshg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:42.716: INFO: Pod daemon-set-bbshg is not available
Apr 28 16:28:42.716: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:43.721: INFO: Wrong image for pod: daemon-set-7s2t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:43.721: INFO: Wrong image for pod: daemon-set-bbshg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:43.721: INFO: Pod daemon-set-bbshg is not available
Apr 28 16:28:43.721: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:44.716: INFO: Wrong image for pod: daemon-set-7s2t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:44.716: INFO: Wrong image for pod: daemon-set-bbshg. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:44.716: INFO: Pod daemon-set-bbshg is not available
Apr 28 16:28:44.716: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:45.718: INFO: Wrong image for pod: daemon-set-7s2t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:45.718: INFO: Pod daemon-set-8jtpb is not available
Apr 28 16:28:45.718: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:46.715: INFO: Wrong image for pod: daemon-set-7s2t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:46.715: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:47.715: INFO: Wrong image for pod: daemon-set-7s2t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:47.715: INFO: Pod daemon-set-7s2t2 is not available
Apr 28 16:28:47.715: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:48.714: INFO: Pod daemon-set-lk542 is not available
Apr 28 16:28:48.714: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:49.718: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:50.717: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:50.717: INFO: Pod daemon-set-qjcp2 is not available
Apr 28 16:28:51.717: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:51.718: INFO: Pod daemon-set-qjcp2 is not available
Apr 28 16:28:52.716: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:52.716: INFO: Pod daemon-set-qjcp2 is not available
Apr 28 16:28:53.717: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:53.717: INFO: Pod daemon-set-qjcp2 is not available
Apr 28 16:28:54.716: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:54.716: INFO: Pod daemon-set-qjcp2 is not available
Apr 28 16:28:55.718: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:55.718: INFO: Pod daemon-set-qjcp2 is not available
Apr 28 16:28:56.715: INFO: Wrong image for pod: daemon-set-qjcp2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 28 16:28:56.715: INFO: Pod daemon-set-qjcp2 is not available
Apr 28 16:28:57.717: INFO: Pod daemon-set-7rpnb is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Apr 28 16:28:57.750: INFO: Number of nodes with available pods: 2
Apr 28 16:28:57.750: INFO: Node ip-172-31-76-85 is running more than one daemon pod
Apr 28 16:28:58.766: INFO: Number of nodes with available pods: 3
Apr 28 16:28:58.766: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3711, will wait for the garbage collector to delete the pods
Apr 28 16:28:58.868: INFO: Deleting DaemonSet.extensions daemon-set took: 12.111164ms
Apr 28 16:28:59.468: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.15584ms
Apr 28 16:29:11.775: INFO: Number of nodes with available pods: 0
Apr 28 16:29:11.775: INFO: Number of running nodes: 0, number of available pods: 0
Apr 28 16:29:11.780: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26354"},"items":null}

Apr 28 16:29:11.785: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26354"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:29:11.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3711" for this suite.

• [SLOW TEST:35.384 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":229,"skipped":3813,"failed":0}
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:29:11.818: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-52
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:29:12.022: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:29:18.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-52" for this suite.

• [SLOW TEST:6.581 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":230,"skipped":3813,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:29:18.400: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3171
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-3171
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 28 16:29:18.662: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 28 16:29:18.791: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 28 16:29:20.818: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:29:22.801: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:29:24.801: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:29:26.798: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:29:28.799: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:29:30.800: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:29:32.801: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 28 16:29:32.810: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 28 16:29:34.817: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 28 16:29:36.816: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr 28 16:29:36.825: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Apr 28 16:29:38.864: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Apr 28 16:29:38.864: INFO: Breadth first check of 10.1.62.230 on host 172.31.13.33...
Apr 28 16:29:38.868: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.62.231:9080/dial?request=hostname&protocol=http&host=10.1.62.230&port=8080&tries=1'] Namespace:pod-network-test-3171 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 16:29:38.868: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 16:29:38.984: INFO: Waiting for responses: map[]
Apr 28 16:29:38.984: INFO: reached 10.1.62.230 after 0/1 tries
Apr 28 16:29:38.984: INFO: Breadth first check of 10.1.99.52 on host 172.31.30.112...
Apr 28 16:29:38.990: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.62.231:9080/dial?request=hostname&protocol=http&host=10.1.99.52&port=8080&tries=1'] Namespace:pod-network-test-3171 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 16:29:38.990: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 16:29:39.101: INFO: Waiting for responses: map[]
Apr 28 16:29:39.101: INFO: reached 10.1.99.52 after 0/1 tries
Apr 28 16:29:39.101: INFO: Breadth first check of 10.1.39.36 on host 172.31.76.85...
Apr 28 16:29:39.105: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.62.231:9080/dial?request=hostname&protocol=http&host=10.1.39.36&port=8080&tries=1'] Namespace:pod-network-test-3171 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 16:29:39.105: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 16:29:39.185: INFO: Waiting for responses: map[]
Apr 28 16:29:39.185: INFO: reached 10.1.39.36 after 0/1 tries
Apr 28 16:29:39.185: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:29:39.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3171" for this suite.

• [SLOW TEST:20.803 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":231,"skipped":3879,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:29:39.203: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2029
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:29:39.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2029" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":232,"skipped":3887,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:29:39.500: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1787
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 28 16:29:41.764: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:29:41.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1787" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":233,"skipped":3896,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:29:41.802: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1322
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1322.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1322.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1322.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1322.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1322.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1322.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1322.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1322.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1322.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1322.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1322.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1322.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1322.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 169.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.169_udp@PTR;check="$$(dig +tcp +noall +answer +search 169.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.169_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1322.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1322.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1322.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1322.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1322.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1322.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1322.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1322.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1322.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1322.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1322.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1322.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1322.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 169.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.169_udp@PTR;check="$$(dig +tcp +noall +answer +search 169.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.169_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 28 16:29:44.054: INFO: Unable to read wheezy_udp@dns-test-service.dns-1322.svc.cluster.local from pod dns-1322/dns-test-a883c81c-baee-451e-8e6c-3764d4271d19: the server could not find the requested resource (get pods dns-test-a883c81c-baee-451e-8e6c-3764d4271d19)
Apr 28 16:29:44.060: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1322.svc.cluster.local from pod dns-1322/dns-test-a883c81c-baee-451e-8e6c-3764d4271d19: the server could not find the requested resource (get pods dns-test-a883c81c-baee-451e-8e6c-3764d4271d19)
Apr 28 16:29:44.065: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1322.svc.cluster.local from pod dns-1322/dns-test-a883c81c-baee-451e-8e6c-3764d4271d19: the server could not find the requested resource (get pods dns-test-a883c81c-baee-451e-8e6c-3764d4271d19)
Apr 28 16:29:44.069: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1322.svc.cluster.local from pod dns-1322/dns-test-a883c81c-baee-451e-8e6c-3764d4271d19: the server could not find the requested resource (get pods dns-test-a883c81c-baee-451e-8e6c-3764d4271d19)
Apr 28 16:29:44.103: INFO: Unable to read jessie_udp@dns-test-service.dns-1322.svc.cluster.local from pod dns-1322/dns-test-a883c81c-baee-451e-8e6c-3764d4271d19: the server could not find the requested resource (get pods dns-test-a883c81c-baee-451e-8e6c-3764d4271d19)
Apr 28 16:29:44.109: INFO: Unable to read jessie_tcp@dns-test-service.dns-1322.svc.cluster.local from pod dns-1322/dns-test-a883c81c-baee-451e-8e6c-3764d4271d19: the server could not find the requested resource (get pods dns-test-a883c81c-baee-451e-8e6c-3764d4271d19)
Apr 28 16:29:44.114: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1322.svc.cluster.local from pod dns-1322/dns-test-a883c81c-baee-451e-8e6c-3764d4271d19: the server could not find the requested resource (get pods dns-test-a883c81c-baee-451e-8e6c-3764d4271d19)
Apr 28 16:29:44.118: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1322.svc.cluster.local from pod dns-1322/dns-test-a883c81c-baee-451e-8e6c-3764d4271d19: the server could not find the requested resource (get pods dns-test-a883c81c-baee-451e-8e6c-3764d4271d19)
Apr 28 16:29:44.145: INFO: Lookups using dns-1322/dns-test-a883c81c-baee-451e-8e6c-3764d4271d19 failed for: [wheezy_udp@dns-test-service.dns-1322.svc.cluster.local wheezy_tcp@dns-test-service.dns-1322.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1322.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1322.svc.cluster.local jessie_udp@dns-test-service.dns-1322.svc.cluster.local jessie_tcp@dns-test-service.dns-1322.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1322.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1322.svc.cluster.local]

Apr 28 16:29:49.255: INFO: DNS probes using dns-1322/dns-test-a883c81c-baee-451e-8e6c-3764d4271d19 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:29:49.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1322" for this suite.

• [SLOW TEST:7.544 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":234,"skipped":3920,"failed":0}
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:29:49.347: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1040
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-1e1ca54d-0d7e-4516-a7ee-2e9dd151dee3
STEP: Creating a pod to test consume configMaps
Apr 28 16:29:49.566: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6b445d06-a1ae-49b1-b2ee-5e06f893a742" in namespace "projected-1040" to be "Succeeded or Failed"
Apr 28 16:29:49.570: INFO: Pod "pod-projected-configmaps-6b445d06-a1ae-49b1-b2ee-5e06f893a742": Phase="Pending", Reason="", readiness=false. Elapsed: 4.403394ms
Apr 28 16:29:51.580: INFO: Pod "pod-projected-configmaps-6b445d06-a1ae-49b1-b2ee-5e06f893a742": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013774421s
STEP: Saw pod success
Apr 28 16:29:51.580: INFO: Pod "pod-projected-configmaps-6b445d06-a1ae-49b1-b2ee-5e06f893a742" satisfied condition "Succeeded or Failed"
Apr 28 16:29:51.583: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-projected-configmaps-6b445d06-a1ae-49b1-b2ee-5e06f893a742 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 28 16:29:51.617: INFO: Waiting for pod pod-projected-configmaps-6b445d06-a1ae-49b1-b2ee-5e06f893a742 to disappear
Apr 28 16:29:51.621: INFO: Pod pod-projected-configmaps-6b445d06-a1ae-49b1-b2ee-5e06f893a742 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:29:51.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1040" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":235,"skipped":3920,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:29:51.632: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6553
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 28 16:29:52.163: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 28 16:29:54.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224192, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224192, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224192, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755224192, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 16:29:57.209: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:29:57.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6553" for this suite.
STEP: Destroying namespace "webhook-6553-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.799 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":236,"skipped":3928,"failed":0}
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:29:57.432: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-93ec6a59-45e9-4e63-b0ce-c92a6d39e9ab in namespace container-probe-340
Apr 28 16:29:59.647: INFO: Started pod liveness-93ec6a59-45e9-4e63-b0ce-c92a6d39e9ab in namespace container-probe-340
STEP: checking the pod's current state and verifying that restartCount is present
Apr 28 16:29:59.650: INFO: Initial restart count of pod liveness-93ec6a59-45e9-4e63-b0ce-c92a6d39e9ab is 0
Apr 28 16:30:17.733: INFO: Restart count of pod container-probe-340/liveness-93ec6a59-45e9-4e63-b0ce-c92a6d39e9ab is now 1 (18.083143727s elapsed)
Apr 28 16:30:37.829: INFO: Restart count of pod container-probe-340/liveness-93ec6a59-45e9-4e63-b0ce-c92a6d39e9ab is now 2 (38.178819752s elapsed)
Apr 28 16:30:57.916: INFO: Restart count of pod container-probe-340/liveness-93ec6a59-45e9-4e63-b0ce-c92a6d39e9ab is now 3 (58.26546228s elapsed)
Apr 28 16:31:18.001: INFO: Restart count of pod container-probe-340/liveness-93ec6a59-45e9-4e63-b0ce-c92a6d39e9ab is now 4 (1m18.350623811s elapsed)
Apr 28 16:32:26.319: INFO: Restart count of pod container-probe-340/liveness-93ec6a59-45e9-4e63-b0ce-c92a6d39e9ab is now 5 (2m26.668594528s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:32:26.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-340" for this suite.

• [SLOW TEST:148.918 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":237,"skipped":3928,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:32:26.350: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7371
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-cca54ae2-bbed-4dff-9a63-f1ee24f9a1d8
STEP: Creating secret with name secret-projected-all-test-volume-33213d5d-ad80-4dc1-9e53-c12c3576fb0f
STEP: Creating a pod to test Check all projections for projected volume plugin
Apr 28 16:32:26.545: INFO: Waiting up to 5m0s for pod "projected-volume-307b2bee-83fd-44ec-86c9-4bc7175785d2" in namespace "projected-7371" to be "Succeeded or Failed"
Apr 28 16:32:26.552: INFO: Pod "projected-volume-307b2bee-83fd-44ec-86c9-4bc7175785d2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.799462ms
Apr 28 16:32:28.559: INFO: Pod "projected-volume-307b2bee-83fd-44ec-86c9-4bc7175785d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01322806s
STEP: Saw pod success
Apr 28 16:32:28.559: INFO: Pod "projected-volume-307b2bee-83fd-44ec-86c9-4bc7175785d2" satisfied condition "Succeeded or Failed"
Apr 28 16:32:28.565: INFO: Trying to get logs from node ip-172-31-13-33 pod projected-volume-307b2bee-83fd-44ec-86c9-4bc7175785d2 container projected-all-volume-test: <nil>
STEP: delete the pod
Apr 28 16:32:28.610: INFO: Waiting for pod projected-volume-307b2bee-83fd-44ec-86c9-4bc7175785d2 to disappear
Apr 28 16:32:28.615: INFO: Pod projected-volume-307b2bee-83fd-44ec-86c9-4bc7175785d2 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:32:28.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7371" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":238,"skipped":3956,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:32:28.632: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5898
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Apr 28 16:32:28.853: INFO: Waiting up to 5m0s for pod "var-expansion-64dd5cbb-ead0-490f-9d57-75a158943d26" in namespace "var-expansion-5898" to be "Succeeded or Failed"
Apr 28 16:32:28.860: INFO: Pod "var-expansion-64dd5cbb-ead0-490f-9d57-75a158943d26": Phase="Pending", Reason="", readiness=false. Elapsed: 6.698079ms
Apr 28 16:32:30.871: INFO: Pod "var-expansion-64dd5cbb-ead0-490f-9d57-75a158943d26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017219203s
STEP: Saw pod success
Apr 28 16:32:30.871: INFO: Pod "var-expansion-64dd5cbb-ead0-490f-9d57-75a158943d26" satisfied condition "Succeeded or Failed"
Apr 28 16:32:30.876: INFO: Trying to get logs from node ip-172-31-13-33 pod var-expansion-64dd5cbb-ead0-490f-9d57-75a158943d26 container dapi-container: <nil>
STEP: delete the pod
Apr 28 16:32:30.903: INFO: Waiting for pod var-expansion-64dd5cbb-ead0-490f-9d57-75a158943d26 to disappear
Apr 28 16:32:30.908: INFO: Pod var-expansion-64dd5cbb-ead0-490f-9d57-75a158943d26 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:32:30.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5898" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":239,"skipped":3967,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:32:30.922: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8808
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 28 16:32:31.109: INFO: Waiting up to 5m0s for pod "pod-ff394a2b-5860-4efe-aee5-510ed705f821" in namespace "emptydir-8808" to be "Succeeded or Failed"
Apr 28 16:32:31.120: INFO: Pod "pod-ff394a2b-5860-4efe-aee5-510ed705f821": Phase="Pending", Reason="", readiness=false. Elapsed: 11.189847ms
Apr 28 16:32:33.130: INFO: Pod "pod-ff394a2b-5860-4efe-aee5-510ed705f821": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021615591s
STEP: Saw pod success
Apr 28 16:32:33.130: INFO: Pod "pod-ff394a2b-5860-4efe-aee5-510ed705f821" satisfied condition "Succeeded or Failed"
Apr 28 16:32:33.134: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-ff394a2b-5860-4efe-aee5-510ed705f821 container test-container: <nil>
STEP: delete the pod
Apr 28 16:32:33.160: INFO: Waiting for pod pod-ff394a2b-5860-4efe-aee5-510ed705f821 to disappear
Apr 28 16:32:33.164: INFO: Pod pod-ff394a2b-5860-4efe-aee5-510ed705f821 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:32:33.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8808" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":240,"skipped":3985,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:32:33.178: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5537
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:32:49.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5537" for this suite.

• [SLOW TEST:16.347 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":241,"skipped":4000,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:32:49.525: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2598
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-2598
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 28 16:32:49.687: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 28 16:32:49.763: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 28 16:32:51.772: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:32:53.777: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:32:55.772: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:32:57.769: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:32:59.774: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:33:01.773: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:33:03.772: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:33:05.787: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:33:07.772: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 28 16:33:07.781: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 28 16:33:09.789: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr 28 16:33:09.798: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Apr 28 16:33:11.849: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Apr 28 16:33:11.849: INFO: Going to poll 10.1.62.240 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Apr 28 16:33:11.852: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.1.62.240 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2598 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 16:33:11.852: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 16:33:12.948: INFO: Found all 1 expected endpoints: [netserver-0]
Apr 28 16:33:12.948: INFO: Going to poll 10.1.99.53 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Apr 28 16:33:12.955: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.1.99.53 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2598 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 16:33:12.955: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 16:33:14.051: INFO: Found all 1 expected endpoints: [netserver-1]
Apr 28 16:33:14.051: INFO: Going to poll 10.1.39.37 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Apr 28 16:33:14.059: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.1.39.37 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2598 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 16:33:14.059: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 16:33:15.150: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:33:15.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2598" for this suite.

• [SLOW TEST:25.650 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":242,"skipped":4002,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:33:15.176: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9685
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0428 16:33:25.489660      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0428 16:33:25.489675      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0428 16:33:25.489679      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 28 16:33:25.489: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Apr 28 16:33:25.489: INFO: Deleting pod "simpletest-rc-to-be-deleted-684qr" in namespace "gc-9685"
Apr 28 16:33:25.527: INFO: Deleting pod "simpletest-rc-to-be-deleted-8srz2" in namespace "gc-9685"
Apr 28 16:33:25.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkrf4" in namespace "gc-9685"
Apr 28 16:33:25.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9bbf" in namespace "gc-9685"
Apr 28 16:33:25.594: INFO: Deleting pod "simpletest-rc-to-be-deleted-ksrzj" in namespace "gc-9685"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:33:25.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9685" for this suite.

• [SLOW TEST:10.465 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":243,"skipped":4039,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:33:25.641: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 28 16:33:28.411: INFO: Successfully updated pod "pod-update-activedeadlineseconds-92d0c35e-c131-4323-a24f-340e91698aa0"
Apr 28 16:33:28.411: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-92d0c35e-c131-4323-a24f-340e91698aa0" in namespace "pods-9271" to be "terminated due to deadline exceeded"
Apr 28 16:33:28.416: INFO: Pod "pod-update-activedeadlineseconds-92d0c35e-c131-4323-a24f-340e91698aa0": Phase="Running", Reason="", readiness=true. Elapsed: 5.013195ms
Apr 28 16:33:30.423: INFO: Pod "pod-update-activedeadlineseconds-92d0c35e-c131-4323-a24f-340e91698aa0": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.012642141s
Apr 28 16:33:30.424: INFO: Pod "pod-update-activedeadlineseconds-92d0c35e-c131-4323-a24f-340e91698aa0" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:33:30.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9271" for this suite.
•{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":244,"skipped":4045,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:33:30.439: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-1369
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:33:30.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1369" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":245,"skipped":4050,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:33:30.653: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-500
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-db55d339-ecb9-420a-bae0-53a94cb2412f
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-db55d339-ecb9-420a-bae0-53a94cb2412f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:35:01.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-500" for this suite.

• [SLOW TEST:90.717 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":246,"skipped":4050,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:35:01.370: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-632
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Apr 28 16:35:01.570: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-632  769d0e1f-ad09-4dc6-9406-f32517494986 27942 0 2021-04-28 16:35:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-28 16:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 28 16:35:01.570: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-632  769d0e1f-ad09-4dc6-9406-f32517494986 27943 0 2021-04-28 16:35:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-28 16:35:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 28 16:35:01.570: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-632  769d0e1f-ad09-4dc6-9406-f32517494986 27944 0 2021-04-28 16:35:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-28 16:35:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Apr 28 16:35:11.621: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-632  769d0e1f-ad09-4dc6-9406-f32517494986 27976 0 2021-04-28 16:35:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-28 16:35:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 28 16:35:11.621: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-632  769d0e1f-ad09-4dc6-9406-f32517494986 27977 0 2021-04-28 16:35:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-28 16:35:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 28 16:35:11.621: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-632  769d0e1f-ad09-4dc6-9406-f32517494986 27978 0 2021-04-28 16:35:01 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-28 16:35:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:35:11.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-632" for this suite.

• [SLOW TEST:10.266 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":247,"skipped":4056,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:35:11.636: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9146
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Apr 28 16:35:11.820: INFO: Waiting up to 5m0s for pod "pod-cf04ada1-e18c-46be-959b-a236c8839c0e" in namespace "emptydir-9146" to be "Succeeded or Failed"
Apr 28 16:35:11.830: INFO: Pod "pod-cf04ada1-e18c-46be-959b-a236c8839c0e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.309987ms
Apr 28 16:35:13.838: INFO: Pod "pod-cf04ada1-e18c-46be-959b-a236c8839c0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018094273s
STEP: Saw pod success
Apr 28 16:35:13.838: INFO: Pod "pod-cf04ada1-e18c-46be-959b-a236c8839c0e" satisfied condition "Succeeded or Failed"
Apr 28 16:35:13.844: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-cf04ada1-e18c-46be-959b-a236c8839c0e container test-container: <nil>
STEP: delete the pod
Apr 28 16:35:13.875: INFO: Waiting for pod pod-cf04ada1-e18c-46be-959b-a236c8839c0e to disappear
Apr 28 16:35:13.883: INFO: Pod pod-cf04ada1-e18c-46be-959b-a236c8839c0e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:35:13.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9146" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":248,"skipped":4084,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:35:13.895: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-863a8bb9-0454-443c-88b6-a6ed2ec623bb
STEP: Creating a pod to test consume secrets
Apr 28 16:35:14.091: INFO: Waiting up to 5m0s for pod "pod-secrets-ca27258f-6c25-484f-8abd-523da1676de1" in namespace "secrets-9553" to be "Succeeded or Failed"
Apr 28 16:35:14.103: INFO: Pod "pod-secrets-ca27258f-6c25-484f-8abd-523da1676de1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.721683ms
Apr 28 16:35:16.113: INFO: Pod "pod-secrets-ca27258f-6c25-484f-8abd-523da1676de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022190979s
STEP: Saw pod success
Apr 28 16:35:16.113: INFO: Pod "pod-secrets-ca27258f-6c25-484f-8abd-523da1676de1" satisfied condition "Succeeded or Failed"
Apr 28 16:35:16.117: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-secrets-ca27258f-6c25-484f-8abd-523da1676de1 container secret-volume-test: <nil>
STEP: delete the pod
Apr 28 16:35:16.146: INFO: Waiting for pod pod-secrets-ca27258f-6c25-484f-8abd-523da1676de1 to disappear
Apr 28 16:35:16.153: INFO: Pod pod-secrets-ca27258f-6c25-484f-8abd-523da1676de1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:35:16.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9553" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":249,"skipped":4107,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:35:16.169: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0428 16:35:22.388634      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0428 16:35:22.388649      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0428 16:35:22.388653      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 28 16:35:22.388: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:35:22.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8401" for this suite.

• [SLOW TEST:6.233 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":250,"skipped":4117,"failed":0}
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:35:22.402: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3715
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0428 16:36:02.612739      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0428 16:36:02.612756      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0428 16:36:02.612759      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 28 16:36:02.612: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Apr 28 16:36:02.612: INFO: Deleting pod "simpletest.rc-5q5zq" in namespace "gc-3715"
Apr 28 16:36:02.628: INFO: Deleting pod "simpletest.rc-85qkf" in namespace "gc-3715"
Apr 28 16:36:02.653: INFO: Deleting pod "simpletest.rc-92vzx" in namespace "gc-3715"
Apr 28 16:36:02.668: INFO: Deleting pod "simpletest.rc-fgrhc" in namespace "gc-3715"
Apr 28 16:36:02.685: INFO: Deleting pod "simpletest.rc-jrwdh" in namespace "gc-3715"
Apr 28 16:36:02.705: INFO: Deleting pod "simpletest.rc-lmltv" in namespace "gc-3715"
Apr 28 16:36:02.723: INFO: Deleting pod "simpletest.rc-mkhfb" in namespace "gc-3715"
Apr 28 16:36:02.751: INFO: Deleting pod "simpletest.rc-nq9ts" in namespace "gc-3715"
Apr 28 16:36:02.769: INFO: Deleting pod "simpletest.rc-qhgh4" in namespace "gc-3715"
Apr 28 16:36:02.784: INFO: Deleting pod "simpletest.rc-vz5ss" in namespace "gc-3715"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:36:02.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3715" for this suite.

• [SLOW TEST:40.412 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":251,"skipped":4117,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:36:02.815: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3787
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:36:19.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3787" for this suite.

• [SLOW TEST:16.388 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":252,"skipped":4141,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:36:19.203: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-516
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-21e8a834-d74f-463b-acdf-beb8120e017b
STEP: Creating a pod to test consume configMaps
Apr 28 16:36:19.412: INFO: Waiting up to 5m0s for pod "pod-configmaps-a7d9acb5-3482-431e-8d73-531d3fa7bf48" in namespace "configmap-516" to be "Succeeded or Failed"
Apr 28 16:36:19.422: INFO: Pod "pod-configmaps-a7d9acb5-3482-431e-8d73-531d3fa7bf48": Phase="Pending", Reason="", readiness=false. Elapsed: 9.698849ms
Apr 28 16:36:21.432: INFO: Pod "pod-configmaps-a7d9acb5-3482-431e-8d73-531d3fa7bf48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019500025s
STEP: Saw pod success
Apr 28 16:36:21.432: INFO: Pod "pod-configmaps-a7d9acb5-3482-431e-8d73-531d3fa7bf48" satisfied condition "Succeeded or Failed"
Apr 28 16:36:21.437: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-configmaps-a7d9acb5-3482-431e-8d73-531d3fa7bf48 container agnhost-container: <nil>
STEP: delete the pod
Apr 28 16:36:21.462: INFO: Waiting for pod pod-configmaps-a7d9acb5-3482-431e-8d73-531d3fa7bf48 to disappear
Apr 28 16:36:21.466: INFO: Pod pod-configmaps-a7d9acb5-3482-431e-8d73-531d3fa7bf48 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:36:21.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-516" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":253,"skipped":4159,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:36:21.481: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8666
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 28 16:36:21.668: INFO: Waiting up to 5m0s for pod "pod-8e183809-54b1-410c-a261-de49d3f76bd4" in namespace "emptydir-8666" to be "Succeeded or Failed"
Apr 28 16:36:21.674: INFO: Pod "pod-8e183809-54b1-410c-a261-de49d3f76bd4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.641925ms
Apr 28 16:36:23.691: INFO: Pod "pod-8e183809-54b1-410c-a261-de49d3f76bd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022440887s
STEP: Saw pod success
Apr 28 16:36:23.691: INFO: Pod "pod-8e183809-54b1-410c-a261-de49d3f76bd4" satisfied condition "Succeeded or Failed"
Apr 28 16:36:23.695: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-8e183809-54b1-410c-a261-de49d3f76bd4 container test-container: <nil>
STEP: delete the pod
Apr 28 16:36:23.736: INFO: Waiting for pod pod-8e183809-54b1-410c-a261-de49d3f76bd4 to disappear
Apr 28 16:36:23.744: INFO: Pod pod-8e183809-54b1-410c-a261-de49d3f76bd4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:36:23.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8666" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":254,"skipped":4199,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:36:23.760: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9684
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:36:23.935: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Apr 28 16:36:25.986: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:36:26.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9684" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":255,"skipped":4217,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:36:27.012: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8537
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8537
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8537
I0428 16:36:27.260518      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8537, replica count: 2
I0428 16:36:30.310861      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 28 16:36:30.310: INFO: Creating new exec pod
Apr 28 16:36:33.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-8537 exec execpodrkbbv -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Apr 28 16:36:33.733: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 28 16:36:33.733: INFO: stdout: ""
Apr 28 16:36:33.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-8537 exec execpodrkbbv -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.181 80'
Apr 28 16:36:33.857: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.181 80\nConnection to 10.152.183.181 80 port [tcp/http] succeeded!\n"
Apr 28 16:36:33.857: INFO: stdout: ""
Apr 28 16:36:33.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-8537 exec execpodrkbbv -- /bin/sh -x -c nc -zv -t -w 2 172.31.76.85 32749'
Apr 28 16:36:33.983: INFO: stderr: "+ nc -zv -t -w 2 172.31.76.85 32749\nConnection to 172.31.76.85 32749 port [tcp/32749] succeeded!\n"
Apr 28 16:36:33.983: INFO: stdout: ""
Apr 28 16:36:33.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-8537 exec execpodrkbbv -- /bin/sh -x -c nc -zv -t -w 2 172.31.30.112 32749'
Apr 28 16:36:34.118: INFO: stderr: "+ nc -zv -t -w 2 172.31.30.112 32749\nConnection to 172.31.30.112 32749 port [tcp/32749] succeeded!\n"
Apr 28 16:36:34.118: INFO: stdout: ""
Apr 28 16:36:34.118: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:36:34.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8537" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:7.155 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":256,"skipped":4220,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:36:34.167: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2300
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Apr 28 16:36:34.351: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 16:36:36.072: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:36:46.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2300" for this suite.

• [SLOW TEST:12.670 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":257,"skipped":4251,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:36:46.837: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3957
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:36:47.048: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Apr 28 16:36:47.061: INFO: Number of nodes with available pods: 0
Apr 28 16:36:47.061: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Apr 28 16:36:47.093: INFO: Number of nodes with available pods: 0
Apr 28 16:36:47.093: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 16:36:48.100: INFO: Number of nodes with available pods: 1
Apr 28 16:36:48.100: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Apr 28 16:36:48.129: INFO: Number of nodes with available pods: 1
Apr 28 16:36:48.129: INFO: Number of running nodes: 0, number of available pods: 1
Apr 28 16:36:49.136: INFO: Number of nodes with available pods: 0
Apr 28 16:36:49.136: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Apr 28 16:36:49.151: INFO: Number of nodes with available pods: 0
Apr 28 16:36:49.151: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 16:36:50.158: INFO: Number of nodes with available pods: 0
Apr 28 16:36:50.158: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 16:36:51.158: INFO: Number of nodes with available pods: 0
Apr 28 16:36:51.158: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 16:36:52.161: INFO: Number of nodes with available pods: 0
Apr 28 16:36:52.161: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 16:36:53.158: INFO: Number of nodes with available pods: 1
Apr 28 16:36:53.158: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3957, will wait for the garbage collector to delete the pods
Apr 28 16:36:53.231: INFO: Deleting DaemonSet.extensions daemon-set took: 11.155383ms
Apr 28 16:36:53.832: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.141449ms
Apr 28 16:37:05.139: INFO: Number of nodes with available pods: 0
Apr 28 16:37:05.139: INFO: Number of running nodes: 0, number of available pods: 0
Apr 28 16:37:05.143: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29045"},"items":null}

Apr 28 16:37:05.148: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29045"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:37:05.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3957" for this suite.

• [SLOW TEST:18.354 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":258,"skipped":4261,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:37:05.192: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4724
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 28 16:37:05.380: INFO: Waiting up to 5m0s for pod "pod-01e60a4b-8d94-4f02-a296-9afb7d58954a" in namespace "emptydir-4724" to be "Succeeded or Failed"
Apr 28 16:37:05.389: INFO: Pod "pod-01e60a4b-8d94-4f02-a296-9afb7d58954a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.857703ms
Apr 28 16:37:07.404: INFO: Pod "pod-01e60a4b-8d94-4f02-a296-9afb7d58954a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02377496s
STEP: Saw pod success
Apr 28 16:37:07.404: INFO: Pod "pod-01e60a4b-8d94-4f02-a296-9afb7d58954a" satisfied condition "Succeeded or Failed"
Apr 28 16:37:07.410: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-01e60a4b-8d94-4f02-a296-9afb7d58954a container test-container: <nil>
STEP: delete the pod
Apr 28 16:37:07.444: INFO: Waiting for pod pod-01e60a4b-8d94-4f02-a296-9afb7d58954a to disappear
Apr 28 16:37:07.450: INFO: Pod pod-01e60a4b-8d94-4f02-a296-9afb7d58954a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:37:07.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4724" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":259,"skipped":4264,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:37:07.471: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1866
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1866
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1866
STEP: creating replication controller externalsvc in namespace services-1866
I0428 16:37:07.682423      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1866, replica count: 2
I0428 16:37:10.732678      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Apr 28 16:37:10.764: INFO: Creating new exec pod
Apr 28 16:37:12.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-1866 exec execpodcnpqc -- /bin/sh -x -c nslookup clusterip-service.services-1866.svc.cluster.local'
Apr 28 16:37:12.963: INFO: stderr: "+ nslookup clusterip-service.services-1866.svc.cluster.local\n"
Apr 28 16:37:12.963: INFO: stdout: "Server:\t\t10.152.183.87\nAddress:\t10.152.183.87#53\n\nclusterip-service.services-1866.svc.cluster.local\tcanonical name = externalsvc.services-1866.svc.cluster.local.\nName:\texternalsvc.services-1866.svc.cluster.local\nAddress: 10.152.183.178\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1866, will wait for the garbage collector to delete the pods
Apr 28 16:37:13.032: INFO: Deleting ReplicationController externalsvc took: 13.102849ms
Apr 28 16:37:13.632: INFO: Terminating ReplicationController externalsvc pods took: 600.139653ms
Apr 28 16:37:25.185: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:37:25.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1866" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:17.766 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":260,"skipped":4266,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:37:25.237: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1661
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Apr 28 16:37:25.429: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-1661 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:37:25.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1661" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":261,"skipped":4282,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:37:25.513: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:37:25.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-869" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":262,"skipped":4316,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:37:25.785: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Apr 28 16:37:25.961: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-4381 proxy --unix-socket=/tmp/kubectl-proxy-unix655872900/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:37:26.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4381" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":263,"skipped":4325,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:37:26.016: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-4373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:37:26.537: INFO: Checking APIGroup: apiregistration.k8s.io
Apr 28 16:37:26.538: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Apr 28 16:37:26.538: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Apr 28 16:37:26.538: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Apr 28 16:37:26.538: INFO: Checking APIGroup: apps
Apr 28 16:37:26.540: INFO: PreferredVersion.GroupVersion: apps/v1
Apr 28 16:37:26.540: INFO: Versions found [{apps/v1 v1}]
Apr 28 16:37:26.540: INFO: apps/v1 matches apps/v1
Apr 28 16:37:26.540: INFO: Checking APIGroup: events.k8s.io
Apr 28 16:37:26.541: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Apr 28 16:37:26.541: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Apr 28 16:37:26.541: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Apr 28 16:37:26.541: INFO: Checking APIGroup: authentication.k8s.io
Apr 28 16:37:26.543: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Apr 28 16:37:26.543: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Apr 28 16:37:26.543: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Apr 28 16:37:26.543: INFO: Checking APIGroup: authorization.k8s.io
Apr 28 16:37:26.544: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Apr 28 16:37:26.544: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Apr 28 16:37:26.544: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Apr 28 16:37:26.544: INFO: Checking APIGroup: autoscaling
Apr 28 16:37:26.545: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Apr 28 16:37:26.545: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Apr 28 16:37:26.545: INFO: autoscaling/v1 matches autoscaling/v1
Apr 28 16:37:26.545: INFO: Checking APIGroup: batch
Apr 28 16:37:26.546: INFO: PreferredVersion.GroupVersion: batch/v1
Apr 28 16:37:26.546: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Apr 28 16:37:26.546: INFO: batch/v1 matches batch/v1
Apr 28 16:37:26.546: INFO: Checking APIGroup: certificates.k8s.io
Apr 28 16:37:26.547: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Apr 28 16:37:26.547: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Apr 28 16:37:26.547: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Apr 28 16:37:26.547: INFO: Checking APIGroup: networking.k8s.io
Apr 28 16:37:26.549: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Apr 28 16:37:26.549: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Apr 28 16:37:26.549: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Apr 28 16:37:26.549: INFO: Checking APIGroup: extensions
Apr 28 16:37:26.550: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Apr 28 16:37:26.550: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Apr 28 16:37:26.550: INFO: extensions/v1beta1 matches extensions/v1beta1
Apr 28 16:37:26.550: INFO: Checking APIGroup: policy
Apr 28 16:37:26.551: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Apr 28 16:37:26.551: INFO: Versions found [{policy/v1beta1 v1beta1}]
Apr 28 16:37:26.551: INFO: policy/v1beta1 matches policy/v1beta1
Apr 28 16:37:26.551: INFO: Checking APIGroup: rbac.authorization.k8s.io
Apr 28 16:37:26.552: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Apr 28 16:37:26.552: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Apr 28 16:37:26.552: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Apr 28 16:37:26.552: INFO: Checking APIGroup: storage.k8s.io
Apr 28 16:37:26.553: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Apr 28 16:37:26.553: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Apr 28 16:37:26.553: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Apr 28 16:37:26.553: INFO: Checking APIGroup: admissionregistration.k8s.io
Apr 28 16:37:26.555: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Apr 28 16:37:26.555: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Apr 28 16:37:26.555: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Apr 28 16:37:26.555: INFO: Checking APIGroup: apiextensions.k8s.io
Apr 28 16:37:26.557: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Apr 28 16:37:26.557: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Apr 28 16:37:26.557: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Apr 28 16:37:26.557: INFO: Checking APIGroup: scheduling.k8s.io
Apr 28 16:37:26.558: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Apr 28 16:37:26.558: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Apr 28 16:37:26.558: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Apr 28 16:37:26.558: INFO: Checking APIGroup: coordination.k8s.io
Apr 28 16:37:26.559: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Apr 28 16:37:26.559: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Apr 28 16:37:26.559: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Apr 28 16:37:26.559: INFO: Checking APIGroup: node.k8s.io
Apr 28 16:37:26.561: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Apr 28 16:37:26.561: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Apr 28 16:37:26.561: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Apr 28 16:37:26.561: INFO: Checking APIGroup: discovery.k8s.io
Apr 28 16:37:26.562: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Apr 28 16:37:26.562: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Apr 28 16:37:26.562: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Apr 28 16:37:26.562: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Apr 28 16:37:26.563: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Apr 28 16:37:26.563: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Apr 28 16:37:26.563: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Apr 28 16:37:26.563: INFO: Checking APIGroup: metrics.k8s.io
Apr 28 16:37:26.564: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Apr 28 16:37:26.564: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Apr 28 16:37:26.564: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:37:26.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-4373" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":264,"skipped":4367,"failed":0}

------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:37:26.578: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9057
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9057
STEP: creating service affinity-nodeport-transition in namespace services-9057
STEP: creating replication controller affinity-nodeport-transition in namespace services-9057
I0428 16:37:26.778933      24 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-9057, replica count: 3
I0428 16:37:29.829166      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 28 16:37:29.845: INFO: Creating new exec pod
Apr 28 16:37:32.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9057 exec execpod-affinity2ts7k -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Apr 28 16:37:33.035: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Apr 28 16:37:33.035: INFO: stdout: ""
Apr 28 16:37:33.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9057 exec execpod-affinity2ts7k -- /bin/sh -x -c nc -zv -t -w 2 10.152.183.164 80'
Apr 28 16:37:33.163: INFO: stderr: "+ nc -zv -t -w 2 10.152.183.164 80\nConnection to 10.152.183.164 80 port [tcp/http] succeeded!\n"
Apr 28 16:37:33.163: INFO: stdout: ""
Apr 28 16:37:33.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9057 exec execpod-affinity2ts7k -- /bin/sh -x -c nc -zv -t -w 2 172.31.13.33 31422'
Apr 28 16:37:33.295: INFO: stderr: "+ nc -zv -t -w 2 172.31.13.33 31422\nConnection to 172.31.13.33 31422 port [tcp/31422] succeeded!\n"
Apr 28 16:37:33.296: INFO: stdout: ""
Apr 28 16:37:33.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9057 exec execpod-affinity2ts7k -- /bin/sh -x -c nc -zv -t -w 2 172.31.76.85 31422'
Apr 28 16:37:33.419: INFO: stderr: "+ nc -zv -t -w 2 172.31.76.85 31422\nConnection to 172.31.76.85 31422 port [tcp/31422] succeeded!\n"
Apr 28 16:37:33.419: INFO: stdout: ""
Apr 28 16:37:33.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9057 exec execpod-affinity2ts7k -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.13.33:31422/ ; done'
Apr 28 16:37:33.637: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n"
Apr 28 16:37:33.637: INFO: stdout: "\naffinity-nodeport-transition-wfck9\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-wfck9\naffinity-nodeport-transition-wfck9\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-wfck9\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-wfck9\naffinity-nodeport-transition-wfck9\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885"
Apr 28 16:37:33.637: INFO: Received response from host: affinity-nodeport-transition-wfck9
Apr 28 16:37:33.637: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.637: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.637: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.637: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.637: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.637: INFO: Received response from host: affinity-nodeport-transition-wfck9
Apr 28 16:37:33.637: INFO: Received response from host: affinity-nodeport-transition-wfck9
Apr 28 16:37:33.637: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.637: INFO: Received response from host: affinity-nodeport-transition-wfck9
Apr 28 16:37:33.637: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.637: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.637: INFO: Received response from host: affinity-nodeport-transition-wfck9
Apr 28 16:37:33.637: INFO: Received response from host: affinity-nodeport-transition-wfck9
Apr 28 16:37:33.637: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.637: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=services-9057 exec execpod-affinity2ts7k -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.13.33:31422/ ; done'
Apr 28 16:37:33.844: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.13.33:31422/\n"
Apr 28 16:37:33.845: INFO: stdout: "\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885\naffinity-nodeport-transition-62885"
Apr 28 16:37:33.845: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.845: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.845: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.845: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.845: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.845: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.845: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.845: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.845: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.845: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.845: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.845: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.845: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.845: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.845: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.845: INFO: Received response from host: affinity-nodeport-transition-62885
Apr 28 16:37:33.845: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9057, will wait for the garbage collector to delete the pods
Apr 28 16:37:33.926: INFO: Deleting ReplicationController affinity-nodeport-transition took: 11.569509ms
Apr 28 16:37:34.026: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.160017ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:37:45.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9057" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:18.714 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":265,"skipped":4367,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:37:45.292: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7482
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 28 16:37:45.545: INFO: Waiting up to 5m0s for pod "downward-api-ba7b7c9e-fe8a-47b8-8465-45bfd0ce286d" in namespace "downward-api-7482" to be "Succeeded or Failed"
Apr 28 16:37:45.550: INFO: Pod "downward-api-ba7b7c9e-fe8a-47b8-8465-45bfd0ce286d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.017897ms
Apr 28 16:37:47.562: INFO: Pod "downward-api-ba7b7c9e-fe8a-47b8-8465-45bfd0ce286d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01678624s
STEP: Saw pod success
Apr 28 16:37:47.562: INFO: Pod "downward-api-ba7b7c9e-fe8a-47b8-8465-45bfd0ce286d" satisfied condition "Succeeded or Failed"
Apr 28 16:37:47.570: INFO: Trying to get logs from node ip-172-31-13-33 pod downward-api-ba7b7c9e-fe8a-47b8-8465-45bfd0ce286d container dapi-container: <nil>
STEP: delete the pod
Apr 28 16:37:47.631: INFO: Waiting for pod downward-api-ba7b7c9e-fe8a-47b8-8465-45bfd0ce286d to disappear
Apr 28 16:37:47.637: INFO: Pod downward-api-ba7b7c9e-fe8a-47b8-8465-45bfd0ce286d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:37:47.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7482" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":266,"skipped":4395,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:37:47.655: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9957
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-325ec6b0-c406-409e-92cb-05229894ebde
STEP: Creating a pod to test consume secrets
Apr 28 16:37:47.848: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-40f23b32-582b-4c64-b654-cdb5ede85fad" in namespace "projected-9957" to be "Succeeded or Failed"
Apr 28 16:37:47.868: INFO: Pod "pod-projected-secrets-40f23b32-582b-4c64-b654-cdb5ede85fad": Phase="Pending", Reason="", readiness=false. Elapsed: 20.395076ms
Apr 28 16:37:49.873: INFO: Pod "pod-projected-secrets-40f23b32-582b-4c64-b654-cdb5ede85fad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025726666s
STEP: Saw pod success
Apr 28 16:37:49.873: INFO: Pod "pod-projected-secrets-40f23b32-582b-4c64-b654-cdb5ede85fad" satisfied condition "Succeeded or Failed"
Apr 28 16:37:49.879: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-projected-secrets-40f23b32-582b-4c64-b654-cdb5ede85fad container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 28 16:37:49.905: INFO: Waiting for pod pod-projected-secrets-40f23b32-582b-4c64-b654-cdb5ede85fad to disappear
Apr 28 16:37:49.910: INFO: Pod pod-projected-secrets-40f23b32-582b-4c64-b654-cdb5ede85fad no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:37:49.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9957" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":267,"skipped":4427,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:37:49.924: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7054
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-e4cd6c92-8122-4e03-9702-0a818b656eca
STEP: Creating a pod to test consume configMaps
Apr 28 16:37:50.127: INFO: Waiting up to 5m0s for pod "pod-configmaps-35ecf9f8-99da-4e1c-8b56-d1068d5abd4a" in namespace "configmap-7054" to be "Succeeded or Failed"
Apr 28 16:37:50.136: INFO: Pod "pod-configmaps-35ecf9f8-99da-4e1c-8b56-d1068d5abd4a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.373074ms
Apr 28 16:37:52.155: INFO: Pod "pod-configmaps-35ecf9f8-99da-4e1c-8b56-d1068d5abd4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027724111s
STEP: Saw pod success
Apr 28 16:37:52.155: INFO: Pod "pod-configmaps-35ecf9f8-99da-4e1c-8b56-d1068d5abd4a" satisfied condition "Succeeded or Failed"
Apr 28 16:37:52.160: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-configmaps-35ecf9f8-99da-4e1c-8b56-d1068d5abd4a container configmap-volume-test: <nil>
STEP: delete the pod
Apr 28 16:37:52.186: INFO: Waiting for pod pod-configmaps-35ecf9f8-99da-4e1c-8b56-d1068d5abd4a to disappear
Apr 28 16:37:52.192: INFO: Pod pod-configmaps-35ecf9f8-99da-4e1c-8b56-d1068d5abd4a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:37:52.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7054" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":268,"skipped":4438,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:37:52.208: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8670
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-60678cf9-e27a-4be8-918d-134524f45c33
STEP: Creating a pod to test consume configMaps
Apr 28 16:37:52.399: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d2864ec4-622d-4432-af8e-1c70442058de" in namespace "projected-8670" to be "Succeeded or Failed"
Apr 28 16:37:52.406: INFO: Pod "pod-projected-configmaps-d2864ec4-622d-4432-af8e-1c70442058de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.320008ms
Apr 28 16:37:54.415: INFO: Pod "pod-projected-configmaps-d2864ec4-622d-4432-af8e-1c70442058de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015672995s
STEP: Saw pod success
Apr 28 16:37:54.415: INFO: Pod "pod-projected-configmaps-d2864ec4-622d-4432-af8e-1c70442058de" satisfied condition "Succeeded or Failed"
Apr 28 16:37:54.421: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-projected-configmaps-d2864ec4-622d-4432-af8e-1c70442058de container agnhost-container: <nil>
STEP: delete the pod
Apr 28 16:37:54.451: INFO: Waiting for pod pod-projected-configmaps-d2864ec4-622d-4432-af8e-1c70442058de to disappear
Apr 28 16:37:54.457: INFO: Pod pod-projected-configmaps-d2864ec4-622d-4432-af8e-1c70442058de no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:37:54.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8670" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":269,"skipped":4449,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:37:54.472: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6660
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:37:54.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6660" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":270,"skipped":4456,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:37:54.739: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6390
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:37:54.931: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-eca4eddc-ff0c-45a2-8e77-9de7a2ba1574" in namespace "security-context-test-6390" to be "Succeeded or Failed"
Apr 28 16:37:54.946: INFO: Pod "alpine-nnp-false-eca4eddc-ff0c-45a2-8e77-9de7a2ba1574": Phase="Pending", Reason="", readiness=false. Elapsed: 15.115137ms
Apr 28 16:37:56.956: INFO: Pod "alpine-nnp-false-eca4eddc-ff0c-45a2-8e77-9de7a2ba1574": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024479609s
Apr 28 16:37:56.956: INFO: Pod "alpine-nnp-false-eca4eddc-ff0c-45a2-8e77-9de7a2ba1574" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:37:56.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6390" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":271,"skipped":4469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:37:56.983: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-202
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Apr 28 16:37:57.875: INFO: Pod name wrapped-volume-race-b4359cd8-29b7-4a8c-af69-f279db4ac899: Found 0 pods out of 5
Apr 28 16:38:02.893: INFO: Pod name wrapped-volume-race-b4359cd8-29b7-4a8c-af69-f279db4ac899: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b4359cd8-29b7-4a8c-af69-f279db4ac899 in namespace emptydir-wrapper-202, will wait for the garbage collector to delete the pods
Apr 28 16:38:12.998: INFO: Deleting ReplicationController wrapped-volume-race-b4359cd8-29b7-4a8c-af69-f279db4ac899 took: 13.839583ms
Apr 28 16:38:13.699: INFO: Terminating ReplicationController wrapped-volume-race-b4359cd8-29b7-4a8c-af69-f279db4ac899 pods took: 700.127073ms
STEP: Creating RC which spawns configmap-volume pods
Apr 28 16:38:25.146: INFO: Pod name wrapped-volume-race-654dadb4-9e51-4fad-8bb2-2f2c17f2930a: Found 0 pods out of 5
Apr 28 16:38:30.155: INFO: Pod name wrapped-volume-race-654dadb4-9e51-4fad-8bb2-2f2c17f2930a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-654dadb4-9e51-4fad-8bb2-2f2c17f2930a in namespace emptydir-wrapper-202, will wait for the garbage collector to delete the pods
Apr 28 16:38:40.300: INFO: Deleting ReplicationController wrapped-volume-race-654dadb4-9e51-4fad-8bb2-2f2c17f2930a took: 16.450126ms
Apr 28 16:38:40.400: INFO: Terminating ReplicationController wrapped-volume-race-654dadb4-9e51-4fad-8bb2-2f2c17f2930a pods took: 100.117214ms
STEP: Creating RC which spawns configmap-volume pods
Apr 28 16:38:45.240: INFO: Pod name wrapped-volume-race-5d102c08-3f9e-4843-99e9-26a1c7803cfd: Found 0 pods out of 5
Apr 28 16:38:50.248: INFO: Pod name wrapped-volume-race-5d102c08-3f9e-4843-99e9-26a1c7803cfd: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5d102c08-3f9e-4843-99e9-26a1c7803cfd in namespace emptydir-wrapper-202, will wait for the garbage collector to delete the pods
Apr 28 16:39:00.349: INFO: Deleting ReplicationController wrapped-volume-race-5d102c08-3f9e-4843-99e9-26a1c7803cfd took: 11.843769ms
Apr 28 16:39:01.050: INFO: Terminating ReplicationController wrapped-volume-race-5d102c08-3f9e-4843-99e9-26a1c7803cfd pods took: 700.126364ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:39:12.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-202" for this suite.

• [SLOW TEST:75.454 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":272,"skipped":4491,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:39:12.438: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8173
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Apr 28 16:39:12.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-8173 cluster-info'
Apr 28 16:39:12.822: INFO: stderr: ""
Apr 28 16:39:12.822: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:39:12.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8173" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":273,"skipped":4505,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:39:12.835: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1859
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Apr 28 16:39:12.988: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:39:16.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1859" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":274,"skipped":4526,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:39:16.104: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2109
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 28 16:39:16.302: INFO: Waiting up to 5m0s for pod "pod-c3d2c866-f2d1-4c5c-bc9c-9c400ab2f935" in namespace "emptydir-2109" to be "Succeeded or Failed"
Apr 28 16:39:16.314: INFO: Pod "pod-c3d2c866-f2d1-4c5c-bc9c-9c400ab2f935": Phase="Pending", Reason="", readiness=false. Elapsed: 11.401909ms
Apr 28 16:39:18.325: INFO: Pod "pod-c3d2c866-f2d1-4c5c-bc9c-9c400ab2f935": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022642s
STEP: Saw pod success
Apr 28 16:39:18.325: INFO: Pod "pod-c3d2c866-f2d1-4c5c-bc9c-9c400ab2f935" satisfied condition "Succeeded or Failed"
Apr 28 16:39:18.328: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-c3d2c866-f2d1-4c5c-bc9c-9c400ab2f935 container test-container: <nil>
STEP: delete the pod
Apr 28 16:39:18.366: INFO: Waiting for pod pod-c3d2c866-f2d1-4c5c-bc9c-9c400ab2f935 to disappear
Apr 28 16:39:18.371: INFO: Pod pod-c3d2c866-f2d1-4c5c-bc9c-9c400ab2f935 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:39:18.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2109" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":275,"skipped":4536,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:39:18.383: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4162
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Apr 28 16:39:18.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=kubectl-4162 api-versions'
Apr 28 16:39:18.593: INFO: stderr: ""
Apr 28 16:39:18.593: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:39:18.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4162" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":276,"skipped":4573,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:39:18.612: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8747
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:39:18.808: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-05fa4ed5-2532-4ef0-b2b3-1d39f058b8a2" in namespace "security-context-test-8747" to be "Succeeded or Failed"
Apr 28 16:39:18.812: INFO: Pod "busybox-privileged-false-05fa4ed5-2532-4ef0-b2b3-1d39f058b8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.367564ms
Apr 28 16:39:20.820: INFO: Pod "busybox-privileged-false-05fa4ed5-2532-4ef0-b2b3-1d39f058b8a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011757065s
Apr 28 16:39:20.820: INFO: Pod "busybox-privileged-false-05fa4ed5-2532-4ef0-b2b3-1d39f058b8a2" satisfied condition "Succeeded or Failed"
Apr 28 16:39:20.831: INFO: Got logs for pod "busybox-privileged-false-05fa4ed5-2532-4ef0-b2b3-1d39f058b8a2": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:39:20.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8747" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":277,"skipped":4590,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:39:20.850: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-7933
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 28 16:39:21.058: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 28 16:40:21.082: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Apr 28 16:40:21.122: INFO: Created pod: pod0-sched-preemption-low-priority
Apr 28 16:40:21.149: INFO: Created pod: pod1-sched-preemption-medium-priority
Apr 28 16:40:21.177: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:40:45.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7933" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:84.504 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":278,"skipped":4614,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:40:45.354: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1077
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-245a3569-3455-4065-aacb-d24f8111cf99
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:40:45.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1077" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":279,"skipped":4626,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:40:45.549: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-820
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-820
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 28 16:40:45.710: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 28 16:40:45.770: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 28 16:40:47.783: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:40:49.783: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:40:51.781: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:40:53.780: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:40:55.786: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:40:57.775: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:40:59.781: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:41:01.781: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:41:03.777: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:41:05.776: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 28 16:41:05.787: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr 28 16:41:05.796: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Apr 28 16:41:07.849: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Apr 28 16:41:07.849: INFO: Going to poll 10.1.62.45 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Apr 28 16:41:07.853: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.1.62.45:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-820 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 16:41:07.853: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 16:41:07.943: INFO: Found all 1 expected endpoints: [netserver-0]
Apr 28 16:41:07.943: INFO: Going to poll 10.1.99.68 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Apr 28 16:41:07.947: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.1.99.68:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-820 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 16:41:07.947: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 16:41:07.996: INFO: Found all 1 expected endpoints: [netserver-1]
Apr 28 16:41:07.996: INFO: Going to poll 10.1.39.47 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Apr 28 16:41:08.001: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.1.39.47:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-820 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 16:41:08.001: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 16:41:08.087: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:41:08.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-820" for this suite.

• [SLOW TEST:22.552 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":280,"skipped":4639,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:41:08.101: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-815
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 28 16:41:08.266: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 28 16:41:08.276: INFO: Waiting for terminating namespaces to be deleted...
Apr 28 16:41:08.284: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-13-33 before test
Apr 28 16:41:08.289: INFO: nginx-ingress-controller-kubernetes-worker-rjf5q from ingress-nginx-kubernetes-worker started at 2021-04-28 16:25:55 +0000 UTC (1 container statuses recorded)
Apr 28 16:41:08.289: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Apr 28 16:41:08.289: INFO: host-test-container-pod from pod-network-test-820 started at 2021-04-28 16:41:05 +0000 UTC (1 container statuses recorded)
Apr 28 16:41:08.289: INFO: 	Container agnhost-container ready: true, restart count 0
Apr 28 16:41:08.289: INFO: netserver-0 from pod-network-test-820 started at 2021-04-28 16:40:45 +0000 UTC (1 container statuses recorded)
Apr 28 16:41:08.289: INFO: 	Container webserver ready: true, restart count 0
Apr 28 16:41:08.289: INFO: test-container-pod from pod-network-test-820 started at 2021-04-28 16:41:05 +0000 UTC (1 container statuses recorded)
Apr 28 16:41:08.289: INFO: 	Container webserver ready: true, restart count 0
Apr 28 16:41:08.289: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-30-112 before test
Apr 28 16:41:08.296: INFO: nginx-ingress-controller-kubernetes-worker-9plpg from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:47 +0000 UTC (1 container statuses recorded)
Apr 28 16:41:08.296: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Apr 28 16:41:08.296: INFO: kube-state-metrics-7f55b9fcd7-jjs4x from kube-system started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 16:41:08.296: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 28 16:41:08.296: INFO: metrics-server-v0.3.6-f6cf867b4-52bpg from kube-system started at 2021-04-28 16:25:17 +0000 UTC (2 container statuses recorded)
Apr 28 16:41:08.296: INFO: 	Container metrics-server ready: true, restart count 0
Apr 28 16:41:08.296: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Apr 28 16:41:08.296: INFO: dashboard-metrics-scraper-74757fb5b7-x9lrp from kubernetes-dashboard started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 16:41:08.296: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr 28 16:41:08.296: INFO: kubernetes-dashboard-64f87676d4-mshn4 from kubernetes-dashboard started at 2021-04-28 14:28:48 +0000 UTC (1 container statuses recorded)
Apr 28 16:41:08.296: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr 28 16:41:08.296: INFO: netserver-1 from pod-network-test-820 started at 2021-04-28 16:40:45 +0000 UTC (1 container statuses recorded)
Apr 28 16:41:08.296: INFO: 	Container webserver ready: true, restart count 0
Apr 28 16:41:08.296: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-76-85 before test
Apr 28 16:41:08.305: INFO: default-http-backend-kubernetes-worker-6494cbc7fd-74bdv from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:58 +0000 UTC (1 container statuses recorded)
Apr 28 16:41:08.305: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
Apr 28 16:41:08.305: INFO: nginx-ingress-controller-kubernetes-worker-bjnb5 from ingress-nginx-kubernetes-worker started at 2021-04-28 14:28:49 +0000 UTC (1 container statuses recorded)
Apr 28 16:41:08.305: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
Apr 28 16:41:08.305: INFO: coredns-7bb4d77796-mgrfg from kube-system started at 2021-04-28 16:04:35 +0000 UTC (1 container statuses recorded)
Apr 28 16:41:08.305: INFO: 	Container coredns ready: true, restart count 0
Apr 28 16:41:08.305: INFO: netserver-2 from pod-network-test-820 started at 2021-04-28 16:40:45 +0000 UTC (1 container statuses recorded)
Apr 28 16:41:08.305: INFO: 	Container webserver ready: true, restart count 0
Apr 28 16:41:08.305: INFO: sonobuoy from sonobuoy started at 2021-04-28 15:27:11 +0000 UTC (1 container statuses recorded)
Apr 28 16:41:08.305: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 28 16:41:08.305: INFO: sonobuoy-e2e-job-c0dbab055ba045c5 from sonobuoy started at 2021-04-28 15:27:15 +0000 UTC (2 container statuses recorded)
Apr 28 16:41:08.305: INFO: 	Container e2e ready: true, restart count 0
Apr 28 16:41:08.305: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node ip-172-31-13-33
STEP: verifying the node has the label node ip-172-31-30-112
STEP: verifying the node has the label node ip-172-31-76-85
Apr 28 16:41:08.421: INFO: Pod default-http-backend-kubernetes-worker-6494cbc7fd-74bdv requesting resource cpu=10m on Node ip-172-31-76-85
Apr 28 16:41:08.421: INFO: Pod nginx-ingress-controller-kubernetes-worker-9plpg requesting resource cpu=0m on Node ip-172-31-30-112
Apr 28 16:41:08.421: INFO: Pod nginx-ingress-controller-kubernetes-worker-bjnb5 requesting resource cpu=0m on Node ip-172-31-76-85
Apr 28 16:41:08.421: INFO: Pod nginx-ingress-controller-kubernetes-worker-rjf5q requesting resource cpu=0m on Node ip-172-31-13-33
Apr 28 16:41:08.421: INFO: Pod coredns-7bb4d77796-mgrfg requesting resource cpu=100m on Node ip-172-31-76-85
Apr 28 16:41:08.421: INFO: Pod kube-state-metrics-7f55b9fcd7-jjs4x requesting resource cpu=0m on Node ip-172-31-30-112
Apr 28 16:41:08.421: INFO: Pod metrics-server-v0.3.6-f6cf867b4-52bpg requesting resource cpu=53m on Node ip-172-31-30-112
Apr 28 16:41:08.421: INFO: Pod dashboard-metrics-scraper-74757fb5b7-x9lrp requesting resource cpu=0m on Node ip-172-31-30-112
Apr 28 16:41:08.421: INFO: Pod kubernetes-dashboard-64f87676d4-mshn4 requesting resource cpu=0m on Node ip-172-31-30-112
Apr 28 16:41:08.421: INFO: Pod host-test-container-pod requesting resource cpu=0m on Node ip-172-31-13-33
Apr 28 16:41:08.421: INFO: Pod netserver-0 requesting resource cpu=0m on Node ip-172-31-13-33
Apr 28 16:41:08.421: INFO: Pod netserver-1 requesting resource cpu=0m on Node ip-172-31-30-112
Apr 28 16:41:08.421: INFO: Pod netserver-2 requesting resource cpu=0m on Node ip-172-31-76-85
Apr 28 16:41:08.421: INFO: Pod test-container-pod requesting resource cpu=0m on Node ip-172-31-13-33
Apr 28 16:41:08.421: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-76-85
Apr 28 16:41:08.421: INFO: Pod sonobuoy-e2e-job-c0dbab055ba045c5 requesting resource cpu=0m on Node ip-172-31-76-85
STEP: Starting Pods to consume most of the cluster CPU.
Apr 28 16:41:08.421: INFO: Creating a pod which consumes cpu=2800m on Node ip-172-31-13-33
Apr 28 16:41:08.432: INFO: Creating a pod which consumes cpu=2762m on Node ip-172-31-30-112
Apr 28 16:41:08.441: INFO: Creating a pod which consumes cpu=2723m on Node ip-172-31-76-85
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2b57708d-c530-422e-afb8-5991ea478378.167a1325a82225b9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-815/filler-pod-2b57708d-c530-422e-afb8-5991ea478378 to ip-172-31-13-33]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2b57708d-c530-422e-afb8-5991ea478378.167a1325c41f473d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2b57708d-c530-422e-afb8-5991ea478378.167a1325c6681589], Reason = [Created], Message = [Created container filler-pod-2b57708d-c530-422e-afb8-5991ea478378]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2b57708d-c530-422e-afb8-5991ea478378.167a1325cb215273], Reason = [Started], Message = [Started container filler-pod-2b57708d-c530-422e-afb8-5991ea478378]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-327d3667-e785-4ca8-9dff-29c8ebb71d54.167a1325a8647302], Reason = [Scheduled], Message = [Successfully assigned sched-pred-815/filler-pod-327d3667-e785-4ca8-9dff-29c8ebb71d54 to ip-172-31-30-112]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-327d3667-e785-4ca8-9dff-29c8ebb71d54.167a1325c5bc2d96], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-327d3667-e785-4ca8-9dff-29c8ebb71d54.167a1325c7bb0959], Reason = [Created], Message = [Created container filler-pod-327d3667-e785-4ca8-9dff-29c8ebb71d54]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-327d3667-e785-4ca8-9dff-29c8ebb71d54.167a1325cb2f1388], Reason = [Started], Message = [Started container filler-pod-327d3667-e785-4ca8-9dff-29c8ebb71d54]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b3f14c2d-9162-4c18-88c8-a8b9cf996529.167a1325a99c8dfa], Reason = [Scheduled], Message = [Successfully assigned sched-pred-815/filler-pod-b3f14c2d-9162-4c18-88c8-a8b9cf996529 to ip-172-31-76-85]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b3f14c2d-9162-4c18-88c8-a8b9cf996529.167a1325c5f8bb7d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b3f14c2d-9162-4c18-88c8-a8b9cf996529.167a1325c82a8c76], Reason = [Created], Message = [Created container filler-pod-b3f14c2d-9162-4c18-88c8-a8b9cf996529]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b3f14c2d-9162-4c18-88c8-a8b9cf996529.167a1325cbd58477], Reason = [Started], Message = [Started container filler-pod-b3f14c2d-9162-4c18-88c8-a8b9cf996529]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.167a13262280214d], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.167a132623ad82d4], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node ip-172-31-13-33
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-30-112
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-76-85
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:41:11.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-815" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":281,"skipped":4660,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:41:11.585: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2709
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 28 16:41:15.870: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 28 16:41:15.874: INFO: Pod pod-with-poststart-http-hook still exists
Apr 28 16:41:17.875: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 28 16:41:17.884: INFO: Pod pod-with-poststart-http-hook still exists
Apr 28 16:41:19.874: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 28 16:41:19.884: INFO: Pod pod-with-poststart-http-hook still exists
Apr 28 16:41:21.875: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 28 16:41:21.886: INFO: Pod pod-with-poststart-http-hook still exists
Apr 28 16:41:23.875: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 28 16:41:23.886: INFO: Pod pod-with-poststart-http-hook still exists
Apr 28 16:41:25.875: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 28 16:41:25.886: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:41:25.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2709" for this suite.

• [SLOW TEST:14.315 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":282,"skipped":4676,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:41:25.900: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4876
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Apr 28 16:41:26.076: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:41:41.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4876" for this suite.

• [SLOW TEST:15.238 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":283,"skipped":4685,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:41:41.138: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6701
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-f1ae670a-f8a0-4bbc-a904-997b736b58dc
STEP: Creating a pod to test consume configMaps
Apr 28 16:41:41.341: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-701c67e0-052c-4b33-a8f3-5dd239afe80c" in namespace "projected-6701" to be "Succeeded or Failed"
Apr 28 16:41:41.351: INFO: Pod "pod-projected-configmaps-701c67e0-052c-4b33-a8f3-5dd239afe80c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.588911ms
Apr 28 16:41:43.359: INFO: Pod "pod-projected-configmaps-701c67e0-052c-4b33-a8f3-5dd239afe80c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017961466s
STEP: Saw pod success
Apr 28 16:41:43.359: INFO: Pod "pod-projected-configmaps-701c67e0-052c-4b33-a8f3-5dd239afe80c" satisfied condition "Succeeded or Failed"
Apr 28 16:41:43.365: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-projected-configmaps-701c67e0-052c-4b33-a8f3-5dd239afe80c container agnhost-container: <nil>
STEP: delete the pod
Apr 28 16:41:43.392: INFO: Waiting for pod pod-projected-configmaps-701c67e0-052c-4b33-a8f3-5dd239afe80c to disappear
Apr 28 16:41:43.396: INFO: Pod pod-projected-configmaps-701c67e0-052c-4b33-a8f3-5dd239afe80c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:41:43.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6701" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":284,"skipped":4694,"failed":0}
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:41:43.407: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4865
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 28 16:41:47.648: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 28 16:41:47.654: INFO: Pod pod-with-prestop-http-hook still exists
Apr 28 16:41:49.654: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 28 16:41:49.663: INFO: Pod pod-with-prestop-http-hook still exists
Apr 28 16:41:51.654: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 28 16:41:51.662: INFO: Pod pod-with-prestop-http-hook still exists
Apr 28 16:41:53.654: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 28 16:41:53.662: INFO: Pod pod-with-prestop-http-hook still exists
Apr 28 16:41:55.654: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 28 16:41:55.663: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:41:55.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4865" for this suite.

• [SLOW TEST:12.295 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":285,"skipped":4696,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:41:55.703: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9493
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:42:23.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9493" for this suite.

• [SLOW TEST:28.276 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":286,"skipped":4711,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:42:23.979: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3866
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 16:42:24.169: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a282c28-6762-44d6-a1b5-76e55197cbed" in namespace "projected-3866" to be "Succeeded or Failed"
Apr 28 16:42:24.178: INFO: Pod "downwardapi-volume-1a282c28-6762-44d6-a1b5-76e55197cbed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.957965ms
Apr 28 16:42:26.188: INFO: Pod "downwardapi-volume-1a282c28-6762-44d6-a1b5-76e55197cbed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018405277s
STEP: Saw pod success
Apr 28 16:42:26.188: INFO: Pod "downwardapi-volume-1a282c28-6762-44d6-a1b5-76e55197cbed" satisfied condition "Succeeded or Failed"
Apr 28 16:42:26.192: INFO: Trying to get logs from node ip-172-31-13-33 pod downwardapi-volume-1a282c28-6762-44d6-a1b5-76e55197cbed container client-container: <nil>
STEP: delete the pod
Apr 28 16:42:26.221: INFO: Waiting for pod downwardapi-volume-1a282c28-6762-44d6-a1b5-76e55197cbed to disappear
Apr 28 16:42:26.225: INFO: Pod downwardapi-volume-1a282c28-6762-44d6-a1b5-76e55197cbed no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:42:26.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3866" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":287,"skipped":4765,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:42:26.237: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5723
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-a540ea5f-20e2-46a0-a705-1c54a0e4c1dc
STEP: Creating a pod to test consume secrets
Apr 28 16:42:26.416: INFO: Waiting up to 5m0s for pod "pod-secrets-a8225819-bd15-482d-aafe-5a3049801daa" in namespace "secrets-5723" to be "Succeeded or Failed"
Apr 28 16:42:26.421: INFO: Pod "pod-secrets-a8225819-bd15-482d-aafe-5a3049801daa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.806212ms
Apr 28 16:42:28.428: INFO: Pod "pod-secrets-a8225819-bd15-482d-aafe-5a3049801daa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011409106s
STEP: Saw pod success
Apr 28 16:42:28.428: INFO: Pod "pod-secrets-a8225819-bd15-482d-aafe-5a3049801daa" satisfied condition "Succeeded or Failed"
Apr 28 16:42:28.433: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-secrets-a8225819-bd15-482d-aafe-5a3049801daa container secret-volume-test: <nil>
STEP: delete the pod
Apr 28 16:42:28.459: INFO: Waiting for pod pod-secrets-a8225819-bd15-482d-aafe-5a3049801daa to disappear
Apr 28 16:42:28.465: INFO: Pod pod-secrets-a8225819-bd15-482d-aafe-5a3049801daa no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:42:28.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5723" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":288,"skipped":4767,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:42:28.477: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-447
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-447
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Apr 28 16:42:28.673: INFO: Found 0 stateful pods, waiting for 3
Apr 28 16:42:38.680: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 28 16:42:38.680: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 28 16:42:38.680: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr 28 16:42:38.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-447 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 28 16:42:38.833: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 28 16:42:38.833: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 28 16:42:38.833: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Apr 28 16:42:48.893: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Apr 28 16:42:58.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-447 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 16:42:59.057: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 28 16:42:59.057: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 28 16:42:59.057: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 28 16:43:19.099: INFO: Waiting for StatefulSet statefulset-447/ss2 to complete update
Apr 28 16:43:19.099: INFO: Waiting for Pod statefulset-447/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Apr 28 16:43:29.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-447 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 28 16:43:29.267: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 28 16:43:29.267: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 28 16:43:29.267: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 28 16:43:39.316: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Apr 28 16:43:49.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=statefulset-447 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 28 16:43:49.482: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 28 16:43:49.482: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 28 16:43:49.482: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 28 16:44:09.525: INFO: Waiting for StatefulSet statefulset-447/ss2 to complete update
Apr 28 16:44:09.525: INFO: Waiting for Pod statefulset-447/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 28 16:44:19.545: INFO: Deleting all statefulset in ns statefulset-447
Apr 28 16:44:19.549: INFO: Scaling statefulset ss2 to 0
Apr 28 16:44:29.581: INFO: Waiting for statefulset status.replicas updated to 0
Apr 28 16:44:29.585: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:44:29.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-447" for this suite.

• [SLOW TEST:121.145 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":289,"skipped":4769,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:44:29.623: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2060
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:44:29.958: INFO: Create a RollingUpdate DaemonSet
Apr 28 16:44:29.965: INFO: Check that daemon pods launch on every node of the cluster
Apr 28 16:44:29.974: INFO: Number of nodes with available pods: 0
Apr 28 16:44:29.974: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 16:44:30.985: INFO: Number of nodes with available pods: 2
Apr 28 16:44:30.985: INFO: Node ip-172-31-13-33 is running more than one daemon pod
Apr 28 16:44:31.988: INFO: Number of nodes with available pods: 3
Apr 28 16:44:31.988: INFO: Number of running nodes: 3, number of available pods: 3
Apr 28 16:44:31.988: INFO: Update the DaemonSet to trigger a rollout
Apr 28 16:44:32.002: INFO: Updating DaemonSet daemon-set
Apr 28 16:44:42.022: INFO: Roll back the DaemonSet before rollout is complete
Apr 28 16:44:42.034: INFO: Updating DaemonSet daemon-set
Apr 28 16:44:42.034: INFO: Make sure DaemonSet rollback is complete
Apr 28 16:44:42.038: INFO: Wrong image for pod: daemon-set-9cvnl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 28 16:44:42.038: INFO: Pod daemon-set-9cvnl is not available
Apr 28 16:44:43.048: INFO: Wrong image for pod: daemon-set-9cvnl. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 28 16:44:43.048: INFO: Pod daemon-set-9cvnl is not available
Apr 28 16:44:44.050: INFO: Pod daemon-set-582dd is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2060, will wait for the garbage collector to delete the pods
Apr 28 16:44:44.133: INFO: Deleting DaemonSet.extensions daemon-set took: 14.629668ms
Apr 28 16:44:44.733: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.130355ms
Apr 28 16:44:57.049: INFO: Number of nodes with available pods: 0
Apr 28 16:44:57.049: INFO: Number of running nodes: 0, number of available pods: 0
Apr 28 16:44:57.053: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32356"},"items":null}

Apr 28 16:44:57.056: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32356"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:44:57.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2060" for this suite.

• [SLOW TEST:27.494 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":290,"skipped":4775,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:44:57.117: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6675
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:44:57.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6675" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":291,"skipped":4781,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:44:57.411: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3426
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-3426
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 28 16:44:57.584: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 28 16:44:57.895: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 28 16:44:59.902: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:45:01.903: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:45:03.904: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:45:05.900: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:45:07.916: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:45:09.901: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:45:11.903: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:45:13.902: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:45:15.905: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 28 16:45:17.906: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 28 16:45:17.915: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr 28 16:45:17.923: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Apr 28 16:45:19.988: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Apr 28 16:45:19.988: INFO: Breadth first check of 10.1.62.65 on host 172.31.13.33...
Apr 28 16:45:19.994: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.62.66:9080/dial?request=hostname&protocol=udp&host=10.1.62.65&port=8081&tries=1'] Namespace:pod-network-test-3426 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 16:45:19.994: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 16:45:20.087: INFO: Waiting for responses: map[]
Apr 28 16:45:20.087: INFO: reached 10.1.62.65 after 0/1 tries
Apr 28 16:45:20.087: INFO: Breadth first check of 10.1.99.73 on host 172.31.30.112...
Apr 28 16:45:20.092: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.62.66:9080/dial?request=hostname&protocol=udp&host=10.1.99.73&port=8081&tries=1'] Namespace:pod-network-test-3426 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 16:45:20.092: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 16:45:20.180: INFO: Waiting for responses: map[]
Apr 28 16:45:20.180: INFO: reached 10.1.99.73 after 0/1 tries
Apr 28 16:45:20.180: INFO: Breadth first check of 10.1.39.50 on host 172.31.76.85...
Apr 28 16:45:20.186: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.62.66:9080/dial?request=hostname&protocol=udp&host=10.1.39.50&port=8081&tries=1'] Namespace:pod-network-test-3426 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 16:45:20.186: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 16:45:20.273: INFO: Waiting for responses: map[]
Apr 28 16:45:20.273: INFO: reached 10.1.39.50 after 0/1 tries
Apr 28 16:45:20.273: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:45:20.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3426" for this suite.

• [SLOW TEST:22.876 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":292,"skipped":4797,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:45:20.288: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:45:20.468: INFO: Creating deployment "webserver-deployment"
Apr 28 16:45:20.474: INFO: Waiting for observed generation 1
Apr 28 16:45:22.492: INFO: Waiting for all required pods to come up
Apr 28 16:45:22.497: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Apr 28 16:45:24.512: INFO: Waiting for deployment "webserver-deployment" to complete
Apr 28 16:45:24.521: INFO: Updating deployment "webserver-deployment" with a non-existent image
Apr 28 16:45:24.541: INFO: Updating deployment webserver-deployment
Apr 28 16:45:24.541: INFO: Waiting for observed generation 2
Apr 28 16:45:26.558: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr 28 16:45:26.562: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr 28 16:45:26.565: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 28 16:45:26.577: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr 28 16:45:26.577: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr 28 16:45:26.582: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 28 16:45:26.589: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Apr 28 16:45:26.589: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Apr 28 16:45:26.602: INFO: Updating deployment webserver-deployment
Apr 28 16:45:26.602: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Apr 28 16:45:26.612: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr 28 16:45:26.618: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 28 16:45:26.639: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7109  5f0a9ba4-0ed3-4ec5-9cbd-acf484723025 32742 3 2021-04-28 16:45:20 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-04-28 16:45:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-28 16:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0071d1e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-04-28 16:45:24 +0000 UTC,LastTransitionTime:2021-04-28 16:45:20 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-04-28 16:45:26 +0000 UTC,LastTransitionTime:2021-04-28 16:45:26 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Apr 28 16:45:26.653: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-7109  5860a18e-a1fa-4310-ae1a-743954839c4c 32739 3 2021-04-28 16:45:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 5f0a9ba4-0ed3-4ec5-9cbd-acf484723025 0xc00349f8a7 0xc00349f8a8}] []  [{kube-controller-manager Update apps/v1 2021-04-28 16:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0a9ba4-0ed3-4ec5-9cbd-acf484723025\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00349f928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 28 16:45:26.653: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Apr 28 16:45:26.653: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-7109  5d286ea8-afd7-4978-b5d7-d61d350cccdf 32737 3 2021-04-28 16:45:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 5f0a9ba4-0ed3-4ec5-9cbd-acf484723025 0xc00349f987 0xc00349f988}] []  [{kube-controller-manager Update apps/v1 2021-04-28 16:45:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f0a9ba4-0ed3-4ec5-9cbd-acf484723025\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00349f9f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Apr 28 16:45:26.665: INFO: Pod "webserver-deployment-795d758f88-bhtdp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bhtdp webserver-deployment-795d758f88- deployment-7109  b9decc4a-747e-4ae3-913d-e73c2ce0e0c2 32745 0 2021-04-28 16:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5860a18e-a1fa-4310-ae1a-743954839c4c 0xc00349fe77 0xc00349fe78}] []  [{kube-controller-manager Update v1 2021-04-28 16:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5860a18e-a1fa-4310-ae1a-743954839c4c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dvx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dvx54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dvx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 28 16:45:26.665: INFO: Pod "webserver-deployment-795d758f88-f69zj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-f69zj webserver-deployment-795d758f88- deployment-7109  5a82ea1e-9ec0-4d33-a21a-fb5c71e740bd 32706 0 2021-04-28 16:45:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5860a18e-a1fa-4310-ae1a-743954839c4c 0xc00349ff87 0xc00349ff88}] []  [{kube-controller-manager Update v1 2021-04-28 16:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5860a18e-a1fa-4310-ae1a-743954839c4c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:45:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.62.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dvx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dvx54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dvx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.33,PodIP:10.1.62.72,StartTime:2021-04-28 16:45:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.62.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 28 16:45:26.665: INFO: Pod "webserver-deployment-795d758f88-hgdxb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hgdxb webserver-deployment-795d758f88- deployment-7109  c056b426-8c3a-4f23-8771-eea043ae7f57 32726 0 2021-04-28 16:45:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5860a18e-a1fa-4310-ae1a-743954839c4c 0xc0037c07b0 0xc0037c07b1}] []  [{kube-controller-manager Update v1 2021-04-28 16:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5860a18e-a1fa-4310-ae1a-743954839c4c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:45:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.39.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dvx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dvx54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dvx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-76-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.76.85,PodIP:10.1.39.54,StartTime:2021-04-28 16:45:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.39.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 28 16:45:26.666: INFO: Pod "webserver-deployment-795d758f88-nkxdf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nkxdf webserver-deployment-795d758f88- deployment-7109  fded81eb-ee3a-4f73-be2e-56a057b5557b 32730 0 2021-04-28 16:45:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5860a18e-a1fa-4310-ae1a-743954839c4c 0xc0037c0980 0xc0037c0981}] []  [{kube-controller-manager Update v1 2021-04-28 16:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5860a18e-a1fa-4310-ae1a-743954839c4c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:45:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.99.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dvx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dvx54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dvx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-30-112,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.30.112,PodIP:10.1.99.76,StartTime:2021-04-28 16:45:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.99.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 28 16:45:26.666: INFO: Pod "webserver-deployment-795d758f88-nrscd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nrscd webserver-deployment-795d758f88- deployment-7109  0697d8ef-51a6-4117-9fc3-5f0916df1daf 32731 0 2021-04-28 16:45:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5860a18e-a1fa-4310-ae1a-743954839c4c 0xc0037c0d50 0xc0037c0d51}] []  [{kube-controller-manager Update v1 2021-04-28 16:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5860a18e-a1fa-4310-ae1a-743954839c4c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:45:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.62.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dvx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dvx54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dvx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.33,PodIP:10.1.62.74,StartTime:2021-04-28 16:45:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.62.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 28 16:45:26.666: INFO: Pod "webserver-deployment-795d758f88-zvk5s" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zvk5s webserver-deployment-795d758f88- deployment-7109  6b580a75-cf32-4004-a1a7-7caed6e7a232 32700 0 2021-04-28 16:45:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 5860a18e-a1fa-4310-ae1a-743954839c4c 0xc0037c1030 0xc0037c1031}] []  [{kube-controller-manager Update v1 2021-04-28 16:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5860a18e-a1fa-4310-ae1a-743954839c4c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:45:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.62.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dvx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dvx54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dvx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.33,PodIP:10.1.62.73,StartTime:2021-04-28 16:45:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.62.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 28 16:45:26.666: INFO: Pod "webserver-deployment-dd94f59b7-5n75x" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5n75x webserver-deployment-dd94f59b7- deployment-7109  ba21e438-ecc6-4d17-aef5-5c232ce09cc7 32593 0 2021-04-28 16:45:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5d286ea8-afd7-4978-b5d7-d61d350cccdf 0xc0037c1430 0xc0037c1431}] []  [{kube-controller-manager Update v1 2021-04-28 16:45:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d286ea8-afd7-4978-b5d7-d61d350cccdf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:45:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.62.67\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dvx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dvx54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dvx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.33,PodIP:10.1.62.67,StartTime:2021-04-28 16:45:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-28 16:45:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://52015b22b555b80bf6c41c5c59d2a67e9fce6763ba7fae1affaf9c847dc4d423,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.62.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 28 16:45:26.666: INFO: Pod "webserver-deployment-dd94f59b7-79dtr" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-79dtr webserver-deployment-dd94f59b7- deployment-7109  684eedd5-2406-4cf9-ad56-0a5775d99b00 32600 0 2021-04-28 16:45:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5d286ea8-afd7-4978-b5d7-d61d350cccdf 0xc0037c19b0 0xc0037c19b1}] []  [{kube-controller-manager Update v1 2021-04-28 16:45:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d286ea8-afd7-4978-b5d7-d61d350cccdf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:45:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.39.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dvx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dvx54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dvx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-76-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.76.85,PodIP:10.1.39.52,StartTime:2021-04-28 16:45:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-28 16:45:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://676894477d42926866737d7fed44b80d5253808fca9f61d4e72af830253778dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.39.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 28 16:45:26.666: INFO: Pod "webserver-deployment-dd94f59b7-7qdlt" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7qdlt webserver-deployment-dd94f59b7- deployment-7109  a0324f08-fd85-42bf-97ba-bf173f46b0c2 32606 0 2021-04-28 16:45:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5d286ea8-afd7-4978-b5d7-d61d350cccdf 0xc0037c1c40 0xc0037c1c41}] []  [{kube-controller-manager Update v1 2021-04-28 16:45:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d286ea8-afd7-4978-b5d7-d61d350cccdf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:45:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.39.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dvx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dvx54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dvx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-76-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.76.85,PodIP:10.1.39.51,StartTime:2021-04-28 16:45:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-28 16:45:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://39f5f0c2f425038491442edd966e7046977dee5672dc6b5aa028132ad5b4e6b7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.39.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 28 16:45:26.666: INFO: Pod "webserver-deployment-dd94f59b7-9hpxp" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9hpxp webserver-deployment-dd94f59b7- deployment-7109  f70219eb-3843-4bfd-9335-fad38f56e589 32597 0 2021-04-28 16:45:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5d286ea8-afd7-4978-b5d7-d61d350cccdf 0xc0037c1e80 0xc0037c1e81}] []  [{kube-controller-manager Update v1 2021-04-28 16:45:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d286ea8-afd7-4978-b5d7-d61d350cccdf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:45:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.39.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dvx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dvx54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dvx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-76-85,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.76.85,PodIP:10.1.39.53,StartTime:2021-04-28 16:45:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-28 16:45:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://1df7cf83c5d7320d2798c278d8dd84f4de70855fd61f7dc431aef4eee809a2ff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.39.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 28 16:45:26.667: INFO: Pod "webserver-deployment-dd94f59b7-g2b79" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-g2b79 webserver-deployment-dd94f59b7- deployment-7109  dd318a81-0111-440d-9977-183f3cf5ceca 32589 0 2021-04-28 16:45:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5d286ea8-afd7-4978-b5d7-d61d350cccdf 0xc002ce4030 0xc002ce4031}] []  [{kube-controller-manager Update v1 2021-04-28 16:45:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d286ea8-afd7-4978-b5d7-d61d350cccdf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:45:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.62.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dvx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dvx54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dvx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.33,PodIP:10.1.62.69,StartTime:2021-04-28 16:45:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-28 16:45:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://4262b453b933c3cce5481978df480d98bccb5da355e3ccf8716120948a8fae18,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.62.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 28 16:45:26.667: INFO: Pod "webserver-deployment-dd94f59b7-l759c" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-l759c webserver-deployment-dd94f59b7- deployment-7109  6a565013-a4c9-4f13-be9f-250aac5f8952 32621 0 2021-04-28 16:45:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5d286ea8-afd7-4978-b5d7-d61d350cccdf 0xc002ce41d0 0xc002ce41d1}] []  [{kube-controller-manager Update v1 2021-04-28 16:45:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d286ea8-afd7-4978-b5d7-d61d350cccdf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:45:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.62.70\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dvx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dvx54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dvx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.13.33,PodIP:10.1.62.70,StartTime:2021-04-28 16:45:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-28 16:45:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://bb87850f8016380da64fb2a5960b27c2d9533d9d7ee9f1cee84b41d841fec4af,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.62.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 28 16:45:26.667: INFO: Pod "webserver-deployment-dd94f59b7-p2grw" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-p2grw webserver-deployment-dd94f59b7- deployment-7109  9e2eb65f-7bc2-4d8b-a897-928d5714c987 32748 0 2021-04-28 16:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5d286ea8-afd7-4978-b5d7-d61d350cccdf 0xc002ce4370 0xc002ce4371}] []  [{kube-controller-manager Update v1 2021-04-28 16:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d286ea8-afd7-4978-b5d7-d61d350cccdf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dvx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dvx54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dvx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 28 16:45:26.667: INFO: Pod "webserver-deployment-dd94f59b7-qm6qh" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qm6qh webserver-deployment-dd94f59b7- deployment-7109  e00b4190-f040-43fa-87f3-4fa8cfda804c 32604 0 2021-04-28 16:45:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5d286ea8-afd7-4978-b5d7-d61d350cccdf 0xc002ce4477 0xc002ce4478}] []  [{kube-controller-manager Update v1 2021-04-28 16:45:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d286ea8-afd7-4978-b5d7-d61d350cccdf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:45:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.99.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dvx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dvx54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dvx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-30-112,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.30.112,PodIP:10.1.99.74,StartTime:2021-04-28 16:45:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-28 16:45:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://a07758d38199c6ac4e5a715110a155eff141c4f989a36e2e5ab1657d11ae4b5d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.99.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 28 16:45:26.667: INFO: Pod "webserver-deployment-dd94f59b7-wwhxq" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wwhxq webserver-deployment-dd94f59b7- deployment-7109  de6c22db-8f80-4b37-890d-fcceacf88107 32751 0 2021-04-28 16:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5d286ea8-afd7-4978-b5d7-d61d350cccdf 0xc002ce4620 0xc002ce4621}] []  [{kube-controller-manager Update v1 2021-04-28 16:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d286ea8-afd7-4978-b5d7-d61d350cccdf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dvx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dvx54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dvx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-30-112,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 28 16:45:26.667: INFO: Pod "webserver-deployment-dd94f59b7-zhtkj" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zhtkj webserver-deployment-dd94f59b7- deployment-7109  be574eb1-f014-46c5-8fef-9a5d1a834702 32599 0 2021-04-28 16:45:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5d286ea8-afd7-4978-b5d7-d61d350cccdf 0xc002ce4750 0xc002ce4751}] []  [{kube-controller-manager Update v1 2021-04-28 16:45:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d286ea8-afd7-4978-b5d7-d61d350cccdf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-28 16:45:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.99.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dvx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dvx54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dvx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-30-112,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.30.112,PodIP:10.1.99.75,StartTime:2021-04-28 16:45:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-28 16:45:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://eed4340e05e951d203da3354e1e060fecf7f0ce6acf6ca75242bba2254c2f18a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.99.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 28 16:45:26.667: INFO: Pod "webserver-deployment-dd94f59b7-zjmkz" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zjmkz webserver-deployment-dd94f59b7- deployment-7109  32e8b176-55b9-4611-b819-610f6392caaf 32743 0 2021-04-28 16:45:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5d286ea8-afd7-4978-b5d7-d61d350cccdf 0xc002ce48f0 0xc002ce48f1}] []  [{kube-controller-manager Update v1 2021-04-28 16:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d286ea8-afd7-4978-b5d7-d61d350cccdf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dvx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dvx54,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dvx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-33,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-28 16:45:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:45:26.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7109" for this suite.

• [SLOW TEST:6.462 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":293,"skipped":4805,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:45:26.750: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7651
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Apr 28 16:45:27.041: INFO: Waiting up to 5m0s for pod "client-containers-7d6322b0-f3ff-4968-ad82-bf36a2619050" in namespace "containers-7651" to be "Succeeded or Failed"
Apr 28 16:45:27.053: INFO: Pod "client-containers-7d6322b0-f3ff-4968-ad82-bf36a2619050": Phase="Pending", Reason="", readiness=false. Elapsed: 12.025089ms
Apr 28 16:45:29.060: INFO: Pod "client-containers-7d6322b0-f3ff-4968-ad82-bf36a2619050": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019322305s
Apr 28 16:45:31.068: INFO: Pod "client-containers-7d6322b0-f3ff-4968-ad82-bf36a2619050": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027485335s
Apr 28 16:45:33.078: INFO: Pod "client-containers-7d6322b0-f3ff-4968-ad82-bf36a2619050": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037086124s
STEP: Saw pod success
Apr 28 16:45:33.078: INFO: Pod "client-containers-7d6322b0-f3ff-4968-ad82-bf36a2619050" satisfied condition "Succeeded or Failed"
Apr 28 16:45:33.084: INFO: Trying to get logs from node ip-172-31-13-33 pod client-containers-7d6322b0-f3ff-4968-ad82-bf36a2619050 container agnhost-container: <nil>
STEP: delete the pod
Apr 28 16:45:33.131: INFO: Waiting for pod client-containers-7d6322b0-f3ff-4968-ad82-bf36a2619050 to disappear
Apr 28 16:45:33.138: INFO: Pod client-containers-7d6322b0-f3ff-4968-ad82-bf36a2619050 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:45:33.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7651" for this suite.

• [SLOW TEST:6.400 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":294,"skipped":4841,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:45:33.150: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5421
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:45:36.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5421" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":295,"skipped":4848,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:45:36.440: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3382
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:45:36.653: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 28 16:45:39.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-3382 --namespace=crd-publish-openapi-3382 create -f -'
Apr 28 16:45:39.799: INFO: stderr: ""
Apr 28 16:45:39.799: INFO: stdout: "e2e-test-crd-publish-openapi-351-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 28 16:45:39.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-3382 --namespace=crd-publish-openapi-3382 delete e2e-test-crd-publish-openapi-351-crds test-cr'
Apr 28 16:45:39.866: INFO: stderr: ""
Apr 28 16:45:39.866: INFO: stdout: "e2e-test-crd-publish-openapi-351-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Apr 28 16:45:39.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-3382 --namespace=crd-publish-openapi-3382 apply -f -'
Apr 28 16:45:40.019: INFO: stderr: ""
Apr 28 16:45:40.019: INFO: stdout: "e2e-test-crd-publish-openapi-351-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 28 16:45:40.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-3382 --namespace=crd-publish-openapi-3382 delete e2e-test-crd-publish-openapi-351-crds test-cr'
Apr 28 16:45:40.161: INFO: stderr: ""
Apr 28 16:45:40.161: INFO: stdout: "e2e-test-crd-publish-openapi-351-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr 28 16:45:40.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-091929713 --namespace=crd-publish-openapi-3382 explain e2e-test-crd-publish-openapi-351-crds'
Apr 28 16:45:40.356: INFO: stderr: ""
Apr 28 16:45:40.356: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-351-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:45:43.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3382" for this suite.

• [SLOW TEST:7.176 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":296,"skipped":4878,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:45:43.616: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3467
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 28 16:45:44.033: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 16:45:47.061: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:45:47.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3467" for this suite.
STEP: Destroying namespace "webhook-3467-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":297,"skipped":4887,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:45:47.453: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5843
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Apr 28 16:45:47.624: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5843  be9cb582-2093-44a6-a0f7-925f01eb9818 33227 0 2021-04-28 16:45:47 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-04-28 16:45:47 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6sghx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6sghx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6sghx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 28 16:45:47.629: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Apr 28 16:45:49.642: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Apr 28 16:45:51.635: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Apr 28 16:45:51.635: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5843 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 16:45:51.635: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Verifying customized DNS server is configured on pod...
Apr 28 16:45:51.722: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5843 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 28 16:45:51.722: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
Apr 28 16:45:51.796: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:45:51.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5843" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":298,"skipped":4902,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:45:51.831: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2416
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-baee3b54-7241-46de-9e1f-185575cf5f1d
STEP: Creating a pod to test consume secrets
Apr 28 16:45:52.010: INFO: Waiting up to 5m0s for pod "pod-secrets-b13d50ef-dfef-4be3-8fac-7da750758338" in namespace "secrets-2416" to be "Succeeded or Failed"
Apr 28 16:45:52.022: INFO: Pod "pod-secrets-b13d50ef-dfef-4be3-8fac-7da750758338": Phase="Pending", Reason="", readiness=false. Elapsed: 12.47041ms
Apr 28 16:45:54.030: INFO: Pod "pod-secrets-b13d50ef-dfef-4be3-8fac-7da750758338": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020773604s
STEP: Saw pod success
Apr 28 16:45:54.030: INFO: Pod "pod-secrets-b13d50ef-dfef-4be3-8fac-7da750758338" satisfied condition "Succeeded or Failed"
Apr 28 16:45:54.035: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-secrets-b13d50ef-dfef-4be3-8fac-7da750758338 container secret-volume-test: <nil>
STEP: delete the pod
Apr 28 16:45:54.059: INFO: Waiting for pod pod-secrets-b13d50ef-dfef-4be3-8fac-7da750758338 to disappear
Apr 28 16:45:54.064: INFO: Pod pod-secrets-b13d50ef-dfef-4be3-8fac-7da750758338 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:45:54.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2416" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":299,"skipped":4928,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:45:54.075: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Apr 28 16:45:54.266: INFO: Waiting up to 5m0s for pod "client-containers-cf95df98-fb2b-43fa-b4a6-4eba93476a43" in namespace "containers-4856" to be "Succeeded or Failed"
Apr 28 16:45:54.276: INFO: Pod "client-containers-cf95df98-fb2b-43fa-b4a6-4eba93476a43": Phase="Pending", Reason="", readiness=false. Elapsed: 9.155244ms
Apr 28 16:45:56.285: INFO: Pod "client-containers-cf95df98-fb2b-43fa-b4a6-4eba93476a43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019020919s
STEP: Saw pod success
Apr 28 16:45:56.285: INFO: Pod "client-containers-cf95df98-fb2b-43fa-b4a6-4eba93476a43" satisfied condition "Succeeded or Failed"
Apr 28 16:45:56.291: INFO: Trying to get logs from node ip-172-31-13-33 pod client-containers-cf95df98-fb2b-43fa-b4a6-4eba93476a43 container agnhost-container: <nil>
STEP: delete the pod
Apr 28 16:45:56.315: INFO: Waiting for pod client-containers-cf95df98-fb2b-43fa-b4a6-4eba93476a43 to disappear
Apr 28 16:45:56.319: INFO: Pod client-containers-cf95df98-fb2b-43fa-b4a6-4eba93476a43 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:45:56.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4856" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":300,"skipped":4965,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:45:56.331: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4452
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 28 16:45:56.510: INFO: Waiting up to 5m0s for pod "downwardapi-volume-18d32a32-5b78-4362-9912-99e10b73510b" in namespace "downward-api-4452" to be "Succeeded or Failed"
Apr 28 16:45:56.516: INFO: Pod "downwardapi-volume-18d32a32-5b78-4362-9912-99e10b73510b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.245391ms
Apr 28 16:45:58.523: INFO: Pod "downwardapi-volume-18d32a32-5b78-4362-9912-99e10b73510b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012680433s
STEP: Saw pod success
Apr 28 16:45:58.523: INFO: Pod "downwardapi-volume-18d32a32-5b78-4362-9912-99e10b73510b" satisfied condition "Succeeded or Failed"
Apr 28 16:45:58.528: INFO: Trying to get logs from node ip-172-31-13-33 pod downwardapi-volume-18d32a32-5b78-4362-9912-99e10b73510b container client-container: <nil>
STEP: delete the pod
Apr 28 16:45:58.553: INFO: Waiting for pod downwardapi-volume-18d32a32-5b78-4362-9912-99e10b73510b to disappear
Apr 28 16:45:58.557: INFO: Pod downwardapi-volume-18d32a32-5b78-4362-9912-99e10b73510b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:45:58.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4452" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":301,"skipped":4991,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:45:58.574: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-780
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-183729b3-f759-4ae5-acb4-38082192b499 in namespace container-probe-780
Apr 28 16:46:00.791: INFO: Started pod busybox-183729b3-f759-4ae5-acb4-38082192b499 in namespace container-probe-780
STEP: checking the pod's current state and verifying that restartCount is present
Apr 28 16:46:00.795: INFO: Initial restart count of pod busybox-183729b3-f759-4ae5-acb4-38082192b499 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:50:01.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-780" for this suite.

• [SLOW TEST:243.400 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":302,"skipped":4992,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:50:01.974: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8765
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:50:04.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8765" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":303,"skipped":5000,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:50:04.241: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3228
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:50:29.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3228" for this suite.

• [SLOW TEST:25.560 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":304,"skipped":5006,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:50:29.802: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-2958
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Apr 28 16:50:29.986: INFO: created test-event-1
Apr 28 16:50:29.992: INFO: created test-event-2
Apr 28 16:50:29.998: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Apr 28 16:50:30.003: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Apr 28 16:50:30.034: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:50:30.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2958" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":305,"skipped":5105,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:50:30.050: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6544
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 28 16:50:30.235: INFO: Waiting up to 5m0s for pod "pod-55617453-3844-4482-80d2-35f62ee6a4ea" in namespace "emptydir-6544" to be "Succeeded or Failed"
Apr 28 16:50:30.242: INFO: Pod "pod-55617453-3844-4482-80d2-35f62ee6a4ea": Phase="Pending", Reason="", readiness=false. Elapsed: 7.21415ms
Apr 28 16:50:32.249: INFO: Pod "pod-55617453-3844-4482-80d2-35f62ee6a4ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013847627s
STEP: Saw pod success
Apr 28 16:50:32.249: INFO: Pod "pod-55617453-3844-4482-80d2-35f62ee6a4ea" satisfied condition "Succeeded or Failed"
Apr 28 16:50:32.252: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-55617453-3844-4482-80d2-35f62ee6a4ea container test-container: <nil>
STEP: delete the pod
Apr 28 16:50:32.276: INFO: Waiting for pod pod-55617453-3844-4482-80d2-35f62ee6a4ea to disappear
Apr 28 16:50:32.282: INFO: Pod pod-55617453-3844-4482-80d2-35f62ee6a4ea no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:50:32.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6544" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":306,"skipped":5108,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:50:32.297: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4893
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 28 16:50:32.778: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 28 16:50:35.813: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:50:35.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4893" for this suite.
STEP: Destroying namespace "webhook-4893-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":307,"skipped":5204,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:50:35.981: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1266
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 28 16:50:36.188: INFO: Waiting up to 5m0s for pod "pod-22c75448-959e-4bfc-9a8a-c4de61c13855" in namespace "emptydir-1266" to be "Succeeded or Failed"
Apr 28 16:50:36.192: INFO: Pod "pod-22c75448-959e-4bfc-9a8a-c4de61c13855": Phase="Pending", Reason="", readiness=false. Elapsed: 3.412033ms
Apr 28 16:50:38.211: INFO: Pod "pod-22c75448-959e-4bfc-9a8a-c4de61c13855": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022706611s
STEP: Saw pod success
Apr 28 16:50:38.211: INFO: Pod "pod-22c75448-959e-4bfc-9a8a-c4de61c13855" satisfied condition "Succeeded or Failed"
Apr 28 16:50:38.217: INFO: Trying to get logs from node ip-172-31-13-33 pod pod-22c75448-959e-4bfc-9a8a-c4de61c13855 container test-container: <nil>
STEP: delete the pod
Apr 28 16:50:38.318: INFO: Waiting for pod pod-22c75448-959e-4bfc-9a8a-c4de61c13855 to disappear
Apr 28 16:50:38.322: INFO: Pod pod-22c75448-959e-4bfc-9a8a-c4de61c13855 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:50:38.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1266" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":308,"skipped":5205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:50:38.339: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6899
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:50:40.556: INFO: Deleting pod "var-expansion-91516ef9-3ab0-4716-b86a-12a7adb9a84c" in namespace "var-expansion-6899"
Apr 28 16:50:40.568: INFO: Wait up to 5m0s for pod "var-expansion-91516ef9-3ab0-4716-b86a-12a7adb9a84c" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:50:42.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6899" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":309,"skipped":5242,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:50:42.598: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-4545
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 28 16:50:42.776: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 28 16:51:42.797: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:51:42.801: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-9662
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:51:43.051: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Apr 28 16:51:43.055: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:51:43.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9662" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:51:43.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4545" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.654 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":310,"skipped":5260,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 28 16:51:43.253: INFO: >>> kubeConfig: /tmp/kubeconfig-091929713
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8214
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 28 16:51:43.504: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"1a513cd8-c6bd-4823-a79c-680ac3607544", Controller:(*bool)(0xc0038f28e6), BlockOwnerDeletion:(*bool)(0xc0038f28e7)}}
Apr 28 16:51:43.520: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"86fb6043-7947-43fb-ae4d-79c731d3f2af", Controller:(*bool)(0xc0035b136e), BlockOwnerDeletion:(*bool)(0xc0035b136f)}}
Apr 28 16:51:43.531: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"ec34dd1e-962c-4f38-9779-a6c06b86886b", Controller:(*bool)(0xc0038f2ac6), BlockOwnerDeletion:(*bool)(0xc0038f2ac7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 28 16:51:48.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8214" for this suite.

• [SLOW TEST:5.326 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":311,"skipped":5329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSApr 28 16:51:48.580: INFO: Running AfterSuite actions on all nodes
Apr 28 16:51:48.580: INFO: Running AfterSuite actions on node 1
Apr 28 16:51:48.580: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 5065.100 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
PASS

Ginkgo ran 1 suite in 1h24m26.206250923s
Test Suite Passed
