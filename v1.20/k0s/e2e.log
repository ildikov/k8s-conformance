I0303 08:53:45.532589      24 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-613311166
I0303 08:53:45.532995      24 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0303 08:53:45.533313      24 e2e.go:129] Starting e2e run "670f6e94-0271-41aa-b53d-e1904003cb9e" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1614761624 - Will randomize all specs
Will run 311 of 5667 specs

Mar  3 08:53:45.547: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
E0303 08:53:45.547777      24 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Mar  3 08:53:45.550: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar  3 08:53:45.656: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  3 08:53:45.730: INFO: 12 / 12 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  3 08:53:45.730: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Mar  3 08:53:45.730: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  3 08:53:45.749: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Mar  3 08:53:45.749: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
Mar  3 08:53:45.749: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar  3 08:53:45.749: INFO: e2e test version: v1.20.4
Mar  3 08:53:45.750: INFO: kube-apiserver version: v1.20.4-k0s1
Mar  3 08:53:45.750: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 08:53:45.755: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:53:45.758: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename security-context-test
Mar  3 08:53:45.785: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Mar  3 08:53:45.790: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 08:53:45.798: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-70af0848-6a43-4ec8-8c42-6abdc3cebc6e" in namespace "security-context-test-8253" to be "Succeeded or Failed"
Mar  3 08:53:45.800: INFO: Pod "busybox-readonly-false-70af0848-6a43-4ec8-8c42-6abdc3cebc6e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.674413ms
Mar  3 08:53:47.803: INFO: Pod "busybox-readonly-false-70af0848-6a43-4ec8-8c42-6abdc3cebc6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005474067s
Mar  3 08:53:49.811: INFO: Pod "busybox-readonly-false-70af0848-6a43-4ec8-8c42-6abdc3cebc6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012867617s
Mar  3 08:53:49.811: INFO: Pod "busybox-readonly-false-70af0848-6a43-4ec8-8c42-6abdc3cebc6e" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:53:49.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8253" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":1,"skipped":61,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:53:49.818: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar  3 08:53:49.854: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 08:53:53.563: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:54:07.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4459" for this suite.

• [SLOW TEST:18.061 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":2,"skipped":97,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:54:07.880: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 08:54:07.922: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f2d7161-b597-447d-8c3e-a3e798183d09" in namespace "downward-api-5646" to be "Succeeded or Failed"
Mar  3 08:54:07.926: INFO: Pod "downwardapi-volume-1f2d7161-b597-447d-8c3e-a3e798183d09": Phase="Pending", Reason="", readiness=false. Elapsed: 3.825917ms
Mar  3 08:54:09.935: INFO: Pod "downwardapi-volume-1f2d7161-b597-447d-8c3e-a3e798183d09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012913284s
Mar  3 08:54:11.945: INFO: Pod "downwardapi-volume-1f2d7161-b597-447d-8c3e-a3e798183d09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022662317s
Mar  3 08:54:13.951: INFO: Pod "downwardapi-volume-1f2d7161-b597-447d-8c3e-a3e798183d09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029010725s
STEP: Saw pod success
Mar  3 08:54:13.951: INFO: Pod "downwardapi-volume-1f2d7161-b597-447d-8c3e-a3e798183d09" satisfied condition "Succeeded or Failed"
Mar  3 08:54:13.953: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-1f2d7161-b597-447d-8c3e-a3e798183d09 container client-container: <nil>
STEP: delete the pod
Mar  3 08:54:13.978: INFO: Waiting for pod downwardapi-volume-1f2d7161-b597-447d-8c3e-a3e798183d09 to disappear
Mar  3 08:54:13.980: INFO: Pod downwardapi-volume-1f2d7161-b597-447d-8c3e-a3e798183d09 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:54:13.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5646" for this suite.

• [SLOW TEST:6.106 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":3,"skipped":104,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:54:13.987: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Mar  3 08:54:14.032: INFO: Waiting up to 5m0s for pod "client-containers-c2bf968a-e0df-417c-b1b2-a9d2794ba734" in namespace "containers-8912" to be "Succeeded or Failed"
Mar  3 08:54:14.034: INFO: Pod "client-containers-c2bf968a-e0df-417c-b1b2-a9d2794ba734": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188413ms
Mar  3 08:54:16.037: INFO: Pod "client-containers-c2bf968a-e0df-417c-b1b2-a9d2794ba734": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005103987s
STEP: Saw pod success
Mar  3 08:54:16.037: INFO: Pod "client-containers-c2bf968a-e0df-417c-b1b2-a9d2794ba734" satisfied condition "Succeeded or Failed"
Mar  3 08:54:16.039: INFO: Trying to get logs from node worker-1 pod client-containers-c2bf968a-e0df-417c-b1b2-a9d2794ba734 container agnhost-container: <nil>
STEP: delete the pod
Mar  3 08:54:16.051: INFO: Waiting for pod client-containers-c2bf968a-e0df-417c-b1b2-a9d2794ba734 to disappear
Mar  3 08:54:16.053: INFO: Pod client-containers-c2bf968a-e0df-417c-b1b2-a9d2794ba734 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:54:16.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8912" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":4,"skipped":105,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:54:16.064: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 08:54:16.110: INFO: Create a RollingUpdate DaemonSet
Mar  3 08:54:16.116: INFO: Check that daemon pods launch on every node of the cluster
Mar  3 08:54:16.121: INFO: Number of nodes with available pods: 0
Mar  3 08:54:16.121: INFO: Node controller-0 is running more than one daemon pod
Mar  3 08:54:17.130: INFO: Number of nodes with available pods: 0
Mar  3 08:54:17.130: INFO: Node controller-0 is running more than one daemon pod
Mar  3 08:54:18.129: INFO: Number of nodes with available pods: 0
Mar  3 08:54:18.129: INFO: Node controller-0 is running more than one daemon pod
Mar  3 08:54:19.130: INFO: Number of nodes with available pods: 0
Mar  3 08:54:19.130: INFO: Node controller-0 is running more than one daemon pod
Mar  3 08:54:20.134: INFO: Number of nodes with available pods: 0
Mar  3 08:54:20.134: INFO: Node controller-0 is running more than one daemon pod
Mar  3 08:54:21.135: INFO: Number of nodes with available pods: 0
Mar  3 08:54:21.135: INFO: Node controller-0 is running more than one daemon pod
Mar  3 08:54:22.212: INFO: Number of nodes with available pods: 0
Mar  3 08:54:22.212: INFO: Node controller-0 is running more than one daemon pod
Mar  3 08:54:23.134: INFO: Number of nodes with available pods: 1
Mar  3 08:54:23.134: INFO: Node controller-0 is running more than one daemon pod
Mar  3 08:54:24.130: INFO: Number of nodes with available pods: 3
Mar  3 08:54:24.130: INFO: Number of running nodes: 3, number of available pods: 3
Mar  3 08:54:24.130: INFO: Update the DaemonSet to trigger a rollout
Mar  3 08:54:24.137: INFO: Updating DaemonSet daemon-set
Mar  3 08:54:31.157: INFO: Roll back the DaemonSet before rollout is complete
Mar  3 08:54:31.163: INFO: Updating DaemonSet daemon-set
Mar  3 08:54:31.163: INFO: Make sure DaemonSet rollback is complete
Mar  3 08:54:31.167: INFO: Wrong image for pod: daemon-set-2tzd4. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  3 08:54:31.167: INFO: Pod daemon-set-2tzd4 is not available
Mar  3 08:54:32.176: INFO: Wrong image for pod: daemon-set-2tzd4. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  3 08:54:32.176: INFO: Pod daemon-set-2tzd4 is not available
Mar  3 08:54:33.177: INFO: Wrong image for pod: daemon-set-2tzd4. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  3 08:54:33.177: INFO: Pod daemon-set-2tzd4 is not available
Mar  3 08:54:34.177: INFO: Wrong image for pod: daemon-set-2tzd4. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  3 08:54:34.177: INFO: Pod daemon-set-2tzd4 is not available
Mar  3 08:54:35.178: INFO: Wrong image for pod: daemon-set-2tzd4. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  3 08:54:35.178: INFO: Pod daemon-set-2tzd4 is not available
Mar  3 08:54:36.177: INFO: Wrong image for pod: daemon-set-2tzd4. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  3 08:54:36.177: INFO: Pod daemon-set-2tzd4 is not available
Mar  3 08:54:37.175: INFO: Wrong image for pod: daemon-set-2tzd4. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  3 08:54:37.175: INFO: Pod daemon-set-2tzd4 is not available
Mar  3 08:54:38.177: INFO: Wrong image for pod: daemon-set-2tzd4. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  3 08:54:38.177: INFO: Pod daemon-set-2tzd4 is not available
Mar  3 08:54:39.178: INFO: Wrong image for pod: daemon-set-2tzd4. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  3 08:54:39.178: INFO: Pod daemon-set-2tzd4 is not available
Mar  3 08:54:40.178: INFO: Wrong image for pod: daemon-set-2tzd4. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  3 08:54:40.178: INFO: Pod daemon-set-2tzd4 is not available
Mar  3 08:54:41.176: INFO: Pod daemon-set-rwmnx is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3535, will wait for the garbage collector to delete the pods
Mar  3 08:54:41.252: INFO: Deleting DaemonSet.extensions daemon-set took: 14.82008ms
Mar  3 08:54:41.952: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.273621ms
Mar  3 08:56:01.058: INFO: Number of nodes with available pods: 0
Mar  3 08:56:01.058: INFO: Number of running nodes: 0, number of available pods: 0
Mar  3 08:56:01.063: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1370"},"items":null}

Mar  3 08:56:01.065: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1370"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:56:01.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3535" for this suite.

• [SLOW TEST:105.016 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":5,"skipped":112,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:56:01.085: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-a2faa874-fbaf-4175-b3da-a90bf1d1728c
STEP: Creating a pod to test consume secrets
Mar  3 08:56:01.165: INFO: Waiting up to 5m0s for pod "pod-secrets-66be38ac-6945-482b-86de-3011fb5caf93" in namespace "secrets-3066" to be "Succeeded or Failed"
Mar  3 08:56:01.167: INFO: Pod "pod-secrets-66be38ac-6945-482b-86de-3011fb5caf93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.295185ms
Mar  3 08:56:03.176: INFO: Pod "pod-secrets-66be38ac-6945-482b-86de-3011fb5caf93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011340465s
STEP: Saw pod success
Mar  3 08:56:03.176: INFO: Pod "pod-secrets-66be38ac-6945-482b-86de-3011fb5caf93" satisfied condition "Succeeded or Failed"
Mar  3 08:56:03.178: INFO: Trying to get logs from node worker-1 pod pod-secrets-66be38ac-6945-482b-86de-3011fb5caf93 container secret-volume-test: <nil>
STEP: delete the pod
Mar  3 08:56:03.205: INFO: Waiting for pod pod-secrets-66be38ac-6945-482b-86de-3011fb5caf93 to disappear
Mar  3 08:56:03.207: INFO: Pod pod-secrets-66be38ac-6945-482b-86de-3011fb5caf93 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:56:03.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3066" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":6,"skipped":129,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:56:03.214: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Mar  3 08:56:03.247: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-8334 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:56:03.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8334" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":7,"skipped":134,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:56:03.324: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3035
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Mar  3 08:56:03.376: INFO: Found 0 stateful pods, waiting for 3
Mar  3 08:56:13.382: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  3 08:56:13.382: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  3 08:56:13.382: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar  3 08:56:13.406: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar  3 08:56:23.449: INFO: Updating stateful set ss2
Mar  3 08:56:23.492: INFO: Waiting for Pod statefulset-3035/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Mar  3 08:56:33.570: INFO: Found 2 stateful pods, waiting for 3
Mar  3 08:56:43.583: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  3 08:56:43.583: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  3 08:56:43.583: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar  3 08:56:43.606: INFO: Updating stateful set ss2
Mar  3 08:56:43.634: INFO: Waiting for Pod statefulset-3035/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  3 08:56:53.671: INFO: Updating stateful set ss2
Mar  3 08:56:53.684: INFO: Waiting for StatefulSet statefulset-3035/ss2 to complete update
Mar  3 08:56:53.684: INFO: Waiting for Pod statefulset-3035/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  3 08:57:03.695: INFO: Waiting for StatefulSet statefulset-3035/ss2 to complete update
Mar  3 08:57:03.695: INFO: Waiting for Pod statefulset-3035/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  3 08:57:13.696: INFO: Waiting for StatefulSet statefulset-3035/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  3 08:57:23.697: INFO: Deleting all statefulset in ns statefulset-3035
Mar  3 08:57:23.699: INFO: Scaling statefulset ss2 to 0
Mar  3 08:57:53.728: INFO: Waiting for statefulset status.replicas updated to 0
Mar  3 08:57:53.730: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:57:53.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3035" for this suite.

• [SLOW TEST:110.457 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":8,"skipped":148,"failed":0}
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:57:53.782: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 08:57:55.834: INFO: Deleting pod "var-expansion-b46576f0-501f-4ef5-920d-81b122295344" in namespace "var-expansion-2486"
Mar  3 08:57:55.839: INFO: Wait up to 5m0s for pod "var-expansion-b46576f0-501f-4ef5-920d-81b122295344" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:57:59.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2486" for this suite.

• [SLOW TEST:6.075 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":9,"skipped":154,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:57:59.857: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  3 08:57:59.895: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2710  12fb26e9-75de-41b1-90ca-f902dba8f6c2 1932 0 2021-03-03 08:57:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-03 08:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  3 08:57:59.895: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2710  12fb26e9-75de-41b1-90ca-f902dba8f6c2 1932 0 2021-03-03 08:57:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-03 08:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar  3 08:58:09.905: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2710  12fb26e9-75de-41b1-90ca-f902dba8f6c2 1949 0 2021-03-03 08:57:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-03 08:58:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  3 08:58:09.905: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2710  12fb26e9-75de-41b1-90ca-f902dba8f6c2 1949 0 2021-03-03 08:57:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-03 08:58:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  3 08:58:19.923: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2710  12fb26e9-75de-41b1-90ca-f902dba8f6c2 1954 0 2021-03-03 08:57:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-03 08:58:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  3 08:58:19.923: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2710  12fb26e9-75de-41b1-90ca-f902dba8f6c2 1954 0 2021-03-03 08:57:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-03 08:58:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar  3 08:58:29.935: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2710  12fb26e9-75de-41b1-90ca-f902dba8f6c2 1959 0 2021-03-03 08:57:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-03 08:58:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  3 08:58:29.935: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2710  12fb26e9-75de-41b1-90ca-f902dba8f6c2 1959 0 2021-03-03 08:57:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-03 08:58:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  3 08:58:39.950: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2710  264a816b-c9db-4c66-a974-a94a58ba7628 1964 0 2021-03-03 08:58:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-03 08:58:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  3 08:58:39.950: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2710  264a816b-c9db-4c66-a974-a94a58ba7628 1964 0 2021-03-03 08:58:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-03 08:58:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar  3 08:58:49.960: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2710  264a816b-c9db-4c66-a974-a94a58ba7628 1969 0 2021-03-03 08:58:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-03 08:58:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  3 08:58:49.960: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2710  264a816b-c9db-4c66-a974-a94a58ba7628 1969 0 2021-03-03 08:58:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-03 08:58:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:58:59.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2710" for this suite.

• [SLOW TEST:60.122 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":10,"skipped":176,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:58:59.983: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  3 08:59:01.162: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 08:59:04.206: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 08:59:04.211: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3386-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:59:05.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2503" for this suite.
STEP: Destroying namespace "webhook-2503-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.518 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":11,"skipped":181,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:59:05.507: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  3 08:59:13.588: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 08:59:13.591: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 08:59:15.591: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 08:59:15.597: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 08:59:17.591: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 08:59:17.595: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 08:59:19.591: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 08:59:19.596: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 08:59:21.591: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 08:59:21.602: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 08:59:23.591: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 08:59:23.598: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 08:59:25.591: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 08:59:25.601: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 08:59:27.591: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 08:59:27.627: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  3 08:59:29.591: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  3 08:59:29.599: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:59:29.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5468" for this suite.

• [SLOW TEST:24.119 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":12,"skipped":226,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:59:29.628: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:59:31.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8703" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":13,"skipped":257,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:59:31.711: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 08:59:31.743: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  3 08:59:35.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-1258 --namespace=crd-publish-openapi-1258 create -f -'
Mar  3 08:59:36.038: INFO: stderr: ""
Mar  3 08:59:36.038: INFO: stdout: "e2e-test-crd-publish-openapi-2858-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  3 08:59:36.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-1258 --namespace=crd-publish-openapi-1258 delete e2e-test-crd-publish-openapi-2858-crds test-cr'
Mar  3 08:59:36.130: INFO: stderr: ""
Mar  3 08:59:36.130: INFO: stdout: "e2e-test-crd-publish-openapi-2858-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar  3 08:59:36.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-1258 --namespace=crd-publish-openapi-1258 apply -f -'
Mar  3 08:59:36.358: INFO: stderr: ""
Mar  3 08:59:36.358: INFO: stdout: "e2e-test-crd-publish-openapi-2858-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  3 08:59:36.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-1258 --namespace=crd-publish-openapi-1258 delete e2e-test-crd-publish-openapi-2858-crds test-cr'
Mar  3 08:59:36.454: INFO: stderr: ""
Mar  3 08:59:36.454: INFO: stdout: "e2e-test-crd-publish-openapi-2858-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar  3 08:59:36.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-1258 explain e2e-test-crd-publish-openapi-2858-crds'
Mar  3 08:59:36.693: INFO: stderr: ""
Mar  3 08:59:36.693: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2858-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:59:40.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1258" for this suite.

• [SLOW TEST:8.654 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":14,"skipped":274,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:59:40.365: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Mar  3 08:59:40.402: INFO: Waiting up to 5m0s for pod "client-containers-27e9dd8f-ade5-4137-a398-e6cfada9bf6a" in namespace "containers-2834" to be "Succeeded or Failed"
Mar  3 08:59:40.405: INFO: Pod "client-containers-27e9dd8f-ade5-4137-a398-e6cfada9bf6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.766368ms
Mar  3 08:59:42.408: INFO: Pod "client-containers-27e9dd8f-ade5-4137-a398-e6cfada9bf6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005283098s
Mar  3 08:59:44.416: INFO: Pod "client-containers-27e9dd8f-ade5-4137-a398-e6cfada9bf6a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013859328s
Mar  3 08:59:46.423: INFO: Pod "client-containers-27e9dd8f-ade5-4137-a398-e6cfada9bf6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020824768s
STEP: Saw pod success
Mar  3 08:59:46.423: INFO: Pod "client-containers-27e9dd8f-ade5-4137-a398-e6cfada9bf6a" satisfied condition "Succeeded or Failed"
Mar  3 08:59:46.426: INFO: Trying to get logs from node controller-0 pod client-containers-27e9dd8f-ade5-4137-a398-e6cfada9bf6a container agnhost-container: <nil>
STEP: delete the pod
Mar  3 08:59:46.457: INFO: Waiting for pod client-containers-27e9dd8f-ade5-4137-a398-e6cfada9bf6a to disappear
Mar  3 08:59:46.461: INFO: Pod client-containers-27e9dd8f-ade5-4137-a398-e6cfada9bf6a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:59:46.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2834" for this suite.

• [SLOW TEST:6.109 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":15,"skipped":275,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:59:46.477: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-3f4b2a67-464d-4c19-9b3e-2d771336e0f7
STEP: Creating a pod to test consume secrets
Mar  3 08:59:46.551: INFO: Waiting up to 5m0s for pod "pod-secrets-81fe99c6-db10-4ddc-b23f-01618308bfbe" in namespace "secrets-1816" to be "Succeeded or Failed"
Mar  3 08:59:46.554: INFO: Pod "pod-secrets-81fe99c6-db10-4ddc-b23f-01618308bfbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.992877ms
Mar  3 08:59:48.559: INFO: Pod "pod-secrets-81fe99c6-db10-4ddc-b23f-01618308bfbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007704317s
STEP: Saw pod success
Mar  3 08:59:48.559: INFO: Pod "pod-secrets-81fe99c6-db10-4ddc-b23f-01618308bfbe" satisfied condition "Succeeded or Failed"
Mar  3 08:59:48.561: INFO: Trying to get logs from node controller-0 pod pod-secrets-81fe99c6-db10-4ddc-b23f-01618308bfbe container secret-volume-test: <nil>
STEP: delete the pod
Mar  3 08:59:48.578: INFO: Waiting for pod pod-secrets-81fe99c6-db10-4ddc-b23f-01618308bfbe to disappear
Mar  3 08:59:48.582: INFO: Pod pod-secrets-81fe99c6-db10-4ddc-b23f-01618308bfbe no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:59:48.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1816" for this suite.
STEP: Destroying namespace "secret-namespace-7136" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":16,"skipped":343,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:59:48.594: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 08:59:48.618: INFO: Creating deployment "webserver-deployment"
Mar  3 08:59:48.623: INFO: Waiting for observed generation 1
Mar  3 08:59:50.643: INFO: Waiting for all required pods to come up
Mar  3 08:59:50.651: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar  3 08:59:52.675: INFO: Waiting for deployment "webserver-deployment" to complete
Mar  3 08:59:52.679: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar  3 08:59:52.686: INFO: Updating deployment webserver-deployment
Mar  3 08:59:52.686: INFO: Waiting for observed generation 2
Mar  3 08:59:54.697: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  3 08:59:54.699: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  3 08:59:54.702: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  3 08:59:54.707: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  3 08:59:54.707: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  3 08:59:54.709: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  3 08:59:54.712: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar  3 08:59:54.712: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar  3 08:59:54.719: INFO: Updating deployment webserver-deployment
Mar  3 08:59:54.719: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar  3 08:59:54.724: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  3 08:59:54.732: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  3 08:59:54.762: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8403  56884fc0-68f5-43b9-be9d-ed032bcde3b3 2547 3 2021-03-03 08:59:48 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-03 08:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003eaf578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-03-03 08:59:52 +0000 UTC,LastTransitionTime:2021-03-03 08:59:48 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-03-03 08:59:54 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar  3 08:59:54.831: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-8403  c72af2d5-794f-4f99-81e9-8515dc8ef717 2543 3 2021-03-03 08:59:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 56884fc0-68f5-43b9-be9d-ed032bcde3b3 0xc003eca217 0xc003eca218}] []  [{kube-controller-manager Update apps/v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56884fc0-68f5-43b9-be9d-ed032bcde3b3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003eca2f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  3 08:59:54.831: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar  3 08:59:54.831: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-8403  57dc5074-85cc-4d1a-b8bd-2026bcda7d15 2540 3 2021-03-03 08:59:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 56884fc0-68f5-43b9-be9d-ed032bcde3b3 0xc003eca357 0xc003eca358}] []  [{kube-controller-manager Update apps/v1 2021-03-03 08:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56884fc0-68f5-43b9-be9d-ed032bcde3b3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003eca3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar  3 08:59:54.865: INFO: Pod "webserver-deployment-795d758f88-4gdjz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4gdjz webserver-deployment-795d758f88- deployment-8403  4cf46e8c-f8c1-4fa3-806b-ffb5e65ec11e 2567 0 2021-03-03 08:59:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c72af2d5-794f-4f99-81e9-8515dc8ef717 0xc003eafcc0 0xc003eafcc1}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c72af2d5-794f-4f99-81e9-8515dc8ef717\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.865: INFO: Pod "webserver-deployment-795d758f88-4k5cm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4k5cm webserver-deployment-795d758f88- deployment-8403  20266e60-8971-483e-974e-ca942ff5d840 2506 0 2021-03-03 08:59:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.226.85/32 cni.projectcalico.org/podIPs:10.244.226.85/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c72af2d5-794f-4f99-81e9-8515dc8ef717 0xc003eafe87 0xc003eafe88}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c72af2d5-794f-4f99-81e9-8515dc8ef717\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-03 08:59:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.47.86,PodIP:,StartTime:2021-03-03 08:59:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.865: INFO: Pod "webserver-deployment-795d758f88-72t29" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-72t29 webserver-deployment-795d758f88- deployment-8403  1485d049-c11f-4c7c-a580-7748a563d02c 2559 0 2021-03-03 08:59:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c72af2d5-794f-4f99-81e9-8515dc8ef717 0xc003ef40e0 0xc003ef40e1}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c72af2d5-794f-4f99-81e9-8515dc8ef717\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.866: INFO: Pod "webserver-deployment-795d758f88-7fcml" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7fcml webserver-deployment-795d758f88- deployment-8403  aef70d93-2d89-49fd-840e-8f593618bea2 2492 0 2021-03-03 08:59:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.192.79/32 cni.projectcalico.org/podIPs:10.244.192.79/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c72af2d5-794f-4f99-81e9-8515dc8ef717 0xc003ef4320 0xc003ef4321}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c72af2d5-794f-4f99-81e9-8515dc8ef717\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-03 08:59:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.49.5,PodIP:,StartTime:2021-03-03 08:59:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.866: INFO: Pod "webserver-deployment-795d758f88-7l8hr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7l8hr webserver-deployment-795d758f88- deployment-8403  6e3aabf4-28c3-4a61-b3e9-4abebddfb74e 2572 0 2021-03-03 08:59:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c72af2d5-794f-4f99-81e9-8515dc8ef717 0xc003ef4600 0xc003ef4601}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c72af2d5-794f-4f99-81e9-8515dc8ef717\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.49.5,PodIP:,StartTime:2021-03-03 08:59:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.866: INFO: Pod "webserver-deployment-795d758f88-9ggjk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9ggjk webserver-deployment-795d758f88- deployment-8403  e1bdb0e0-ce53-4077-9f21-85305c53e29a 2496 0 2021-03-03 08:59:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.43.9/32 cni.projectcalico.org/podIPs:10.244.43.9/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c72af2d5-794f-4f99-81e9-8515dc8ef717 0xc003ef48f0 0xc003ef48f1}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c72af2d5-794f-4f99-81e9-8515dc8ef717\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-03 08:59:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.23,PodIP:,StartTime:2021-03-03 08:59:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.866: INFO: Pod "webserver-deployment-795d758f88-cfq27" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-cfq27 webserver-deployment-795d758f88- deployment-8403  cc662ff9-b48a-47a0-83dc-f8c50bb0290d 2569 0 2021-03-03 08:59:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c72af2d5-794f-4f99-81e9-8515dc8ef717 0xc003ef4ca0 0xc003ef4ca1}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c72af2d5-794f-4f99-81e9-8515dc8ef717\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.866: INFO: Pod "webserver-deployment-795d758f88-npbn7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-npbn7 webserver-deployment-795d758f88- deployment-8403  fb5aaa85-934c-4783-8bfa-d218d88936b2 2566 0 2021-03-03 08:59:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c72af2d5-794f-4f99-81e9-8515dc8ef717 0xc003ef4e87 0xc003ef4e88}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c72af2d5-794f-4f99-81e9-8515dc8ef717\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.866: INFO: Pod "webserver-deployment-795d758f88-pht2z" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pht2z webserver-deployment-795d758f88- deployment-8403  7d12f469-4f03-444c-8abf-0287ea60b881 2570 0 2021-03-03 08:59:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c72af2d5-794f-4f99-81e9-8515dc8ef717 0xc003ef5097 0xc003ef5098}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c72af2d5-794f-4f99-81e9-8515dc8ef717\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.867: INFO: Pod "webserver-deployment-795d758f88-qlcqm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qlcqm webserver-deployment-795d758f88- deployment-8403  5dd36c32-8746-45d3-ae5e-7a0374c77147 2508 0 2021-03-03 08:59:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.226.86/32 cni.projectcalico.org/podIPs:10.244.226.86/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c72af2d5-794f-4f99-81e9-8515dc8ef717 0xc003ef5227 0xc003ef5228}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c72af2d5-794f-4f99-81e9-8515dc8ef717\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-03 08:59:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.47.86,PodIP:,StartTime:2021-03-03 08:59:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.867: INFO: Pod "webserver-deployment-795d758f88-vdkdf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vdkdf webserver-deployment-795d758f88- deployment-8403  16069fd7-0d94-46f6-9d5b-74f2ba77f935 2507 0 2021-03-03 08:59:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.43.10/32 cni.projectcalico.org/podIPs:10.244.43.10/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c72af2d5-794f-4f99-81e9-8515dc8ef717 0xc003ef5470 0xc003ef5471}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c72af2d5-794f-4f99-81e9-8515dc8ef717\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-03 08:59:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.23,PodIP:,StartTime:2021-03-03 08:59:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.867: INFO: Pod "webserver-deployment-795d758f88-x27ds" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-x27ds webserver-deployment-795d758f88- deployment-8403  44c9c70c-aede-4c28-8494-e1b2e0ecce1d 2557 0 2021-03-03 08:59:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c72af2d5-794f-4f99-81e9-8515dc8ef717 0xc003ef5720 0xc003ef5721}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c72af2d5-794f-4f99-81e9-8515dc8ef717\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.867: INFO: Pod "webserver-deployment-dd94f59b7-2tqsv" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2tqsv webserver-deployment-dd94f59b7- deployment-8403  23d7fad9-937c-4462-b216-f5df4b70aca0 2421 0 2021-03-03 08:59:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.43.6/32 cni.projectcalico.org/podIPs:10.244.43.6/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003ef5900 0xc003ef5901}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-03 08:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.43.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.23,PodIP:10.244.43.6,StartTime:2021-03-03 08:59:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-03 08:59:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://da19f504fc873dff0a5a545d563d0eb3d919fcd7083e9cf2613d36bc3469d90d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.43.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.867: INFO: Pod "webserver-deployment-dd94f59b7-59cdd" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-59cdd webserver-deployment-dd94f59b7- deployment-8403  9047e85e-c4bc-4dce-ba0f-fd4ebc04f38a 2429 0 2021-03-03 08:59:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.192.77/32 cni.projectcalico.org/podIPs:10.244.192.77/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003ef5b80 0xc003ef5b81}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-03 08:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.192.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.49.5,PodIP:10.244.192.77,StartTime:2021-03-03 08:59:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-03 08:59:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://d72ae683c80569ac07a0578956efe379795107c979fbe05f808eeb34896af480,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.192.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.868: INFO: Pod "webserver-deployment-dd94f59b7-5mwqh" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5mwqh webserver-deployment-dd94f59b7- deployment-8403  161c8c5f-6b55-4096-9954-fc971ef089ee 2354 0 2021-03-03 08:59:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.226.83/32 cni.projectcalico.org/podIPs:10.244.226.83/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003ef5e20 0xc003ef5e21}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-03 08:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-03 08:59:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.226.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.47.86,PodIP:10.244.226.83,StartTime:2021-03-03 08:59:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-03 08:59:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://7245b33c6112260f294ffc43ac60ce590860c5cb7edfdfb036b303fd1287c0ad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.226.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.868: INFO: Pod "webserver-deployment-dd94f59b7-65mlk" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-65mlk webserver-deployment-dd94f59b7- deployment-8403  78ddec43-1a10-447f-b3bd-cef74e9276f7 2580 0 2021-03-03 08:59:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003f160e0 0xc003f160e1}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.868: INFO: Pod "webserver-deployment-dd94f59b7-6zngd" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6zngd webserver-deployment-dd94f59b7- deployment-8403  d1a886e0-5363-4a12-919f-beabee143676 2555 0 2021-03-03 08:59:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003f16287 0xc003f16288}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.868: INFO: Pod "webserver-deployment-dd94f59b7-cbvss" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-cbvss webserver-deployment-dd94f59b7- deployment-8403  74a0133e-872b-40a6-83c9-aa8a8865862a 2423 0 2021-03-03 08:59:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.43.7/32 cni.projectcalico.org/podIPs:10.244.43.7/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003f16440 0xc003f16441}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-03 08:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.43.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.23,PodIP:10.244.43.7,StartTime:2021-03-03 08:59:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-03 08:59:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://b02cfbeaeeb9f2b9d4460be894107db55b59a54c2d792fd217ad02b29a4c69dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.43.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.868: INFO: Pod "webserver-deployment-dd94f59b7-fslfl" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fslfl webserver-deployment-dd94f59b7- deployment-8403  220211a2-28e5-4baf-9888-2dcf86eef6d8 2581 0 2021-03-03 08:59:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003f167c0 0xc003f167c1}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.868: INFO: Pod "webserver-deployment-dd94f59b7-g88vv" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-g88vv webserver-deployment-dd94f59b7- deployment-8403  4244cc79-5d6b-42db-9d86-0002bc5da0ad 2352 0 2021-03-03 08:59:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.226.84/32 cni.projectcalico.org/podIPs:10.244.226.84/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003f16947 0xc003f16948}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-03 08:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-03 08:59:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.226.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.47.86,PodIP:10.244.226.84,StartTime:2021-03-03 08:59:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-03 08:59:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://b64a38b8d09a3e67b17666ddb20c99ac4ff0df18397573d4849cbf18a4dfbe23,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.226.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.869: INFO: Pod "webserver-deployment-dd94f59b7-gmdf7" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gmdf7 webserver-deployment-dd94f59b7- deployment-8403  db5bd792-4ef6-4cf1-9329-217ef0d3f985 2418 0 2021-03-03 08:59:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.43.8/32 cni.projectcalico.org/podIPs:10.244.43.8/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003f16bc0 0xc003f16bc1}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-03 08:59:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.43.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.23,PodIP:10.244.43.8,StartTime:2021-03-03 08:59:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-03 08:59:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://1949d3e6e814f9ad629c7f9334ee02bb05d5acdcebbc883b6282965d148a4863,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.43.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.869: INFO: Pod "webserver-deployment-dd94f59b7-htp8p" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-htp8p webserver-deployment-dd94f59b7- deployment-8403  46c5ebc1-ffe7-447a-88cf-c3372ba46ceb 2576 0 2021-03-03 08:59:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003f16e00 0xc003f16e01}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.869: INFO: Pod "webserver-deployment-dd94f59b7-m89n4" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-m89n4 webserver-deployment-dd94f59b7- deployment-8403  1a61566f-af72-4405-a321-cd33673ba3a4 2578 0 2021-03-03 08:59:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003f16ff0 0xc003f16ff1}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.23,PodIP:,StartTime:2021-03-03 08:59:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.869: INFO: Pod "webserver-deployment-dd94f59b7-nbq5q" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nbq5q webserver-deployment-dd94f59b7- deployment-8403  a1cd2669-d24e-4926-a391-9663dc9c24e0 2568 0 2021-03-03 08:59:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003f17220 0xc003f17221}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.869: INFO: Pod "webserver-deployment-dd94f59b7-qvwck" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qvwck webserver-deployment-dd94f59b7- deployment-8403  045b6a3f-880a-4b4d-ae71-547cabd4a99d 2571 0 2021-03-03 08:59:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003f17400 0xc003f17401}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.869: INFO: Pod "webserver-deployment-dd94f59b7-tlntr" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-tlntr webserver-deployment-dd94f59b7- deployment-8403  e533aee7-3e2c-495a-9e9f-c94e77bd77b6 2433 0 2021-03-03 08:59:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.192.76/32 cni.projectcalico.org/podIPs:10.244.192.76/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003f175b0 0xc003f175b1}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-03 08:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.192.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.49.5,PodIP:10.244.192.76,StartTime:2021-03-03 08:59:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-03 08:59:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://937ed798da70d2e46538676afea8c98d3eaedd6c65c2e167e6f6d6fc9736f4c8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.192.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.869: INFO: Pod "webserver-deployment-dd94f59b7-wqj7n" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wqj7n webserver-deployment-dd94f59b7- deployment-8403  a3b4d5d4-c6f9-4374-86e1-4a47a98ce96a 2565 0 2021-03-03 08:59:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003f17840 0xc003f17841}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.47.86,PodIP:,StartTime:2021-03-03 08:59:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.870: INFO: Pod "webserver-deployment-dd94f59b7-wvqg9" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wvqg9 webserver-deployment-dd94f59b7- deployment-8403  bbe76dc3-fe6c-4004-a912-a05ea078abe7 2573 0 2021-03-03 08:59:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003f17a50 0xc003f17a51}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.870: INFO: Pod "webserver-deployment-dd94f59b7-x69kv" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-x69kv webserver-deployment-dd94f59b7- deployment-8403  43403bdb-063e-4910-bbe3-a438df5ea314 2427 0 2021-03-03 08:59:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.192.78/32 cni.projectcalico.org/podIPs:10.244.192.78/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003f17bd0 0xc003f17bd1}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-03 08:59:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-03 08:59:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.192.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 08:59:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.49.5,PodIP:10.244.192.78,StartTime:2021-03-03 08:59:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-03 08:59:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://d3c6db92e91a8c135c4fec5d9282be166b294af0318dbad07eed61740b0d4d10,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.192.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 08:59:54.870: INFO: Pod "webserver-deployment-dd94f59b7-x6pkz" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-x6pkz webserver-deployment-dd94f59b7- deployment-8403  6b0f619b-de70-49e9-9f72-c6042b13d5d6 2579 0 2021-03-03 08:59:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 57dc5074-85cc-4d1a-b8bd-2026bcda7d15 0xc003f17e20 0xc003f17e21}] []  [{kube-controller-manager Update v1 2021-03-03 08:59:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57dc5074-85cc-4d1a-b8bd-2026bcda7d15\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sb2kt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sb2kt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sb2kt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:59:54.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8403" for this suite.

• [SLOW TEST:6.317 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":17,"skipped":406,"failed":0}
SSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:59:54.912: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 08:59:55.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5310" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":18,"skipped":410,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 08:59:55.223: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 08:59:55.593: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  3 08:59:55.648: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  3 09:00:00.686: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  3 09:00:00.686: INFO: Creating deployment "test-rolling-update-deployment"
Mar  3 09:00:00.726: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  3 09:00:00.778: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  3 09:00:02.880: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  3 09:00:02.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750358800, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750358800, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750358800, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750358800, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  3 09:00:04.911: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  3 09:00:04.918: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3745  f3b2e68c-7e2b-4912-bee2-99294a3a39b8 3074 1 2021-03-03 09:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-03-03 09:00:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-03 09:00:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041335f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-03 09:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-03-03 09:00:03 +0000 UTC,LastTransitionTime:2021-03-03 09:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  3 09:00:04.920: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-3745  4f4d0722-5dbc-4371-a257-921fdda5bf2e 3055 1 2021-03-03 09:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment f3b2e68c-7e2b-4912-bee2-99294a3a39b8 0xc004133b87 0xc004133b88}] []  [{kube-controller-manager Update apps/v1 2021-03-03 09:00:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2e68c-7e2b-4912-bee2-99294a3a39b8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004133c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  3 09:00:04.920: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  3 09:00:04.921: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3745  ff32a818-87b3-42c2-83f3-0b9e9af475bb 3072 2 2021-03-03 08:59:55 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment f3b2e68c-7e2b-4912-bee2-99294a3a39b8 0xc004133a5f 0xc004133a70}] []  [{e2e.test Update apps/v1 2021-03-03 08:59:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-03 09:00:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2e68c-7e2b-4912-bee2-99294a3a39b8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004133b18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  3 09:00:04.923: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-dsbmn" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-dsbmn test-rolling-update-deployment-6b6bf9df46- deployment-3745  79d26129-8f68-4145-86be-84457aeb7f46 3053 0 2021-03-03 09:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[cni.projectcalico.org/podIP:10.244.226.92/32 cni.projectcalico.org/podIPs:10.244.226.92/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 4f4d0722-5dbc-4371-a257-921fdda5bf2e 0xc0041841e7 0xc0041841e8}] []  [{kube-controller-manager Update v1 2021-03-03 09:00:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f4d0722-5dbc-4371-a257-921fdda5bf2e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-03 09:00:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-03 09:00:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.226.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g7j4g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g7j4g,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g7j4g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:00:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:00:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:00:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:00:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.47.86,PodIP:10.244.226.92,StartTime:2021-03-03 09:00:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-03 09:00:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://9d35035e781df2d8294b9ed730c7302440c9414bd65c464d7c2f1c959c75e110,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.226.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:00:04.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3745" for this suite.

• [SLOW TEST:9.710 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":19,"skipped":416,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:00:04.934: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9603
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-9603
I0303 09:00:05.006630      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-9603, replica count: 2
Mar  3 09:00:08.058: INFO: Creating new exec pod
I0303 09:00:08.058167      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  3 09:00:11.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-9603 exec execpodvrjhw -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar  3 09:00:11.262: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  3 09:00:11.262: INFO: stdout: ""
Mar  3 09:00:11.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-9603 exec execpodvrjhw -- /bin/sh -x -c nc -zv -t -w 2 10.101.163.247 80'
Mar  3 09:00:11.429: INFO: stderr: "+ nc -zv -t -w 2 10.101.163.247 80\nConnection to 10.101.163.247 80 port [tcp/http] succeeded!\n"
Mar  3 09:00:11.429: INFO: stdout: ""
Mar  3 09:00:11.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-9603 exec execpodvrjhw -- /bin/sh -x -c nc -zv -t -w 2 10.0.49.5 31796'
Mar  3 09:00:11.575: INFO: stderr: "+ nc -zv -t -w 2 10.0.49.5 31796\nConnection to 10.0.49.5 31796 port [tcp/31796] succeeded!\n"
Mar  3 09:00:11.575: INFO: stdout: ""
Mar  3 09:00:11.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-9603 exec execpodvrjhw -- /bin/sh -x -c nc -zv -t -w 2 10.0.47.86 31796'
Mar  3 09:00:11.745: INFO: stderr: "+ nc -zv -t -w 2 10.0.47.86 31796\nConnection to 10.0.47.86 31796 port [tcp/31796] succeeded!\n"
Mar  3 09:00:11.745: INFO: stdout: ""
Mar  3 09:00:11.745: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:00:11.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9603" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.857 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":20,"skipped":417,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:00:11.791: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:00:11.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5544" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":21,"skipped":426,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:00:11.892: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:00:11.974: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:00:12.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-848" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":22,"skipped":427,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:00:13.006: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  3 09:00:13.033: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  3 09:00:13.039: INFO: Waiting for terminating namespaces to be deleted...
Mar  3 09:00:13.042: INFO: 
Logging pods the apiserver thinks is on node controller-0 before test
Mar  3 09:00:13.046: INFO: calico-kube-controllers-5f6546844f-gnl9f from kube-system started at 2021-03-03 08:47:09 +0000 UTC (1 container statuses recorded)
Mar  3 09:00:13.046: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  3 09:00:13.046: INFO: calico-node-468vb from kube-system started at 2021-03-03 08:46:49 +0000 UTC (1 container statuses recorded)
Mar  3 09:00:13.046: INFO: 	Container calico-node ready: true, restart count 0
Mar  3 09:00:13.046: INFO: coredns-5c98d7d4d8-96hpd from kube-system started at 2021-03-03 08:47:11 +0000 UTC (1 container statuses recorded)
Mar  3 09:00:13.046: INFO: 	Container coredns ready: true, restart count 0
Mar  3 09:00:13.046: INFO: konnectivity-agent-882qs from kube-system started at 2021-03-03 08:47:01 +0000 UTC (1 container statuses recorded)
Mar  3 09:00:13.046: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  3 09:00:13.047: INFO: kube-proxy-snqpb from kube-system started at 2021-03-03 08:46:30 +0000 UTC (1 container statuses recorded)
Mar  3 09:00:13.047: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  3 09:00:13.047: INFO: metrics-server-6fbcd86f7b-r2c26 from kube-system started at 2021-03-03 08:47:09 +0000 UTC (1 container statuses recorded)
Mar  3 09:00:13.047: INFO: 	Container metrics-server ready: true, restart count 0
Mar  3 09:00:13.047: INFO: execpodvrjhw from services-9603 started at 2021-03-03 09:00:08 +0000 UTC (1 container statuses recorded)
Mar  3 09:00:13.047: INFO: 	Container agnhost-container ready: true, restart count 0
Mar  3 09:00:13.047: INFO: externalname-service-9nndb from services-9603 started at 2021-03-03 09:00:05 +0000 UTC (1 container statuses recorded)
Mar  3 09:00:13.047: INFO: 	Container externalname-service ready: true, restart count 0
Mar  3 09:00:13.047: INFO: sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-8fglb from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 09:00:13.047: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  3 09:00:13.047: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  3 09:00:13.047: INFO: 
Logging pods the apiserver thinks is on node worker-0 before test
Mar  3 09:00:13.053: INFO: calico-node-4vzkv from kube-system started at 2021-03-03 08:50:42 +0000 UTC (1 container statuses recorded)
Mar  3 09:00:13.053: INFO: 	Container calico-node ready: true, restart count 0
Mar  3 09:00:13.053: INFO: konnectivity-agent-hrn26 from kube-system started at 2021-03-03 08:51:02 +0000 UTC (1 container statuses recorded)
Mar  3 09:00:13.053: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  3 09:00:13.053: INFO: kube-proxy-xwt8k from kube-system started at 2021-03-03 08:50:42 +0000 UTC (1 container statuses recorded)
Mar  3 09:00:13.053: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  3 09:00:13.053: INFO: externalname-service-dps66 from services-9603 started at 2021-03-03 09:00:05 +0000 UTC (1 container statuses recorded)
Mar  3 09:00:13.053: INFO: 	Container externalname-service ready: true, restart count 0
Mar  3 09:00:13.053: INFO: sonobuoy-e2e-job-a123a876f42443b1 from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 09:00:13.053: INFO: 	Container e2e ready: true, restart count 0
Mar  3 09:00:13.053: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  3 09:00:13.053: INFO: sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-xjvxl from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 09:00:13.053: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  3 09:00:13.054: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  3 09:00:13.054: INFO: 
Logging pods the apiserver thinks is on node worker-1 before test
Mar  3 09:00:13.059: INFO: calico-node-d2bzg from kube-system started at 2021-03-03 08:52:07 +0000 UTC (1 container statuses recorded)
Mar  3 09:00:13.059: INFO: 	Container calico-node ready: true, restart count 0
Mar  3 09:00:13.059: INFO: konnectivity-agent-b4b2k from kube-system started at 2021-03-03 08:52:27 +0000 UTC (1 container statuses recorded)
Mar  3 09:00:13.059: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  3 09:00:13.059: INFO: kube-proxy-c7zpn from kube-system started at 2021-03-03 08:52:07 +0000 UTC (1 container statuses recorded)
Mar  3 09:00:13.059: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  3 09:00:13.059: INFO: busybox-scheduling-a595aaea-eeb6-4013-89e0-c21f43c1ae5b from kubelet-test-8703 started at 2021-03-03 08:59:29 +0000 UTC (1 container statuses recorded)
Mar  3 09:00:13.059: INFO: 	Container busybox-scheduling-a595aaea-eeb6-4013-89e0-c21f43c1ae5b ready: false, restart count 0
Mar  3 09:00:13.059: INFO: sonobuoy from sonobuoy started at 2021-03-03 08:53:20 +0000 UTC (1 container statuses recorded)
Mar  3 09:00:13.059: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  3 09:00:13.059: INFO: sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-x4pmv from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 09:00:13.059: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  3 09:00:13.059: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1668c97ed8f96c79], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:00:14.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2238" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":23,"skipped":453,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:00:14.090: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4688
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Mar  3 09:00:14.139: INFO: Found 0 stateful pods, waiting for 3
Mar  3 09:00:24.145: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  3 09:00:24.145: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  3 09:00:24.145: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  3 09:00:24.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-4688 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  3 09:00:24.350: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  3 09:00:24.350: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  3 09:00:24.350: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar  3 09:00:34.383: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar  3 09:00:44.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-4688 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  3 09:00:44.715: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  3 09:00:44.715: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  3 09:00:44.715: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  3 09:00:54.731: INFO: Waiting for StatefulSet statefulset-4688/ss2 to complete update
Mar  3 09:00:54.731: INFO: Waiting for Pod statefulset-4688/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  3 09:00:54.731: INFO: Waiting for Pod statefulset-4688/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  3 09:01:04.746: INFO: Waiting for StatefulSet statefulset-4688/ss2 to complete update
Mar  3 09:01:04.746: INFO: Waiting for Pod statefulset-4688/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  3 09:01:04.746: INFO: Waiting for Pod statefulset-4688/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  3 09:01:14.750: INFO: Waiting for StatefulSet statefulset-4688/ss2 to complete update
Mar  3 09:01:14.750: INFO: Waiting for Pod statefulset-4688/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Mar  3 09:01:24.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-4688 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  3 09:01:24.901: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  3 09:01:24.901: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  3 09:01:24.901: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  3 09:01:34.944: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar  3 09:01:44.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-4688 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  3 09:01:45.127: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  3 09:01:45.127: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  3 09:01:45.127: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  3 09:02:05.153: INFO: Waiting for StatefulSet statefulset-4688/ss2 to complete update
Mar  3 09:02:05.153: INFO: Waiting for Pod statefulset-4688/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  3 09:02:15.166: INFO: Deleting all statefulset in ns statefulset-4688
Mar  3 09:02:15.168: INFO: Scaling statefulset ss2 to 0
Mar  3 09:02:45.192: INFO: Waiting for statefulset status.replicas updated to 0
Mar  3 09:02:45.194: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:02:45.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4688" for this suite.

• [SLOW TEST:151.141 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":24,"skipped":464,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:02:45.232: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:02:45.273: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-5fc1b017-163d-43e1-b2c1-08c6847c2fb6" in namespace "security-context-test-1485" to be "Succeeded or Failed"
Mar  3 09:02:45.276: INFO: Pod "busybox-privileged-false-5fc1b017-163d-43e1-b2c1-08c6847c2fb6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.135604ms
Mar  3 09:02:47.279: INFO: Pod "busybox-privileged-false-5fc1b017-163d-43e1-b2c1-08c6847c2fb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005938186s
Mar  3 09:02:47.279: INFO: Pod "busybox-privileged-false-5fc1b017-163d-43e1-b2c1-08c6847c2fb6" satisfied condition "Succeeded or Failed"
Mar  3 09:02:47.296: INFO: Got logs for pod "busybox-privileged-false-5fc1b017-163d-43e1-b2c1-08c6847c2fb6": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:02:47.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1485" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":25,"skipped":500,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:02:47.303: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  3 09:02:47.340: INFO: Waiting up to 5m0s for pod "pod-ffb1742d-8a44-488d-91c6-cc3c0abbdf81" in namespace "emptydir-3544" to be "Succeeded or Failed"
Mar  3 09:02:47.342: INFO: Pod "pod-ffb1742d-8a44-488d-91c6-cc3c0abbdf81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.786401ms
Mar  3 09:02:49.347: INFO: Pod "pod-ffb1742d-8a44-488d-91c6-cc3c0abbdf81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00739436s
STEP: Saw pod success
Mar  3 09:02:49.347: INFO: Pod "pod-ffb1742d-8a44-488d-91c6-cc3c0abbdf81" satisfied condition "Succeeded or Failed"
Mar  3 09:02:49.350: INFO: Trying to get logs from node worker-1 pod pod-ffb1742d-8a44-488d-91c6-cc3c0abbdf81 container test-container: <nil>
STEP: delete the pod
Mar  3 09:02:49.364: INFO: Waiting for pod pod-ffb1742d-8a44-488d-91c6-cc3c0abbdf81 to disappear
Mar  3 09:02:49.366: INFO: Pod pod-ffb1742d-8a44-488d-91c6-cc3c0abbdf81 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:02:49.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3544" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":26,"skipped":501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:02:49.377: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar  3 09:02:52.442: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:02:53.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3147" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":27,"skipped":541,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:02:53.472: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 09:02:53.510: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6939de60-401f-4d03-8235-4dfbc4c96838" in namespace "downward-api-8823" to be "Succeeded or Failed"
Mar  3 09:02:53.517: INFO: Pod "downwardapi-volume-6939de60-401f-4d03-8235-4dfbc4c96838": Phase="Pending", Reason="", readiness=false. Elapsed: 7.116013ms
Mar  3 09:02:55.522: INFO: Pod "downwardapi-volume-6939de60-401f-4d03-8235-4dfbc4c96838": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011546443s
STEP: Saw pod success
Mar  3 09:02:55.522: INFO: Pod "downwardapi-volume-6939de60-401f-4d03-8235-4dfbc4c96838" satisfied condition "Succeeded or Failed"
Mar  3 09:02:55.524: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-6939de60-401f-4d03-8235-4dfbc4c96838 container client-container: <nil>
STEP: delete the pod
Mar  3 09:02:55.547: INFO: Waiting for pod downwardapi-volume-6939de60-401f-4d03-8235-4dfbc4c96838 to disappear
Mar  3 09:02:55.549: INFO: Pod downwardapi-volume-6939de60-401f-4d03-8235-4dfbc4c96838 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:02:55.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8823" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":28,"skipped":551,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:02:55.555: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-caf34f2c-bcad-422b-8a2a-580af4671750 in namespace container-probe-4062
Mar  3 09:02:57.651: INFO: Started pod liveness-caf34f2c-bcad-422b-8a2a-580af4671750 in namespace container-probe-4062
STEP: checking the pod's current state and verifying that restartCount is present
Mar  3 09:02:57.653: INFO: Initial restart count of pod liveness-caf34f2c-bcad-422b-8a2a-580af4671750 is 0
Mar  3 09:03:17.736: INFO: Restart count of pod container-probe-4062/liveness-caf34f2c-bcad-422b-8a2a-580af4671750 is now 1 (20.083089579s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:03:17.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4062" for this suite.

• [SLOW TEST:22.231 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":29,"skipped":564,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:03:17.790: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-be6e9ec7-f5d7-4283-bc96-a6210d647978
STEP: Creating a pod to test consume configMaps
Mar  3 09:03:17.869: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f5f7b9b1-fa17-4315-a18c-c6436e5fce12" in namespace "projected-8127" to be "Succeeded or Failed"
Mar  3 09:03:17.874: INFO: Pod "pod-projected-configmaps-f5f7b9b1-fa17-4315-a18c-c6436e5fce12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.569096ms
Mar  3 09:03:19.886: INFO: Pod "pod-projected-configmaps-f5f7b9b1-fa17-4315-a18c-c6436e5fce12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016937867s
STEP: Saw pod success
Mar  3 09:03:19.886: INFO: Pod "pod-projected-configmaps-f5f7b9b1-fa17-4315-a18c-c6436e5fce12" satisfied condition "Succeeded or Failed"
Mar  3 09:03:19.891: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-f5f7b9b1-fa17-4315-a18c-c6436e5fce12 container agnhost-container: <nil>
STEP: delete the pod
Mar  3 09:03:19.906: INFO: Waiting for pod pod-projected-configmaps-f5f7b9b1-fa17-4315-a18c-c6436e5fce12 to disappear
Mar  3 09:03:19.909: INFO: Pod pod-projected-configmaps-f5f7b9b1-fa17-4315-a18c-c6436e5fce12 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:03:19.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8127" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":30,"skipped":571,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:03:19.919: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  3 09:03:19.963: INFO: Waiting up to 5m0s for pod "pod-5bccabd2-671a-4040-b614-b6df55f1ed9d" in namespace "emptydir-3805" to be "Succeeded or Failed"
Mar  3 09:03:19.967: INFO: Pod "pod-5bccabd2-671a-4040-b614-b6df55f1ed9d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.513517ms
Mar  3 09:03:21.974: INFO: Pod "pod-5bccabd2-671a-4040-b614-b6df55f1ed9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010860634s
STEP: Saw pod success
Mar  3 09:03:21.974: INFO: Pod "pod-5bccabd2-671a-4040-b614-b6df55f1ed9d" satisfied condition "Succeeded or Failed"
Mar  3 09:03:21.976: INFO: Trying to get logs from node worker-1 pod pod-5bccabd2-671a-4040-b614-b6df55f1ed9d container test-container: <nil>
STEP: delete the pod
Mar  3 09:03:21.993: INFO: Waiting for pod pod-5bccabd2-671a-4040-b614-b6df55f1ed9d to disappear
Mar  3 09:03:21.995: INFO: Pod pod-5bccabd2-671a-4040-b614-b6df55f1ed9d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:03:21.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3805" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":31,"skipped":593,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:03:22.003: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-28ef31de-f253-4cb8-85f6-0f5779dd6cf1 in namespace container-probe-5909
Mar  3 09:03:24.095: INFO: Started pod busybox-28ef31de-f253-4cb8-85f6-0f5779dd6cf1 in namespace container-probe-5909
STEP: checking the pod's current state and verifying that restartCount is present
Mar  3 09:03:24.097: INFO: Initial restart count of pod busybox-28ef31de-f253-4cb8-85f6-0f5779dd6cf1 is 0
Mar  3 09:04:08.266: INFO: Restart count of pod container-probe-5909/busybox-28ef31de-f253-4cb8-85f6-0f5779dd6cf1 is now 1 (44.168523934s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:04:08.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5909" for this suite.

• [SLOW TEST:46.279 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":32,"skipped":599,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:04:08.286: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 09:04:08.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e5c2824-a0f5-4dbe-885a-3028ad4673ba" in namespace "downward-api-8095" to be "Succeeded or Failed"
Mar  3 09:04:08.347: INFO: Pod "downwardapi-volume-0e5c2824-a0f5-4dbe-885a-3028ad4673ba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.503827ms
Mar  3 09:04:10.354: INFO: Pod "downwardapi-volume-0e5c2824-a0f5-4dbe-885a-3028ad4673ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013072176s
STEP: Saw pod success
Mar  3 09:04:10.354: INFO: Pod "downwardapi-volume-0e5c2824-a0f5-4dbe-885a-3028ad4673ba" satisfied condition "Succeeded or Failed"
Mar  3 09:04:10.356: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-0e5c2824-a0f5-4dbe-885a-3028ad4673ba container client-container: <nil>
STEP: delete the pod
Mar  3 09:04:10.370: INFO: Waiting for pod downwardapi-volume-0e5c2824-a0f5-4dbe-885a-3028ad4673ba to disappear
Mar  3 09:04:10.373: INFO: Pod downwardapi-volume-0e5c2824-a0f5-4dbe-885a-3028ad4673ba no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:04:10.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8095" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":33,"skipped":625,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:04:10.380: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:04:12.437: INFO: Deleting pod "var-expansion-857044b8-13a3-40a6-ad02-2cd016b4e9d7" in namespace "var-expansion-6746"
Mar  3 09:04:12.442: INFO: Wait up to 5m0s for pod "var-expansion-857044b8-13a3-40a6-ad02-2cd016b4e9d7" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:04:14.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6746" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":34,"skipped":640,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:04:14.459: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Mar  3 09:04:14.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6726 create -f -'
Mar  3 09:04:14.781: INFO: stderr: ""
Mar  3 09:04:14.781: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  3 09:04:14.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6726 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  3 09:04:14.867: INFO: stderr: ""
Mar  3 09:04:14.867: INFO: stdout: "update-demo-nautilus-x97ws update-demo-nautilus-xmms2 "
Mar  3 09:04:14.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6726 get pods update-demo-nautilus-x97ws -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  3 09:04:14.947: INFO: stderr: ""
Mar  3 09:04:14.947: INFO: stdout: ""
Mar  3 09:04:14.947: INFO: update-demo-nautilus-x97ws is created but not running
Mar  3 09:04:19.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6726 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  3 09:04:20.029: INFO: stderr: ""
Mar  3 09:04:20.030: INFO: stdout: "update-demo-nautilus-x97ws update-demo-nautilus-xmms2 "
Mar  3 09:04:20.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6726 get pods update-demo-nautilus-x97ws -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  3 09:04:20.109: INFO: stderr: ""
Mar  3 09:04:20.109: INFO: stdout: "true"
Mar  3 09:04:20.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6726 get pods update-demo-nautilus-x97ws -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  3 09:04:20.188: INFO: stderr: ""
Mar  3 09:04:20.188: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  3 09:04:20.188: INFO: validating pod update-demo-nautilus-x97ws
Mar  3 09:04:20.197: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  3 09:04:20.197: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  3 09:04:20.197: INFO: update-demo-nautilus-x97ws is verified up and running
Mar  3 09:04:20.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6726 get pods update-demo-nautilus-xmms2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  3 09:04:20.274: INFO: stderr: ""
Mar  3 09:04:20.274: INFO: stdout: "true"
Mar  3 09:04:20.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6726 get pods update-demo-nautilus-xmms2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  3 09:04:20.351: INFO: stderr: ""
Mar  3 09:04:20.351: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  3 09:04:20.351: INFO: validating pod update-demo-nautilus-xmms2
Mar  3 09:04:20.359: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  3 09:04:20.359: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  3 09:04:20.359: INFO: update-demo-nautilus-xmms2 is verified up and running
STEP: using delete to clean up resources
Mar  3 09:04:20.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6726 delete --grace-period=0 --force -f -'
Mar  3 09:04:20.442: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  3 09:04:20.442: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  3 09:04:20.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6726 get rc,svc -l name=update-demo --no-headers'
Mar  3 09:04:20.525: INFO: stderr: "No resources found in kubectl-6726 namespace.\n"
Mar  3 09:04:20.526: INFO: stdout: ""
Mar  3 09:04:20.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6726 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  3 09:04:20.608: INFO: stderr: ""
Mar  3 09:04:20.608: INFO: stdout: "update-demo-nautilus-x97ws\nupdate-demo-nautilus-xmms2\n"
Mar  3 09:04:21.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6726 get rc,svc -l name=update-demo --no-headers'
Mar  3 09:04:21.195: INFO: stderr: "No resources found in kubectl-6726 namespace.\n"
Mar  3 09:04:21.195: INFO: stdout: ""
Mar  3 09:04:21.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6726 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  3 09:04:21.280: INFO: stderr: ""
Mar  3 09:04:21.280: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:04:21.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6726" for this suite.

• [SLOW TEST:6.830 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":35,"skipped":654,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:04:21.289: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Mar  3 09:04:23.863: INFO: Successfully updated pod "annotationupdate6240b875-490d-425f-b10c-3b4cb26d124f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:04:27.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5553" for this suite.

• [SLOW TEST:6.612 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":36,"skipped":664,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:04:27.902: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6621, will wait for the garbage collector to delete the pods
Mar  3 09:04:30.000: INFO: Deleting Job.batch foo took: 5.746003ms
Mar  3 09:04:30.700: INFO: Terminating Job.batch foo pods took: 700.259846ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:05:07.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6621" for this suite.

• [SLOW TEST:39.922 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":37,"skipped":666,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:05:07.831: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  3 09:05:08.390: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 09:05:11.407: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:05:11.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5035" for this suite.
STEP: Destroying namespace "webhook-5035-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":38,"skipped":682,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:05:11.499: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1559.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1559.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  3 09:05:21.619: INFO: DNS probes using dns-1559/dns-test-ff0d964f-fc70-46b1-8456-169d71cf7da7 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:05:21.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1559" for this suite.

• [SLOW TEST:10.182 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":39,"skipped":689,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:05:21.681: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar  3 09:05:21.718: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Mar  3 09:05:22.574: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar  3 09:05:24.620: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  3 09:05:26.629: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  3 09:05:28.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  3 09:05:30.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  3 09:05:32.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  3 09:05:34.629: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359122, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  3 09:05:37.659: INFO: Waited 1.024706613s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:05:38.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6164" for this suite.

• [SLOW TEST:16.868 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":40,"skipped":712,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:05:38.554: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-0be6a628-ad05-47f7-9400-1522b0dabd85
STEP: Creating a pod to test consume configMaps
Mar  3 09:05:38.610: INFO: Waiting up to 5m0s for pod "pod-configmaps-f0b8eca4-220e-4191-8949-db04b8ef25ef" in namespace "configmap-8284" to be "Succeeded or Failed"
Mar  3 09:05:38.618: INFO: Pod "pod-configmaps-f0b8eca4-220e-4191-8949-db04b8ef25ef": Phase="Pending", Reason="", readiness=false. Elapsed: 8.527431ms
Mar  3 09:05:40.622: INFO: Pod "pod-configmaps-f0b8eca4-220e-4191-8949-db04b8ef25ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012411534s
STEP: Saw pod success
Mar  3 09:05:40.622: INFO: Pod "pod-configmaps-f0b8eca4-220e-4191-8949-db04b8ef25ef" satisfied condition "Succeeded or Failed"
Mar  3 09:05:40.624: INFO: Trying to get logs from node worker-1 pod pod-configmaps-f0b8eca4-220e-4191-8949-db04b8ef25ef container agnhost-container: <nil>
STEP: delete the pod
Mar  3 09:05:40.649: INFO: Waiting for pod pod-configmaps-f0b8eca4-220e-4191-8949-db04b8ef25ef to disappear
Mar  3 09:05:40.651: INFO: Pod pod-configmaps-f0b8eca4-220e-4191-8949-db04b8ef25ef no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:05:40.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8284" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":41,"skipped":714,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:05:40.657: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Mar  3 09:05:40.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-8382 create -f -'
Mar  3 09:05:41.012: INFO: stderr: ""
Mar  3 09:05:41.012: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Mar  3 09:05:41.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-8382 diff -f -'
Mar  3 09:05:41.395: INFO: rc: 1
Mar  3 09:05:41.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-8382 delete -f -'
Mar  3 09:05:41.482: INFO: stderr: ""
Mar  3 09:05:41.482: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:05:41.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8382" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":42,"skipped":717,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:05:41.496: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-807d8217-7191-4d27-8673-91459ec376d9
STEP: Creating configMap with name cm-test-opt-upd-9fda01e4-555d-4ff2-a56b-11354f206572
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-807d8217-7191-4d27-8673-91459ec376d9
STEP: Updating configmap cm-test-opt-upd-9fda01e4-555d-4ff2-a56b-11354f206572
STEP: Creating configMap with name cm-test-opt-create-7c004ebd-e4bf-4bd1-bf2a-9d7485f6d0a6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:05:45.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1616" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":43,"skipped":770,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:05:45.651: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  3 09:05:45.713: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  3 09:06:45.736: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:06:45.739: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:06:45.789: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Mar  3 09:06:45.792: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:06:45.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1244" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:06:45.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9391" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.217 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":44,"skipped":771,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:06:45.870: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8846.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8846.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8846.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8846.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  3 09:06:57.933: INFO: DNS probes using dns-test-7ab73d3b-6e8e-4f52-a44d-c8055c5a0156 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8846.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8846.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8846.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8846.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  3 09:06:59.987: INFO: File wheezy_udp@dns-test-service-3.dns-8846.svc.cluster.local from pod  dns-8846/dns-test-685bb292-732c-4701-9997-8710fa94f826 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  3 09:06:59.990: INFO: File jessie_udp@dns-test-service-3.dns-8846.svc.cluster.local from pod  dns-8846/dns-test-685bb292-732c-4701-9997-8710fa94f826 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  3 09:06:59.990: INFO: Lookups using dns-8846/dns-test-685bb292-732c-4701-9997-8710fa94f826 failed for: [wheezy_udp@dns-test-service-3.dns-8846.svc.cluster.local jessie_udp@dns-test-service-3.dns-8846.svc.cluster.local]

Mar  3 09:07:04.995: INFO: File wheezy_udp@dns-test-service-3.dns-8846.svc.cluster.local from pod  dns-8846/dns-test-685bb292-732c-4701-9997-8710fa94f826 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  3 09:07:04.998: INFO: File jessie_udp@dns-test-service-3.dns-8846.svc.cluster.local from pod  dns-8846/dns-test-685bb292-732c-4701-9997-8710fa94f826 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  3 09:07:04.998: INFO: Lookups using dns-8846/dns-test-685bb292-732c-4701-9997-8710fa94f826 failed for: [wheezy_udp@dns-test-service-3.dns-8846.svc.cluster.local jessie_udp@dns-test-service-3.dns-8846.svc.cluster.local]

Mar  3 09:07:09.995: INFO: File wheezy_udp@dns-test-service-3.dns-8846.svc.cluster.local from pod  dns-8846/dns-test-685bb292-732c-4701-9997-8710fa94f826 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  3 09:07:09.999: INFO: File jessie_udp@dns-test-service-3.dns-8846.svc.cluster.local from pod  dns-8846/dns-test-685bb292-732c-4701-9997-8710fa94f826 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  3 09:07:09.999: INFO: Lookups using dns-8846/dns-test-685bb292-732c-4701-9997-8710fa94f826 failed for: [wheezy_udp@dns-test-service-3.dns-8846.svc.cluster.local jessie_udp@dns-test-service-3.dns-8846.svc.cluster.local]

Mar  3 09:07:14.995: INFO: File wheezy_udp@dns-test-service-3.dns-8846.svc.cluster.local from pod  dns-8846/dns-test-685bb292-732c-4701-9997-8710fa94f826 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  3 09:07:14.999: INFO: File jessie_udp@dns-test-service-3.dns-8846.svc.cluster.local from pod  dns-8846/dns-test-685bb292-732c-4701-9997-8710fa94f826 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  3 09:07:14.999: INFO: Lookups using dns-8846/dns-test-685bb292-732c-4701-9997-8710fa94f826 failed for: [wheezy_udp@dns-test-service-3.dns-8846.svc.cluster.local jessie_udp@dns-test-service-3.dns-8846.svc.cluster.local]

Mar  3 09:07:19.998: INFO: DNS probes using dns-test-685bb292-732c-4701-9997-8710fa94f826 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8846.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8846.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8846.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8846.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  3 09:07:22.164: INFO: DNS probes using dns-test-1c63bc9d-7ba7-4aa7-bcc6-a0892597baa9 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:07:22.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8846" for this suite.

• [SLOW TEST:36.371 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":45,"skipped":784,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:07:22.241: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-329.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-329.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-329.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-329.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-329.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-329.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-329.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-329.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-329.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-329.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  3 09:07:24.325: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:24.330: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:24.333: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:24.337: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:24.346: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:24.349: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:24.355: INFO: Unable to read jessie_udp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:24.359: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:24.365: INFO: Lookups using dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local wheezy_udp@dns-test-service-2.dns-329.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-329.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local jessie_udp@dns-test-service-2.dns-329.svc.cluster.local jessie_tcp@dns-test-service-2.dns-329.svc.cluster.local]

Mar  3 09:07:29.370: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:29.373: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:29.376: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:29.381: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:29.390: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:29.393: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:29.396: INFO: Unable to read jessie_udp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:29.399: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:29.405: INFO: Lookups using dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local wheezy_udp@dns-test-service-2.dns-329.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-329.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local jessie_udp@dns-test-service-2.dns-329.svc.cluster.local jessie_tcp@dns-test-service-2.dns-329.svc.cluster.local]

Mar  3 09:07:34.370: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:34.373: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:34.376: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:34.382: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:34.391: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:34.394: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:34.397: INFO: Unable to read jessie_udp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:34.400: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:34.406: INFO: Lookups using dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local wheezy_udp@dns-test-service-2.dns-329.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-329.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local jessie_udp@dns-test-service-2.dns-329.svc.cluster.local jessie_tcp@dns-test-service-2.dns-329.svc.cluster.local]

Mar  3 09:07:39.370: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:39.373: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:39.376: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:39.379: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:39.387: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:39.390: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:39.393: INFO: Unable to read jessie_udp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:39.396: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:39.401: INFO: Lookups using dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local wheezy_udp@dns-test-service-2.dns-329.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-329.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local jessie_udp@dns-test-service-2.dns-329.svc.cluster.local jessie_tcp@dns-test-service-2.dns-329.svc.cluster.local]

Mar  3 09:07:44.369: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:44.373: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:44.376: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:44.379: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:44.388: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:44.391: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:44.394: INFO: Unable to read jessie_udp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:44.397: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:44.403: INFO: Lookups using dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local wheezy_udp@dns-test-service-2.dns-329.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-329.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local jessie_udp@dns-test-service-2.dns-329.svc.cluster.local jessie_tcp@dns-test-service-2.dns-329.svc.cluster.local]

Mar  3 09:07:49.370: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:49.373: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:49.376: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:49.378: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:49.387: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:49.390: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:49.392: INFO: Unable to read jessie_udp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:49.395: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-329.svc.cluster.local from pod dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc: the server could not find the requested resource (get pods dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc)
Mar  3 09:07:49.401: INFO: Lookups using dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local wheezy_udp@dns-test-service-2.dns-329.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-329.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-329.svc.cluster.local jessie_udp@dns-test-service-2.dns-329.svc.cluster.local jessie_tcp@dns-test-service-2.dns-329.svc.cluster.local]

Mar  3 09:07:54.400: INFO: DNS probes using dns-329/dns-test-cb8afcb6-771c-42eb-941e-f7fd132446dc succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:07:54.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-329" for this suite.

• [SLOW TEST:32.235 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":46,"skipped":790,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:07:54.476: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Mar  3 09:07:54.502: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:08:13.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4837" for this suite.

• [SLOW TEST:18.733 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":47,"skipped":795,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:08:13.209: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Mar  3 09:08:13.252: INFO: Waiting up to 5m0s for pod "var-expansion-376f1b35-f77a-4693-903f-04e4810c108f" in namespace "var-expansion-1299" to be "Succeeded or Failed"
Mar  3 09:08:13.261: INFO: Pod "var-expansion-376f1b35-f77a-4693-903f-04e4810c108f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.019728ms
Mar  3 09:08:15.265: INFO: Pod "var-expansion-376f1b35-f77a-4693-903f-04e4810c108f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012977822s
STEP: Saw pod success
Mar  3 09:08:15.265: INFO: Pod "var-expansion-376f1b35-f77a-4693-903f-04e4810c108f" satisfied condition "Succeeded or Failed"
Mar  3 09:08:15.268: INFO: Trying to get logs from node worker-1 pod var-expansion-376f1b35-f77a-4693-903f-04e4810c108f container dapi-container: <nil>
STEP: delete the pod
Mar  3 09:08:15.292: INFO: Waiting for pod var-expansion-376f1b35-f77a-4693-903f-04e4810c108f to disappear
Mar  3 09:08:15.295: INFO: Pod var-expansion-376f1b35-f77a-4693-903f-04e4810c108f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:08:15.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1299" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":48,"skipped":811,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:08:15.302: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:08:15.340: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:08:22.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-539" for this suite.

• [SLOW TEST:7.213 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":49,"skipped":832,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:08:22.517: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:08:22.559: INFO: Creating ReplicaSet my-hostname-basic-0aa47c1e-b323-4f9c-9703-08e8effe731b
Mar  3 09:08:22.567: INFO: Pod name my-hostname-basic-0aa47c1e-b323-4f9c-9703-08e8effe731b: Found 0 pods out of 1
Mar  3 09:08:27.579: INFO: Pod name my-hostname-basic-0aa47c1e-b323-4f9c-9703-08e8effe731b: Found 1 pods out of 1
Mar  3 09:08:27.579: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0aa47c1e-b323-4f9c-9703-08e8effe731b" is running
Mar  3 09:08:27.581: INFO: Pod "my-hostname-basic-0aa47c1e-b323-4f9c-9703-08e8effe731b-q5zsc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-03 09:08:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-03 09:08:24 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-03 09:08:24 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-03 09:08:22 +0000 UTC Reason: Message:}])
Mar  3 09:08:27.581: INFO: Trying to dial the pod
Mar  3 09:08:32.596: INFO: Controller my-hostname-basic-0aa47c1e-b323-4f9c-9703-08e8effe731b: Got expected result from replica 1 [my-hostname-basic-0aa47c1e-b323-4f9c-9703-08e8effe731b-q5zsc]: "my-hostname-basic-0aa47c1e-b323-4f9c-9703-08e8effe731b-q5zsc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:08:32.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8227" for this suite.

• [SLOW TEST:10.087 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":50,"skipped":850,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:08:32.608: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  3 09:08:32.663: INFO: Number of nodes with available pods: 0
Mar  3 09:08:32.663: INFO: Node controller-0 is running more than one daemon pod
Mar  3 09:08:33.672: INFO: Number of nodes with available pods: 0
Mar  3 09:08:33.672: INFO: Node controller-0 is running more than one daemon pod
Mar  3 09:08:34.672: INFO: Number of nodes with available pods: 3
Mar  3 09:08:34.672: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar  3 09:08:34.687: INFO: Number of nodes with available pods: 2
Mar  3 09:08:34.687: INFO: Node worker-0 is running more than one daemon pod
Mar  3 09:08:35.702: INFO: Number of nodes with available pods: 2
Mar  3 09:08:35.702: INFO: Node worker-0 is running more than one daemon pod
Mar  3 09:08:36.699: INFO: Number of nodes with available pods: 2
Mar  3 09:08:36.699: INFO: Node worker-0 is running more than one daemon pod
Mar  3 09:08:37.700: INFO: Number of nodes with available pods: 2
Mar  3 09:08:37.700: INFO: Node worker-0 is running more than one daemon pod
Mar  3 09:08:38.701: INFO: Number of nodes with available pods: 2
Mar  3 09:08:38.701: INFO: Node worker-0 is running more than one daemon pod
Mar  3 09:08:39.700: INFO: Number of nodes with available pods: 2
Mar  3 09:08:39.700: INFO: Node worker-0 is running more than one daemon pod
Mar  3 09:08:40.699: INFO: Number of nodes with available pods: 2
Mar  3 09:08:40.699: INFO: Node worker-0 is running more than one daemon pod
Mar  3 09:08:41.701: INFO: Number of nodes with available pods: 2
Mar  3 09:08:41.701: INFO: Node worker-0 is running more than one daemon pod
Mar  3 09:08:42.701: INFO: Number of nodes with available pods: 2
Mar  3 09:08:42.701: INFO: Node worker-0 is running more than one daemon pod
Mar  3 09:08:43.701: INFO: Number of nodes with available pods: 2
Mar  3 09:08:43.701: INFO: Node worker-0 is running more than one daemon pod
Mar  3 09:08:44.701: INFO: Number of nodes with available pods: 3
Mar  3 09:08:44.701: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7167, will wait for the garbage collector to delete the pods
Mar  3 09:08:44.763: INFO: Deleting DaemonSet.extensions daemon-set took: 5.511294ms
Mar  3 09:08:44.863: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.252644ms
Mar  3 09:08:57.767: INFO: Number of nodes with available pods: 0
Mar  3 09:08:57.767: INFO: Number of running nodes: 0, number of available pods: 0
Mar  3 09:08:57.769: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5465"},"items":null}

Mar  3 09:08:57.771: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5465"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:08:57.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7167" for this suite.

• [SLOW TEST:25.181 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":51,"skipped":860,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:08:57.789: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-716
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-716
STEP: Deleting pre-stop pod
Mar  3 09:09:06.882: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:09:06.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-716" for this suite.

• [SLOW TEST:9.153 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":52,"skipped":863,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:09:06.943: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-105
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  3 09:09:06.978: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  3 09:09:07.007: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  3 09:09:09.013: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 09:09:11.011: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 09:09:13.014: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 09:09:15.016: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 09:09:17.013: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 09:09:19.013: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  3 09:09:19.018: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  3 09:09:19.021: INFO: The status of Pod netserver-2 is Running (Ready = false)
Mar  3 09:09:21.026: INFO: The status of Pod netserver-2 is Running (Ready = false)
Mar  3 09:09:23.029: INFO: The status of Pod netserver-2 is Running (Ready = false)
Mar  3 09:09:25.030: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar  3 09:09:27.053: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  3 09:09:27.053: INFO: Breadth first check of 10.244.192.91 on host 10.0.49.5...
Mar  3 09:09:27.055: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.226.125:9080/dial?request=hostname&protocol=http&host=10.244.192.91&port=8080&tries=1'] Namespace:pod-network-test-105 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:09:27.055: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:09:27.133: INFO: Waiting for responses: map[]
Mar  3 09:09:27.133: INFO: reached 10.244.192.91 after 0/1 tries
Mar  3 09:09:27.133: INFO: Breadth first check of 10.244.43.24 on host 10.0.40.23...
Mar  3 09:09:27.136: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.226.125:9080/dial?request=hostname&protocol=http&host=10.244.43.24&port=8080&tries=1'] Namespace:pod-network-test-105 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:09:27.136: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:09:27.211: INFO: Waiting for responses: map[]
Mar  3 09:09:27.211: INFO: reached 10.244.43.24 after 0/1 tries
Mar  3 09:09:27.211: INFO: Breadth first check of 10.244.226.124 on host 10.0.47.86...
Mar  3 09:09:27.214: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.226.125:9080/dial?request=hostname&protocol=http&host=10.244.226.124&port=8080&tries=1'] Namespace:pod-network-test-105 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:09:27.214: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:09:27.278: INFO: Waiting for responses: map[]
Mar  3 09:09:27.279: INFO: reached 10.244.226.124 after 0/1 tries
Mar  3 09:09:27.279: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:09:27.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-105" for this suite.

• [SLOW TEST:20.349 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":53,"skipped":867,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:09:27.292: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-6269
STEP: creating service affinity-clusterip-transition in namespace services-6269
STEP: creating replication controller affinity-clusterip-transition in namespace services-6269
I0303 09:09:27.353045      24 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-6269, replica count: 3
I0303 09:09:30.403556      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  3 09:09:30.413: INFO: Creating new exec pod
Mar  3 09:09:33.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-6269 exec execpod-affinityvkn29 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Mar  3 09:09:33.625: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar  3 09:09:33.625: INFO: stdout: ""
Mar  3 09:09:33.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-6269 exec execpod-affinityvkn29 -- /bin/sh -x -c nc -zv -t -w 2 10.110.206.210 80'
Mar  3 09:09:33.778: INFO: stderr: "+ nc -zv -t -w 2 10.110.206.210 80\nConnection to 10.110.206.210 80 port [tcp/http] succeeded!\n"
Mar  3 09:09:33.778: INFO: stdout: ""
Mar  3 09:09:33.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-6269 exec execpod-affinityvkn29 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.206.210:80/ ; done'
Mar  3 09:09:34.039: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n"
Mar  3 09:09:34.039: INFO: stdout: "\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ffbj6\naffinity-clusterip-transition-9bh7d\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-9bh7d\naffinity-clusterip-transition-9bh7d\naffinity-clusterip-transition-9bh7d\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-9bh7d\naffinity-clusterip-transition-ffbj6\naffinity-clusterip-transition-9bh7d\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-9bh7d\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ksm67"
Mar  3 09:09:34.039: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:09:34.039: INFO: Received response from host: affinity-clusterip-transition-ffbj6
Mar  3 09:09:34.039: INFO: Received response from host: affinity-clusterip-transition-9bh7d
Mar  3 09:09:34.039: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:09:34.039: INFO: Received response from host: affinity-clusterip-transition-9bh7d
Mar  3 09:09:34.039: INFO: Received response from host: affinity-clusterip-transition-9bh7d
Mar  3 09:09:34.039: INFO: Received response from host: affinity-clusterip-transition-9bh7d
Mar  3 09:09:34.039: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:09:34.039: INFO: Received response from host: affinity-clusterip-transition-9bh7d
Mar  3 09:09:34.039: INFO: Received response from host: affinity-clusterip-transition-ffbj6
Mar  3 09:09:34.039: INFO: Received response from host: affinity-clusterip-transition-9bh7d
Mar  3 09:09:34.039: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:09:34.039: INFO: Received response from host: affinity-clusterip-transition-9bh7d
Mar  3 09:09:34.039: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:09:34.039: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:09:34.039: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:09:34.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-6269 exec execpod-affinityvkn29 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.206.210:80/ ; done'
Mar  3 09:09:34.306: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n"
Mar  3 09:09:34.306: INFO: stdout: "\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-9bh7d\naffinity-clusterip-transition-ffbj6\naffinity-clusterip-transition-9bh7d\naffinity-clusterip-transition-9bh7d\naffinity-clusterip-transition-ffbj6\naffinity-clusterip-transition-ffbj6\naffinity-clusterip-transition-ffbj6\naffinity-clusterip-transition-9bh7d\naffinity-clusterip-transition-ffbj6\naffinity-clusterip-transition-ffbj6\naffinity-clusterip-transition-9bh7d\naffinity-clusterip-transition-9bh7d\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-9bh7d\naffinity-clusterip-transition-9bh7d"
Mar  3 09:09:34.307: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:09:34.307: INFO: Received response from host: affinity-clusterip-transition-9bh7d
Mar  3 09:09:34.307: INFO: Received response from host: affinity-clusterip-transition-ffbj6
Mar  3 09:09:34.307: INFO: Received response from host: affinity-clusterip-transition-9bh7d
Mar  3 09:09:34.307: INFO: Received response from host: affinity-clusterip-transition-9bh7d
Mar  3 09:09:34.307: INFO: Received response from host: affinity-clusterip-transition-ffbj6
Mar  3 09:09:34.307: INFO: Received response from host: affinity-clusterip-transition-ffbj6
Mar  3 09:09:34.307: INFO: Received response from host: affinity-clusterip-transition-ffbj6
Mar  3 09:09:34.307: INFO: Received response from host: affinity-clusterip-transition-9bh7d
Mar  3 09:09:34.307: INFO: Received response from host: affinity-clusterip-transition-ffbj6
Mar  3 09:09:34.307: INFO: Received response from host: affinity-clusterip-transition-ffbj6
Mar  3 09:09:34.307: INFO: Received response from host: affinity-clusterip-transition-9bh7d
Mar  3 09:09:34.307: INFO: Received response from host: affinity-clusterip-transition-9bh7d
Mar  3 09:09:34.307: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:09:34.307: INFO: Received response from host: affinity-clusterip-transition-9bh7d
Mar  3 09:09:34.307: INFO: Received response from host: affinity-clusterip-transition-9bh7d
Mar  3 09:10:04.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-6269 exec execpod-affinityvkn29 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.206.210:80/ ; done'
Mar  3 09:10:04.810: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.206.210:80/\n"
Mar  3 09:10:04.810: INFO: stdout: "\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ksm67\naffinity-clusterip-transition-ksm67"
Mar  3 09:10:04.810: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:10:04.810: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:10:04.810: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:10:04.810: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:10:04.810: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:10:04.810: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:10:04.810: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:10:04.810: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:10:04.810: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:10:04.810: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:10:04.810: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:10:04.810: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:10:04.810: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:10:04.810: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:10:04.810: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:10:04.810: INFO: Received response from host: affinity-clusterip-transition-ksm67
Mar  3 09:10:04.810: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6269, will wait for the garbage collector to delete the pods
Mar  3 09:10:04.896: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.281639ms
Mar  3 09:10:05.596: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 700.279313ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:10:17.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6269" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:50.441 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":54,"skipped":889,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:10:17.734: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-b8d190c8-7912-46ae-b035-acde844295c1
STEP: Creating a pod to test consume secrets
Mar  3 09:10:17.792: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5d6c5c9f-3ada-4569-93a0-b7d7c5e57b5e" in namespace "projected-7362" to be "Succeeded or Failed"
Mar  3 09:10:17.795: INFO: Pod "pod-projected-secrets-5d6c5c9f-3ada-4569-93a0-b7d7c5e57b5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.804859ms
Mar  3 09:10:19.799: INFO: Pod "pod-projected-secrets-5d6c5c9f-3ada-4569-93a0-b7d7c5e57b5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007367678s
Mar  3 09:10:21.804: INFO: Pod "pod-projected-secrets-5d6c5c9f-3ada-4569-93a0-b7d7c5e57b5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011992195s
STEP: Saw pod success
Mar  3 09:10:21.804: INFO: Pod "pod-projected-secrets-5d6c5c9f-3ada-4569-93a0-b7d7c5e57b5e" satisfied condition "Succeeded or Failed"
Mar  3 09:10:21.806: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-5d6c5c9f-3ada-4569-93a0-b7d7c5e57b5e container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  3 09:10:21.833: INFO: Waiting for pod pod-projected-secrets-5d6c5c9f-3ada-4569-93a0-b7d7c5e57b5e to disappear
Mar  3 09:10:21.835: INFO: Pod pod-projected-secrets-5d6c5c9f-3ada-4569-93a0-b7d7c5e57b5e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:10:21.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7362" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":55,"skipped":966,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:10:21.844: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  3 09:10:21.878: INFO: Waiting up to 5m0s for pod "pod-f3486890-7301-4699-a218-09e3bfaa63a8" in namespace "emptydir-5566" to be "Succeeded or Failed"
Mar  3 09:10:21.881: INFO: Pod "pod-f3486890-7301-4699-a218-09e3bfaa63a8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.392565ms
Mar  3 09:10:23.888: INFO: Pod "pod-f3486890-7301-4699-a218-09e3bfaa63a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01020953s
STEP: Saw pod success
Mar  3 09:10:23.888: INFO: Pod "pod-f3486890-7301-4699-a218-09e3bfaa63a8" satisfied condition "Succeeded or Failed"
Mar  3 09:10:23.890: INFO: Trying to get logs from node worker-1 pod pod-f3486890-7301-4699-a218-09e3bfaa63a8 container test-container: <nil>
STEP: delete the pod
Mar  3 09:10:23.909: INFO: Waiting for pod pod-f3486890-7301-4699-a218-09e3bfaa63a8 to disappear
Mar  3 09:10:23.912: INFO: Pod pod-f3486890-7301-4699-a218-09e3bfaa63a8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:10:23.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5566" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":56,"skipped":991,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:10:23.920: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:10:48.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6834" for this suite.

• [SLOW TEST:24.268 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":57,"skipped":1012,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:10:48.188: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 09:10:48.233: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ae4e94e-062a-4d1b-9614-53f517056717" in namespace "downward-api-9786" to be "Succeeded or Failed"
Mar  3 09:10:48.237: INFO: Pod "downwardapi-volume-4ae4e94e-062a-4d1b-9614-53f517056717": Phase="Pending", Reason="", readiness=false. Elapsed: 4.522672ms
Mar  3 09:10:50.245: INFO: Pod "downwardapi-volume-4ae4e94e-062a-4d1b-9614-53f517056717": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012084061s
STEP: Saw pod success
Mar  3 09:10:50.245: INFO: Pod "downwardapi-volume-4ae4e94e-062a-4d1b-9614-53f517056717" satisfied condition "Succeeded or Failed"
Mar  3 09:10:50.247: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-4ae4e94e-062a-4d1b-9614-53f517056717 container client-container: <nil>
STEP: delete the pod
Mar  3 09:10:50.265: INFO: Waiting for pod downwardapi-volume-4ae4e94e-062a-4d1b-9614-53f517056717 to disappear
Mar  3 09:10:50.267: INFO: Pod downwardapi-volume-4ae4e94e-062a-4d1b-9614-53f517056717 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:10:50.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9786" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":58,"skipped":1022,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:10:50.274: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:10:50.303: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:10:52.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2606" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":59,"skipped":1035,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:10:52.385: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  3 09:10:52.424: INFO: Waiting up to 5m0s for pod "pod-89fc9ff6-cf68-4110-92f4-99a0a570eb91" in namespace "emptydir-7848" to be "Succeeded or Failed"
Mar  3 09:10:52.427: INFO: Pod "pod-89fc9ff6-cf68-4110-92f4-99a0a570eb91": Phase="Pending", Reason="", readiness=false. Elapsed: 3.542242ms
Mar  3 09:10:54.433: INFO: Pod "pod-89fc9ff6-cf68-4110-92f4-99a0a570eb91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009233726s
STEP: Saw pod success
Mar  3 09:10:54.433: INFO: Pod "pod-89fc9ff6-cf68-4110-92f4-99a0a570eb91" satisfied condition "Succeeded or Failed"
Mar  3 09:10:54.436: INFO: Trying to get logs from node worker-0 pod pod-89fc9ff6-cf68-4110-92f4-99a0a570eb91 container test-container: <nil>
STEP: delete the pod
Mar  3 09:10:54.462: INFO: Waiting for pod pod-89fc9ff6-cf68-4110-92f4-99a0a570eb91 to disappear
Mar  3 09:10:54.464: INFO: Pod pod-89fc9ff6-cf68-4110-92f4-99a0a570eb91 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:10:54.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7848" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":60,"skipped":1058,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:10:54.471: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  3 09:10:54.505: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  3 09:10:54.510: INFO: Waiting for terminating namespaces to be deleted...
Mar  3 09:10:54.512: INFO: 
Logging pods the apiserver thinks is on node controller-0 before test
Mar  3 09:10:54.517: INFO: calico-kube-controllers-5f6546844f-gnl9f from kube-system started at 2021-03-03 08:47:09 +0000 UTC (1 container statuses recorded)
Mar  3 09:10:54.517: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  3 09:10:54.517: INFO: calico-node-468vb from kube-system started at 2021-03-03 08:46:49 +0000 UTC (1 container statuses recorded)
Mar  3 09:10:54.517: INFO: 	Container calico-node ready: true, restart count 0
Mar  3 09:10:54.517: INFO: coredns-5c98d7d4d8-96hpd from kube-system started at 2021-03-03 08:47:11 +0000 UTC (1 container statuses recorded)
Mar  3 09:10:54.517: INFO: 	Container coredns ready: true, restart count 0
Mar  3 09:10:54.517: INFO: konnectivity-agent-882qs from kube-system started at 2021-03-03 08:47:01 +0000 UTC (1 container statuses recorded)
Mar  3 09:10:54.517: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  3 09:10:54.517: INFO: kube-proxy-snqpb from kube-system started at 2021-03-03 08:46:30 +0000 UTC (1 container statuses recorded)
Mar  3 09:10:54.517: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  3 09:10:54.517: INFO: metrics-server-6fbcd86f7b-r2c26 from kube-system started at 2021-03-03 08:47:09 +0000 UTC (1 container statuses recorded)
Mar  3 09:10:54.517: INFO: 	Container metrics-server ready: true, restart count 0
Mar  3 09:10:54.517: INFO: sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-8fglb from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 09:10:54.517: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  3 09:10:54.517: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  3 09:10:54.517: INFO: 
Logging pods the apiserver thinks is on node worker-0 before test
Mar  3 09:10:54.522: INFO: calico-node-4vzkv from kube-system started at 2021-03-03 08:50:42 +0000 UTC (1 container statuses recorded)
Mar  3 09:10:54.522: INFO: 	Container calico-node ready: true, restart count 0
Mar  3 09:10:54.522: INFO: konnectivity-agent-hrn26 from kube-system started at 2021-03-03 08:51:02 +0000 UTC (1 container statuses recorded)
Mar  3 09:10:54.523: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  3 09:10:54.523: INFO: kube-proxy-xwt8k from kube-system started at 2021-03-03 08:50:42 +0000 UTC (1 container statuses recorded)
Mar  3 09:10:54.524: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  3 09:10:54.524: INFO: sonobuoy-e2e-job-a123a876f42443b1 from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 09:10:54.524: INFO: 	Container e2e ready: true, restart count 0
Mar  3 09:10:54.525: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  3 09:10:54.525: INFO: sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-xjvxl from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 09:10:54.525: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  3 09:10:54.525: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  3 09:10:54.525: INFO: 
Logging pods the apiserver thinks is on node worker-1 before test
Mar  3 09:10:54.530: INFO: calico-node-d2bzg from kube-system started at 2021-03-03 08:52:07 +0000 UTC (1 container statuses recorded)
Mar  3 09:10:54.530: INFO: 	Container calico-node ready: true, restart count 0
Mar  3 09:10:54.530: INFO: konnectivity-agent-b4b2k from kube-system started at 2021-03-03 08:52:27 +0000 UTC (1 container statuses recorded)
Mar  3 09:10:54.530: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  3 09:10:54.530: INFO: kube-proxy-c7zpn from kube-system started at 2021-03-03 08:52:07 +0000 UTC (1 container statuses recorded)
Mar  3 09:10:54.530: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  3 09:10:54.530: INFO: pod-exec-websocket-4b906215-14ba-4a33-9cb3-12f5c24e510a from pods-2606 started at 2021-03-03 09:10:50 +0000 UTC (1 container statuses recorded)
Mar  3 09:10:54.530: INFO: 	Container main ready: true, restart count 0
Mar  3 09:10:54.530: INFO: sonobuoy from sonobuoy started at 2021-03-03 08:53:20 +0000 UTC (1 container statuses recorded)
Mar  3 09:10:54.530: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  3 09:10:54.530: INFO: sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-x4pmv from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 09:10:54.530: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  3 09:10:54.530: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ab7dab79-bf3c-41ef-94fa-4065a4216aeb 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-ab7dab79-bf3c-41ef-94fa-4065a4216aeb off the node worker-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ab7dab79-bf3c-41ef-94fa-4065a4216aeb
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:10:58.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1648" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":61,"skipped":1082,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:10:58.606: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar  3 09:10:58.639: INFO: Waiting up to 5m0s for pod "downward-api-c15443ec-9d3c-4e99-8c10-b6d54f7bccfc" in namespace "downward-api-5282" to be "Succeeded or Failed"
Mar  3 09:10:58.642: INFO: Pod "downward-api-c15443ec-9d3c-4e99-8c10-b6d54f7bccfc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.14619ms
Mar  3 09:11:00.650: INFO: Pod "downward-api-c15443ec-9d3c-4e99-8c10-b6d54f7bccfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01127847s
Mar  3 09:11:02.656: INFO: Pod "downward-api-c15443ec-9d3c-4e99-8c10-b6d54f7bccfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017088547s
STEP: Saw pod success
Mar  3 09:11:02.656: INFO: Pod "downward-api-c15443ec-9d3c-4e99-8c10-b6d54f7bccfc" satisfied condition "Succeeded or Failed"
Mar  3 09:11:02.658: INFO: Trying to get logs from node worker-0 pod downward-api-c15443ec-9d3c-4e99-8c10-b6d54f7bccfc container dapi-container: <nil>
STEP: delete the pod
Mar  3 09:11:02.676: INFO: Waiting for pod downward-api-c15443ec-9d3c-4e99-8c10-b6d54f7bccfc to disappear
Mar  3 09:11:02.679: INFO: Pod downward-api-c15443ec-9d3c-4e99-8c10-b6d54f7bccfc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:11:02.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5282" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":62,"skipped":1121,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:11:02.691: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:11:31.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1788" for this suite.
STEP: Destroying namespace "nsdeletetest-8725" for this suite.
Mar  3 09:11:31.878: INFO: Namespace nsdeletetest-8725 was already deleted
STEP: Destroying namespace "nsdeletetest-8322" for this suite.

• [SLOW TEST:29.190 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":63,"skipped":1141,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:11:31.882: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 09:11:31.918: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66f4796b-363b-4b1c-9ab5-268fae556855" in namespace "projected-4994" to be "Succeeded or Failed"
Mar  3 09:11:31.925: INFO: Pod "downwardapi-volume-66f4796b-363b-4b1c-9ab5-268fae556855": Phase="Pending", Reason="", readiness=false. Elapsed: 7.240423ms
Mar  3 09:11:33.930: INFO: Pod "downwardapi-volume-66f4796b-363b-4b1c-9ab5-268fae556855": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011967589s
STEP: Saw pod success
Mar  3 09:11:33.930: INFO: Pod "downwardapi-volume-66f4796b-363b-4b1c-9ab5-268fae556855" satisfied condition "Succeeded or Failed"
Mar  3 09:11:33.932: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-66f4796b-363b-4b1c-9ab5-268fae556855 container client-container: <nil>
STEP: delete the pod
Mar  3 09:11:33.947: INFO: Waiting for pod downwardapi-volume-66f4796b-363b-4b1c-9ab5-268fae556855 to disappear
Mar  3 09:11:33.948: INFO: Pod downwardapi-volume-66f4796b-363b-4b1c-9ab5-268fae556855 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:11:33.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4994" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":64,"skipped":1173,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:11:33.956: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:11:34.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-3594" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":65,"skipped":1184,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:11:34.022: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:11:34.058: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  3 09:11:39.071: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  3 09:11:39.071: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  3 09:11:39.097: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2623  d28e62dc-6a1a-420b-8623-ece4928aa081 6372 1 2021-03-03 09:11:39 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-03-03 09:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a16af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar  3 09:11:39.107: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-2623  9adf6f01-e933-46bd-b163-fb6f54f05cb3 6374 1 2021-03-03 09:11:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment d28e62dc-6a1a-420b-8623-ece4928aa081 0xc004a17117 0xc004a17118}] []  [{kube-controller-manager Update apps/v1 2021-03-03 09:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d28e62dc-6a1a-420b-8623-ece4928aa081\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a17238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  3 09:11:39.107: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar  3 09:11:39.108: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2623  a3367578-a8b5-4798-9f04-3c80d3575c35 6373 1 2021-03-03 09:11:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment d28e62dc-6a1a-420b-8623-ece4928aa081 0xc004a16fd7 0xc004a16fd8}] []  [{e2e.test Update apps/v1 2021-03-03 09:11:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-03 09:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"d28e62dc-6a1a-420b-8623-ece4928aa081\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004a170b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  3 09:11:39.125: INFO: Pod "test-cleanup-controller-m8jg6" is available:
&Pod{ObjectMeta:{test-cleanup-controller-m8jg6 test-cleanup-controller- deployment-2623  c0207fd3-9b9d-401d-b122-2b7a669ada1b 6352 0 2021-03-03 09:11:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.244.192.93/32 cni.projectcalico.org/podIPs:10.244.192.93/32] [{apps/v1 ReplicaSet test-cleanup-controller a3367578-a8b5-4798-9f04-3c80d3575c35 0xc004a179df 0xc004a17b00}] []  [{calico Update v1 2021-03-03 09:11:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2021-03-03 09:11:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3367578-a8b5-4798-9f04-3c80d3575c35\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-03 09:11:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.192.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvkzn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvkzn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvkzn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:11:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:11:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:11:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:11:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.49.5,PodIP:10.244.192.93,StartTime:2021-03-03 09:11:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-03 09:11:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://c294fd0b1e599de97812e19353532cd58fd2d7c88b60728af24ce763c51e1f1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.192.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 09:11:39.127: INFO: Pod "test-cleanup-deployment-685c4f8568-grdjk" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-685c4f8568-grdjk test-cleanup-deployment-685c4f8568- deployment-2623  06815066-5156-4f07-a362-18c1c467e003 6382 0 2021-03-03 09:11:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-685c4f8568 9adf6f01-e933-46bd-b163-fb6f54f05cb3 0xc004a17d57 0xc004a17d58}] []  [{kube-controller-manager Update v1 2021-03-03 09:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9adf6f01-e933-46bd-b163-fb6f54f05cb3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvkzn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvkzn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvkzn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:11:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:11:39.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2623" for this suite.

• [SLOW TEST:5.129 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":66,"skipped":1191,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:11:39.155: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  3 09:11:39.212: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  3 09:12:39.238: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Mar  3 09:12:39.264: INFO: Created pod: pod0-sched-preemption-low-priority
Mar  3 09:12:39.290: INFO: Created pod: pod1-sched-preemption-medium-priority
Mar  3 09:12:39.308: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:13:03.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4647" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:84.231 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":67,"skipped":1203,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:13:03.388: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Mar  3 09:13:03.438: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Mar  3 09:13:03.443: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  3 09:13:03.443: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Mar  3 09:13:03.450: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  3 09:13:03.450: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Mar  3 09:13:03.465: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar  3 09:13:03.465: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Mar  3 09:13:10.527: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:13:10.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-7475" for this suite.

• [SLOW TEST:7.157 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":68,"skipped":1234,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:13:10.545: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  3 09:13:11.192: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  3 09:13:13.202: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359591, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359591, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359591, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750359591, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 09:13:16.216: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:13:16.222: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:13:17.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-953" for this suite.
STEP: Destroying namespace "webhook-953-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.897 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":69,"skipped":1240,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:13:17.442: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Mar  3 09:13:17.499: INFO: Waiting up to 5m0s for pod "pod-ed14cb23-a487-49ae-a1f2-13a88ce960c2" in namespace "emptydir-2585" to be "Succeeded or Failed"
Mar  3 09:13:17.514: INFO: Pod "pod-ed14cb23-a487-49ae-a1f2-13a88ce960c2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.809855ms
Mar  3 09:13:19.522: INFO: Pod "pod-ed14cb23-a487-49ae-a1f2-13a88ce960c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022634929s
Mar  3 09:13:21.529: INFO: Pod "pod-ed14cb23-a487-49ae-a1f2-13a88ce960c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029928747s
STEP: Saw pod success
Mar  3 09:13:21.529: INFO: Pod "pod-ed14cb23-a487-49ae-a1f2-13a88ce960c2" satisfied condition "Succeeded or Failed"
Mar  3 09:13:21.532: INFO: Trying to get logs from node worker-1 pod pod-ed14cb23-a487-49ae-a1f2-13a88ce960c2 container test-container: <nil>
STEP: delete the pod
Mar  3 09:13:21.553: INFO: Waiting for pod pod-ed14cb23-a487-49ae-a1f2-13a88ce960c2 to disappear
Mar  3 09:13:21.556: INFO: Pod pod-ed14cb23-a487-49ae-a1f2-13a88ce960c2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:13:21.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2585" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":70,"skipped":1252,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:13:21.567: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Mar  3 09:13:21.602: INFO: Major version: 1
STEP: Confirm minor version
Mar  3 09:13:21.602: INFO: cleanMinorVersion: 20
Mar  3 09:13:21.602: INFO: Minor version: 20+
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:13:21.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8705" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":71,"skipped":1261,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:13:21.612: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  3 09:13:21.646: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  3 09:14:21.668: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:14:21.670: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Mar  3 09:14:23.750: INFO: found a healthy node: worker-1
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:14:37.814: INFO: pods created so far: [1 1 1]
Mar  3 09:14:37.814: INFO: length of pods created so far: 3
Mar  3 09:14:47.826: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:14:54.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9250" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:14:54.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7388" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:93.287 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":72,"skipped":1275,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:14:54.902: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar  3 09:14:54.959: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8446  ed9d43f6-1ec5-4799-b7dd-bc9a06bf51fc 7003 0 2021-03-03 09:14:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-03 09:14:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  3 09:14:54.959: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8446  ed9d43f6-1ec5-4799-b7dd-bc9a06bf51fc 7004 0 2021-03-03 09:14:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-03 09:14:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar  3 09:14:54.971: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8446  ed9d43f6-1ec5-4799-b7dd-bc9a06bf51fc 7005 0 2021-03-03 09:14:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-03 09:14:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  3 09:14:54.971: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8446  ed9d43f6-1ec5-4799-b7dd-bc9a06bf51fc 7006 0 2021-03-03 09:14:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-03 09:14:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:14:54.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8446" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":73,"skipped":1318,"failed":0}
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:14:54.981: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Mar  3 09:14:55.008: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:15:00.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7188" for this suite.

• [SLOW TEST:5.873 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":74,"skipped":1323,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:15:00.857: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-7xg2
STEP: Creating a pod to test atomic-volume-subpath
Mar  3 09:15:00.919: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7xg2" in namespace "subpath-3268" to be "Succeeded or Failed"
Mar  3 09:15:00.924: INFO: Pod "pod-subpath-test-configmap-7xg2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.732723ms
Mar  3 09:15:02.934: INFO: Pod "pod-subpath-test-configmap-7xg2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014788557s
Mar  3 09:15:04.941: INFO: Pod "pod-subpath-test-configmap-7xg2": Phase="Running", Reason="", readiness=true. Elapsed: 4.021937459s
Mar  3 09:15:06.946: INFO: Pod "pod-subpath-test-configmap-7xg2": Phase="Running", Reason="", readiness=true. Elapsed: 6.027106197s
Mar  3 09:15:08.954: INFO: Pod "pod-subpath-test-configmap-7xg2": Phase="Running", Reason="", readiness=true. Elapsed: 8.035453895s
Mar  3 09:15:10.964: INFO: Pod "pod-subpath-test-configmap-7xg2": Phase="Running", Reason="", readiness=true. Elapsed: 10.045130361s
Mar  3 09:15:12.968: INFO: Pod "pod-subpath-test-configmap-7xg2": Phase="Running", Reason="", readiness=true. Elapsed: 12.049234659s
Mar  3 09:15:14.973: INFO: Pod "pod-subpath-test-configmap-7xg2": Phase="Running", Reason="", readiness=true. Elapsed: 14.054181128s
Mar  3 09:15:16.980: INFO: Pod "pod-subpath-test-configmap-7xg2": Phase="Running", Reason="", readiness=true. Elapsed: 16.060646677s
Mar  3 09:15:18.986: INFO: Pod "pod-subpath-test-configmap-7xg2": Phase="Running", Reason="", readiness=true. Elapsed: 18.067362361s
Mar  3 09:15:20.992: INFO: Pod "pod-subpath-test-configmap-7xg2": Phase="Running", Reason="", readiness=true. Elapsed: 20.073336576s
Mar  3 09:15:22.997: INFO: Pod "pod-subpath-test-configmap-7xg2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.077590485s
STEP: Saw pod success
Mar  3 09:15:22.997: INFO: Pod "pod-subpath-test-configmap-7xg2" satisfied condition "Succeeded or Failed"
Mar  3 09:15:23.000: INFO: Trying to get logs from node worker-0 pod pod-subpath-test-configmap-7xg2 container test-container-subpath-configmap-7xg2: <nil>
STEP: delete the pod
Mar  3 09:15:23.025: INFO: Waiting for pod pod-subpath-test-configmap-7xg2 to disappear
Mar  3 09:15:23.027: INFO: Pod pod-subpath-test-configmap-7xg2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-7xg2
Mar  3 09:15:23.027: INFO: Deleting pod "pod-subpath-test-configmap-7xg2" in namespace "subpath-3268"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:15:23.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3268" for this suite.

• [SLOW TEST:22.181 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":75,"skipped":1333,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:15:23.038: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  3 09:15:24.085: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:15:24.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2657" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":76,"skipped":1342,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:15:24.103: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:15:24.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8892" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":77,"skipped":1439,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:15:24.179: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-021048e7-ddd0-44ea-b2e5-e81b46cfce3d
STEP: Creating a pod to test consume secrets
Mar  3 09:15:24.214: INFO: Waiting up to 5m0s for pod "pod-secrets-211d4570-3c02-4b59-90ba-ffded1e98a74" in namespace "secrets-6835" to be "Succeeded or Failed"
Mar  3 09:15:24.219: INFO: Pod "pod-secrets-211d4570-3c02-4b59-90ba-ffded1e98a74": Phase="Pending", Reason="", readiness=false. Elapsed: 4.558135ms
Mar  3 09:15:26.222: INFO: Pod "pod-secrets-211d4570-3c02-4b59-90ba-ffded1e98a74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00798384s
STEP: Saw pod success
Mar  3 09:15:26.222: INFO: Pod "pod-secrets-211d4570-3c02-4b59-90ba-ffded1e98a74" satisfied condition "Succeeded or Failed"
Mar  3 09:15:26.224: INFO: Trying to get logs from node worker-1 pod pod-secrets-211d4570-3c02-4b59-90ba-ffded1e98a74 container secret-volume-test: <nil>
STEP: delete the pod
Mar  3 09:15:26.248: INFO: Waiting for pod pod-secrets-211d4570-3c02-4b59-90ba-ffded1e98a74 to disappear
Mar  3 09:15:26.250: INFO: Pod pod-secrets-211d4570-3c02-4b59-90ba-ffded1e98a74 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:15:26.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6835" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":78,"skipped":1449,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:15:26.259: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar  3 09:15:26.296: INFO: Waiting up to 5m0s for pod "downward-api-a392f147-0b3f-4118-805f-6b785adc49ae" in namespace "downward-api-4741" to be "Succeeded or Failed"
Mar  3 09:15:26.298: INFO: Pod "downward-api-a392f147-0b3f-4118-805f-6b785adc49ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.355331ms
Mar  3 09:15:28.305: INFO: Pod "downward-api-a392f147-0b3f-4118-805f-6b785adc49ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008970874s
STEP: Saw pod success
Mar  3 09:15:28.305: INFO: Pod "downward-api-a392f147-0b3f-4118-805f-6b785adc49ae" satisfied condition "Succeeded or Failed"
Mar  3 09:15:28.308: INFO: Trying to get logs from node worker-1 pod downward-api-a392f147-0b3f-4118-805f-6b785adc49ae container dapi-container: <nil>
STEP: delete the pod
Mar  3 09:15:28.322: INFO: Waiting for pod downward-api-a392f147-0b3f-4118-805f-6b785adc49ae to disappear
Mar  3 09:15:28.324: INFO: Pod downward-api-a392f147-0b3f-4118-805f-6b785adc49ae no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:15:28.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4741" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":79,"skipped":1466,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:15:28.333: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 09:15:28.366: INFO: Waiting up to 5m0s for pod "downwardapi-volume-093c9d19-df79-424f-9e15-07aae38ccf27" in namespace "projected-1112" to be "Succeeded or Failed"
Mar  3 09:15:28.369: INFO: Pod "downwardapi-volume-093c9d19-df79-424f-9e15-07aae38ccf27": Phase="Pending", Reason="", readiness=false. Elapsed: 3.010739ms
Mar  3 09:15:30.376: INFO: Pod "downwardapi-volume-093c9d19-df79-424f-9e15-07aae38ccf27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009925793s
STEP: Saw pod success
Mar  3 09:15:30.376: INFO: Pod "downwardapi-volume-093c9d19-df79-424f-9e15-07aae38ccf27" satisfied condition "Succeeded or Failed"
Mar  3 09:15:30.381: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-093c9d19-df79-424f-9e15-07aae38ccf27 container client-container: <nil>
STEP: delete the pod
Mar  3 09:15:30.395: INFO: Waiting for pod downwardapi-volume-093c9d19-df79-424f-9e15-07aae38ccf27 to disappear
Mar  3 09:15:30.397: INFO: Pod downwardapi-volume-093c9d19-df79-424f-9e15-07aae38ccf27 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:15:30.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1112" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":80,"skipped":1472,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:15:30.405: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar  3 09:15:30.438: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  3 09:15:35.446: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:15:36.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9233" for this suite.

• [SLOW TEST:6.069 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":81,"skipped":1496,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:15:36.474: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  3 09:15:36.511: INFO: Waiting up to 5m0s for pod "pod-e4ea0a03-0786-4788-8c0a-6bc9821b2919" in namespace "emptydir-6840" to be "Succeeded or Failed"
Mar  3 09:15:36.513: INFO: Pod "pod-e4ea0a03-0786-4788-8c0a-6bc9821b2919": Phase="Pending", Reason="", readiness=false. Elapsed: 2.42675ms
Mar  3 09:15:38.517: INFO: Pod "pod-e4ea0a03-0786-4788-8c0a-6bc9821b2919": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005820765s
STEP: Saw pod success
Mar  3 09:15:38.517: INFO: Pod "pod-e4ea0a03-0786-4788-8c0a-6bc9821b2919" satisfied condition "Succeeded or Failed"
Mar  3 09:15:38.519: INFO: Trying to get logs from node worker-1 pod pod-e4ea0a03-0786-4788-8c0a-6bc9821b2919 container test-container: <nil>
STEP: delete the pod
Mar  3 09:15:38.534: INFO: Waiting for pod pod-e4ea0a03-0786-4788-8c0a-6bc9821b2919 to disappear
Mar  3 09:15:38.536: INFO: Pod pod-e4ea0a03-0786-4788-8c0a-6bc9821b2919 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:15:38.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6840" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":82,"skipped":1512,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:15:38.543: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 09:15:38.624: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7160c992-8253-4df8-a082-0bf5330dfe19" in namespace "projected-3460" to be "Succeeded or Failed"
Mar  3 09:15:38.630: INFO: Pod "downwardapi-volume-7160c992-8253-4df8-a082-0bf5330dfe19": Phase="Pending", Reason="", readiness=false. Elapsed: 6.160603ms
Mar  3 09:15:40.638: INFO: Pod "downwardapi-volume-7160c992-8253-4df8-a082-0bf5330dfe19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013314384s
STEP: Saw pod success
Mar  3 09:15:40.638: INFO: Pod "downwardapi-volume-7160c992-8253-4df8-a082-0bf5330dfe19" satisfied condition "Succeeded or Failed"
Mar  3 09:15:40.640: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-7160c992-8253-4df8-a082-0bf5330dfe19 container client-container: <nil>
STEP: delete the pod
Mar  3 09:15:40.655: INFO: Waiting for pod downwardapi-volume-7160c992-8253-4df8-a082-0bf5330dfe19 to disappear
Mar  3 09:15:40.657: INFO: Pod downwardapi-volume-7160c992-8253-4df8-a082-0bf5330dfe19 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:15:40.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3460" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":83,"skipped":1526,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:15:40.666: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6920
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-6920
Mar  3 09:15:40.723: INFO: Found 0 stateful pods, waiting for 1
Mar  3 09:15:50.727: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  3 09:15:50.743: INFO: Deleting all statefulset in ns statefulset-6920
Mar  3 09:15:50.746: INFO: Scaling statefulset ss to 0
Mar  3 09:16:20.807: INFO: Waiting for statefulset status.replicas updated to 0
Mar  3 09:16:20.809: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:16:20.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6920" for this suite.

• [SLOW TEST:40.177 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":84,"skipped":1562,"failed":0}
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:16:20.845: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-16385f44-ec30-49e7-b998-9c69dca7da9f in namespace container-probe-5484
Mar  3 09:16:22.914: INFO: Started pod liveness-16385f44-ec30-49e7-b998-9c69dca7da9f in namespace container-probe-5484
STEP: checking the pod's current state and verifying that restartCount is present
Mar  3 09:16:22.916: INFO: Initial restart count of pod liveness-16385f44-ec30-49e7-b998-9c69dca7da9f is 0
Mar  3 09:16:43.004: INFO: Restart count of pod container-probe-5484/liveness-16385f44-ec30-49e7-b998-9c69dca7da9f is now 1 (20.088285112s elapsed)
Mar  3 09:17:03.082: INFO: Restart count of pod container-probe-5484/liveness-16385f44-ec30-49e7-b998-9c69dca7da9f is now 2 (40.166018146s elapsed)
Mar  3 09:17:23.160: INFO: Restart count of pod container-probe-5484/liveness-16385f44-ec30-49e7-b998-9c69dca7da9f is now 3 (1m0.244210274s elapsed)
Mar  3 09:17:43.248: INFO: Restart count of pod container-probe-5484/liveness-16385f44-ec30-49e7-b998-9c69dca7da9f is now 4 (1m20.331982605s elapsed)
Mar  3 09:18:53.514: INFO: Restart count of pod container-probe-5484/liveness-16385f44-ec30-49e7-b998-9c69dca7da9f is now 5 (2m30.598459466s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:18:53.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5484" for this suite.

• [SLOW TEST:152.684 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":85,"skipped":1562,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:18:53.530: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  3 09:18:53.584: INFO: Waiting up to 5m0s for pod "pod-cfc8ca37-ff9b-4338-a27b-977c74d0a302" in namespace "emptydir-6757" to be "Succeeded or Failed"
Mar  3 09:18:53.590: INFO: Pod "pod-cfc8ca37-ff9b-4338-a27b-977c74d0a302": Phase="Pending", Reason="", readiness=false. Elapsed: 5.560323ms
Mar  3 09:18:55.596: INFO: Pod "pod-cfc8ca37-ff9b-4338-a27b-977c74d0a302": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011444343s
STEP: Saw pod success
Mar  3 09:18:55.596: INFO: Pod "pod-cfc8ca37-ff9b-4338-a27b-977c74d0a302" satisfied condition "Succeeded or Failed"
Mar  3 09:18:55.601: INFO: Trying to get logs from node worker-1 pod pod-cfc8ca37-ff9b-4338-a27b-977c74d0a302 container test-container: <nil>
STEP: delete the pod
Mar  3 09:18:55.625: INFO: Waiting for pod pod-cfc8ca37-ff9b-4338-a27b-977c74d0a302 to disappear
Mar  3 09:18:55.628: INFO: Pod pod-cfc8ca37-ff9b-4338-a27b-977c74d0a302 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:18:55.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6757" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":86,"skipped":1589,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:18:55.636: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:19:06.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5867" for this suite.

• [SLOW TEST:11.095 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":87,"skipped":1595,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:19:06.731: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  3 09:19:06.775: INFO: Waiting up to 5m0s for pod "pod-da7c41be-be70-4621-8d43-9be723680aa9" in namespace "emptydir-4180" to be "Succeeded or Failed"
Mar  3 09:19:06.785: INFO: Pod "pod-da7c41be-be70-4621-8d43-9be723680aa9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.172082ms
Mar  3 09:19:08.791: INFO: Pod "pod-da7c41be-be70-4621-8d43-9be723680aa9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016043518s
STEP: Saw pod success
Mar  3 09:19:08.791: INFO: Pod "pod-da7c41be-be70-4621-8d43-9be723680aa9" satisfied condition "Succeeded or Failed"
Mar  3 09:19:08.793: INFO: Trying to get logs from node worker-1 pod pod-da7c41be-be70-4621-8d43-9be723680aa9 container test-container: <nil>
STEP: delete the pod
Mar  3 09:19:08.807: INFO: Waiting for pod pod-da7c41be-be70-4621-8d43-9be723680aa9 to disappear
Mar  3 09:19:08.809: INFO: Pod pod-da7c41be-be70-4621-8d43-9be723680aa9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:19:08.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4180" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":88,"skipped":1623,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:19:08.820: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5461 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5461;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5461 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5461;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5461.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5461.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5461.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5461.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5461.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5461.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5461.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5461.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5461.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5461.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5461.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5461.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5461.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 83.255.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.255.83_udp@PTR;check="$$(dig +tcp +noall +answer +search 83.255.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.255.83_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5461 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5461;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5461 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5461;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5461.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5461.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5461.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5461.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5461.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5461.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5461.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5461.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5461.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5461.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5461.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5461.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5461.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 83.255.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.255.83_udp@PTR;check="$$(dig +tcp +noall +answer +search 83.255.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.255.83_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  3 09:19:10.893: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:10.897: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:10.900: INFO: Unable to read wheezy_udp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:10.903: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:10.906: INFO: Unable to read wheezy_udp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:10.912: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:10.920: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:10.927: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:10.948: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:10.952: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:10.955: INFO: Unable to read jessie_udp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:10.958: INFO: Unable to read jessie_tcp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:10.963: INFO: Unable to read jessie_udp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:10.967: INFO: Unable to read jessie_tcp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:10.971: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:10.975: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:11.004: INFO: Lookups using dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5461 wheezy_tcp@dns-test-service.dns-5461 wheezy_udp@dns-test-service.dns-5461.svc wheezy_tcp@dns-test-service.dns-5461.svc wheezy_udp@_http._tcp.dns-test-service.dns-5461.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5461.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5461 jessie_tcp@dns-test-service.dns-5461 jessie_udp@dns-test-service.dns-5461.svc jessie_tcp@dns-test-service.dns-5461.svc jessie_udp@_http._tcp.dns-test-service.dns-5461.svc jessie_tcp@_http._tcp.dns-test-service.dns-5461.svc]

Mar  3 09:19:16.009: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:16.012: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:16.015: INFO: Unable to read wheezy_udp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:16.018: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:16.021: INFO: Unable to read wheezy_udp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:16.024: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:16.027: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:16.030: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:16.050: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:16.053: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:16.056: INFO: Unable to read jessie_udp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:16.059: INFO: Unable to read jessie_tcp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:16.062: INFO: Unable to read jessie_udp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:16.065: INFO: Unable to read jessie_tcp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:16.067: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:16.070: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:16.094: INFO: Lookups using dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5461 wheezy_tcp@dns-test-service.dns-5461 wheezy_udp@dns-test-service.dns-5461.svc wheezy_tcp@dns-test-service.dns-5461.svc wheezy_udp@_http._tcp.dns-test-service.dns-5461.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5461.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5461 jessie_tcp@dns-test-service.dns-5461 jessie_udp@dns-test-service.dns-5461.svc jessie_tcp@dns-test-service.dns-5461.svc jessie_udp@_http._tcp.dns-test-service.dns-5461.svc jessie_tcp@_http._tcp.dns-test-service.dns-5461.svc]

Mar  3 09:19:21.009: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:21.012: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:21.015: INFO: Unable to read wheezy_udp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:21.018: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:21.021: INFO: Unable to read wheezy_udp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:21.024: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:21.027: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:21.030: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:21.050: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:21.053: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:21.056: INFO: Unable to read jessie_udp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:21.059: INFO: Unable to read jessie_tcp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:21.062: INFO: Unable to read jessie_udp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:21.065: INFO: Unable to read jessie_tcp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:21.068: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:21.071: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:21.087: INFO: Lookups using dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5461 wheezy_tcp@dns-test-service.dns-5461 wheezy_udp@dns-test-service.dns-5461.svc wheezy_tcp@dns-test-service.dns-5461.svc wheezy_udp@_http._tcp.dns-test-service.dns-5461.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5461.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5461 jessie_tcp@dns-test-service.dns-5461 jessie_udp@dns-test-service.dns-5461.svc jessie_tcp@dns-test-service.dns-5461.svc jessie_udp@_http._tcp.dns-test-service.dns-5461.svc jessie_tcp@_http._tcp.dns-test-service.dns-5461.svc]

Mar  3 09:19:26.009: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:26.011: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:26.014: INFO: Unable to read wheezy_udp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:26.017: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:26.021: INFO: Unable to read wheezy_udp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:26.024: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:26.027: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:26.030: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:26.050: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:26.053: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:26.056: INFO: Unable to read jessie_udp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:26.059: INFO: Unable to read jessie_tcp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:26.062: INFO: Unable to read jessie_udp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:26.065: INFO: Unable to read jessie_tcp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:26.068: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:26.071: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:26.092: INFO: Lookups using dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5461 wheezy_tcp@dns-test-service.dns-5461 wheezy_udp@dns-test-service.dns-5461.svc wheezy_tcp@dns-test-service.dns-5461.svc wheezy_udp@_http._tcp.dns-test-service.dns-5461.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5461.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5461 jessie_tcp@dns-test-service.dns-5461 jessie_udp@dns-test-service.dns-5461.svc jessie_tcp@dns-test-service.dns-5461.svc jessie_udp@_http._tcp.dns-test-service.dns-5461.svc jessie_tcp@_http._tcp.dns-test-service.dns-5461.svc]

Mar  3 09:19:31.017: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:31.020: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:31.024: INFO: Unable to read wheezy_udp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:31.026: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:31.029: INFO: Unable to read wheezy_udp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:31.032: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:31.035: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:31.038: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:31.058: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:31.061: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:31.063: INFO: Unable to read jessie_udp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:31.066: INFO: Unable to read jessie_tcp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:31.069: INFO: Unable to read jessie_udp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:31.072: INFO: Unable to read jessie_tcp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:31.075: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:31.078: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:31.093: INFO: Lookups using dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5461 wheezy_tcp@dns-test-service.dns-5461 wheezy_udp@dns-test-service.dns-5461.svc wheezy_tcp@dns-test-service.dns-5461.svc wheezy_udp@_http._tcp.dns-test-service.dns-5461.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5461.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5461 jessie_tcp@dns-test-service.dns-5461 jessie_udp@dns-test-service.dns-5461.svc jessie_tcp@dns-test-service.dns-5461.svc jessie_udp@_http._tcp.dns-test-service.dns-5461.svc jessie_tcp@_http._tcp.dns-test-service.dns-5461.svc]

Mar  3 09:19:36.009: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:36.013: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:36.016: INFO: Unable to read wheezy_udp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:36.019: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:36.022: INFO: Unable to read wheezy_udp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:36.025: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:36.028: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:36.032: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:36.053: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:36.056: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:36.059: INFO: Unable to read jessie_udp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:36.063: INFO: Unable to read jessie_tcp@dns-test-service.dns-5461 from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:36.066: INFO: Unable to read jessie_udp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:36.070: INFO: Unable to read jessie_tcp@dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:36.073: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:36.076: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5461.svc from pod dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402: the server could not find the requested resource (get pods dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402)
Mar  3 09:19:36.094: INFO: Lookups using dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5461 wheezy_tcp@dns-test-service.dns-5461 wheezy_udp@dns-test-service.dns-5461.svc wheezy_tcp@dns-test-service.dns-5461.svc wheezy_udp@_http._tcp.dns-test-service.dns-5461.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5461.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5461 jessie_tcp@dns-test-service.dns-5461 jessie_udp@dns-test-service.dns-5461.svc jessie_tcp@dns-test-service.dns-5461.svc jessie_udp@_http._tcp.dns-test-service.dns-5461.svc jessie_tcp@_http._tcp.dns-test-service.dns-5461.svc]

Mar  3 09:19:41.083: INFO: DNS probes using dns-5461/dns-test-3a01abd5-4cb1-41b0-b765-a6acc16e2402 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:19:41.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5461" for this suite.

• [SLOW TEST:32.430 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":89,"skipped":1646,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:19:41.256: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-ca43b8cb-cc9c-4d87-9b52-82f1a1667a8a
STEP: Creating a pod to test consume secrets
Mar  3 09:19:41.299: INFO: Waiting up to 5m0s for pod "pod-secrets-9fb32cdd-9213-4337-9ef3-763ddaaf7deb" in namespace "secrets-7337" to be "Succeeded or Failed"
Mar  3 09:19:41.301: INFO: Pod "pod-secrets-9fb32cdd-9213-4337-9ef3-763ddaaf7deb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.510882ms
Mar  3 09:19:43.313: INFO: Pod "pod-secrets-9fb32cdd-9213-4337-9ef3-763ddaaf7deb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014588747s
STEP: Saw pod success
Mar  3 09:19:43.314: INFO: Pod "pod-secrets-9fb32cdd-9213-4337-9ef3-763ddaaf7deb" satisfied condition "Succeeded or Failed"
Mar  3 09:19:43.318: INFO: Trying to get logs from node worker-1 pod pod-secrets-9fb32cdd-9213-4337-9ef3-763ddaaf7deb container secret-volume-test: <nil>
STEP: delete the pod
Mar  3 09:19:43.337: INFO: Waiting for pod pod-secrets-9fb32cdd-9213-4337-9ef3-763ddaaf7deb to disappear
Mar  3 09:19:43.339: INFO: Pod pod-secrets-9fb32cdd-9213-4337-9ef3-763ddaaf7deb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:19:43.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7337" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":90,"skipped":1684,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:19:43.348: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-4276
STEP: creating service affinity-nodeport-transition in namespace services-4276
STEP: creating replication controller affinity-nodeport-transition in namespace services-4276
I0303 09:19:43.406875      24 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-4276, replica count: 3
I0303 09:19:46.457303      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  3 09:19:46.468: INFO: Creating new exec pod
Mar  3 09:19:49.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-4276 exec execpod-affinitykhq8p -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Mar  3 09:19:49.667: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar  3 09:19:49.667: INFO: stdout: ""
Mar  3 09:19:49.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-4276 exec execpod-affinitykhq8p -- /bin/sh -x -c nc -zv -t -w 2 10.99.143.106 80'
Mar  3 09:19:49.826: INFO: stderr: "+ nc -zv -t -w 2 10.99.143.106 80\nConnection to 10.99.143.106 80 port [tcp/http] succeeded!\n"
Mar  3 09:19:49.826: INFO: stdout: ""
Mar  3 09:19:49.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-4276 exec execpod-affinitykhq8p -- /bin/sh -x -c nc -zv -t -w 2 10.0.47.86 32009'
Mar  3 09:19:49.989: INFO: stderr: "+ nc -zv -t -w 2 10.0.47.86 32009\nConnection to 10.0.47.86 32009 port [tcp/32009] succeeded!\n"
Mar  3 09:19:49.989: INFO: stdout: ""
Mar  3 09:19:49.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-4276 exec execpod-affinitykhq8p -- /bin/sh -x -c nc -zv -t -w 2 10.0.49.5 32009'
Mar  3 09:19:50.155: INFO: stderr: "+ nc -zv -t -w 2 10.0.49.5 32009\nConnection to 10.0.49.5 32009 port [tcp/32009] succeeded!\n"
Mar  3 09:19:50.155: INFO: stdout: ""
Mar  3 09:19:50.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-4276 exec execpod-affinitykhq8p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.49.5:32009/ ; done'
Mar  3 09:19:50.408: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n"
Mar  3 09:19:50.408: INFO: stdout: "\naffinity-nodeport-transition-m22c5\naffinity-nodeport-transition-6l4sj\naffinity-nodeport-transition-6l4sj\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-m22c5\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-m22c5\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-m22c5\naffinity-nodeport-transition-6l4sj\naffinity-nodeport-transition-m22c5\naffinity-nodeport-transition-m22c5\naffinity-nodeport-transition-6l4sj\naffinity-nodeport-transition-m22c5\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-6l4sj"
Mar  3 09:19:50.408: INFO: Received response from host: affinity-nodeport-transition-m22c5
Mar  3 09:19:50.408: INFO: Received response from host: affinity-nodeport-transition-6l4sj
Mar  3 09:19:50.408: INFO: Received response from host: affinity-nodeport-transition-6l4sj
Mar  3 09:19:50.408: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.408: INFO: Received response from host: affinity-nodeport-transition-m22c5
Mar  3 09:19:50.408: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.408: INFO: Received response from host: affinity-nodeport-transition-m22c5
Mar  3 09:19:50.408: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.408: INFO: Received response from host: affinity-nodeport-transition-m22c5
Mar  3 09:19:50.408: INFO: Received response from host: affinity-nodeport-transition-6l4sj
Mar  3 09:19:50.408: INFO: Received response from host: affinity-nodeport-transition-m22c5
Mar  3 09:19:50.408: INFO: Received response from host: affinity-nodeport-transition-m22c5
Mar  3 09:19:50.408: INFO: Received response from host: affinity-nodeport-transition-6l4sj
Mar  3 09:19:50.408: INFO: Received response from host: affinity-nodeport-transition-m22c5
Mar  3 09:19:50.409: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.409: INFO: Received response from host: affinity-nodeport-transition-6l4sj
Mar  3 09:19:50.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-4276 exec execpod-affinitykhq8p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.49.5:32009/ ; done'
Mar  3 09:19:50.685: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:32009/\n"
Mar  3 09:19:50.685: INFO: stdout: "\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-2t4xt\naffinity-nodeport-transition-2t4xt"
Mar  3 09:19:50.686: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.686: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.686: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.686: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.686: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.686: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.686: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.686: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.686: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.686: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.686: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.686: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.686: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.686: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.686: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.686: INFO: Received response from host: affinity-nodeport-transition-2t4xt
Mar  3 09:19:50.686: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4276, will wait for the garbage collector to delete the pods
Mar  3 09:19:50.785: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.053705ms
Mar  3 09:19:50.885: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.290571ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:20:02.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4276" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:19.193 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":91,"skipped":1688,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:20:02.541: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar  3 09:20:02.867: INFO: Pod name wrapped-volume-race-b296d5fe-221a-4935-bdc5-fe2bdb00ed56: Found 3 pods out of 5
Mar  3 09:20:07.883: INFO: Pod name wrapped-volume-race-b296d5fe-221a-4935-bdc5-fe2bdb00ed56: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b296d5fe-221a-4935-bdc5-fe2bdb00ed56 in namespace emptydir-wrapper-6499, will wait for the garbage collector to delete the pods
Mar  3 09:20:17.960: INFO: Deleting ReplicationController wrapped-volume-race-b296d5fe-221a-4935-bdc5-fe2bdb00ed56 took: 5.333485ms
Mar  3 09:20:18.060: INFO: Terminating ReplicationController wrapped-volume-race-b296d5fe-221a-4935-bdc5-fe2bdb00ed56 pods took: 100.227392ms
STEP: Creating RC which spawns configmap-volume pods
Mar  3 09:20:32.483: INFO: Pod name wrapped-volume-race-d7b84a5f-2e27-48c7-a3b5-5637d9f98ba8: Found 0 pods out of 5
Mar  3 09:20:37.494: INFO: Pod name wrapped-volume-race-d7b84a5f-2e27-48c7-a3b5-5637d9f98ba8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d7b84a5f-2e27-48c7-a3b5-5637d9f98ba8 in namespace emptydir-wrapper-6499, will wait for the garbage collector to delete the pods
Mar  3 09:20:47.573: INFO: Deleting ReplicationController wrapped-volume-race-d7b84a5f-2e27-48c7-a3b5-5637d9f98ba8 took: 4.914972ms
Mar  3 09:20:48.274: INFO: Terminating ReplicationController wrapped-volume-race-d7b84a5f-2e27-48c7-a3b5-5637d9f98ba8 pods took: 700.207498ms
STEP: Creating RC which spawns configmap-volume pods
Mar  3 09:20:57.799: INFO: Pod name wrapped-volume-race-d500f065-3a90-453f-aab3-a022c88d8ebf: Found 0 pods out of 5
Mar  3 09:21:02.814: INFO: Pod name wrapped-volume-race-d500f065-3a90-453f-aab3-a022c88d8ebf: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d500f065-3a90-453f-aab3-a022c88d8ebf in namespace emptydir-wrapper-6499, will wait for the garbage collector to delete the pods
Mar  3 09:21:12.908: INFO: Deleting ReplicationController wrapped-volume-race-d500f065-3a90-453f-aab3-a022c88d8ebf took: 6.018927ms
Mar  3 09:21:13.608: INFO: Terminating ReplicationController wrapped-volume-race-d500f065-3a90-453f-aab3-a022c88d8ebf pods took: 700.278664ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:21:22.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6499" for this suite.

• [SLOW TEST:80.167 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":92,"skipped":1716,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:21:22.711: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:21:22.765: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  3 09:21:27.773: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  3 09:21:27.773: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  3 09:21:29.781: INFO: Creating deployment "test-rollover-deployment"
Mar  3 09:21:29.793: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  3 09:21:31.802: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  3 09:21:31.806: INFO: Ensure that both replica sets have 1 created replica
Mar  3 09:21:31.810: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  3 09:21:31.816: INFO: Updating deployment test-rollover-deployment
Mar  3 09:21:31.816: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  3 09:21:33.827: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  3 09:21:33.831: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  3 09:21:33.835: INFO: all replica sets need to contain the pod-template-hash label
Mar  3 09:21:33.835: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360089, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360089, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360093, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360089, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  3 09:21:35.844: INFO: all replica sets need to contain the pod-template-hash label
Mar  3 09:21:35.844: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360089, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360089, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360093, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360089, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  3 09:21:37.841: INFO: all replica sets need to contain the pod-template-hash label
Mar  3 09:21:37.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360089, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360089, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360093, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360089, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  3 09:21:39.845: INFO: all replica sets need to contain the pod-template-hash label
Mar  3 09:21:39.845: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360089, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360089, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360093, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360089, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  3 09:21:41.847: INFO: all replica sets need to contain the pod-template-hash label
Mar  3 09:21:41.847: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360089, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360089, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360093, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360089, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  3 09:21:43.845: INFO: 
Mar  3 09:21:43.845: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  3 09:21:43.853: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-122  bc6e2f48-59bf-46a2-95fa-d001febc7a91 9190 2 2021-03-03 09:21:29 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-03 09:21:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-03 09:21:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003846b18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-03 09:21:29 +0000 UTC,LastTransitionTime:2021-03-03 09:21:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-03-03 09:21:43 +0000 UTC,LastTransitionTime:2021-03-03 09:21:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  3 09:21:43.857: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-122  86e6bd1d-0bf7-4bd9-887f-b20aef963d8c 9179 2 2021-03-03 09:21:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment bc6e2f48-59bf-46a2-95fa-d001febc7a91 0xc00388c1d7 0xc00388c1d8}] []  [{kube-controller-manager Update apps/v1 2021-03-03 09:21:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc6e2f48-59bf-46a2-95fa-d001febc7a91\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00388c268 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  3 09:21:43.857: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  3 09:21:43.857: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-122  cf22205e-d407-48eb-8d33-a74d40f43155 9189 2 2021-03-03 09:21:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment bc6e2f48-59bf-46a2-95fa-d001febc7a91 0xc00388c0cf 0xc00388c0e0}] []  [{e2e.test Update apps/v1 2021-03-03 09:21:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-03 09:21:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc6e2f48-59bf-46a2-95fa-d001febc7a91\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00388c178 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  3 09:21:43.859: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-122  6670839c-92e1-41ba-9bbf-66716e43b9c2 9156 2 2021-03-03 09:21:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment bc6e2f48-59bf-46a2-95fa-d001febc7a91 0xc00388c2c7 0xc00388c2c8}] []  [{kube-controller-manager Update apps/v1 2021-03-03 09:21:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc6e2f48-59bf-46a2-95fa-d001febc7a91\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00388c358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  3 09:21:43.862: INFO: Pod "test-rollover-deployment-668db69979-qmpzg" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-qmpzg test-rollover-deployment-668db69979- deployment-122  031bb77a-f1b0-466a-82cf-ec8025641b8f 9172 0 2021-03-03 09:21:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[cni.projectcalico.org/podIP:10.244.226.103/32 cni.projectcalico.org/podIPs:10.244.226.103/32] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 86e6bd1d-0bf7-4bd9-887f-b20aef963d8c 0xc00388c987 0xc00388c988}] []  [{kube-controller-manager Update v1 2021-03-03 09:21:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86e6bd1d-0bf7-4bd9-887f-b20aef963d8c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-03 09:21:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-03 09:21:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.226.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-79l77,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-79l77,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-79l77,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:21:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:21:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:21:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:21:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.47.86,PodIP:10.244.226.103,StartTime:2021-03-03 09:21:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-03 09:21:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://6b196e668a46c2003ead9b7bcb10c2e2669a2faca582bdd7b7399de45e090031,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.226.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:21:43.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-122" for this suite.

• [SLOW TEST:21.160 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":93,"skipped":1738,"failed":0}
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:21:43.872: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  3 09:21:47.957: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5911 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:21:47.957: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:21:48.029: INFO: Exec stderr: ""
Mar  3 09:21:48.029: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5911 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:21:48.029: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:21:48.113: INFO: Exec stderr: ""
Mar  3 09:21:48.113: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5911 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:21:48.113: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:21:48.184: INFO: Exec stderr: ""
Mar  3 09:21:48.184: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5911 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:21:48.184: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:21:48.251: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  3 09:21:48.251: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5911 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:21:48.251: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:21:48.321: INFO: Exec stderr: ""
Mar  3 09:21:48.321: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5911 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:21:48.321: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:21:48.416: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  3 09:21:48.416: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5911 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:21:48.416: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:21:48.488: INFO: Exec stderr: ""
Mar  3 09:21:48.488: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5911 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:21:48.488: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:21:48.555: INFO: Exec stderr: ""
Mar  3 09:21:48.555: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5911 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:21:48.555: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:21:48.616: INFO: Exec stderr: ""
Mar  3 09:21:48.616: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5911 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:21:48.617: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:21:48.680: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:21:48.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5911" for this suite.
•{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":94,"skipped":1738,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:21:48.693: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-f7d94fbc-f0c5-49cd-bb77-d471e684d3af
STEP: Creating a pod to test consume configMaps
Mar  3 09:21:48.736: INFO: Waiting up to 5m0s for pod "pod-configmaps-99c554f7-d69a-40bf-b4db-e77da8df6bf9" in namespace "configmap-6030" to be "Succeeded or Failed"
Mar  3 09:21:48.740: INFO: Pod "pod-configmaps-99c554f7-d69a-40bf-b4db-e77da8df6bf9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.503816ms
Mar  3 09:21:50.746: INFO: Pod "pod-configmaps-99c554f7-d69a-40bf-b4db-e77da8df6bf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00997327s
STEP: Saw pod success
Mar  3 09:21:50.746: INFO: Pod "pod-configmaps-99c554f7-d69a-40bf-b4db-e77da8df6bf9" satisfied condition "Succeeded or Failed"
Mar  3 09:21:50.749: INFO: Trying to get logs from node controller-0 pod pod-configmaps-99c554f7-d69a-40bf-b4db-e77da8df6bf9 container agnhost-container: <nil>
STEP: delete the pod
Mar  3 09:21:50.783: INFO: Waiting for pod pod-configmaps-99c554f7-d69a-40bf-b4db-e77da8df6bf9 to disappear
Mar  3 09:21:50.786: INFO: Pod pod-configmaps-99c554f7-d69a-40bf-b4db-e77da8df6bf9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:21:50.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6030" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":95,"skipped":1758,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:21:50.796: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  3 09:21:51.335: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 09:21:54.353: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:21:54.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2158" for this suite.
STEP: Destroying namespace "webhook-2158-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":96,"skipped":1758,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:21:54.585: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:21:54.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6804" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":97,"skipped":1766,"failed":0}

------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:21:54.678: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-bf765fae-4897-45fd-ac6e-9301d0594043
STEP: Creating secret with name s-test-opt-upd-c3824de2-fb4a-4184-829f-cf7d93d6800d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-bf765fae-4897-45fd-ac6e-9301d0594043
STEP: Updating secret s-test-opt-upd-c3824de2-fb4a-4184-829f-cf7d93d6800d
STEP: Creating secret with name s-test-opt-create-29f0588c-de75-4c23-8efe-cb62374fb683
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:22:00.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3645" for this suite.

• [SLOW TEST:6.135 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":98,"skipped":1766,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:22:00.816: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Mar  3 09:22:00.857: INFO: Waiting up to 5m0s for pod "client-containers-41861a8c-d8a1-4a32-91b0-7eb41c390b5d" in namespace "containers-9229" to be "Succeeded or Failed"
Mar  3 09:22:00.867: INFO: Pod "client-containers-41861a8c-d8a1-4a32-91b0-7eb41c390b5d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.405653ms
Mar  3 09:22:02.873: INFO: Pod "client-containers-41861a8c-d8a1-4a32-91b0-7eb41c390b5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015711716s
STEP: Saw pod success
Mar  3 09:22:02.873: INFO: Pod "client-containers-41861a8c-d8a1-4a32-91b0-7eb41c390b5d" satisfied condition "Succeeded or Failed"
Mar  3 09:22:02.876: INFO: Trying to get logs from node controller-0 pod client-containers-41861a8c-d8a1-4a32-91b0-7eb41c390b5d container agnhost-container: <nil>
STEP: delete the pod
Mar  3 09:22:02.893: INFO: Waiting for pod client-containers-41861a8c-d8a1-4a32-91b0-7eb41c390b5d to disappear
Mar  3 09:22:02.896: INFO: Pod client-containers-41861a8c-d8a1-4a32-91b0-7eb41c390b5d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:22:02.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9229" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":99,"skipped":1867,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:22:02.905: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:22:08.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5725" for this suite.

• [SLOW TEST:6.074 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":100,"skipped":1904,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:22:08.980: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8430
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8430
STEP: creating replication controller externalsvc in namespace services-8430
I0303 09:22:09.039388      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8430, replica count: 2
I0303 09:22:12.090288      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar  3 09:22:12.120: INFO: Creating new exec pod
Mar  3 09:22:14.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-8430 exec execpodcsxcn -- /bin/sh -x -c nslookup nodeport-service.services-8430.svc.cluster.local'
Mar  3 09:22:14.559: INFO: stderr: "+ nslookup nodeport-service.services-8430.svc.cluster.local\n"
Mar  3 09:22:14.559: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-8430.svc.cluster.local\tcanonical name = externalsvc.services-8430.svc.cluster.local.\nName:\texternalsvc.services-8430.svc.cluster.local\nAddress: 10.107.74.59\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8430, will wait for the garbage collector to delete the pods
Mar  3 09:22:14.619: INFO: Deleting ReplicationController externalsvc took: 5.775033ms
Mar  3 09:22:14.719: INFO: Terminating ReplicationController externalsvc pods took: 100.237426ms
Mar  3 09:22:27.764: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:22:27.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8430" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:18.823 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":101,"skipped":1905,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:22:27.803: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:22:29.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7165" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":102,"skipped":1915,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:22:29.886: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:22:29.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6535 version'
Mar  3 09:22:29.999: INFO: stderr: ""
Mar  3 09:22:29.999: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.4\", GitCommit:\"e87da0bd6e03ec3fea7933c4b5263d151aafd07c\", GitTreeState:\"clean\", BuildDate:\"2021-02-18T16:12:00Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20+\", GitVersion:\"v1.20.4-k0s1\", GitCommit:\"e87da0bd6e03ec3fea7933c4b5263d151aafd07c\", GitTreeState:\"clean\", BuildDate:\"2021-03-03T07:31:16Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:22:30.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6535" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":103,"skipped":1938,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:22:30.007: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-3735
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  3 09:22:30.035: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  3 09:22:30.091: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  3 09:22:32.098: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 09:22:34.098: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 09:22:36.095: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 09:22:38.099: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 09:22:40.103: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 09:22:42.098: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 09:22:44.096: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 09:22:46.103: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  3 09:22:46.108: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  3 09:22:48.115: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  3 09:22:50.115: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  3 09:22:50.120: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar  3 09:22:52.154: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  3 09:22:52.154: INFO: Going to poll 10.244.192.106 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  3 09:22:52.156: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.192.106 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3735 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:22:52.156: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:22:53.242: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  3 09:22:53.242: INFO: Going to poll 10.244.43.50 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  3 09:22:53.249: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.43.50 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3735 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:22:53.249: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:22:54.324: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  3 09:22:54.324: INFO: Going to poll 10.244.226.107 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  3 09:22:54.332: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.226.107 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3735 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:22:54.332: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:22:55.416: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:22:55.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3735" for this suite.

• [SLOW TEST:25.419 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":104,"skipped":1938,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:22:55.426: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  3 09:22:55.837: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  3 09:22:57.845: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360175, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360175, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360175, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750360175, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 09:23:00.867: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:23:00.872: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7307-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:23:02.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6945" for this suite.
STEP: Destroying namespace "webhook-6945-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.676 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":105,"skipped":1946,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:23:02.106: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 09:23:02.162: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0f2ec14e-8a22-4327-99f2-849fc3fda568" in namespace "projected-5534" to be "Succeeded or Failed"
Mar  3 09:23:02.172: INFO: Pod "downwardapi-volume-0f2ec14e-8a22-4327-99f2-849fc3fda568": Phase="Pending", Reason="", readiness=false. Elapsed: 9.331931ms
Mar  3 09:23:04.179: INFO: Pod "downwardapi-volume-0f2ec14e-8a22-4327-99f2-849fc3fda568": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016799931s
STEP: Saw pod success
Mar  3 09:23:04.179: INFO: Pod "downwardapi-volume-0f2ec14e-8a22-4327-99f2-849fc3fda568" satisfied condition "Succeeded or Failed"
Mar  3 09:23:04.182: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-0f2ec14e-8a22-4327-99f2-849fc3fda568 container client-container: <nil>
STEP: delete the pod
Mar  3 09:23:04.212: INFO: Waiting for pod downwardapi-volume-0f2ec14e-8a22-4327-99f2-849fc3fda568 to disappear
Mar  3 09:23:04.215: INFO: Pod downwardapi-volume-0f2ec14e-8a22-4327-99f2-849fc3fda568 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:23:04.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5534" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":106,"skipped":1954,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:23:04.227: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-2755
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2755 to expose endpoints map[]
Mar  3 09:23:04.305: INFO: successfully validated that service endpoint-test2 in namespace services-2755 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2755
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2755 to expose endpoints map[pod1:[80]]
Mar  3 09:23:06.345: INFO: successfully validated that service endpoint-test2 in namespace services-2755 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-2755
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2755 to expose endpoints map[pod1:[80] pod2:[80]]
Mar  3 09:23:08.372: INFO: successfully validated that service endpoint-test2 in namespace services-2755 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-2755
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2755 to expose endpoints map[pod2:[80]]
Mar  3 09:23:08.411: INFO: successfully validated that service endpoint-test2 in namespace services-2755 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-2755
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2755 to expose endpoints map[]
Mar  3 09:23:08.454: INFO: successfully validated that service endpoint-test2 in namespace services-2755 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:23:08.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2755" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":107,"skipped":1957,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:23:08.503: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-702e59ba-04f3-4059-9b01-54ed908e1171
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-702e59ba-04f3-4059-9b01-54ed908e1171
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:23:12.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1563" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":108,"skipped":1976,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:23:12.663: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Mar  3 09:23:15.244: INFO: Successfully updated pod "labelsupdate19be8ad4-1dd4-4b31-9bdf-06398d2cd4a2"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:23:17.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9479" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":109,"skipped":1993,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:23:17.270: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  3 09:23:17.304: INFO: Waiting up to 5m0s for pod "pod-86c1457a-211e-4d36-9b97-5acee2b50ba2" in namespace "emptydir-3936" to be "Succeeded or Failed"
Mar  3 09:23:17.310: INFO: Pod "pod-86c1457a-211e-4d36-9b97-5acee2b50ba2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.542557ms
Mar  3 09:23:19.317: INFO: Pod "pod-86c1457a-211e-4d36-9b97-5acee2b50ba2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013345698s
STEP: Saw pod success
Mar  3 09:23:19.317: INFO: Pod "pod-86c1457a-211e-4d36-9b97-5acee2b50ba2" satisfied condition "Succeeded or Failed"
Mar  3 09:23:19.320: INFO: Trying to get logs from node worker-1 pod pod-86c1457a-211e-4d36-9b97-5acee2b50ba2 container test-container: <nil>
STEP: delete the pod
Mar  3 09:23:19.334: INFO: Waiting for pod pod-86c1457a-211e-4d36-9b97-5acee2b50ba2 to disappear
Mar  3 09:23:19.337: INFO: Pod pod-86c1457a-211e-4d36-9b97-5acee2b50ba2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:23:19.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3936" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":110,"skipped":2006,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:23:19.344: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  3 09:23:19.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-5537 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Mar  3 09:23:19.459: INFO: stderr: ""
Mar  3 09:23:19.460: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Mar  3 09:23:19.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-5537 delete pods e2e-test-httpd-pod'
Mar  3 09:23:21.934: INFO: stderr: ""
Mar  3 09:23:21.934: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:23:21.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5537" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":111,"skipped":2024,"failed":0}
SSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:23:21.945: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar  3 09:23:22.000: INFO: starting watch
STEP: patching
STEP: updating
Mar  3 09:23:22.010: INFO: waiting for watch events with expected annotations
Mar  3 09:23:22.010: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:23:22.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-2399" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":112,"skipped":2029,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:23:22.040: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar  3 09:23:22.078: INFO: Waiting up to 5m0s for pod "downward-api-6fe162fa-12b4-4839-bc56-231e4b1b816a" in namespace "downward-api-6064" to be "Succeeded or Failed"
Mar  3 09:23:22.082: INFO: Pod "downward-api-6fe162fa-12b4-4839-bc56-231e4b1b816a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.235952ms
Mar  3 09:23:24.089: INFO: Pod "downward-api-6fe162fa-12b4-4839-bc56-231e4b1b816a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010634766s
STEP: Saw pod success
Mar  3 09:23:24.089: INFO: Pod "downward-api-6fe162fa-12b4-4839-bc56-231e4b1b816a" satisfied condition "Succeeded or Failed"
Mar  3 09:23:24.093: INFO: Trying to get logs from node worker-1 pod downward-api-6fe162fa-12b4-4839-bc56-231e4b1b816a container dapi-container: <nil>
STEP: delete the pod
Mar  3 09:23:24.107: INFO: Waiting for pod downward-api-6fe162fa-12b4-4839-bc56-231e4b1b816a to disappear
Mar  3 09:23:24.110: INFO: Pod downward-api-6fe162fa-12b4-4839-bc56-231e4b1b816a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:23:24.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6064" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":113,"skipped":2033,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:23:24.119: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-b9e10eef-85d2-404e-b629-844ffce7ce65
STEP: Creating a pod to test consume configMaps
Mar  3 09:23:24.162: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-81d17627-b417-4786-a3f3-5841dff8b8da" in namespace "projected-3012" to be "Succeeded or Failed"
Mar  3 09:23:24.164: INFO: Pod "pod-projected-configmaps-81d17627-b417-4786-a3f3-5841dff8b8da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.477386ms
Mar  3 09:23:26.169: INFO: Pod "pod-projected-configmaps-81d17627-b417-4786-a3f3-5841dff8b8da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007461222s
STEP: Saw pod success
Mar  3 09:23:26.169: INFO: Pod "pod-projected-configmaps-81d17627-b417-4786-a3f3-5841dff8b8da" satisfied condition "Succeeded or Failed"
Mar  3 09:23:26.172: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-81d17627-b417-4786-a3f3-5841dff8b8da container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  3 09:23:26.186: INFO: Waiting for pod pod-projected-configmaps-81d17627-b417-4786-a3f3-5841dff8b8da to disappear
Mar  3 09:23:26.188: INFO: Pod pod-projected-configmaps-81d17627-b417-4786-a3f3-5841dff8b8da no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:23:26.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3012" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":114,"skipped":2044,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:23:26.198: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:23:26.244: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-3de15cb9-da24-44f3-a3c6-df6575037215" in namespace "security-context-test-7309" to be "Succeeded or Failed"
Mar  3 09:23:26.249: INFO: Pod "alpine-nnp-false-3de15cb9-da24-44f3-a3c6-df6575037215": Phase="Pending", Reason="", readiness=false. Elapsed: 4.942266ms
Mar  3 09:23:28.256: INFO: Pod "alpine-nnp-false-3de15cb9-da24-44f3-a3c6-df6575037215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012319238s
Mar  3 09:23:30.263: INFO: Pod "alpine-nnp-false-3de15cb9-da24-44f3-a3c6-df6575037215": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018830258s
Mar  3 09:23:30.263: INFO: Pod "alpine-nnp-false-3de15cb9-da24-44f3-a3c6-df6575037215" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:23:30.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7309" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":115,"skipped":2052,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:23:30.280: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-64182f41-77ad-4c52-b30d-59c6120fd00c
STEP: Creating a pod to test consume configMaps
Mar  3 09:23:30.320: INFO: Waiting up to 5m0s for pod "pod-configmaps-4fe00a6a-4e27-43b9-9ab4-a48d1bca7e27" in namespace "configmap-4034" to be "Succeeded or Failed"
Mar  3 09:23:30.323: INFO: Pod "pod-configmaps-4fe00a6a-4e27-43b9-9ab4-a48d1bca7e27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.990649ms
Mar  3 09:23:32.329: INFO: Pod "pod-configmaps-4fe00a6a-4e27-43b9-9ab4-a48d1bca7e27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009421663s
STEP: Saw pod success
Mar  3 09:23:32.329: INFO: Pod "pod-configmaps-4fe00a6a-4e27-43b9-9ab4-a48d1bca7e27" satisfied condition "Succeeded or Failed"
Mar  3 09:23:32.332: INFO: Trying to get logs from node worker-1 pod pod-configmaps-4fe00a6a-4e27-43b9-9ab4-a48d1bca7e27 container agnhost-container: <nil>
STEP: delete the pod
Mar  3 09:23:32.345: INFO: Waiting for pod pod-configmaps-4fe00a6a-4e27-43b9-9ab4-a48d1bca7e27 to disappear
Mar  3 09:23:32.348: INFO: Pod pod-configmaps-4fe00a6a-4e27-43b9-9ab4-a48d1bca7e27 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:23:32.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4034" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":116,"skipped":2099,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:23:32.356: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 09:23:32.388: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44a01003-d860-4e5d-b813-c6ddf0c3c500" in namespace "projected-1744" to be "Succeeded or Failed"
Mar  3 09:23:32.392: INFO: Pod "downwardapi-volume-44a01003-d860-4e5d-b813-c6ddf0c3c500": Phase="Pending", Reason="", readiness=false. Elapsed: 3.264384ms
Mar  3 09:23:34.399: INFO: Pod "downwardapi-volume-44a01003-d860-4e5d-b813-c6ddf0c3c500": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010548545s
STEP: Saw pod success
Mar  3 09:23:34.399: INFO: Pod "downwardapi-volume-44a01003-d860-4e5d-b813-c6ddf0c3c500" satisfied condition "Succeeded or Failed"
Mar  3 09:23:34.401: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-44a01003-d860-4e5d-b813-c6ddf0c3c500 container client-container: <nil>
STEP: delete the pod
Mar  3 09:23:34.416: INFO: Waiting for pod downwardapi-volume-44a01003-d860-4e5d-b813-c6ddf0c3c500 to disappear
Mar  3 09:23:34.422: INFO: Pod downwardapi-volume-44a01003-d860-4e5d-b813-c6ddf0c3c500 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:23:34.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1744" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":117,"skipped":2108,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:23:34.433: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:23:34.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7443" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":118,"skipped":2157,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:23:34.470: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-e8a5ce47-c658-4cf7-a324-11f1f1600570
STEP: Creating a pod to test consume secrets
Mar  3 09:23:34.513: INFO: Waiting up to 5m0s for pod "pod-secrets-c9571a3a-83ea-4f49-a4b1-264e3753eae8" in namespace "secrets-2438" to be "Succeeded or Failed"
Mar  3 09:23:34.516: INFO: Pod "pod-secrets-c9571a3a-83ea-4f49-a4b1-264e3753eae8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.07533ms
Mar  3 09:23:36.521: INFO: Pod "pod-secrets-c9571a3a-83ea-4f49-a4b1-264e3753eae8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007964649s
STEP: Saw pod success
Mar  3 09:23:36.521: INFO: Pod "pod-secrets-c9571a3a-83ea-4f49-a4b1-264e3753eae8" satisfied condition "Succeeded or Failed"
Mar  3 09:23:36.523: INFO: Trying to get logs from node worker-1 pod pod-secrets-c9571a3a-83ea-4f49-a4b1-264e3753eae8 container secret-env-test: <nil>
STEP: delete the pod
Mar  3 09:23:36.538: INFO: Waiting for pod pod-secrets-c9571a3a-83ea-4f49-a4b1-264e3753eae8 to disappear
Mar  3 09:23:36.540: INFO: Pod pod-secrets-c9571a3a-83ea-4f49-a4b1-264e3753eae8 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:23:36.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2438" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":119,"skipped":2161,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:23:36.548: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  3 09:23:36.584: INFO: Waiting up to 5m0s for pod "pod-4ebb8e82-7416-4f1a-b024-64ac0ee2adcd" in namespace "emptydir-700" to be "Succeeded or Failed"
Mar  3 09:23:36.587: INFO: Pod "pod-4ebb8e82-7416-4f1a-b024-64ac0ee2adcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.925048ms
Mar  3 09:23:38.591: INFO: Pod "pod-4ebb8e82-7416-4f1a-b024-64ac0ee2adcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007493622s
STEP: Saw pod success
Mar  3 09:23:38.591: INFO: Pod "pod-4ebb8e82-7416-4f1a-b024-64ac0ee2adcd" satisfied condition "Succeeded or Failed"
Mar  3 09:23:38.593: INFO: Trying to get logs from node worker-1 pod pod-4ebb8e82-7416-4f1a-b024-64ac0ee2adcd container test-container: <nil>
STEP: delete the pod
Mar  3 09:23:38.610: INFO: Waiting for pod pod-4ebb8e82-7416-4f1a-b024-64ac0ee2adcd to disappear
Mar  3 09:23:38.613: INFO: Pod pod-4ebb8e82-7416-4f1a-b024-64ac0ee2adcd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:23:38.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-700" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":120,"skipped":2171,"failed":0}

------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:23:38.620: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:23:47.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5699" for this suite.

• [SLOW TEST:8.838 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":121,"skipped":2171,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:23:47.459: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-5854038e-6e68-477b-a6df-8a1a62e676e0 in namespace container-probe-3605
Mar  3 09:23:49.519: INFO: Started pod test-webserver-5854038e-6e68-477b-a6df-8a1a62e676e0 in namespace container-probe-3605
STEP: checking the pod's current state and verifying that restartCount is present
Mar  3 09:23:49.522: INFO: Initial restart count of pod test-webserver-5854038e-6e68-477b-a6df-8a1a62e676e0 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:27:51.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3605" for this suite.

• [SLOW TEST:243.986 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":122,"skipped":2174,"failed":0}
SS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:27:51.446: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-2086
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2086 to expose endpoints map[]
Mar  3 09:27:51.574: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Mar  3 09:27:52.583: INFO: successfully validated that service multi-endpoint-test in namespace services-2086 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2086
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2086 to expose endpoints map[pod1:[100]]
Mar  3 09:27:55.613: INFO: successfully validated that service multi-endpoint-test in namespace services-2086 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-2086
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2086 to expose endpoints map[pod1:[100] pod2:[101]]
Mar  3 09:27:57.661: INFO: successfully validated that service multi-endpoint-test in namespace services-2086 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-2086
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2086 to expose endpoints map[pod2:[101]]
Mar  3 09:27:58.729: INFO: successfully validated that service multi-endpoint-test in namespace services-2086 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-2086
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2086 to expose endpoints map[]
Mar  3 09:27:58.835: INFO: successfully validated that service multi-endpoint-test in namespace services-2086 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:27:58.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2086" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:7.467 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":123,"skipped":2176,"failed":0}
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:27:58.913: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar  3 09:27:58.983: INFO: Waiting up to 5m0s for pod "downward-api-d2d87022-d250-4931-93a1-431968c18a91" in namespace "downward-api-9536" to be "Succeeded or Failed"
Mar  3 09:27:59.001: INFO: Pod "downward-api-d2d87022-d250-4931-93a1-431968c18a91": Phase="Pending", Reason="", readiness=false. Elapsed: 17.946017ms
Mar  3 09:28:01.010: INFO: Pod "downward-api-d2d87022-d250-4931-93a1-431968c18a91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026865479s
STEP: Saw pod success
Mar  3 09:28:01.010: INFO: Pod "downward-api-d2d87022-d250-4931-93a1-431968c18a91" satisfied condition "Succeeded or Failed"
Mar  3 09:28:01.013: INFO: Trying to get logs from node worker-1 pod downward-api-d2d87022-d250-4931-93a1-431968c18a91 container dapi-container: <nil>
STEP: delete the pod
Mar  3 09:28:01.050: INFO: Waiting for pod downward-api-d2d87022-d250-4931-93a1-431968c18a91 to disappear
Mar  3 09:28:01.054: INFO: Pod downward-api-d2d87022-d250-4931-93a1-431968c18a91 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:28:01.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9536" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":124,"skipped":2176,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:28:01.063: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-dptk
STEP: Creating a pod to test atomic-volume-subpath
Mar  3 09:28:01.119: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-dptk" in namespace "subpath-9612" to be "Succeeded or Failed"
Mar  3 09:28:01.148: INFO: Pod "pod-subpath-test-projected-dptk": Phase="Pending", Reason="", readiness=false. Elapsed: 29.044489ms
Mar  3 09:28:03.154: INFO: Pod "pod-subpath-test-projected-dptk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035079858s
Mar  3 09:28:05.165: INFO: Pod "pod-subpath-test-projected-dptk": Phase="Running", Reason="", readiness=true. Elapsed: 4.045702826s
Mar  3 09:28:07.174: INFO: Pod "pod-subpath-test-projected-dptk": Phase="Running", Reason="", readiness=true. Elapsed: 6.055316017s
Mar  3 09:28:09.183: INFO: Pod "pod-subpath-test-projected-dptk": Phase="Running", Reason="", readiness=true. Elapsed: 8.063735397s
Mar  3 09:28:11.197: INFO: Pod "pod-subpath-test-projected-dptk": Phase="Running", Reason="", readiness=true. Elapsed: 10.078255896s
Mar  3 09:28:13.205: INFO: Pod "pod-subpath-test-projected-dptk": Phase="Running", Reason="", readiness=true. Elapsed: 12.086371808s
Mar  3 09:28:15.216: INFO: Pod "pod-subpath-test-projected-dptk": Phase="Running", Reason="", readiness=true. Elapsed: 14.09686104s
Mar  3 09:28:17.226: INFO: Pod "pod-subpath-test-projected-dptk": Phase="Running", Reason="", readiness=true. Elapsed: 16.106589473s
Mar  3 09:28:19.232: INFO: Pod "pod-subpath-test-projected-dptk": Phase="Running", Reason="", readiness=true. Elapsed: 18.112864331s
Mar  3 09:28:21.242: INFO: Pod "pod-subpath-test-projected-dptk": Phase="Running", Reason="", readiness=true. Elapsed: 20.123172406s
Mar  3 09:28:23.251: INFO: Pod "pod-subpath-test-projected-dptk": Phase="Running", Reason="", readiness=true. Elapsed: 22.131843565s
Mar  3 09:28:25.259: INFO: Pod "pod-subpath-test-projected-dptk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.139997384s
STEP: Saw pod success
Mar  3 09:28:25.259: INFO: Pod "pod-subpath-test-projected-dptk" satisfied condition "Succeeded or Failed"
Mar  3 09:28:25.262: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-projected-dptk container test-container-subpath-projected-dptk: <nil>
STEP: delete the pod
Mar  3 09:28:25.287: INFO: Waiting for pod pod-subpath-test-projected-dptk to disappear
Mar  3 09:28:25.289: INFO: Pod pod-subpath-test-projected-dptk no longer exists
STEP: Deleting pod pod-subpath-test-projected-dptk
Mar  3 09:28:25.289: INFO: Deleting pod "pod-subpath-test-projected-dptk" in namespace "subpath-9612"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:28:25.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9612" for this suite.

• [SLOW TEST:24.238 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":125,"skipped":2189,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:28:25.305: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  3 09:28:27.900: INFO: Successfully updated pod "pod-update-activedeadlineseconds-3a5709ae-b326-4d44-83a2-81dd582830dc"
Mar  3 09:28:27.900: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3a5709ae-b326-4d44-83a2-81dd582830dc" in namespace "pods-3333" to be "terminated due to deadline exceeded"
Mar  3 09:28:27.903: INFO: Pod "pod-update-activedeadlineseconds-3a5709ae-b326-4d44-83a2-81dd582830dc": Phase="Running", Reason="", readiness=true. Elapsed: 2.756514ms
Mar  3 09:28:29.910: INFO: Pod "pod-update-activedeadlineseconds-3a5709ae-b326-4d44-83a2-81dd582830dc": Phase="Running", Reason="", readiness=true. Elapsed: 2.009698723s
Mar  3 09:28:31.918: INFO: Pod "pod-update-activedeadlineseconds-3a5709ae-b326-4d44-83a2-81dd582830dc": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.017805764s
Mar  3 09:28:31.918: INFO: Pod "pod-update-activedeadlineseconds-3a5709ae-b326-4d44-83a2-81dd582830dc" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:28:31.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3333" for this suite.

• [SLOW TEST:6.627 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":126,"skipped":2244,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:28:31.931: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-44ad19a1-94ce-4164-9ea6-dc5140853dbc
STEP: Creating a pod to test consume secrets
Mar  3 09:28:32.014: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fd882534-6ec3-499a-9b07-5b0ef76e8f54" in namespace "projected-9845" to be "Succeeded or Failed"
Mar  3 09:28:32.019: INFO: Pod "pod-projected-secrets-fd882534-6ec3-499a-9b07-5b0ef76e8f54": Phase="Pending", Reason="", readiness=false. Elapsed: 5.520688ms
Mar  3 09:28:34.026: INFO: Pod "pod-projected-secrets-fd882534-6ec3-499a-9b07-5b0ef76e8f54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012351445s
STEP: Saw pod success
Mar  3 09:28:34.026: INFO: Pod "pod-projected-secrets-fd882534-6ec3-499a-9b07-5b0ef76e8f54" satisfied condition "Succeeded or Failed"
Mar  3 09:28:34.029: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-fd882534-6ec3-499a-9b07-5b0ef76e8f54 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  3 09:28:34.049: INFO: Waiting for pod pod-projected-secrets-fd882534-6ec3-499a-9b07-5b0ef76e8f54 to disappear
Mar  3 09:28:34.053: INFO: Pod pod-projected-secrets-fd882534-6ec3-499a-9b07-5b0ef76e8f54 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:28:34.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9845" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":127,"skipped":2253,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:28:34.061: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar  3 09:28:34.139: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4050  9c23861a-edd4-4224-afb8-c439de373f34 11070 0 2021-03-03 09:28:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-03-03 09:28:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  3 09:28:34.139: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4050  9c23861a-edd4-4224-afb8-c439de373f34 11071 0 2021-03-03 09:28:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-03-03 09:28:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:28:34.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4050" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":128,"skipped":2256,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:28:34.148: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:28:34.185: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar  3 09:28:35.230: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:28:35.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3389" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":129,"skipped":2261,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:28:35.267: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 09:28:35.340: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92bb1dce-edc0-4e4e-a6df-b782dd8a210b" in namespace "downward-api-6999" to be "Succeeded or Failed"
Mar  3 09:28:35.348: INFO: Pod "downwardapi-volume-92bb1dce-edc0-4e4e-a6df-b782dd8a210b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.046237ms
Mar  3 09:28:37.358: INFO: Pod "downwardapi-volume-92bb1dce-edc0-4e4e-a6df-b782dd8a210b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018369661s
STEP: Saw pod success
Mar  3 09:28:37.358: INFO: Pod "downwardapi-volume-92bb1dce-edc0-4e4e-a6df-b782dd8a210b" satisfied condition "Succeeded or Failed"
Mar  3 09:28:37.362: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-92bb1dce-edc0-4e4e-a6df-b782dd8a210b container client-container: <nil>
STEP: delete the pod
Mar  3 09:28:37.434: INFO: Waiting for pod downwardapi-volume-92bb1dce-edc0-4e4e-a6df-b782dd8a210b to disappear
Mar  3 09:28:37.437: INFO: Pod downwardapi-volume-92bb1dce-edc0-4e4e-a6df-b782dd8a210b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:28:37.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6999" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":130,"skipped":2279,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:28:37.457: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:28:54.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2465" for this suite.

• [SLOW TEST:17.182 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":131,"skipped":2286,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:28:54.643: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  3 09:28:56.742: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:28:56.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2202" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":132,"skipped":2322,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:28:56.780: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:29:56.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5160" for this suite.

• [SLOW TEST:60.094 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":133,"skipped":2341,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:29:56.887: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:30:02.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5286" for this suite.

• [SLOW TEST:5.477 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":134,"skipped":2376,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:30:02.365: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2064
Mar  3 09:30:04.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-2064 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  3 09:30:04.625: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar  3 09:30:04.625: INFO: stdout: "iptables"
Mar  3 09:30:04.625: INFO: proxyMode: iptables
Mar  3 09:30:04.650: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  3 09:30:04.655: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-2064
STEP: creating replication controller affinity-nodeport-timeout in namespace services-2064
I0303 09:30:04.758424      24 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2064, replica count: 3
I0303 09:30:07.808882      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  3 09:30:07.823: INFO: Creating new exec pod
Mar  3 09:30:10.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-2064 exec execpod-affinityx8lvr -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Mar  3 09:30:11.052: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar  3 09:30:11.052: INFO: stdout: ""
Mar  3 09:30:11.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-2064 exec execpod-affinityx8lvr -- /bin/sh -x -c nc -zv -t -w 2 10.101.231.35 80'
Mar  3 09:30:11.211: INFO: stderr: "+ nc -zv -t -w 2 10.101.231.35 80\nConnection to 10.101.231.35 80 port [tcp/http] succeeded!\n"
Mar  3 09:30:11.211: INFO: stdout: ""
Mar  3 09:30:11.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-2064 exec execpod-affinityx8lvr -- /bin/sh -x -c nc -zv -t -w 2 10.0.49.5 31051'
Mar  3 09:30:11.372: INFO: stderr: "+ nc -zv -t -w 2 10.0.49.5 31051\nConnection to 10.0.49.5 31051 port [tcp/31051] succeeded!\n"
Mar  3 09:30:11.372: INFO: stdout: ""
Mar  3 09:30:11.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-2064 exec execpod-affinityx8lvr -- /bin/sh -x -c nc -zv -t -w 2 10.0.40.23 31051'
Mar  3 09:30:11.538: INFO: stderr: "+ nc -zv -t -w 2 10.0.40.23 31051\nConnection to 10.0.40.23 31051 port [tcp/31051] succeeded!\n"
Mar  3 09:30:11.538: INFO: stdout: ""
Mar  3 09:30:11.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-2064 exec execpod-affinityx8lvr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.49.5:31051/ ; done'
Mar  3 09:30:11.772: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n"
Mar  3 09:30:11.772: INFO: stdout: "\naffinity-nodeport-timeout-nnj5s\naffinity-nodeport-timeout-nnj5s\naffinity-nodeport-timeout-nnj5s\naffinity-nodeport-timeout-nnj5s\naffinity-nodeport-timeout-nnj5s\naffinity-nodeport-timeout-nnj5s\naffinity-nodeport-timeout-nnj5s\naffinity-nodeport-timeout-nnj5s\naffinity-nodeport-timeout-nnj5s\naffinity-nodeport-timeout-nnj5s\naffinity-nodeport-timeout-nnj5s\naffinity-nodeport-timeout-nnj5s\naffinity-nodeport-timeout-nnj5s\naffinity-nodeport-timeout-nnj5s\naffinity-nodeport-timeout-nnj5s\naffinity-nodeport-timeout-nnj5s"
Mar  3 09:30:11.772: INFO: Received response from host: affinity-nodeport-timeout-nnj5s
Mar  3 09:30:11.772: INFO: Received response from host: affinity-nodeport-timeout-nnj5s
Mar  3 09:30:11.772: INFO: Received response from host: affinity-nodeport-timeout-nnj5s
Mar  3 09:30:11.772: INFO: Received response from host: affinity-nodeport-timeout-nnj5s
Mar  3 09:30:11.772: INFO: Received response from host: affinity-nodeport-timeout-nnj5s
Mar  3 09:30:11.772: INFO: Received response from host: affinity-nodeport-timeout-nnj5s
Mar  3 09:30:11.772: INFO: Received response from host: affinity-nodeport-timeout-nnj5s
Mar  3 09:30:11.772: INFO: Received response from host: affinity-nodeport-timeout-nnj5s
Mar  3 09:30:11.772: INFO: Received response from host: affinity-nodeport-timeout-nnj5s
Mar  3 09:30:11.772: INFO: Received response from host: affinity-nodeport-timeout-nnj5s
Mar  3 09:30:11.772: INFO: Received response from host: affinity-nodeport-timeout-nnj5s
Mar  3 09:30:11.772: INFO: Received response from host: affinity-nodeport-timeout-nnj5s
Mar  3 09:30:11.773: INFO: Received response from host: affinity-nodeport-timeout-nnj5s
Mar  3 09:30:11.773: INFO: Received response from host: affinity-nodeport-timeout-nnj5s
Mar  3 09:30:11.773: INFO: Received response from host: affinity-nodeport-timeout-nnj5s
Mar  3 09:30:11.773: INFO: Received response from host: affinity-nodeport-timeout-nnj5s
Mar  3 09:30:11.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-2064 exec execpod-affinityx8lvr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.49.5:31051/'
Mar  3 09:30:11.937: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n"
Mar  3 09:30:11.937: INFO: stdout: "affinity-nodeport-timeout-nnj5s"
Mar  3 09:30:31.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-2064 exec execpod-affinityx8lvr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.49.5:31051/'
Mar  3 09:30:32.121: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n"
Mar  3 09:30:32.121: INFO: stdout: "affinity-nodeport-timeout-nnj5s"
Mar  3 09:30:52.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-2064 exec execpod-affinityx8lvr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.49.5:31051/'
Mar  3 09:30:52.322: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.49.5:31051/\n"
Mar  3 09:30:52.322: INFO: stdout: "affinity-nodeport-timeout-6ltlr"
Mar  3 09:30:52.322: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2064, will wait for the garbage collector to delete the pods
Mar  3 09:30:52.411: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 4.460663ms
Mar  3 09:30:53.111: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 700.211189ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:31:07.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2064" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:65.417 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":135,"skipped":2389,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:31:07.785: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  3 09:31:10.443: INFO: Successfully updated pod "pod-update-7c73b9bd-aa99-4682-8fce-7e4acf87caf5"
STEP: verifying the updated pod is in kubernetes
Mar  3 09:31:10.449: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:31:10.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4309" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":136,"skipped":2396,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:31:10.461: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-a1ebe56f-46da-4a70-b237-c8d326673470
STEP: Creating a pod to test consume secrets
Mar  3 09:31:10.519: INFO: Waiting up to 5m0s for pod "pod-secrets-8fc7bc04-27ff-47f9-b057-270ab2734db2" in namespace "secrets-9426" to be "Succeeded or Failed"
Mar  3 09:31:10.539: INFO: Pod "pod-secrets-8fc7bc04-27ff-47f9-b057-270ab2734db2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.247472ms
Mar  3 09:31:12.555: INFO: Pod "pod-secrets-8fc7bc04-27ff-47f9-b057-270ab2734db2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035569782s
STEP: Saw pod success
Mar  3 09:31:12.555: INFO: Pod "pod-secrets-8fc7bc04-27ff-47f9-b057-270ab2734db2" satisfied condition "Succeeded or Failed"
Mar  3 09:31:12.560: INFO: Trying to get logs from node controller-0 pod pod-secrets-8fc7bc04-27ff-47f9-b057-270ab2734db2 container secret-volume-test: <nil>
STEP: delete the pod
Mar  3 09:31:12.595: INFO: Waiting for pod pod-secrets-8fc7bc04-27ff-47f9-b057-270ab2734db2 to disappear
Mar  3 09:31:12.599: INFO: Pod pod-secrets-8fc7bc04-27ff-47f9-b057-270ab2734db2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:31:12.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9426" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":137,"skipped":2398,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:31:12.610: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar  3 09:31:12.669: INFO: Waiting up to 5m0s for pod "pod-07501763-71ec-4794-92f1-239f5308555e" in namespace "emptydir-8951" to be "Succeeded or Failed"
Mar  3 09:31:12.674: INFO: Pod "pod-07501763-71ec-4794-92f1-239f5308555e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.597981ms
Mar  3 09:31:14.682: INFO: Pod "pod-07501763-71ec-4794-92f1-239f5308555e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012599424s
STEP: Saw pod success
Mar  3 09:31:14.682: INFO: Pod "pod-07501763-71ec-4794-92f1-239f5308555e" satisfied condition "Succeeded or Failed"
Mar  3 09:31:14.685: INFO: Trying to get logs from node worker-0 pod pod-07501763-71ec-4794-92f1-239f5308555e container test-container: <nil>
STEP: delete the pod
Mar  3 09:31:14.727: INFO: Waiting for pod pod-07501763-71ec-4794-92f1-239f5308555e to disappear
Mar  3 09:31:14.731: INFO: Pod pod-07501763-71ec-4794-92f1-239f5308555e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:31:14.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8951" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":138,"skipped":2399,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:31:14.742: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Mar  3 09:31:14.779: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  3 09:32:14.802: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:32:14.805: INFO: Starting informer...
STEP: Starting pods...
Mar  3 09:32:15.038: INFO: Pod1 is running on worker-1. Tainting Node
Mar  3 09:32:17.266: INFO: Pod2 is running on worker-1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Mar  3 09:32:27.713: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar  3 09:32:47.682: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:32:47.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8750" for this suite.

• [SLOW TEST:93.018 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":139,"skipped":2419,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:32:47.762: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar  3 09:32:50.350: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7077 pod-service-account-4761ed8a-0bfc-426f-97ae-718e5d1ef7d9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar  3 09:32:50.861: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7077 pod-service-account-4761ed8a-0bfc-426f-97ae-718e5d1ef7d9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar  3 09:32:51.083: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7077 pod-service-account-4761ed8a-0bfc-426f-97ae-718e5d1ef7d9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:32:51.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7077" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":140,"skipped":2425,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:32:51.267: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:32:51.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-4355 create -f -'
Mar  3 09:32:51.660: INFO: stderr: ""
Mar  3 09:32:51.660: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar  3 09:32:51.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-4355 create -f -'
Mar  3 09:32:51.907: INFO: stderr: ""
Mar  3 09:32:51.907: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar  3 09:32:52.914: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  3 09:32:52.914: INFO: Found 1 / 1
Mar  3 09:32:52.914: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  3 09:32:52.930: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  3 09:32:52.930: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  3 09:32:52.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-4355 describe pod agnhost-primary-znrcl'
Mar  3 09:32:53.039: INFO: stderr: ""
Mar  3 09:32:53.039: INFO: stdout: "Name:         agnhost-primary-znrcl\nNamespace:    kubectl-4355\nPriority:     0\nNode:         worker-1/10.0.47.86\nStart Time:   Wed, 03 Mar 2021 09:32:51 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 10.244.226.77/32\n              cni.projectcalico.org/podIPs: 10.244.226.77/32\nStatus:       Running\nIP:           10.244.226.77\nIPs:\n  IP:           10.244.226.77\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://7de9cbc2e135f0cc50e96ea8f77c9241ed5430c2ed949cd4d2d012d9a57d0d1c\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 03 Mar 2021 09:32:52 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-4wgjk (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-4wgjk:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-4wgjk\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-4355/agnhost-primary-znrcl to worker-1\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Mar  3 09:32:53.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-4355 describe rc agnhost-primary'
Mar  3 09:32:53.157: INFO: stderr: ""
Mar  3 09:32:53.158: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4355\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-znrcl\n"
Mar  3 09:32:53.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-4355 describe service agnhost-primary'
Mar  3 09:32:53.246: INFO: stderr: ""
Mar  3 09:32:53.246: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4355\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.105.248.243\nIPs:               10.105.248.243\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.226.77:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  3 09:32:53.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-4355 describe node controller-0'
Mar  3 09:32:53.361: INFO: stderr: ""
Mar  3 09:32:53.361: INFO: stdout: "Name:               controller-0\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=controller-0\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.49.5/19\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.244.192.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 03 Mar 2021 08:46:20 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  controller-0\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 03 Mar 2021 09:32:49 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 03 Mar 2021 08:47:05 +0000   Wed, 03 Mar 2021 08:47:05 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 03 Mar 2021 09:28:57 +0000   Wed, 03 Mar 2021 08:46:20 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 03 Mar 2021 09:28:57 +0000   Wed, 03 Mar 2021 08:46:20 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 03 Mar 2021 09:28:57 +0000   Wed, 03 Mar 2021 08:46:20 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 03 Mar 2021 09:28:57 +0000   Wed, 03 Mar 2021 08:47:00 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.0.49.5\n  Hostname:    controller-0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  10098432Ki\n  hugepages-2Mi:      0\n  memory:             8150800Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  9306714916\n  hugepages-2Mi:      0\n  memory:             8048400Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 0239ca0d4f104ea4b753168a3b52aaf1\n  System UUID:                ec21a43a-6446-f50a-5c15-dd4421225ebe\n  Boot ID:                    b2bb328f-cbc2-437b-bd8e-caa5cc74f070\n  Kernel Version:             5.4.0-1038-aws\n  OS Image:                   Ubuntu 20.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.4.3\n  Kubelet Version:            v1.20.4-k0s1\n  Kube-Proxy Version:         v1.20.4-k0s1\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-5f6546844f-gnl9f                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         46m\n  kube-system                 calico-node-468vb                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         46m\n  kube-system                 coredns-5c98d7d4d8-96hpd                                   100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     46m\n  kube-system                 konnectivity-agent-882qs                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         45m\n  kube-system                 kube-proxy-snqpb                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         46m\n  kube-system                 metrics-server-6fbcd86f7b-r2c26                            10m (0%)      0 (0%)      30M (0%)         0 (0%)         46m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-8fglb    0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                360m (18%)      0 (0%)\n  memory             103400320 (1%)  170Mi (2%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:\n  Type     Reason                   Age                From        Message\n  ----     ------                   ----               ----        -------\n  Normal   Starting                 46m                kubelet     Starting kubelet.\n  Warning  InvalidDiskCapacity      46m                kubelet     invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  46m (x2 over 46m)  kubelet     Node controller-0 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    46m (x2 over 46m)  kubelet     Node controller-0 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     46m (x2 over 46m)  kubelet     Node controller-0 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  46m                kubelet     Updated Node Allocatable limit across pods\n  Normal   Starting                 46m                kube-proxy  Starting kube-proxy.\n  Normal   NodeReady                45m                kubelet     Node controller-0 status is now: NodeReady\n"
Mar  3 09:32:53.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-4355 describe namespace kubectl-4355'
Mar  3 09:32:53.447: INFO: stderr: ""
Mar  3 09:32:53.447: INFO: stdout: "Name:         kubectl-4355\nLabels:       e2e-framework=kubectl\n              e2e-run=670f6e94-0271-41aa-b53d-e1904003cb9e\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:32:53.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4355" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":141,"skipped":2438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:32:53.461: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Mar  3 09:32:56.045: INFO: Successfully updated pod "labelsupdate9ab0a2aa-d1ef-4c98-9054-51c4eddf9ada"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:33:00.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9789" for this suite.

• [SLOW TEST:6.627 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":142,"skipped":2523,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:33:00.088: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar  3 09:33:00.181: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:33:10.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5728" for this suite.

• [SLOW TEST:10.898 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":143,"skipped":2542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:33:10.989: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 1 pods
STEP: Gathering metrics
W0303 09:33:11.709742      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0303 09:33:11.710105      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0303 09:33:11.710846      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  3 09:33:11.711: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:33:11.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2025" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":144,"skipped":2567,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:33:11.719: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-b490bf24-389a-474d-8dd0-ed6ba1b129b4
STEP: Creating a pod to test consume configMaps
Mar  3 09:33:11.816: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d76c8291-058e-48b5-ac69-d50a619c1396" in namespace "projected-1555" to be "Succeeded or Failed"
Mar  3 09:33:11.831: INFO: Pod "pod-projected-configmaps-d76c8291-058e-48b5-ac69-d50a619c1396": Phase="Pending", Reason="", readiness=false. Elapsed: 14.650197ms
Mar  3 09:33:13.837: INFO: Pod "pod-projected-configmaps-d76c8291-058e-48b5-ac69-d50a619c1396": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020887957s
STEP: Saw pod success
Mar  3 09:33:13.837: INFO: Pod "pod-projected-configmaps-d76c8291-058e-48b5-ac69-d50a619c1396" satisfied condition "Succeeded or Failed"
Mar  3 09:33:13.859: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-d76c8291-058e-48b5-ac69-d50a619c1396 container agnhost-container: <nil>
STEP: delete the pod
Mar  3 09:33:13.899: INFO: Waiting for pod pod-projected-configmaps-d76c8291-058e-48b5-ac69-d50a619c1396 to disappear
Mar  3 09:33:13.901: INFO: Pod pod-projected-configmaps-d76c8291-058e-48b5-ac69-d50a619c1396 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:33:13.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1555" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":145,"skipped":2570,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:33:13.913: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:33:13.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6437" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":146,"skipped":2615,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:33:13.967: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 09:33:14.006: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c956e06-da5d-4849-ae23-74997808ad10" in namespace "projected-7989" to be "Succeeded or Failed"
Mar  3 09:33:14.013: INFO: Pod "downwardapi-volume-2c956e06-da5d-4849-ae23-74997808ad10": Phase="Pending", Reason="", readiness=false. Elapsed: 6.38657ms
Mar  3 09:33:16.024: INFO: Pod "downwardapi-volume-2c956e06-da5d-4849-ae23-74997808ad10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017966776s
STEP: Saw pod success
Mar  3 09:33:16.024: INFO: Pod "downwardapi-volume-2c956e06-da5d-4849-ae23-74997808ad10" satisfied condition "Succeeded or Failed"
Mar  3 09:33:16.028: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-2c956e06-da5d-4849-ae23-74997808ad10 container client-container: <nil>
STEP: delete the pod
Mar  3 09:33:16.050: INFO: Waiting for pod downwardapi-volume-2c956e06-da5d-4849-ae23-74997808ad10 to disappear
Mar  3 09:33:16.053: INFO: Pod downwardapi-volume-2c956e06-da5d-4849-ae23-74997808ad10 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:33:16.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7989" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":147,"skipped":2624,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:33:16.071: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1890.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1890.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1890.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1890.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1890.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1890.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  3 09:33:18.301: INFO: DNS probes using dns-1890/dns-test-61306a79-6ebb-4742-846d-f04962a4b425 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:33:18.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1890" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":148,"skipped":2667,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:33:18.335: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Mar  3 09:33:20.394: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-451 PodName:pod-sharedvolume-7cafe783-833c-4ee9-8e54-2102c50b105d ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:33:20.394: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:33:20.513: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:33:20.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-451" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":149,"skipped":2674,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:33:20.528: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-4381f5ee-b114-4901-bbab-94d0b9a39a83 in namespace container-probe-5146
Mar  3 09:33:22.594: INFO: Started pod liveness-4381f5ee-b114-4901-bbab-94d0b9a39a83 in namespace container-probe-5146
STEP: checking the pod's current state and verifying that restartCount is present
Mar  3 09:33:22.596: INFO: Initial restart count of pod liveness-4381f5ee-b114-4901-bbab-94d0b9a39a83 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:37:23.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5146" for this suite.

• [SLOW TEST:243.129 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":150,"skipped":2727,"failed":0}
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:37:23.657: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:37:23.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2104" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":151,"skipped":2730,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:37:23.743: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:37:39.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7483" for this suite.

• [SLOW TEST:16.216 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":152,"skipped":2730,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:37:39.959: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-6527/secret-test-e4366a08-b9bb-4571-bc39-cc4993f1b48c
STEP: Creating a pod to test consume secrets
Mar  3 09:37:40.034: INFO: Waiting up to 5m0s for pod "pod-configmaps-f949409b-3ee9-46f3-8463-d2b697dd1629" in namespace "secrets-6527" to be "Succeeded or Failed"
Mar  3 09:37:40.041: INFO: Pod "pod-configmaps-f949409b-3ee9-46f3-8463-d2b697dd1629": Phase="Pending", Reason="", readiness=false. Elapsed: 6.929128ms
Mar  3 09:37:42.044: INFO: Pod "pod-configmaps-f949409b-3ee9-46f3-8463-d2b697dd1629": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010007087s
STEP: Saw pod success
Mar  3 09:37:42.044: INFO: Pod "pod-configmaps-f949409b-3ee9-46f3-8463-d2b697dd1629" satisfied condition "Succeeded or Failed"
Mar  3 09:37:42.047: INFO: Trying to get logs from node worker-1 pod pod-configmaps-f949409b-3ee9-46f3-8463-d2b697dd1629 container env-test: <nil>
STEP: delete the pod
Mar  3 09:37:42.095: INFO: Waiting for pod pod-configmaps-f949409b-3ee9-46f3-8463-d2b697dd1629 to disappear
Mar  3 09:37:42.103: INFO: Pod pod-configmaps-f949409b-3ee9-46f3-8463-d2b697dd1629 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:37:42.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6527" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":153,"skipped":2734,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:37:42.128: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0303 09:37:43.751774      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0303 09:37:43.751802      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0303 09:37:43.751809      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  3 09:37:43.751: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:37:43.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7007" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":154,"skipped":2762,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:37:43.765: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Mar  3 09:37:43.800: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  3 09:38:43.823: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:38:43.826: INFO: Starting informer...
STEP: Starting pod...
Mar  3 09:38:44.049: INFO: Pod is running on worker-1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Mar  3 09:38:44.103: INFO: Pod wasn't evicted. Proceeding
Mar  3 09:38:44.103: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Mar  3 09:39:59.136: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:39:59.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-8660" for this suite.

• [SLOW TEST:135.407 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":155,"skipped":2795,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:39:59.173: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  3 09:39:59.922: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 09:40:02.946: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:40:03.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1702" for this suite.
STEP: Destroying namespace "webhook-1702-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":156,"skipped":2805,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:40:03.103: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Mar  3 09:40:03.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6881 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar  3 09:40:03.257: INFO: stderr: ""
Mar  3 09:40:03.257: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Mar  3 09:40:03.257: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar  3 09:40:03.257: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6881" to be "running and ready, or succeeded"
Mar  3 09:40:03.267: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004577ms
Mar  3 09:40:05.275: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.017433868s
Mar  3 09:40:05.275: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar  3 09:40:05.275: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar  3 09:40:05.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6881 logs logs-generator logs-generator'
Mar  3 09:40:05.382: INFO: stderr: ""
Mar  3 09:40:05.382: INFO: stdout: "I0303 09:40:04.088360       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/wjd 297\nI0303 09:40:04.288528       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/mxp 480\nI0303 09:40:04.488526       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/4dt 587\nI0303 09:40:04.688509       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/6tq 408\nI0303 09:40:04.888494       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/ttvn 381\nI0303 09:40:05.088547       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/tjwx 549\nI0303 09:40:05.288996       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/pjh 247\n"
Mar  3 09:40:07.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6881 logs logs-generator logs-generator'
Mar  3 09:40:07.484: INFO: stderr: ""
Mar  3 09:40:07.484: INFO: stdout: "I0303 09:40:04.088360       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/wjd 297\nI0303 09:40:04.288528       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/mxp 480\nI0303 09:40:04.488526       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/4dt 587\nI0303 09:40:04.688509       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/6tq 408\nI0303 09:40:04.888494       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/ttvn 381\nI0303 09:40:05.088547       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/tjwx 549\nI0303 09:40:05.288996       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/pjh 247\nI0303 09:40:05.488564       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/xsq 257\nI0303 09:40:05.688550       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/7697 384\nI0303 09:40:05.888501       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/lqkq 310\nI0303 09:40:06.088516       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/xhr 433\nI0303 09:40:06.288513       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/xn9 527\nI0303 09:40:06.488557       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/ch8 392\nI0303 09:40:06.688543       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/nzv 486\nI0303 09:40:06.888533       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/w2jv 244\nI0303 09:40:07.089085       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/fn6 267\nI0303 09:40:07.288500       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/6w9 235\n"
STEP: limiting log lines
Mar  3 09:40:07.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6881 logs logs-generator logs-generator --tail=1'
Mar  3 09:40:07.574: INFO: stderr: ""
Mar  3 09:40:07.574: INFO: stdout: "I0303 09:40:07.488589       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/bj8 252\n"
Mar  3 09:40:07.574: INFO: got output "I0303 09:40:07.488589       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/bj8 252\n"
STEP: limiting log bytes
Mar  3 09:40:07.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6881 logs logs-generator logs-generator --limit-bytes=1'
Mar  3 09:40:07.668: INFO: stderr: ""
Mar  3 09:40:07.668: INFO: stdout: "I"
Mar  3 09:40:07.668: INFO: got output "I"
STEP: exposing timestamps
Mar  3 09:40:07.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6881 logs logs-generator logs-generator --tail=1 --timestamps'
Mar  3 09:40:07.769: INFO: stderr: ""
Mar  3 09:40:07.769: INFO: stdout: "2021-03-03T09:40:07.688640926Z I0303 09:40:07.688483       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/h6x2 370\n"
Mar  3 09:40:07.769: INFO: got output "2021-03-03T09:40:07.688640926Z I0303 09:40:07.688483       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/h6x2 370\n"
STEP: restricting to a time range
Mar  3 09:40:10.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6881 logs logs-generator logs-generator --since=1s'
Mar  3 09:40:10.364: INFO: stderr: ""
Mar  3 09:40:10.364: INFO: stdout: "I0303 09:40:09.488527       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/4kw 257\nI0303 09:40:09.688535       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/kube-system/pods/hj6 350\nI0303 09:40:09.888513       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/xrz 227\nI0303 09:40:10.088502       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/default/pods/nvbp 288\nI0303 09:40:10.288485       1 logs_generator.go:76] 31 GET /api/v1/namespaces/default/pods/ldrd 571\n"
Mar  3 09:40:10.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6881 logs logs-generator logs-generator --since=24h'
Mar  3 09:40:10.462: INFO: stderr: ""
Mar  3 09:40:10.462: INFO: stdout: "I0303 09:40:04.088360       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/wjd 297\nI0303 09:40:04.288528       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/mxp 480\nI0303 09:40:04.488526       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/4dt 587\nI0303 09:40:04.688509       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/6tq 408\nI0303 09:40:04.888494       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/ttvn 381\nI0303 09:40:05.088547       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/tjwx 549\nI0303 09:40:05.288996       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/pjh 247\nI0303 09:40:05.488564       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/xsq 257\nI0303 09:40:05.688550       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/7697 384\nI0303 09:40:05.888501       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/lqkq 310\nI0303 09:40:06.088516       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/xhr 433\nI0303 09:40:06.288513       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/xn9 527\nI0303 09:40:06.488557       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/ch8 392\nI0303 09:40:06.688543       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/nzv 486\nI0303 09:40:06.888533       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/w2jv 244\nI0303 09:40:07.089085       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/fn6 267\nI0303 09:40:07.288500       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/6w9 235\nI0303 09:40:07.488589       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/bj8 252\nI0303 09:40:07.688483       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/h6x2 370\nI0303 09:40:07.889018       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/ptpr 201\nI0303 09:40:08.088517       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/gcm 517\nI0303 09:40:08.288517       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/zqm8 308\nI0303 09:40:08.488551       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/pxb6 448\nI0303 09:40:08.688526       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/hb85 515\nI0303 09:40:08.888517       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/vxl 420\nI0303 09:40:09.088532       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/rg7k 533\nI0303 09:40:09.288482       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/rflm 362\nI0303 09:40:09.488527       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/4kw 257\nI0303 09:40:09.688535       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/kube-system/pods/hj6 350\nI0303 09:40:09.888513       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/xrz 227\nI0303 09:40:10.088502       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/default/pods/nvbp 288\nI0303 09:40:10.288485       1 logs_generator.go:76] 31 GET /api/v1/namespaces/default/pods/ldrd 571\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Mar  3 09:40:10.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6881 delete pod logs-generator'
Mar  3 09:40:22.394: INFO: stderr: ""
Mar  3 09:40:22.394: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:40:22.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6881" for this suite.

• [SLOW TEST:19.302 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":157,"skipped":2811,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:40:22.405: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:40:24.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5808" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":158,"skipped":2824,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:40:24.501: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7058
Mar  3 09:40:26.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-7058 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  3 09:40:26.746: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar  3 09:40:26.746: INFO: stdout: "iptables"
Mar  3 09:40:26.746: INFO: proxyMode: iptables
Mar  3 09:40:26.759: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  3 09:40:26.763: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-7058
STEP: creating replication controller affinity-clusterip-timeout in namespace services-7058
I0303 09:40:26.804497      24 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-7058, replica count: 3
I0303 09:40:29.855269      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  3 09:40:29.866: INFO: Creating new exec pod
Mar  3 09:40:32.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-7058 exec execpod-affinity2xj7l -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Mar  3 09:40:33.068: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar  3 09:40:33.068: INFO: stdout: ""
Mar  3 09:40:33.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-7058 exec execpod-affinity2xj7l -- /bin/sh -x -c nc -zv -t -w 2 10.108.34.60 80'
Mar  3 09:40:33.250: INFO: stderr: "+ nc -zv -t -w 2 10.108.34.60 80\nConnection to 10.108.34.60 80 port [tcp/http] succeeded!\n"
Mar  3 09:40:33.250: INFO: stdout: ""
Mar  3 09:40:33.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-7058 exec execpod-affinity2xj7l -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.34.60:80/ ; done'
Mar  3 09:40:33.485: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n"
Mar  3 09:40:33.485: INFO: stdout: "\naffinity-clusterip-timeout-g6mxd\naffinity-clusterip-timeout-g6mxd\naffinity-clusterip-timeout-g6mxd\naffinity-clusterip-timeout-g6mxd\naffinity-clusterip-timeout-g6mxd\naffinity-clusterip-timeout-g6mxd\naffinity-clusterip-timeout-g6mxd\naffinity-clusterip-timeout-g6mxd\naffinity-clusterip-timeout-g6mxd\naffinity-clusterip-timeout-g6mxd\naffinity-clusterip-timeout-g6mxd\naffinity-clusterip-timeout-g6mxd\naffinity-clusterip-timeout-g6mxd\naffinity-clusterip-timeout-g6mxd\naffinity-clusterip-timeout-g6mxd\naffinity-clusterip-timeout-g6mxd"
Mar  3 09:40:33.485: INFO: Received response from host: affinity-clusterip-timeout-g6mxd
Mar  3 09:40:33.485: INFO: Received response from host: affinity-clusterip-timeout-g6mxd
Mar  3 09:40:33.485: INFO: Received response from host: affinity-clusterip-timeout-g6mxd
Mar  3 09:40:33.485: INFO: Received response from host: affinity-clusterip-timeout-g6mxd
Mar  3 09:40:33.485: INFO: Received response from host: affinity-clusterip-timeout-g6mxd
Mar  3 09:40:33.485: INFO: Received response from host: affinity-clusterip-timeout-g6mxd
Mar  3 09:40:33.485: INFO: Received response from host: affinity-clusterip-timeout-g6mxd
Mar  3 09:40:33.485: INFO: Received response from host: affinity-clusterip-timeout-g6mxd
Mar  3 09:40:33.485: INFO: Received response from host: affinity-clusterip-timeout-g6mxd
Mar  3 09:40:33.485: INFO: Received response from host: affinity-clusterip-timeout-g6mxd
Mar  3 09:40:33.485: INFO: Received response from host: affinity-clusterip-timeout-g6mxd
Mar  3 09:40:33.485: INFO: Received response from host: affinity-clusterip-timeout-g6mxd
Mar  3 09:40:33.485: INFO: Received response from host: affinity-clusterip-timeout-g6mxd
Mar  3 09:40:33.485: INFO: Received response from host: affinity-clusterip-timeout-g6mxd
Mar  3 09:40:33.485: INFO: Received response from host: affinity-clusterip-timeout-g6mxd
Mar  3 09:40:33.485: INFO: Received response from host: affinity-clusterip-timeout-g6mxd
Mar  3 09:40:33.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-7058 exec execpod-affinity2xj7l -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.108.34.60:80/'
Mar  3 09:40:33.655: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n"
Mar  3 09:40:33.655: INFO: stdout: "affinity-clusterip-timeout-g6mxd"
Mar  3 09:40:53.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-7058 exec execpod-affinity2xj7l -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.108.34.60:80/'
Mar  3 09:40:53.833: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.108.34.60:80/\n"
Mar  3 09:40:53.833: INFO: stdout: "affinity-clusterip-timeout-z9nfz"
Mar  3 09:40:53.833: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-7058, will wait for the garbage collector to delete the pods
Mar  3 09:40:53.960: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 34.175458ms
Mar  3 09:40:56.760: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 2.800284056s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:41:11.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7058" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:46.611 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":159,"skipped":2845,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:41:11.112: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:41:18.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2561" for this suite.

• [SLOW TEST:7.124 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":160,"skipped":2858,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:41:18.238: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  3 09:41:18.892: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750361278, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750361278, loc:(*time.Location)(0x797de40)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-6bd9446d55\""}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 09:41:21.966: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
Mar  3 09:41:21.983: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:41:22.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4440" for this suite.
STEP: Destroying namespace "webhook-4440-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":161,"skipped":2906,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:41:22.284: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-b4917c87-06d8-42b8-9786-fdba3209d0e6
STEP: Creating a pod to test consume configMaps
Mar  3 09:41:22.394: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bc6a47f-7f6a-4c68-a094-8cdf327fd931" in namespace "configmap-3237" to be "Succeeded or Failed"
Mar  3 09:41:22.407: INFO: Pod "pod-configmaps-0bc6a47f-7f6a-4c68-a094-8cdf327fd931": Phase="Pending", Reason="", readiness=false. Elapsed: 12.939971ms
Mar  3 09:41:24.416: INFO: Pod "pod-configmaps-0bc6a47f-7f6a-4c68-a094-8cdf327fd931": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0213792s
STEP: Saw pod success
Mar  3 09:41:24.416: INFO: Pod "pod-configmaps-0bc6a47f-7f6a-4c68-a094-8cdf327fd931" satisfied condition "Succeeded or Failed"
Mar  3 09:41:24.419: INFO: Trying to get logs from node worker-1 pod pod-configmaps-0bc6a47f-7f6a-4c68-a094-8cdf327fd931 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  3 09:41:24.437: INFO: Waiting for pod pod-configmaps-0bc6a47f-7f6a-4c68-a094-8cdf327fd931 to disappear
Mar  3 09:41:24.439: INFO: Pod pod-configmaps-0bc6a47f-7f6a-4c68-a094-8cdf327fd931 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:41:24.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3237" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":162,"skipped":2976,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:41:24.450: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  3 09:41:24.498: INFO: Waiting up to 5m0s for pod "pod-66b520d8-a5f3-4d2f-834a-3d415c31d58d" in namespace "emptydir-3721" to be "Succeeded or Failed"
Mar  3 09:41:24.508: INFO: Pod "pod-66b520d8-a5f3-4d2f-834a-3d415c31d58d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.646285ms
Mar  3 09:41:26.516: INFO: Pod "pod-66b520d8-a5f3-4d2f-834a-3d415c31d58d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018014578s
STEP: Saw pod success
Mar  3 09:41:26.516: INFO: Pod "pod-66b520d8-a5f3-4d2f-834a-3d415c31d58d" satisfied condition "Succeeded or Failed"
Mar  3 09:41:26.518: INFO: Trying to get logs from node worker-1 pod pod-66b520d8-a5f3-4d2f-834a-3d415c31d58d container test-container: <nil>
STEP: delete the pod
Mar  3 09:41:26.537: INFO: Waiting for pod pod-66b520d8-a5f3-4d2f-834a-3d415c31d58d to disappear
Mar  3 09:41:26.540: INFO: Pod pod-66b520d8-a5f3-4d2f-834a-3d415c31d58d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:41:26.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3721" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":163,"skipped":2976,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:41:26.563: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 09:41:26.620: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a0f3ebc5-0da3-4e68-b177-9ac77e0e9a73" in namespace "downward-api-8609" to be "Succeeded or Failed"
Mar  3 09:41:26.630: INFO: Pod "downwardapi-volume-a0f3ebc5-0da3-4e68-b177-9ac77e0e9a73": Phase="Pending", Reason="", readiness=false. Elapsed: 10.060815ms
Mar  3 09:41:28.635: INFO: Pod "downwardapi-volume-a0f3ebc5-0da3-4e68-b177-9ac77e0e9a73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015441547s
Mar  3 09:41:30.643: INFO: Pod "downwardapi-volume-a0f3ebc5-0da3-4e68-b177-9ac77e0e9a73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023188581s
STEP: Saw pod success
Mar  3 09:41:30.643: INFO: Pod "downwardapi-volume-a0f3ebc5-0da3-4e68-b177-9ac77e0e9a73" satisfied condition "Succeeded or Failed"
Mar  3 09:41:30.647: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-a0f3ebc5-0da3-4e68-b177-9ac77e0e9a73 container client-container: <nil>
STEP: delete the pod
Mar  3 09:41:30.667: INFO: Waiting for pod downwardapi-volume-a0f3ebc5-0da3-4e68-b177-9ac77e0e9a73 to disappear
Mar  3 09:41:30.670: INFO: Pod downwardapi-volume-a0f3ebc5-0da3-4e68-b177-9ac77e0e9a73 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:41:30.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8609" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":164,"skipped":3017,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:41:30.679: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  3 09:41:30.733: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  3 09:42:30.756: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Mar  3 09:42:30.798: INFO: Created pod: pod0-sched-preemption-low-priority
Mar  3 09:42:30.838: INFO: Created pod: pod1-sched-preemption-medium-priority
Mar  3 09:42:30.972: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:43:03.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5256" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:92.482 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":165,"skipped":3054,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:43:03.163: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-812
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-812
I0303 09:43:03.499825      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-812, replica count: 2
I0303 09:43:06.550259      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  3 09:43:09.550: INFO: Creating new exec pod
I0303 09:43:09.550406      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  3 09:43:12.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-812 exec execpod4r9bq -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar  3 09:43:12.971: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  3 09:43:12.971: INFO: stdout: ""
Mar  3 09:43:12.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-812 exec execpod4r9bq -- /bin/sh -x -c nc -zv -t -w 2 10.97.165.244 80'
Mar  3 09:43:13.125: INFO: stderr: "+ nc -zv -t -w 2 10.97.165.244 80\nConnection to 10.97.165.244 80 port [tcp/http] succeeded!\n"
Mar  3 09:43:13.125: INFO: stdout: ""
Mar  3 09:43:13.125: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:43:13.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-812" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:10.054 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":166,"skipped":3064,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:43:13.217: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Mar  3 09:43:13.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-4392 api-versions'
Mar  3 09:43:13.373: INFO: stderr: ""
Mar  3 09:43:13.373: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nhelm.k0sproject.io/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:43:13.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4392" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":167,"skipped":3076,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:43:13.426: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-ed5374e1-2624-4c25-a0fd-7f87adc4536c in namespace container-probe-505
Mar  3 09:43:17.486: INFO: Started pod busybox-ed5374e1-2624-4c25-a0fd-7f87adc4536c in namespace container-probe-505
STEP: checking the pod's current state and verifying that restartCount is present
Mar  3 09:43:17.491: INFO: Initial restart count of pod busybox-ed5374e1-2624-4c25-a0fd-7f87adc4536c is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:47:19.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-505" for this suite.

• [SLOW TEST:245.603 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":168,"skipped":3077,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:47:19.032: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar  3 09:47:19.755: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 09:47:22.794: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:47:22.800: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:47:23.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-614" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:5.317 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":169,"skipped":3079,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:47:24.353: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar  3 09:47:29.078: INFO: Successfully updated pod "adopt-release-4x4gh"
STEP: Checking that the Job readopts the Pod
Mar  3 09:47:29.078: INFO: Waiting up to 15m0s for pod "adopt-release-4x4gh" in namespace "job-6857" to be "adopted"
Mar  3 09:47:29.103: INFO: Pod "adopt-release-4x4gh": Phase="Running", Reason="", readiness=true. Elapsed: 25.33876ms
Mar  3 09:47:31.111: INFO: Pod "adopt-release-4x4gh": Phase="Running", Reason="", readiness=true. Elapsed: 2.032806591s
Mar  3 09:47:31.111: INFO: Pod "adopt-release-4x4gh" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar  3 09:47:31.645: INFO: Successfully updated pod "adopt-release-4x4gh"
STEP: Checking that the Job releases the Pod
Mar  3 09:47:31.645: INFO: Waiting up to 15m0s for pod "adopt-release-4x4gh" in namespace "job-6857" to be "released"
Mar  3 09:47:31.701: INFO: Pod "adopt-release-4x4gh": Phase="Running", Reason="", readiness=true. Elapsed: 55.13435ms
Mar  3 09:47:31.701: INFO: Pod "adopt-release-4x4gh" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:47:31.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6857" for this suite.

• [SLOW TEST:7.439 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":170,"skipped":3109,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:47:31.792: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:47:31.900: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  3 09:47:35.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-6602 --namespace=crd-publish-openapi-6602 create -f -'
Mar  3 09:47:36.064: INFO: stderr: ""
Mar  3 09:47:36.065: INFO: stdout: "e2e-test-crd-publish-openapi-1468-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  3 09:47:36.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-6602 --namespace=crd-publish-openapi-6602 delete e2e-test-crd-publish-openapi-1468-crds test-cr'
Mar  3 09:47:36.149: INFO: stderr: ""
Mar  3 09:47:36.149: INFO: stdout: "e2e-test-crd-publish-openapi-1468-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar  3 09:47:36.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-6602 --namespace=crd-publish-openapi-6602 apply -f -'
Mar  3 09:47:36.376: INFO: stderr: ""
Mar  3 09:47:36.376: INFO: stdout: "e2e-test-crd-publish-openapi-1468-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  3 09:47:36.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-6602 --namespace=crd-publish-openapi-6602 delete e2e-test-crd-publish-openapi-1468-crds test-cr'
Mar  3 09:47:36.464: INFO: stderr: ""
Mar  3 09:47:36.464: INFO: stdout: "e2e-test-crd-publish-openapi-1468-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar  3 09:47:36.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-6602 explain e2e-test-crd-publish-openapi-1468-crds'
Mar  3 09:47:36.700: INFO: stderr: ""
Mar  3 09:47:36.700: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1468-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:47:40.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6602" for this suite.

• [SLOW TEST:8.566 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":171,"skipped":3118,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:47:40.361: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:47:40.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8436" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":172,"skipped":3136,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:47:40.463: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:47:56.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2416" for this suite.

• [SLOW TEST:16.181 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":173,"skipped":3169,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:47:56.644: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-v27b
STEP: Creating a pod to test atomic-volume-subpath
Mar  3 09:47:56.705: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-v27b" in namespace "subpath-3917" to be "Succeeded or Failed"
Mar  3 09:47:56.712: INFO: Pod "pod-subpath-test-downwardapi-v27b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.449058ms
Mar  3 09:47:58.719: INFO: Pod "pod-subpath-test-downwardapi-v27b": Phase="Running", Reason="", readiness=true. Elapsed: 2.014834157s
Mar  3 09:48:00.730: INFO: Pod "pod-subpath-test-downwardapi-v27b": Phase="Running", Reason="", readiness=true. Elapsed: 4.025214838s
Mar  3 09:48:02.737: INFO: Pod "pod-subpath-test-downwardapi-v27b": Phase="Running", Reason="", readiness=true. Elapsed: 6.032201805s
Mar  3 09:48:04.745: INFO: Pod "pod-subpath-test-downwardapi-v27b": Phase="Running", Reason="", readiness=true. Elapsed: 8.040644694s
Mar  3 09:48:06.753: INFO: Pod "pod-subpath-test-downwardapi-v27b": Phase="Running", Reason="", readiness=true. Elapsed: 10.047939205s
Mar  3 09:48:08.760: INFO: Pod "pod-subpath-test-downwardapi-v27b": Phase="Running", Reason="", readiness=true. Elapsed: 12.055075821s
Mar  3 09:48:10.769: INFO: Pod "pod-subpath-test-downwardapi-v27b": Phase="Running", Reason="", readiness=true. Elapsed: 14.06403576s
Mar  3 09:48:12.776: INFO: Pod "pod-subpath-test-downwardapi-v27b": Phase="Running", Reason="", readiness=true. Elapsed: 16.071396085s
Mar  3 09:48:14.781: INFO: Pod "pod-subpath-test-downwardapi-v27b": Phase="Running", Reason="", readiness=true. Elapsed: 18.076192981s
Mar  3 09:48:16.787: INFO: Pod "pod-subpath-test-downwardapi-v27b": Phase="Running", Reason="", readiness=true. Elapsed: 20.08231319s
Mar  3 09:48:18.817: INFO: Pod "pod-subpath-test-downwardapi-v27b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.112717587s
STEP: Saw pod success
Mar  3 09:48:18.817: INFO: Pod "pod-subpath-test-downwardapi-v27b" satisfied condition "Succeeded or Failed"
Mar  3 09:48:18.823: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-downwardapi-v27b container test-container-subpath-downwardapi-v27b: <nil>
STEP: delete the pod
Mar  3 09:48:18.891: INFO: Waiting for pod pod-subpath-test-downwardapi-v27b to disappear
Mar  3 09:48:18.895: INFO: Pod pod-subpath-test-downwardapi-v27b no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-v27b
Mar  3 09:48:18.895: INFO: Deleting pod "pod-subpath-test-downwardapi-v27b" in namespace "subpath-3917"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:48:18.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3917" for this suite.

• [SLOW TEST:22.298 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":174,"skipped":3184,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:48:18.943: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-428a409b-a4aa-43b5-95be-071e70c87ba0
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:48:19.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6291" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":175,"skipped":3187,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:48:19.030: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Mar  3 09:48:21.126: INFO: Pod pod-hostip-25be25d3-e379-4dc3-b59f-c450093426b4 has hostIP: 10.0.47.86
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:48:21.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-390" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":176,"skipped":3191,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:48:21.137: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3668
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3668
STEP: creating replication controller externalsvc in namespace services-3668
I0303 09:48:21.260626      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3668, replica count: 2
I0303 09:48:24.311214      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar  3 09:48:24.375: INFO: Creating new exec pod
Mar  3 09:48:26.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-3668 exec execpodlhzzj -- /bin/sh -x -c nslookup clusterip-service.services-3668.svc.cluster.local'
Mar  3 09:48:26.583: INFO: stderr: "+ nslookup clusterip-service.services-3668.svc.cluster.local\n"
Mar  3 09:48:26.583: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3668.svc.cluster.local\tcanonical name = externalsvc.services-3668.svc.cluster.local.\nName:\texternalsvc.services-3668.svc.cluster.local\nAddress: 10.107.22.64\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3668, will wait for the garbage collector to delete the pods
Mar  3 09:48:26.646: INFO: Deleting ReplicationController externalsvc took: 6.083203ms
Mar  3 09:48:27.346: INFO: Terminating ReplicationController externalsvc pods took: 700.230614ms
Mar  3 09:48:37.808: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:48:37.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3668" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:16.750 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":177,"skipped":3204,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:48:37.890: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-ba5440d6-6512-4c7d-9757-f2d4c79f33a3
STEP: Creating a pod to test consume configMaps
Mar  3 09:48:37.963: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6813136c-a2fa-4569-b743-ba984dca5dd2" in namespace "projected-8278" to be "Succeeded or Failed"
Mar  3 09:48:37.973: INFO: Pod "pod-projected-configmaps-6813136c-a2fa-4569-b743-ba984dca5dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.24121ms
Mar  3 09:48:39.980: INFO: Pod "pod-projected-configmaps-6813136c-a2fa-4569-b743-ba984dca5dd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016359222s
Mar  3 09:48:41.987: INFO: Pod "pod-projected-configmaps-6813136c-a2fa-4569-b743-ba984dca5dd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023395595s
STEP: Saw pod success
Mar  3 09:48:41.987: INFO: Pod "pod-projected-configmaps-6813136c-a2fa-4569-b743-ba984dca5dd2" satisfied condition "Succeeded or Failed"
Mar  3 09:48:41.990: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-6813136c-a2fa-4569-b743-ba984dca5dd2 container agnhost-container: <nil>
STEP: delete the pod
Mar  3 09:48:42.005: INFO: Waiting for pod pod-projected-configmaps-6813136c-a2fa-4569-b743-ba984dca5dd2 to disappear
Mar  3 09:48:42.011: INFO: Pod pod-projected-configmaps-6813136c-a2fa-4569-b743-ba984dca5dd2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:48:42.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8278" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":178,"skipped":3215,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:48:42.022: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  3 09:48:44.106: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:48:44.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8493" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":179,"skipped":3225,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:48:44.131: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  3 09:48:44.768: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 09:48:47.805: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:48:47.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8094" for this suite.
STEP: Destroying namespace "webhook-8094-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":180,"skipped":3268,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:48:47.976: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-412e4145-9ba1-4e8c-90cd-446fb37d9f63
STEP: Creating a pod to test consume configMaps
Mar  3 09:48:48.108: INFO: Waiting up to 5m0s for pod "pod-configmaps-50cd79b5-edb2-4518-b742-c62971d02de5" in namespace "configmap-837" to be "Succeeded or Failed"
Mar  3 09:48:48.111: INFO: Pod "pod-configmaps-50cd79b5-edb2-4518-b742-c62971d02de5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.279653ms
Mar  3 09:48:50.118: INFO: Pod "pod-configmaps-50cd79b5-edb2-4518-b742-c62971d02de5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009591015s
STEP: Saw pod success
Mar  3 09:48:50.118: INFO: Pod "pod-configmaps-50cd79b5-edb2-4518-b742-c62971d02de5" satisfied condition "Succeeded or Failed"
Mar  3 09:48:50.120: INFO: Trying to get logs from node worker-1 pod pod-configmaps-50cd79b5-edb2-4518-b742-c62971d02de5 container agnhost-container: <nil>
STEP: delete the pod
Mar  3 09:48:50.139: INFO: Waiting for pod pod-configmaps-50cd79b5-edb2-4518-b742-c62971d02de5 to disappear
Mar  3 09:48:50.142: INFO: Pod pod-configmaps-50cd79b5-edb2-4518-b742-c62971d02de5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:48:50.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-837" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":181,"skipped":3282,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:48:50.162: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:48:50.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8960" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":182,"skipped":3314,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:48:50.239: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:48:50.277: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:48:50.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8765" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":183,"skipped":3332,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:48:50.846: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-e85957b3-0e10-4398-a3db-77cd563887b2
Mar  3 09:48:50.901: INFO: Pod name my-hostname-basic-e85957b3-0e10-4398-a3db-77cd563887b2: Found 0 pods out of 1
Mar  3 09:48:55.908: INFO: Pod name my-hostname-basic-e85957b3-0e10-4398-a3db-77cd563887b2: Found 1 pods out of 1
Mar  3 09:48:55.908: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e85957b3-0e10-4398-a3db-77cd563887b2" are running
Mar  3 09:48:55.911: INFO: Pod "my-hostname-basic-e85957b3-0e10-4398-a3db-77cd563887b2-ds22d" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-03 09:48:50 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-03 09:48:51 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-03 09:48:51 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-03 09:48:50 +0000 UTC Reason: Message:}])
Mar  3 09:48:55.911: INFO: Trying to dial the pod
Mar  3 09:49:00.943: INFO: Controller my-hostname-basic-e85957b3-0e10-4398-a3db-77cd563887b2: Got expected result from replica 1 [my-hostname-basic-e85957b3-0e10-4398-a3db-77cd563887b2-ds22d]: "my-hostname-basic-e85957b3-0e10-4398-a3db-77cd563887b2-ds22d", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:49:00.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8946" for this suite.

• [SLOW TEST:10.112 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":184,"skipped":3361,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:49:00.959: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  3 09:49:01.385: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 09:49:04.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:49:04.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7236" for this suite.
STEP: Destroying namespace "webhook-7236-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":185,"skipped":3368,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:49:04.576: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-8731/configmap-test-8f575526-6698-4a81-9984-c58f2fd46b85
STEP: Creating a pod to test consume configMaps
Mar  3 09:49:04.680: INFO: Waiting up to 5m0s for pod "pod-configmaps-d384bd2e-81ed-4cd5-a5fb-7dffd543f857" in namespace "configmap-8731" to be "Succeeded or Failed"
Mar  3 09:49:04.697: INFO: Pod "pod-configmaps-d384bd2e-81ed-4cd5-a5fb-7dffd543f857": Phase="Pending", Reason="", readiness=false. Elapsed: 16.322702ms
Mar  3 09:49:06.700: INFO: Pod "pod-configmaps-d384bd2e-81ed-4cd5-a5fb-7dffd543f857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019867631s
STEP: Saw pod success
Mar  3 09:49:06.700: INFO: Pod "pod-configmaps-d384bd2e-81ed-4cd5-a5fb-7dffd543f857" satisfied condition "Succeeded or Failed"
Mar  3 09:49:06.719: INFO: Trying to get logs from node controller-0 pod pod-configmaps-d384bd2e-81ed-4cd5-a5fb-7dffd543f857 container env-test: <nil>
STEP: delete the pod
Mar  3 09:49:06.764: INFO: Waiting for pod pod-configmaps-d384bd2e-81ed-4cd5-a5fb-7dffd543f857 to disappear
Mar  3 09:49:06.767: INFO: Pod pod-configmaps-d384bd2e-81ed-4cd5-a5fb-7dffd543f857 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:49:06.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8731" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":186,"skipped":3391,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:49:06.801: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Mar  3 09:49:06.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 create -f -'
Mar  3 09:49:07.378: INFO: stderr: ""
Mar  3 09:49:07.378: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  3 09:49:07.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  3 09:49:07.461: INFO: stderr: ""
Mar  3 09:49:07.461: INFO: stdout: "update-demo-nautilus-l9v8r update-demo-nautilus-x8dp8 "
Mar  3 09:49:07.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods update-demo-nautilus-l9v8r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  3 09:49:07.544: INFO: stderr: ""
Mar  3 09:49:07.544: INFO: stdout: ""
Mar  3 09:49:07.544: INFO: update-demo-nautilus-l9v8r is created but not running
Mar  3 09:49:12.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  3 09:49:12.638: INFO: stderr: ""
Mar  3 09:49:12.638: INFO: stdout: "update-demo-nautilus-l9v8r update-demo-nautilus-x8dp8 "
Mar  3 09:49:12.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods update-demo-nautilus-l9v8r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  3 09:49:12.717: INFO: stderr: ""
Mar  3 09:49:12.717: INFO: stdout: "true"
Mar  3 09:49:12.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods update-demo-nautilus-l9v8r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  3 09:49:12.796: INFO: stderr: ""
Mar  3 09:49:12.796: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  3 09:49:12.796: INFO: validating pod update-demo-nautilus-l9v8r
Mar  3 09:49:12.805: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  3 09:49:12.805: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  3 09:49:12.805: INFO: update-demo-nautilus-l9v8r is verified up and running
Mar  3 09:49:12.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods update-demo-nautilus-x8dp8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  3 09:49:12.887: INFO: stderr: ""
Mar  3 09:49:12.887: INFO: stdout: "true"
Mar  3 09:49:12.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods update-demo-nautilus-x8dp8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  3 09:49:12.967: INFO: stderr: ""
Mar  3 09:49:12.967: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  3 09:49:12.967: INFO: validating pod update-demo-nautilus-x8dp8
Mar  3 09:49:12.976: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  3 09:49:12.976: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  3 09:49:12.976: INFO: update-demo-nautilus-x8dp8 is verified up and running
STEP: scaling down the replication controller
Mar  3 09:49:12.980: INFO: scanned /root for discovery docs: <nil>
Mar  3 09:49:12.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar  3 09:49:14.134: INFO: stderr: ""
Mar  3 09:49:14.135: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  3 09:49:14.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  3 09:49:14.213: INFO: stderr: ""
Mar  3 09:49:14.213: INFO: stdout: "update-demo-nautilus-l9v8r update-demo-nautilus-x8dp8 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  3 09:49:19.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  3 09:49:19.306: INFO: stderr: ""
Mar  3 09:49:19.306: INFO: stdout: "update-demo-nautilus-l9v8r update-demo-nautilus-x8dp8 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  3 09:49:24.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  3 09:49:24.392: INFO: stderr: ""
Mar  3 09:49:24.392: INFO: stdout: "update-demo-nautilus-x8dp8 "
Mar  3 09:49:24.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods update-demo-nautilus-x8dp8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  3 09:49:24.472: INFO: stderr: ""
Mar  3 09:49:24.472: INFO: stdout: "true"
Mar  3 09:49:24.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods update-demo-nautilus-x8dp8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  3 09:49:24.550: INFO: stderr: ""
Mar  3 09:49:24.550: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  3 09:49:24.550: INFO: validating pod update-demo-nautilus-x8dp8
Mar  3 09:49:24.554: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  3 09:49:24.554: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  3 09:49:24.554: INFO: update-demo-nautilus-x8dp8 is verified up and running
STEP: scaling up the replication controller
Mar  3 09:49:24.556: INFO: scanned /root for discovery docs: <nil>
Mar  3 09:49:24.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar  3 09:49:25.667: INFO: stderr: ""
Mar  3 09:49:25.667: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  3 09:49:25.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  3 09:49:25.788: INFO: stderr: ""
Mar  3 09:49:25.788: INFO: stdout: "update-demo-nautilus-kkm4k update-demo-nautilus-x8dp8 "
Mar  3 09:49:25.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods update-demo-nautilus-kkm4k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  3 09:49:25.867: INFO: stderr: ""
Mar  3 09:49:25.867: INFO: stdout: ""
Mar  3 09:49:25.867: INFO: update-demo-nautilus-kkm4k is created but not running
Mar  3 09:49:30.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  3 09:49:30.954: INFO: stderr: ""
Mar  3 09:49:30.954: INFO: stdout: "update-demo-nautilus-kkm4k update-demo-nautilus-x8dp8 "
Mar  3 09:49:30.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods update-demo-nautilus-kkm4k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  3 09:49:31.029: INFO: stderr: ""
Mar  3 09:49:31.030: INFO: stdout: "true"
Mar  3 09:49:31.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods update-demo-nautilus-kkm4k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  3 09:49:31.120: INFO: stderr: ""
Mar  3 09:49:31.121: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  3 09:49:31.121: INFO: validating pod update-demo-nautilus-kkm4k
Mar  3 09:49:31.133: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  3 09:49:31.133: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  3 09:49:31.133: INFO: update-demo-nautilus-kkm4k is verified up and running
Mar  3 09:49:31.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods update-demo-nautilus-x8dp8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  3 09:49:31.209: INFO: stderr: ""
Mar  3 09:49:31.209: INFO: stdout: "true"
Mar  3 09:49:31.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods update-demo-nautilus-x8dp8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  3 09:49:31.287: INFO: stderr: ""
Mar  3 09:49:31.287: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  3 09:49:31.287: INFO: validating pod update-demo-nautilus-x8dp8
Mar  3 09:49:31.292: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  3 09:49:31.292: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  3 09:49:31.292: INFO: update-demo-nautilus-x8dp8 is verified up and running
STEP: using delete to clean up resources
Mar  3 09:49:31.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 delete --grace-period=0 --force -f -'
Mar  3 09:49:31.401: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  3 09:49:31.401: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  3 09:49:31.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get rc,svc -l name=update-demo --no-headers'
Mar  3 09:49:31.485: INFO: stderr: "No resources found in kubectl-3367 namespace.\n"
Mar  3 09:49:31.485: INFO: stdout: ""
Mar  3 09:49:31.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  3 09:49:31.566: INFO: stderr: ""
Mar  3 09:49:31.566: INFO: stdout: "update-demo-nautilus-kkm4k\nupdate-demo-nautilus-x8dp8\n"
Mar  3 09:49:32.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get rc,svc -l name=update-demo --no-headers'
Mar  3 09:49:32.198: INFO: stderr: "No resources found in kubectl-3367 namespace.\n"
Mar  3 09:49:32.198: INFO: stdout: ""
Mar  3 09:49:32.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3367 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  3 09:49:32.312: INFO: stderr: ""
Mar  3 09:49:32.312: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:49:32.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3367" for this suite.

• [SLOW TEST:25.525 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":187,"skipped":3419,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:49:32.327: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-175a4a7b-e598-4cd7-b162-13d68080ef4b
STEP: Creating a pod to test consume secrets
Mar  3 09:49:32.403: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bbc51fbb-e2da-4b77-8f7b-6a4f6525c99e" in namespace "projected-4326" to be "Succeeded or Failed"
Mar  3 09:49:32.407: INFO: Pod "pod-projected-secrets-bbc51fbb-e2da-4b77-8f7b-6a4f6525c99e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.658557ms
Mar  3 09:49:34.415: INFO: Pod "pod-projected-secrets-bbc51fbb-e2da-4b77-8f7b-6a4f6525c99e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012044983s
STEP: Saw pod success
Mar  3 09:49:34.415: INFO: Pod "pod-projected-secrets-bbc51fbb-e2da-4b77-8f7b-6a4f6525c99e" satisfied condition "Succeeded or Failed"
Mar  3 09:49:34.418: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-bbc51fbb-e2da-4b77-8f7b-6a4f6525c99e container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  3 09:49:34.439: INFO: Waiting for pod pod-projected-secrets-bbc51fbb-e2da-4b77-8f7b-6a4f6525c99e to disappear
Mar  3 09:49:34.443: INFO: Pod pod-projected-secrets-bbc51fbb-e2da-4b77-8f7b-6a4f6525c99e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:49:34.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4326" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":188,"skipped":3436,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:49:34.457: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-4c431985-eefa-4c18-95b5-3409fb7fca99
STEP: Creating a pod to test consume secrets
Mar  3 09:49:34.504: INFO: Waiting up to 5m0s for pod "pod-secrets-40f5289c-21c7-4685-a882-19f15f153f09" in namespace "secrets-3186" to be "Succeeded or Failed"
Mar  3 09:49:34.511: INFO: Pod "pod-secrets-40f5289c-21c7-4685-a882-19f15f153f09": Phase="Pending", Reason="", readiness=false. Elapsed: 6.461586ms
Mar  3 09:49:36.515: INFO: Pod "pod-secrets-40f5289c-21c7-4685-a882-19f15f153f09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010364604s
Mar  3 09:49:38.523: INFO: Pod "pod-secrets-40f5289c-21c7-4685-a882-19f15f153f09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018260833s
STEP: Saw pod success
Mar  3 09:49:38.523: INFO: Pod "pod-secrets-40f5289c-21c7-4685-a882-19f15f153f09" satisfied condition "Succeeded or Failed"
Mar  3 09:49:38.525: INFO: Trying to get logs from node worker-1 pod pod-secrets-40f5289c-21c7-4685-a882-19f15f153f09 container secret-volume-test: <nil>
STEP: delete the pod
Mar  3 09:49:38.547: INFO: Waiting for pod pod-secrets-40f5289c-21c7-4685-a882-19f15f153f09 to disappear
Mar  3 09:49:38.550: INFO: Pod pod-secrets-40f5289c-21c7-4685-a882-19f15f153f09 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:49:38.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3186" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":189,"skipped":3439,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:49:38.558: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-ee77915e-a6ca-4b01-b001-6ae978587a0b
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:49:38.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3656" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":190,"skipped":3443,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:49:38.627: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Mar  3 09:49:38.677: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  3 09:49:38.677: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  3 09:49:38.700: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  3 09:49:38.700: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  3 09:49:38.758: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  3 09:49:38.758: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  3 09:49:39.583: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  3 09:49:39.583: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  3 09:49:39.882: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Mar  3 09:49:39.892: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Mar  3 09:49:39.894: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 0
Mar  3 09:49:39.894: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 0
Mar  3 09:49:39.894: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 0
Mar  3 09:49:39.894: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 0
Mar  3 09:49:39.894: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 0
Mar  3 09:49:39.894: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 0
Mar  3 09:49:39.895: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1
Mar  3 09:49:39.895: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1
Mar  3 09:49:39.896: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 2
Mar  3 09:49:39.896: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 2
Mar  3 09:49:39.897: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 2
Mar  3 09:49:39.897: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 2
Mar  3 09:49:39.933: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 2
Mar  3 09:49:39.934: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 2
Mar  3 09:49:39.957: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 2
Mar  3 09:49:39.957: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 2
Mar  3 09:49:39.968: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1
STEP: listing Deployments
Mar  3 09:49:39.974: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Mar  3 09:49:39.990: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Mar  3 09:49:40.005: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  3 09:49:40.068: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  3 09:49:40.081: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  3 09:49:40.155: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  3 09:49:40.183: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  3 09:49:40.201: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  3 09:49:40.212: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Mar  3 09:49:41.001: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1
Mar  3 09:49:41.002: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1
Mar  3 09:49:41.003: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1
Mar  3 09:49:41.003: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1
Mar  3 09:49:41.004: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1
Mar  3 09:49:41.004: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1
Mar  3 09:49:41.004: INFO: observed Deployment test-deployment in namespace deployment-6662 with ReadyReplicas 1
STEP: deleting the Deployment
Mar  3 09:49:41.025: INFO: observed event type MODIFIED
Mar  3 09:49:41.025: INFO: observed event type MODIFIED
Mar  3 09:49:41.025: INFO: observed event type MODIFIED
Mar  3 09:49:41.025: INFO: observed event type MODIFIED
Mar  3 09:49:41.026: INFO: observed event type MODIFIED
Mar  3 09:49:41.026: INFO: observed event type MODIFIED
Mar  3 09:49:41.026: INFO: observed event type MODIFIED
Mar  3 09:49:41.026: INFO: observed event type MODIFIED
Mar  3 09:49:41.026: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  3 09:49:41.039: INFO: Log out all the ReplicaSets if there is no deployment created
Mar  3 09:49:41.049: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-6662  85be42b4-4820-40fc-82c2-a530602aa261 14902 3 2021-03-03 09:49:40 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e9cf2cd3-5592-46eb-ae0b-084376f709af 0xc004a17667 0xc004a17668}] []  [{kube-controller-manager Update apps/v1 2021-03-03 09:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9cf2cd3-5592-46eb-ae0b-084376f709af\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004a17740 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Mar  3 09:49:41.084: INFO: pod: "test-deployment-768947d6f5-5mz2r":
&Pod{ObjectMeta:{test-deployment-768947d6f5-5mz2r test-deployment-768947d6f5- deployment-6662  08246be5-49ca-4f4d-b32a-13a7c26889a2 14899 0 2021-03-03 09:49:40 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-768947d6f5 85be42b4-4820-40fc-82c2-a530602aa261 0xc004a17de7 0xc004a17de8}] []  [{kube-controller-manager Update v1 2021-03-03 09:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85be42b4-4820-40fc-82c2-a530602aa261\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-03 09:49:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-b259l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-b259l,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-b259l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:49:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:49:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:49:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:49:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.23,PodIP:,StartTime:2021-03-03 09:49:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar  3 09:49:41.084: INFO: pod: "test-deployment-768947d6f5-77j4n":
&Pod{ObjectMeta:{test-deployment-768947d6f5-77j4n test-deployment-768947d6f5- deployment-6662  30c3ab61-05f8-421b-9ab2-f3d0da186484 14883 0 2021-03-03 09:49:40 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[cni.projectcalico.org/podIP:10.244.226.109/32 cni.projectcalico.org/podIPs:10.244.226.109/32] [{apps/v1 ReplicaSet test-deployment-768947d6f5 85be42b4-4820-40fc-82c2-a530602aa261 0xc0005d63c7 0xc0005d63c8}] []  [{calico Update v1 2021-03-03 09:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2021-03-03 09:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85be42b4-4820-40fc-82c2-a530602aa261\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-03 09:49:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.226.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-b259l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-b259l,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-b259l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:49:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:49:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:49:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 09:49:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.47.86,PodIP:10.244.226.109,StartTime:2021-03-03 09:49:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-03 09:49:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://08806f5d7fec183b852ffba0a2f7ac033e1f100710ba3a9bf9fc087f9eba91f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.226.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar  3 09:49:41.084: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-6662  d2f3bd9c-2ab7-4c69-a41d-c5fd1a7929ee 14904 4 2021-03-03 09:49:39 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment e9cf2cd3-5592-46eb-ae0b-084376f709af 0xc004a17857 0xc004a17858}] []  [{kube-controller-manager Update apps/v1 2021-03-03 09:49:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9cf2cd3-5592-46eb-ae0b-084376f709af\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004a17918 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar  3 09:49:41.099: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-6662  f634b76b-917b-4791-9174-9ae3a531ee96 14839 2 2021-03-03 09:49:38 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment e9cf2cd3-5592-46eb-ae0b-084376f709af 0xc004a179a7 0xc004a179a8}] []  [{kube-controller-manager Update apps/v1 2021-03-03 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9cf2cd3-5592-46eb-ae0b-084376f709af\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004a17b00 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:49:41.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6662" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":191,"skipped":3445,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:49:41.146: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:49:41.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4259" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":192,"skipped":3455,"failed":0}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:49:41.201: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  3 09:49:45.296: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  3 09:49:45.307: INFO: Pod pod-with-poststart-http-hook still exists
Mar  3 09:49:47.308: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  3 09:49:47.316: INFO: Pod pod-with-poststart-http-hook still exists
Mar  3 09:49:49.308: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  3 09:49:49.317: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:49:49.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5995" for this suite.

• [SLOW TEST:8.126 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":193,"skipped":3458,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:49:49.329: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar  3 09:49:49.441: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar  3 09:50:03.472: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 09:50:07.168: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:50:21.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1296" for this suite.

• [SLOW TEST:32.199 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":194,"skipped":3500,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:50:21.528: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Mar  3 09:50:25.601: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7641 PodName:var-expansion-26fae3e5-66f9-4b4d-8074-ff9be12fe18b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:50:25.601: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: test for file in mounted path
Mar  3 09:50:25.675: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7641 PodName:var-expansion-26fae3e5-66f9-4b4d-8074-ff9be12fe18b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:50:25.675: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: updating the annotation value
Mar  3 09:50:26.262: INFO: Successfully updated pod "var-expansion-26fae3e5-66f9-4b4d-8074-ff9be12fe18b"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Mar  3 09:50:26.265: INFO: Deleting pod "var-expansion-26fae3e5-66f9-4b4d-8074-ff9be12fe18b" in namespace "var-expansion-7641"
Mar  3 09:50:26.270: INFO: Wait up to 5m0s for pod "var-expansion-26fae3e5-66f9-4b4d-8074-ff9be12fe18b" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:51:08.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7641" for this suite.

• [SLOW TEST:46.772 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":195,"skipped":3501,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:51:08.300: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  3 09:51:08.945: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 09:51:12.036: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:51:24.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9788" for this suite.
STEP: Destroying namespace "webhook-9788-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:16.010 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":196,"skipped":3509,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:51:24.310: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Mar  3 09:51:24.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-8466 cluster-info'
Mar  3 09:51:24.449: INFO: stderr: ""
Mar  3 09:51:24.449: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:51:24.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8466" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":197,"skipped":3526,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:51:24.469: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:51:24.521: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:51:26.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-626" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":198,"skipped":3559,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:51:26.590: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  3 09:51:27.374: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  3 09:51:29.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750361887, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750361887, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750361887, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750361887, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 09:51:32.427: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar  3 09:51:34.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=webhook-4395 attach --namespace=webhook-4395 to-be-attached-pod -i -c=container1'
Mar  3 09:51:34.583: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:51:34.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4395" for this suite.
STEP: Destroying namespace "webhook-4395-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:8.125 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":199,"skipped":3571,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:51:34.717: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-884c6d62-c1d3-4447-863d-f23d3735a749
STEP: Creating a pod to test consume secrets
Mar  3 09:51:34.813: INFO: Waiting up to 5m0s for pod "pod-secrets-00159445-4c5a-4b99-ab24-85aae49c233b" in namespace "secrets-8297" to be "Succeeded or Failed"
Mar  3 09:51:34.817: INFO: Pod "pod-secrets-00159445-4c5a-4b99-ab24-85aae49c233b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.929108ms
Mar  3 09:51:36.823: INFO: Pod "pod-secrets-00159445-4c5a-4b99-ab24-85aae49c233b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010127638s
STEP: Saw pod success
Mar  3 09:51:36.823: INFO: Pod "pod-secrets-00159445-4c5a-4b99-ab24-85aae49c233b" satisfied condition "Succeeded or Failed"
Mar  3 09:51:36.826: INFO: Trying to get logs from node controller-0 pod pod-secrets-00159445-4c5a-4b99-ab24-85aae49c233b container secret-volume-test: <nil>
STEP: delete the pod
Mar  3 09:51:36.962: INFO: Waiting for pod pod-secrets-00159445-4c5a-4b99-ab24-85aae49c233b to disappear
Mar  3 09:51:36.967: INFO: Pod pod-secrets-00159445-4c5a-4b99-ab24-85aae49c233b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:51:36.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8297" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":200,"skipped":3584,"failed":0}
S
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:51:36.988: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:51:37.054: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: creating replication controller svc-latency-rc in namespace svc-latency-611
I0303 09:51:37.067005      24 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-611, replica count: 1
I0303 09:51:38.118471      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  3 09:51:38.262: INFO: Created: latency-svc-wxnpt
Mar  3 09:51:38.262: INFO: Got endpoints: latency-svc-wxnpt [44.243608ms]
Mar  3 09:51:38.307: INFO: Created: latency-svc-p4c9n
Mar  3 09:51:38.320: INFO: Got endpoints: latency-svc-p4c9n [57.45217ms]
Mar  3 09:51:38.346: INFO: Created: latency-svc-92tfj
Mar  3 09:51:38.358: INFO: Got endpoints: latency-svc-92tfj [94.620729ms]
Mar  3 09:51:38.377: INFO: Created: latency-svc-h9pk7
Mar  3 09:51:38.401: INFO: Got endpoints: latency-svc-h9pk7 [137.210165ms]
Mar  3 09:51:38.427: INFO: Created: latency-svc-sx6gp
Mar  3 09:51:38.440: INFO: Got endpoints: latency-svc-sx6gp [176.37567ms]
Mar  3 09:51:38.441: INFO: Created: latency-svc-hwznc
Mar  3 09:51:38.455: INFO: Created: latency-svc-5gkdg
Mar  3 09:51:38.466: INFO: Created: latency-svc-z74hx
Mar  3 09:51:38.488: INFO: Got endpoints: latency-svc-5gkdg [222.771537ms]
Mar  3 09:51:38.488: INFO: Got endpoints: latency-svc-hwznc [225.274893ms]
Mar  3 09:51:38.491: INFO: Got endpoints: latency-svc-z74hx [225.423652ms]
Mar  3 09:51:38.516: INFO: Created: latency-svc-v7kmc
Mar  3 09:51:38.516: INFO: Got endpoints: latency-svc-v7kmc [250.677396ms]
Mar  3 09:51:38.525: INFO: Created: latency-svc-x42r7
Mar  3 09:51:38.532: INFO: Got endpoints: latency-svc-x42r7 [266.323896ms]
Mar  3 09:51:38.545: INFO: Created: latency-svc-6cjfl
Mar  3 09:51:38.553: INFO: Got endpoints: latency-svc-6cjfl [287.232625ms]
Mar  3 09:51:38.561: INFO: Created: latency-svc-wzgtg
Mar  3 09:51:38.579: INFO: Created: latency-svc-6fplm
Mar  3 09:51:38.579: INFO: Got endpoints: latency-svc-wzgtg [313.341952ms]
Mar  3 09:51:38.624: INFO: Created: latency-svc-msw5r
Mar  3 09:51:38.624: INFO: Got endpoints: latency-svc-6fplm [357.716633ms]
Mar  3 09:51:38.637: INFO: Got endpoints: latency-svc-msw5r [370.708238ms]
Mar  3 09:51:38.651: INFO: Created: latency-svc-2vc2s
Mar  3 09:51:38.661: INFO: Got endpoints: latency-svc-2vc2s [397.425199ms]
Mar  3 09:51:38.681: INFO: Created: latency-svc-ws8vf
Mar  3 09:51:38.718: INFO: Created: latency-svc-s2snm
Mar  3 09:51:38.718: INFO: Got endpoints: latency-svc-ws8vf [452.509559ms]
Mar  3 09:51:38.760: INFO: Created: latency-svc-j7nfv
Mar  3 09:51:38.773: INFO: Got endpoints: latency-svc-s2snm [451.713512ms]
Mar  3 09:51:38.776: INFO: Got endpoints: latency-svc-j7nfv [417.890687ms]
Mar  3 09:51:38.783: INFO: Created: latency-svc-lv5zf
Mar  3 09:51:38.793: INFO: Got endpoints: latency-svc-lv5zf [392.50716ms]
Mar  3 09:51:38.809: INFO: Created: latency-svc-g97mc
Mar  3 09:51:38.812: INFO: Got endpoints: latency-svc-g97mc [372.451664ms]
Mar  3 09:51:38.844: INFO: Created: latency-svc-fx69w
Mar  3 09:51:38.847: INFO: Got endpoints: latency-svc-fx69w [359.378171ms]
Mar  3 09:51:38.896: INFO: Created: latency-svc-tzvm4
Mar  3 09:51:38.930: INFO: Got endpoints: latency-svc-tzvm4 [441.034033ms]
Mar  3 09:51:38.938: INFO: Created: latency-svc-4qsqt
Mar  3 09:51:38.953: INFO: Created: latency-svc-2bnnz
Mar  3 09:51:38.956: INFO: Got endpoints: latency-svc-4qsqt [439.155003ms]
Mar  3 09:51:38.966: INFO: Got endpoints: latency-svc-2bnnz [475.045939ms]
Mar  3 09:51:38.983: INFO: Created: latency-svc-fkjt8
Mar  3 09:51:38.990: INFO: Got endpoints: latency-svc-fkjt8 [458.345465ms]
Mar  3 09:51:39.025: INFO: Created: latency-svc-mqdzz
Mar  3 09:51:39.037: INFO: Created: latency-svc-5lf7n
Mar  3 09:51:39.063: INFO: Created: latency-svc-grr6w
Mar  3 09:51:39.071: INFO: Got endpoints: latency-svc-5lf7n [491.348952ms]
Mar  3 09:51:39.071: INFO: Got endpoints: latency-svc-mqdzz [517.735047ms]
Mar  3 09:51:39.086: INFO: Got endpoints: latency-svc-grr6w [461.613653ms]
Mar  3 09:51:39.087: INFO: Created: latency-svc-4xzsb
Mar  3 09:51:39.101: INFO: Got endpoints: latency-svc-4xzsb [464.249818ms]
Mar  3 09:51:39.144: INFO: Created: latency-svc-rc9m2
Mar  3 09:51:39.163: INFO: Got endpoints: latency-svc-rc9m2 [501.807114ms]
Mar  3 09:51:39.172: INFO: Created: latency-svc-9t4zj
Mar  3 09:51:39.180: INFO: Got endpoints: latency-svc-9t4zj [461.434213ms]
Mar  3 09:51:39.193: INFO: Created: latency-svc-ml56l
Mar  3 09:51:39.200: INFO: Got endpoints: latency-svc-ml56l [427.140618ms]
Mar  3 09:51:39.217: INFO: Created: latency-svc-lp82c
Mar  3 09:51:39.227: INFO: Got endpoints: latency-svc-lp82c [451.128065ms]
Mar  3 09:51:39.237: INFO: Created: latency-svc-mlqrv
Mar  3 09:51:39.252: INFO: Got endpoints: latency-svc-mlqrv [459.068656ms]
Mar  3 09:51:39.309: INFO: Created: latency-svc-jzddf
Mar  3 09:51:39.349: INFO: Got endpoints: latency-svc-jzddf [535.998882ms]
Mar  3 09:51:39.352: INFO: Created: latency-svc-q66b4
Mar  3 09:51:39.369: INFO: Got endpoints: latency-svc-q66b4 [521.426806ms]
Mar  3 09:51:39.396: INFO: Created: latency-svc-kfqcn
Mar  3 09:51:39.405: INFO: Created: latency-svc-4rhbn
Mar  3 09:51:39.426: INFO: Got endpoints: latency-svc-kfqcn [496.202779ms]
Mar  3 09:51:39.432: INFO: Created: latency-svc-7l65r
Mar  3 09:51:39.441: INFO: Got endpoints: latency-svc-4rhbn [92.411321ms]
Mar  3 09:51:39.455: INFO: Got endpoints: latency-svc-7l65r [498.281231ms]
Mar  3 09:51:39.482: INFO: Created: latency-svc-mdmh6
Mar  3 09:51:39.482: INFO: Created: latency-svc-vxqvv
Mar  3 09:51:39.482: INFO: Got endpoints: latency-svc-vxqvv [516.46897ms]
Mar  3 09:51:39.511: INFO: Got endpoints: latency-svc-mdmh6 [521.04808ms]
Mar  3 09:51:39.515: INFO: Created: latency-svc-jgcbb
Mar  3 09:51:39.527: INFO: Created: latency-svc-4r2hc
Mar  3 09:51:39.531: INFO: Got endpoints: latency-svc-jgcbb [459.890801ms]
Mar  3 09:51:39.541: INFO: Got endpoints: latency-svc-4r2hc [470.044673ms]
Mar  3 09:51:39.552: INFO: Created: latency-svc-j9t88
Mar  3 09:51:39.562: INFO: Got endpoints: latency-svc-j9t88 [476.342703ms]
Mar  3 09:51:39.582: INFO: Created: latency-svc-2pkr8
Mar  3 09:51:39.582: INFO: Got endpoints: latency-svc-2pkr8 [480.287252ms]
Mar  3 09:51:39.626: INFO: Created: latency-svc-bkjfc
Mar  3 09:51:39.641: INFO: Created: latency-svc-6p494
Mar  3 09:51:39.655: INFO: Got endpoints: latency-svc-bkjfc [492.225403ms]
Mar  3 09:51:39.679: INFO: Got endpoints: latency-svc-6p494 [498.651555ms]
Mar  3 09:51:39.685: INFO: Created: latency-svc-6wksf
Mar  3 09:51:39.712: INFO: Created: latency-svc-fztkg
Mar  3 09:51:39.751: INFO: Created: latency-svc-55vh4
Mar  3 09:51:39.756: INFO: Created: latency-svc-wwrkr
Mar  3 09:51:39.793: INFO: Created: latency-svc-qcw6q
Mar  3 09:51:39.809: INFO: Got endpoints: latency-svc-55vh4 [556.176935ms]
Mar  3 09:51:39.810: INFO: Got endpoints: latency-svc-fztkg [582.776053ms]
Mar  3 09:51:39.810: INFO: Got endpoints: latency-svc-wwrkr [441.094565ms]
Mar  3 09:51:39.811: INFO: Got endpoints: latency-svc-6wksf [611.069436ms]
Mar  3 09:51:39.813: INFO: Got endpoints: latency-svc-qcw6q [387.114866ms]
Mar  3 09:51:39.817: INFO: Created: latency-svc-k6q6m
Mar  3 09:51:39.843: INFO: Created: latency-svc-snxbh
Mar  3 09:51:39.848: INFO: Got endpoints: latency-svc-k6q6m [407.04207ms]
Mar  3 09:51:39.906: INFO: Got endpoints: latency-svc-snxbh [451.524266ms]
Mar  3 09:51:39.917: INFO: Created: latency-svc-cq2cw
Mar  3 09:51:39.930: INFO: Got endpoints: latency-svc-cq2cw [447.761007ms]
Mar  3 09:51:39.941: INFO: Created: latency-svc-h9tjv
Mar  3 09:51:39.943: INFO: Got endpoints: latency-svc-h9tjv [431.784734ms]
Mar  3 09:51:39.958: INFO: Created: latency-svc-52hdp
Mar  3 09:51:40.009: INFO: Created: latency-svc-fwsxw
Mar  3 09:51:40.010: INFO: Got endpoints: latency-svc-52hdp [478.503796ms]
Mar  3 09:51:40.020: INFO: Created: latency-svc-p4wnz
Mar  3 09:51:40.035: INFO: Created: latency-svc-hxps9
Mar  3 09:51:40.051: INFO: Got endpoints: latency-svc-p4wnz [489.045367ms]
Mar  3 09:51:40.051: INFO: Got endpoints: latency-svc-fwsxw [510.294456ms]
Mar  3 09:51:40.052: INFO: Got endpoints: latency-svc-hxps9 [470.051515ms]
Mar  3 09:51:40.078: INFO: Created: latency-svc-q85qj
Mar  3 09:51:40.150: INFO: Created: latency-svc-l26mq
Mar  3 09:51:40.158: INFO: Got endpoints: latency-svc-q85qj [503.508386ms]
Mar  3 09:51:40.166: INFO: Got endpoints: latency-svc-l26mq [487.616964ms]
Mar  3 09:51:40.187: INFO: Created: latency-svc-kh5dd
Mar  3 09:51:40.195: INFO: Got endpoints: latency-svc-kh5dd [386.302593ms]
Mar  3 09:51:40.220: INFO: Created: latency-svc-d9xqv
Mar  3 09:51:40.226: INFO: Got endpoints: latency-svc-d9xqv [415.681375ms]
Mar  3 09:51:40.265: INFO: Created: latency-svc-p69sw
Mar  3 09:51:40.292: INFO: Got endpoints: latency-svc-p69sw [481.305583ms]
Mar  3 09:51:40.374: INFO: Created: latency-svc-7q2ks
Mar  3 09:51:40.388: INFO: Created: latency-svc-9mwpp
Mar  3 09:51:40.398: INFO: Got endpoints: latency-svc-9mwpp [586.255801ms]
Mar  3 09:51:40.398: INFO: Got endpoints: latency-svc-7q2ks [584.667853ms]
Mar  3 09:51:40.441: INFO: Created: latency-svc-2bsfg
Mar  3 09:51:40.459: INFO: Got endpoints: latency-svc-2bsfg [611.088957ms]
Mar  3 09:51:40.492: INFO: Created: latency-svc-6nvbb
Mar  3 09:51:40.499: INFO: Got endpoints: latency-svc-6nvbb [592.78307ms]
Mar  3 09:51:40.538: INFO: Created: latency-svc-57j4g
Mar  3 09:51:40.547: INFO: Got endpoints: latency-svc-57j4g [616.977866ms]
Mar  3 09:51:40.670: INFO: Created: latency-svc-ktt67
Mar  3 09:51:40.711: INFO: Got endpoints: latency-svc-ktt67 [767.874766ms]
Mar  3 09:51:40.761: INFO: Created: latency-svc-8zkqp
Mar  3 09:51:40.844: INFO: Created: latency-svc-dkxsr
Mar  3 09:51:40.848: INFO: Got endpoints: latency-svc-8zkqp [837.817685ms]
Mar  3 09:51:40.888: INFO: Got endpoints: latency-svc-dkxsr [837.17789ms]
Mar  3 09:51:40.940: INFO: Created: latency-svc-vcgvz
Mar  3 09:51:40.962: INFO: Got endpoints: latency-svc-vcgvz [911.025903ms]
Mar  3 09:51:40.971: INFO: Created: latency-svc-xtzgs
Mar  3 09:51:40.995: INFO: Created: latency-svc-4xzt8
Mar  3 09:51:40.997: INFO: Got endpoints: latency-svc-xtzgs [944.822811ms]
Mar  3 09:51:41.018: INFO: Got endpoints: latency-svc-4xzt8 [859.720488ms]
Mar  3 09:51:41.058: INFO: Created: latency-svc-2rss8
Mar  3 09:51:41.088: INFO: Created: latency-svc-h6ppf
Mar  3 09:51:41.099: INFO: Got endpoints: latency-svc-2rss8 [933.106556ms]
Mar  3 09:51:41.103: INFO: Got endpoints: latency-svc-h6ppf [908.390515ms]
Mar  3 09:51:41.149: INFO: Created: latency-svc-rzmqf
Mar  3 09:51:41.160: INFO: Got endpoints: latency-svc-rzmqf [933.915564ms]
Mar  3 09:51:41.176: INFO: Created: latency-svc-fzgxg
Mar  3 09:51:41.218: INFO: Got endpoints: latency-svc-fzgxg [925.972284ms]
Mar  3 09:51:41.225: INFO: Created: latency-svc-8nw7j
Mar  3 09:51:41.236: INFO: Created: latency-svc-mp4bl
Mar  3 09:51:41.248: INFO: Created: latency-svc-qq689
Mar  3 09:51:41.250: INFO: Got endpoints: latency-svc-8nw7j [852.842088ms]
Mar  3 09:51:41.332: INFO: Got endpoints: latency-svc-qq689 [872.598366ms]
Mar  3 09:51:41.332: INFO: Got endpoints: latency-svc-mp4bl [934.338764ms]
Mar  3 09:51:41.355: INFO: Created: latency-svc-8stdm
Mar  3 09:51:41.400: INFO: Created: latency-svc-v2x6j
Mar  3 09:51:41.401: INFO: Got endpoints: latency-svc-8stdm [901.765505ms]
Mar  3 09:51:41.444: INFO: Got endpoints: latency-svc-v2x6j [896.354376ms]
Mar  3 09:51:41.482: INFO: Created: latency-svc-zjzzh
Mar  3 09:51:41.503: INFO: Created: latency-svc-66jlq
Mar  3 09:51:41.503: INFO: Created: latency-svc-59ncz
Mar  3 09:51:41.507: INFO: Got endpoints: latency-svc-zjzzh [795.451394ms]
Mar  3 09:51:41.561: INFO: Created: latency-svc-hdsq5
Mar  3 09:51:41.568: INFO: Got endpoints: latency-svc-66jlq [679.536709ms]
Mar  3 09:51:41.568: INFO: Got endpoints: latency-svc-59ncz [719.73905ms]
Mar  3 09:51:41.610: INFO: Created: latency-svc-lldj6
Mar  3 09:51:41.617: INFO: Got endpoints: latency-svc-hdsq5 [654.37779ms]
Mar  3 09:51:41.628: INFO: Got endpoints: latency-svc-lldj6 [630.99401ms]
Mar  3 09:51:41.663: INFO: Created: latency-svc-6v2n5
Mar  3 09:51:41.686: INFO: Got endpoints: latency-svc-6v2n5 [667.805872ms]
Mar  3 09:51:41.702: INFO: Created: latency-svc-w9gqz
Mar  3 09:51:41.737: INFO: Got endpoints: latency-svc-w9gqz [637.566092ms]
Mar  3 09:51:41.752: INFO: Created: latency-svc-6l7cl
Mar  3 09:51:41.752: INFO: Got endpoints: latency-svc-6l7cl [648.784415ms]
Mar  3 09:51:41.770: INFO: Created: latency-svc-8jdsw
Mar  3 09:51:41.774: INFO: Got endpoints: latency-svc-8jdsw [614.453519ms]
Mar  3 09:51:41.794: INFO: Created: latency-svc-kjbrj
Mar  3 09:51:41.808: INFO: Got endpoints: latency-svc-kjbrj [590.245037ms]
Mar  3 09:51:41.824: INFO: Created: latency-svc-dk9xj
Mar  3 09:51:41.842: INFO: Created: latency-svc-5x6bp
Mar  3 09:51:41.857: INFO: Got endpoints: latency-svc-dk9xj [606.430304ms]
Mar  3 09:51:41.861: INFO: Got endpoints: latency-svc-5x6bp [529.071169ms]
Mar  3 09:51:41.871: INFO: Created: latency-svc-vwvz9
Mar  3 09:51:41.879: INFO: Got endpoints: latency-svc-vwvz9 [547.346285ms]
Mar  3 09:51:41.904: INFO: Created: latency-svc-769cw
Mar  3 09:51:41.928: INFO: Got endpoints: latency-svc-769cw [526.182438ms]
Mar  3 09:51:41.933: INFO: Created: latency-svc-wrbkr
Mar  3 09:51:41.967: INFO: Created: latency-svc-92vqw
Mar  3 09:51:41.968: INFO: Got endpoints: latency-svc-wrbkr [524.82599ms]
Mar  3 09:51:41.993: INFO: Got endpoints: latency-svc-92vqw [486.478206ms]
Mar  3 09:51:41.996: INFO: Created: latency-svc-rrgt6
Mar  3 09:51:42.014: INFO: Got endpoints: latency-svc-rrgt6 [445.693323ms]
Mar  3 09:51:42.027: INFO: Created: latency-svc-l2s69
Mar  3 09:51:42.049: INFO: Got endpoints: latency-svc-l2s69 [480.246029ms]
Mar  3 09:51:42.053: INFO: Created: latency-svc-9vqd2
Mar  3 09:51:42.085: INFO: Got endpoints: latency-svc-9vqd2 [468.057953ms]
Mar  3 09:51:42.085: INFO: Created: latency-svc-fvwfv
Mar  3 09:51:42.165: INFO: Got endpoints: latency-svc-fvwfv [536.908643ms]
Mar  3 09:51:42.165: INFO: Created: latency-svc-qxnb9
Mar  3 09:51:42.165: INFO: Got endpoints: latency-svc-qxnb9 [477.737763ms]
Mar  3 09:51:42.180: INFO: Created: latency-svc-sgqgw
Mar  3 09:51:42.189: INFO: Got endpoints: latency-svc-sgqgw [451.873749ms]
Mar  3 09:51:42.232: INFO: Created: latency-svc-jf9tp
Mar  3 09:51:42.255: INFO: Got endpoints: latency-svc-jf9tp [503.22747ms]
Mar  3 09:51:42.260: INFO: Created: latency-svc-zjhnr
Mar  3 09:51:42.301: INFO: Got endpoints: latency-svc-zjhnr [526.838711ms]
Mar  3 09:51:42.386: INFO: Created: latency-svc-rr6vx
Mar  3 09:51:42.408: INFO: Got endpoints: latency-svc-rr6vx [599.046214ms]
Mar  3 09:51:42.466: INFO: Created: latency-svc-qgzxs
Mar  3 09:51:42.494: INFO: Created: latency-svc-2r8qh
Mar  3 09:51:42.515: INFO: Created: latency-svc-gwbdt
Mar  3 09:51:42.517: INFO: Got endpoints: latency-svc-2r8qh [655.996356ms]
Mar  3 09:51:42.517: INFO: Got endpoints: latency-svc-qgzxs [660.306538ms]
Mar  3 09:51:42.539: INFO: Created: latency-svc-vq8j5
Mar  3 09:51:42.545: INFO: Got endpoints: latency-svc-gwbdt [665.735864ms]
Mar  3 09:51:42.554: INFO: Created: latency-svc-jphzc
Mar  3 09:51:42.561: INFO: Got endpoints: latency-svc-vq8j5 [633.489767ms]
Mar  3 09:51:42.567: INFO: Got endpoints: latency-svc-jphzc [598.067126ms]
Mar  3 09:51:42.576: INFO: Created: latency-svc-pxpdm
Mar  3 09:51:42.591: INFO: Created: latency-svc-hdb5s
Mar  3 09:51:42.593: INFO: Got endpoints: latency-svc-pxpdm [599.769933ms]
Mar  3 09:51:42.614: INFO: Got endpoints: latency-svc-hdb5s [599.737672ms]
Mar  3 09:51:42.637: INFO: Created: latency-svc-fxhnr
Mar  3 09:51:42.637: INFO: Created: latency-svc-fn98z
Mar  3 09:51:42.655: INFO: Created: latency-svc-gkqht
Mar  3 09:51:42.669: INFO: Created: latency-svc-hljxx
Mar  3 09:51:42.674: INFO: Got endpoints: latency-svc-fxhnr [624.357545ms]
Mar  3 09:51:42.691: INFO: Created: latency-svc-64c47
Mar  3 09:51:42.703: INFO: Created: latency-svc-tmhls
Mar  3 09:51:42.727: INFO: Created: latency-svc-bvwfr
Mar  3 09:51:42.731: INFO: Got endpoints: latency-svc-fn98z [645.534492ms]
Mar  3 09:51:42.762: INFO: Got endpoints: latency-svc-gkqht [596.785187ms]
Mar  3 09:51:42.794: INFO: Created: latency-svc-6qzmh
Mar  3 09:51:42.811: INFO: Got endpoints: latency-svc-hljxx [645.971053ms]
Mar  3 09:51:42.814: INFO: Created: latency-svc-4rzvn
Mar  3 09:51:42.844: INFO: Created: latency-svc-tqbjp
Mar  3 09:51:42.851: INFO: Created: latency-svc-8b6hn
Mar  3 09:51:42.865: INFO: Got endpoints: latency-svc-64c47 [675.850939ms]
Mar  3 09:51:42.910: INFO: Created: latency-svc-klbkk
Mar  3 09:51:42.952: INFO: Got endpoints: latency-svc-tmhls [696.654703ms]
Mar  3 09:51:42.972: INFO: Got endpoints: latency-svc-bvwfr [670.80103ms]
Mar  3 09:51:42.980: INFO: Created: latency-svc-xnw9m
Mar  3 09:51:43.005: INFO: Created: latency-svc-dc2l2
Mar  3 09:51:43.024: INFO: Got endpoints: latency-svc-6qzmh [615.496807ms]
Mar  3 09:51:43.046: INFO: Created: latency-svc-kd58b
Mar  3 09:51:43.063: INFO: Got endpoints: latency-svc-4rzvn [545.374925ms]
Mar  3 09:51:43.065: INFO: Created: latency-svc-dlwjp
Mar  3 09:51:43.108: INFO: Created: latency-svc-867f6
Mar  3 09:51:43.118: INFO: Got endpoints: latency-svc-tqbjp [600.894615ms]
Mar  3 09:51:43.132: INFO: Created: latency-svc-kc84m
Mar  3 09:51:43.151: INFO: Created: latency-svc-7mggx
Mar  3 09:51:43.172: INFO: Got endpoints: latency-svc-8b6hn [626.541758ms]
Mar  3 09:51:43.184: INFO: Created: latency-svc-lbb46
Mar  3 09:51:43.200: INFO: Created: latency-svc-9kbj4
Mar  3 09:51:43.216: INFO: Got endpoints: latency-svc-klbkk [654.925161ms]
Mar  3 09:51:43.239: INFO: Created: latency-svc-7gz7v
Mar  3 09:51:43.282: INFO: Created: latency-svc-6hhm6
Mar  3 09:51:43.310: INFO: Got endpoints: latency-svc-xnw9m [743.672522ms]
Mar  3 09:51:43.332: INFO: Got endpoints: latency-svc-dc2l2 [739.161436ms]
Mar  3 09:51:43.360: INFO: Created: latency-svc-hdmjk
Mar  3 09:51:43.372: INFO: Got endpoints: latency-svc-kd58b [758.677929ms]
Mar  3 09:51:43.376: INFO: Created: latency-svc-h4ttc
Mar  3 09:51:43.395: INFO: Created: latency-svc-7g6bl
Mar  3 09:51:43.398: INFO: Created: latency-svc-nxtf7
Mar  3 09:51:43.438: INFO: Got endpoints: latency-svc-dlwjp [764.09595ms]
Mar  3 09:51:43.443: INFO: Created: latency-svc-9xjxn
Mar  3 09:51:43.461: INFO: Created: latency-svc-kl5xw
Mar  3 09:51:43.468: INFO: Got endpoints: latency-svc-867f6 [737.055995ms]
Mar  3 09:51:43.485: INFO: Created: latency-svc-qw2xc
Mar  3 09:51:43.489: INFO: Created: latency-svc-b7t7z
Mar  3 09:51:43.528: INFO: Got endpoints: latency-svc-kc84m [765.570328ms]
Mar  3 09:51:43.530: INFO: Created: latency-svc-njhvl
Mar  3 09:51:43.547: INFO: Created: latency-svc-xj9t5
Mar  3 09:51:43.558: INFO: Got endpoints: latency-svc-7mggx [747.575725ms]
Mar  3 09:51:43.574: INFO: Created: latency-svc-gzvj8
Mar  3 09:51:43.615: INFO: Got endpoints: latency-svc-lbb46 [749.492422ms]
Mar  3 09:51:43.627: INFO: Created: latency-svc-mvh6k
Mar  3 09:51:43.672: INFO: Got endpoints: latency-svc-9kbj4 [719.906557ms]
Mar  3 09:51:43.690: INFO: Created: latency-svc-nplf7
Mar  3 09:51:43.712: INFO: Got endpoints: latency-svc-7gz7v [739.66051ms]
Mar  3 09:51:43.728: INFO: Created: latency-svc-w4dvx
Mar  3 09:51:43.763: INFO: Got endpoints: latency-svc-6hhm6 [739.792912ms]
Mar  3 09:51:43.783: INFO: Created: latency-svc-gtdcg
Mar  3 09:51:43.813: INFO: Got endpoints: latency-svc-hdmjk [694.481861ms]
Mar  3 09:51:43.849: INFO: Created: latency-svc-pbsw8
Mar  3 09:51:43.864: INFO: Got endpoints: latency-svc-h4ttc [800.163125ms]
Mar  3 09:51:43.889: INFO: Created: latency-svc-jfkd6
Mar  3 09:51:43.913: INFO: Got endpoints: latency-svc-7g6bl [741.32871ms]
Mar  3 09:51:43.929: INFO: Created: latency-svc-h64x2
Mar  3 09:51:43.963: INFO: Got endpoints: latency-svc-nxtf7 [747.254949ms]
Mar  3 09:51:43.977: INFO: Created: latency-svc-j84pt
Mar  3 09:51:44.012: INFO: Got endpoints: latency-svc-9xjxn [701.851271ms]
Mar  3 09:51:44.024: INFO: Created: latency-svc-8qs9w
Mar  3 09:51:44.064: INFO: Got endpoints: latency-svc-kl5xw [730.574274ms]
Mar  3 09:51:44.077: INFO: Created: latency-svc-nkqtl
Mar  3 09:51:44.113: INFO: Got endpoints: latency-svc-qw2xc [741.013392ms]
Mar  3 09:51:44.125: INFO: Created: latency-svc-mfqfb
Mar  3 09:51:44.159: INFO: Got endpoints: latency-svc-b7t7z [721.134153ms]
Mar  3 09:51:44.177: INFO: Created: latency-svc-sh2j6
Mar  3 09:51:44.211: INFO: Got endpoints: latency-svc-njhvl [743.682987ms]
Mar  3 09:51:44.231: INFO: Created: latency-svc-xlgpw
Mar  3 09:51:44.261: INFO: Got endpoints: latency-svc-xj9t5 [732.926914ms]
Mar  3 09:51:44.303: INFO: Created: latency-svc-bkmm6
Mar  3 09:51:44.315: INFO: Got endpoints: latency-svc-gzvj8 [756.794466ms]
Mar  3 09:51:44.350: INFO: Created: latency-svc-vx8rb
Mar  3 09:51:44.376: INFO: Got endpoints: latency-svc-mvh6k [761.606141ms]
Mar  3 09:51:44.394: INFO: Created: latency-svc-pjvh5
Mar  3 09:51:44.433: INFO: Got endpoints: latency-svc-nplf7 [760.966002ms]
Mar  3 09:51:44.491: INFO: Got endpoints: latency-svc-w4dvx [776.870637ms]
Mar  3 09:51:44.493: INFO: Created: latency-svc-7h6kx
Mar  3 09:51:44.538: INFO: Got endpoints: latency-svc-gtdcg [774.738726ms]
Mar  3 09:51:44.547: INFO: Created: latency-svc-f95wl
Mar  3 09:51:44.577: INFO: Got endpoints: latency-svc-pbsw8 [764.246022ms]
Mar  3 09:51:44.591: INFO: Created: latency-svc-hdcjm
Mar  3 09:51:44.632: INFO: Got endpoints: latency-svc-jfkd6 [768.131275ms]
Mar  3 09:51:44.633: INFO: Created: latency-svc-4c82w
Mar  3 09:51:44.675: INFO: Got endpoints: latency-svc-h64x2 [761.647173ms]
Mar  3 09:51:44.693: INFO: Created: latency-svc-7gdkh
Mar  3 09:51:44.733: INFO: Got endpoints: latency-svc-j84pt [769.336868ms]
Mar  3 09:51:44.740: INFO: Created: latency-svc-mw6s7
Mar  3 09:51:44.815: INFO: Got endpoints: latency-svc-8qs9w [803.154448ms]
Mar  3 09:51:44.821: INFO: Created: latency-svc-v8zx9
Mar  3 09:51:44.849: INFO: Created: latency-svc-fh8bz
Mar  3 09:51:44.850: INFO: Got endpoints: latency-svc-nkqtl [786.495853ms]
Mar  3 09:51:44.864: INFO: Got endpoints: latency-svc-mfqfb [750.328791ms]
Mar  3 09:51:44.919: INFO: Created: latency-svc-6vkgl
Mar  3 09:51:44.933: INFO: Got endpoints: latency-svc-sh2j6 [773.907495ms]
Mar  3 09:51:44.978: INFO: Created: latency-svc-mfmcr
Mar  3 09:51:44.982: INFO: Got endpoints: latency-svc-xlgpw [771.064464ms]
Mar  3 09:51:44.992: INFO: Created: latency-svc-bjtsd
Mar  3 09:51:45.023: INFO: Got endpoints: latency-svc-bkmm6 [762.752069ms]
Mar  3 09:51:45.025: INFO: Created: latency-svc-9vpfj
Mar  3 09:51:45.095: INFO: Got endpoints: latency-svc-vx8rb [778.626ms]
Mar  3 09:51:45.097: INFO: Created: latency-svc-frrfr
Mar  3 09:51:45.190: INFO: Got endpoints: latency-svc-7h6kx [757.233734ms]
Mar  3 09:51:45.191: INFO: Got endpoints: latency-svc-pjvh5 [810.542992ms]
Mar  3 09:51:45.197: INFO: Created: latency-svc-66vfx
Mar  3 09:51:45.243: INFO: Created: latency-svc-m7n9l
Mar  3 09:51:45.292: INFO: Created: latency-svc-fd647
Mar  3 09:51:45.329: INFO: Got endpoints: latency-svc-hdcjm [791.108771ms]
Mar  3 09:51:45.330: INFO: Got endpoints: latency-svc-f95wl [839.595352ms]
Mar  3 09:51:45.379: INFO: Got endpoints: latency-svc-7gdkh [747.398727ms]
Mar  3 09:51:45.381: INFO: Got endpoints: latency-svc-4c82w [803.391528ms]
Mar  3 09:51:45.403: INFO: Created: latency-svc-ktt9g
Mar  3 09:51:45.410: INFO: Got endpoints: latency-svc-mw6s7 [735.033128ms]
Mar  3 09:51:45.427: INFO: Created: latency-svc-9jgk5
Mar  3 09:51:45.440: INFO: Created: latency-svc-nqhtp
Mar  3 09:51:45.451: INFO: Created: latency-svc-jv6q9
Mar  3 09:51:45.474: INFO: Got endpoints: latency-svc-v8zx9 [741.062507ms]
Mar  3 09:51:45.475: INFO: Created: latency-svc-jhpm6
Mar  3 09:51:45.493: INFO: Created: latency-svc-xtl7w
Mar  3 09:51:45.513: INFO: Got endpoints: latency-svc-fh8bz [697.91203ms]
Mar  3 09:51:45.527: INFO: Created: latency-svc-bf8js
Mar  3 09:51:45.561: INFO: Got endpoints: latency-svc-6vkgl [710.372023ms]
Mar  3 09:51:45.576: INFO: Created: latency-svc-wgkql
Mar  3 09:51:45.608: INFO: Got endpoints: latency-svc-mfmcr [744.55478ms]
Mar  3 09:51:45.652: INFO: Created: latency-svc-dkc2n
Mar  3 09:51:45.659: INFO: Got endpoints: latency-svc-bjtsd [725.909025ms]
Mar  3 09:51:45.674: INFO: Created: latency-svc-8zl9z
Mar  3 09:51:45.748: INFO: Got endpoints: latency-svc-9vpfj [765.562477ms]
Mar  3 09:51:45.766: INFO: Got endpoints: latency-svc-frrfr [742.644414ms]
Mar  3 09:51:45.767: INFO: Created: latency-svc-slmkt
Mar  3 09:51:45.781: INFO: Created: latency-svc-n9xdc
Mar  3 09:51:45.808: INFO: Got endpoints: latency-svc-66vfx [712.601525ms]
Mar  3 09:51:45.849: INFO: Created: latency-svc-9jncd
Mar  3 09:51:45.859: INFO: Got endpoints: latency-svc-m7n9l [667.961575ms]
Mar  3 09:51:45.875: INFO: Created: latency-svc-vmhpb
Mar  3 09:51:45.913: INFO: Got endpoints: latency-svc-fd647 [722.738785ms]
Mar  3 09:51:45.927: INFO: Created: latency-svc-jw6ff
Mar  3 09:51:45.978: INFO: Got endpoints: latency-svc-ktt9g [646.895521ms]
Mar  3 09:51:45.995: INFO: Created: latency-svc-wbq99
Mar  3 09:51:46.014: INFO: Got endpoints: latency-svc-9jgk5 [682.779615ms]
Mar  3 09:51:46.062: INFO: Got endpoints: latency-svc-nqhtp [681.571185ms]
Mar  3 09:51:46.068: INFO: Created: latency-svc-djwdw
Mar  3 09:51:46.104: INFO: Created: latency-svc-d6b8f
Mar  3 09:51:46.115: INFO: Got endpoints: latency-svc-jv6q9 [733.932577ms]
Mar  3 09:51:46.162: INFO: Got endpoints: latency-svc-jhpm6 [750.850998ms]
Mar  3 09:51:46.215: INFO: Got endpoints: latency-svc-xtl7w [740.710281ms]
Mar  3 09:51:46.283: INFO: Got endpoints: latency-svc-bf8js [769.905948ms]
Mar  3 09:51:46.315: INFO: Got endpoints: latency-svc-wgkql [754.046687ms]
Mar  3 09:51:46.368: INFO: Got endpoints: latency-svc-dkc2n [759.86682ms]
Mar  3 09:51:46.414: INFO: Got endpoints: latency-svc-8zl9z [754.555454ms]
Mar  3 09:51:46.458: INFO: Got endpoints: latency-svc-slmkt [710.085931ms]
Mar  3 09:51:46.548: INFO: Got endpoints: latency-svc-n9xdc [782.29882ms]
Mar  3 09:51:46.564: INFO: Got endpoints: latency-svc-9jncd [755.880541ms]
Mar  3 09:51:46.613: INFO: Got endpoints: latency-svc-vmhpb [753.896803ms]
Mar  3 09:51:46.663: INFO: Got endpoints: latency-svc-jw6ff [749.581851ms]
Mar  3 09:51:46.711: INFO: Got endpoints: latency-svc-wbq99 [733.0855ms]
Mar  3 09:51:46.758: INFO: Got endpoints: latency-svc-djwdw [743.791178ms]
Mar  3 09:51:46.813: INFO: Got endpoints: latency-svc-d6b8f [750.439503ms]
Mar  3 09:51:46.813: INFO: Latencies: [57.45217ms 92.411321ms 94.620729ms 137.210165ms 176.37567ms 222.771537ms 225.274893ms 225.423652ms 250.677396ms 266.323896ms 287.232625ms 313.341952ms 357.716633ms 359.378171ms 370.708238ms 372.451664ms 386.302593ms 387.114866ms 392.50716ms 397.425199ms 407.04207ms 415.681375ms 417.890687ms 427.140618ms 431.784734ms 439.155003ms 441.034033ms 441.094565ms 445.693323ms 447.761007ms 451.128065ms 451.524266ms 451.713512ms 451.873749ms 452.509559ms 458.345465ms 459.068656ms 459.890801ms 461.434213ms 461.613653ms 464.249818ms 468.057953ms 470.044673ms 470.051515ms 475.045939ms 476.342703ms 477.737763ms 478.503796ms 480.246029ms 480.287252ms 481.305583ms 486.478206ms 487.616964ms 489.045367ms 491.348952ms 492.225403ms 496.202779ms 498.281231ms 498.651555ms 501.807114ms 503.22747ms 503.508386ms 510.294456ms 516.46897ms 517.735047ms 521.04808ms 521.426806ms 524.82599ms 526.182438ms 526.838711ms 529.071169ms 535.998882ms 536.908643ms 545.374925ms 547.346285ms 556.176935ms 582.776053ms 584.667853ms 586.255801ms 590.245037ms 592.78307ms 596.785187ms 598.067126ms 599.046214ms 599.737672ms 599.769933ms 600.894615ms 606.430304ms 611.069436ms 611.088957ms 614.453519ms 615.496807ms 616.977866ms 624.357545ms 626.541758ms 630.99401ms 633.489767ms 637.566092ms 645.534492ms 645.971053ms 646.895521ms 648.784415ms 654.37779ms 654.925161ms 655.996356ms 660.306538ms 665.735864ms 667.805872ms 667.961575ms 670.80103ms 675.850939ms 679.536709ms 681.571185ms 682.779615ms 694.481861ms 696.654703ms 697.91203ms 701.851271ms 710.085931ms 710.372023ms 712.601525ms 719.73905ms 719.906557ms 721.134153ms 722.738785ms 725.909025ms 730.574274ms 732.926914ms 733.0855ms 733.932577ms 735.033128ms 737.055995ms 739.161436ms 739.66051ms 739.792912ms 740.710281ms 741.013392ms 741.062507ms 741.32871ms 742.644414ms 743.672522ms 743.682987ms 743.791178ms 744.55478ms 747.254949ms 747.398727ms 747.575725ms 749.492422ms 749.581851ms 750.328791ms 750.439503ms 750.850998ms 753.896803ms 754.046687ms 754.555454ms 755.880541ms 756.794466ms 757.233734ms 758.677929ms 759.86682ms 760.966002ms 761.606141ms 761.647173ms 762.752069ms 764.09595ms 764.246022ms 765.562477ms 765.570328ms 767.874766ms 768.131275ms 769.336868ms 769.905948ms 771.064464ms 773.907495ms 774.738726ms 776.870637ms 778.626ms 782.29882ms 786.495853ms 791.108771ms 795.451394ms 800.163125ms 803.154448ms 803.391528ms 810.542992ms 837.17789ms 837.817685ms 839.595352ms 852.842088ms 859.720488ms 872.598366ms 896.354376ms 901.765505ms 908.390515ms 911.025903ms 925.972284ms 933.106556ms 933.915564ms 934.338764ms 944.822811ms]
Mar  3 09:51:46.814: INFO: 50 %ile: 646.895521ms
Mar  3 09:51:46.814: INFO: 90 %ile: 795.451394ms
Mar  3 09:51:46.814: INFO: 99 %ile: 934.338764ms
Mar  3 09:51:46.814: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:51:46.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-611" for this suite.

• [SLOW TEST:9.837 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":201,"skipped":3585,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:51:46.828: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 09:51:46.884: INFO: Waiting up to 5m0s for pod "downwardapi-volume-758787e1-1f5b-4b66-ab90-9ff4cf8ecec8" in namespace "downward-api-5521" to be "Succeeded or Failed"
Mar  3 09:51:46.898: INFO: Pod "downwardapi-volume-758787e1-1f5b-4b66-ab90-9ff4cf8ecec8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.38891ms
Mar  3 09:51:48.928: INFO: Pod "downwardapi-volume-758787e1-1f5b-4b66-ab90-9ff4cf8ecec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043839161s
STEP: Saw pod success
Mar  3 09:51:48.928: INFO: Pod "downwardapi-volume-758787e1-1f5b-4b66-ab90-9ff4cf8ecec8" satisfied condition "Succeeded or Failed"
Mar  3 09:51:48.940: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-758787e1-1f5b-4b66-ab90-9ff4cf8ecec8 container client-container: <nil>
STEP: delete the pod
Mar  3 09:51:48.967: INFO: Waiting for pod downwardapi-volume-758787e1-1f5b-4b66-ab90-9ff4cf8ecec8 to disappear
Mar  3 09:51:48.969: INFO: Pod downwardapi-volume-758787e1-1f5b-4b66-ab90-9ff4cf8ecec8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:51:48.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5521" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":202,"skipped":3604,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:51:48.978: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5217
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-5217
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5217
Mar  3 09:51:49.039: INFO: Found 0 stateful pods, waiting for 1
Mar  3 09:51:59.135: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  3 09:51:59.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-5217 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  3 09:51:59.762: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  3 09:51:59.762: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  3 09:51:59.762: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  3 09:51:59.771: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  3 09:52:09.783: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  3 09:52:09.783: INFO: Waiting for statefulset status.replicas updated to 0
Mar  3 09:52:09.802: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Mar  3 09:52:09.802: INFO: ss-0  controller-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  }]
Mar  3 09:52:09.802: INFO: 
Mar  3 09:52:09.802: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  3 09:52:10.806: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994092592s
Mar  3 09:52:11.811: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989634123s
Mar  3 09:52:12.818: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984447359s
Mar  3 09:52:13.824: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978376469s
Mar  3 09:52:14.831: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972159304s
Mar  3 09:52:15.836: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964537342s
Mar  3 09:52:16.846: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.960296199s
Mar  3 09:52:17.852: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.949897824s
Mar  3 09:52:18.859: INFO: Verifying statefulset ss doesn't scale past 3 for another 943.721399ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5217
Mar  3 09:52:19.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-5217 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  3 09:52:20.057: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  3 09:52:20.057: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  3 09:52:20.057: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  3 09:52:20.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-5217 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  3 09:52:20.208: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  3 09:52:20.209: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  3 09:52:20.209: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  3 09:52:20.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-5217 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  3 09:52:20.367: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  3 09:52:20.367: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  3 09:52:20.368: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  3 09:52:20.383: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  3 09:52:20.383: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  3 09:52:20.383: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar  3 09:52:20.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-5217 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  3 09:52:20.597: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  3 09:52:20.597: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  3 09:52:20.597: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  3 09:52:20.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-5217 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  3 09:52:20.762: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  3 09:52:20.762: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  3 09:52:20.762: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  3 09:52:20.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-5217 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  3 09:52:20.913: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  3 09:52:20.914: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  3 09:52:20.914: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  3 09:52:20.914: INFO: Waiting for statefulset status.replicas updated to 0
Mar  3 09:52:20.917: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar  3 09:52:30.943: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  3 09:52:30.943: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  3 09:52:30.943: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  3 09:52:30.991: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Mar  3 09:52:30.991: INFO: ss-0  controller-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  }]
Mar  3 09:52:30.991: INFO: ss-1  worker-1      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  }]
Mar  3 09:52:30.991: INFO: ss-2  worker-0      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  }]
Mar  3 09:52:30.991: INFO: 
Mar  3 09:52:30.991: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  3 09:52:32.006: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Mar  3 09:52:32.006: INFO: ss-0  controller-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  }]
Mar  3 09:52:32.006: INFO: ss-1  worker-1      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  }]
Mar  3 09:52:32.006: INFO: ss-2  worker-0      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  }]
Mar  3 09:52:32.006: INFO: 
Mar  3 09:52:32.006: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  3 09:52:33.009: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Mar  3 09:52:33.010: INFO: ss-0  controller-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  }]
Mar  3 09:52:33.010: INFO: ss-1  worker-1      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  }]
Mar  3 09:52:33.010: INFO: 
Mar  3 09:52:33.010: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  3 09:52:34.014: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Mar  3 09:52:34.014: INFO: ss-0  controller-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  }]
Mar  3 09:52:34.014: INFO: ss-1  worker-1      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  }]
Mar  3 09:52:34.014: INFO: 
Mar  3 09:52:34.014: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  3 09:52:35.023: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Mar  3 09:52:35.023: INFO: ss-0  controller-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  }]
Mar  3 09:52:35.024: INFO: ss-1  worker-1      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  }]
Mar  3 09:52:35.024: INFO: 
Mar  3 09:52:35.024: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  3 09:52:36.031: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Mar  3 09:52:36.031: INFO: ss-0  controller-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  }]
Mar  3 09:52:36.031: INFO: ss-1  worker-1      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  }]
Mar  3 09:52:36.031: INFO: 
Mar  3 09:52:36.031: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  3 09:52:37.036: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Mar  3 09:52:37.036: INFO: ss-0  controller-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  }]
Mar  3 09:52:37.036: INFO: ss-1  worker-1      Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:09 +0000 UTC  }]
Mar  3 09:52:37.036: INFO: 
Mar  3 09:52:37.036: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  3 09:52:38.042: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Mar  3 09:52:38.042: INFO: ss-0  controller-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  }]
Mar  3 09:52:38.042: INFO: 
Mar  3 09:52:38.042: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  3 09:52:39.048: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Mar  3 09:52:39.048: INFO: ss-0  controller-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  }]
Mar  3 09:52:39.048: INFO: 
Mar  3 09:52:39.048: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  3 09:52:40.055: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Mar  3 09:52:40.055: INFO: ss-0  controller-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:52:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 09:51:49 +0000 UTC  }]
Mar  3 09:52:40.055: INFO: 
Mar  3 09:52:40.055: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5217
Mar  3 09:52:41.061: INFO: Scaling statefulset ss to 0
Mar  3 09:52:41.070: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  3 09:52:41.072: INFO: Deleting all statefulset in ns statefulset-5217
Mar  3 09:52:41.074: INFO: Scaling statefulset ss to 0
Mar  3 09:52:41.081: INFO: Waiting for statefulset status.replicas updated to 0
Mar  3 09:52:41.084: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:52:41.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5217" for this suite.

• [SLOW TEST:52.161 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":203,"skipped":3607,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:52:41.141: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-2d7b9060-0c1b-4b42-bea1-c36382ff2fef
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:52:43.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5686" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":204,"skipped":3610,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:52:43.242: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-4090
STEP: creating service affinity-nodeport in namespace services-4090
STEP: creating replication controller affinity-nodeport in namespace services-4090
I0303 09:52:43.376898      24 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-4090, replica count: 3
I0303 09:52:46.428036      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0303 09:52:49.428283      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  3 09:52:49.442: INFO: Creating new exec pod
Mar  3 09:52:52.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-4090 exec execpod-affinity8h4jx -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Mar  3 09:52:52.648: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar  3 09:52:52.648: INFO: stdout: ""
Mar  3 09:52:52.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-4090 exec execpod-affinity8h4jx -- /bin/sh -x -c nc -zv -t -w 2 10.102.161.195 80'
Mar  3 09:52:52.823: INFO: stderr: "+ nc -zv -t -w 2 10.102.161.195 80\nConnection to 10.102.161.195 80 port [tcp/http] succeeded!\n"
Mar  3 09:52:52.824: INFO: stdout: ""
Mar  3 09:52:52.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-4090 exec execpod-affinity8h4jx -- /bin/sh -x -c nc -zv -t -w 2 10.0.40.23 31947'
Mar  3 09:52:52.998: INFO: stderr: "+ nc -zv -t -w 2 10.0.40.23 31947\nConnection to 10.0.40.23 31947 port [tcp/31947] succeeded!\n"
Mar  3 09:52:52.998: INFO: stdout: ""
Mar  3 09:52:52.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-4090 exec execpod-affinity8h4jx -- /bin/sh -x -c nc -zv -t -w 2 10.0.47.86 31947'
Mar  3 09:52:53.158: INFO: stderr: "+ nc -zv -t -w 2 10.0.47.86 31947\nConnection to 10.0.47.86 31947 port [tcp/31947] succeeded!\n"
Mar  3 09:52:53.158: INFO: stdout: ""
Mar  3 09:52:53.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-4090 exec execpod-affinity8h4jx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.49.5:31947/ ; done'
Mar  3 09:52:53.407: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31947/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31947/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31947/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31947/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31947/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31947/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31947/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31947/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31947/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31947/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31947/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31947/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31947/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31947/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31947/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.49.5:31947/\n"
Mar  3 09:52:53.407: INFO: stdout: "\naffinity-nodeport-vnhmf\naffinity-nodeport-vnhmf\naffinity-nodeport-vnhmf\naffinity-nodeport-vnhmf\naffinity-nodeport-vnhmf\naffinity-nodeport-vnhmf\naffinity-nodeport-vnhmf\naffinity-nodeport-vnhmf\naffinity-nodeport-vnhmf\naffinity-nodeport-vnhmf\naffinity-nodeport-vnhmf\naffinity-nodeport-vnhmf\naffinity-nodeport-vnhmf\naffinity-nodeport-vnhmf\naffinity-nodeport-vnhmf\naffinity-nodeport-vnhmf"
Mar  3 09:52:53.407: INFO: Received response from host: affinity-nodeport-vnhmf
Mar  3 09:52:53.407: INFO: Received response from host: affinity-nodeport-vnhmf
Mar  3 09:52:53.407: INFO: Received response from host: affinity-nodeport-vnhmf
Mar  3 09:52:53.407: INFO: Received response from host: affinity-nodeport-vnhmf
Mar  3 09:52:53.407: INFO: Received response from host: affinity-nodeport-vnhmf
Mar  3 09:52:53.407: INFO: Received response from host: affinity-nodeport-vnhmf
Mar  3 09:52:53.407: INFO: Received response from host: affinity-nodeport-vnhmf
Mar  3 09:52:53.407: INFO: Received response from host: affinity-nodeport-vnhmf
Mar  3 09:52:53.407: INFO: Received response from host: affinity-nodeport-vnhmf
Mar  3 09:52:53.407: INFO: Received response from host: affinity-nodeport-vnhmf
Mar  3 09:52:53.407: INFO: Received response from host: affinity-nodeport-vnhmf
Mar  3 09:52:53.407: INFO: Received response from host: affinity-nodeport-vnhmf
Mar  3 09:52:53.407: INFO: Received response from host: affinity-nodeport-vnhmf
Mar  3 09:52:53.407: INFO: Received response from host: affinity-nodeport-vnhmf
Mar  3 09:52:53.407: INFO: Received response from host: affinity-nodeport-vnhmf
Mar  3 09:52:53.407: INFO: Received response from host: affinity-nodeport-vnhmf
Mar  3 09:52:53.407: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4090, will wait for the garbage collector to delete the pods
Mar  3 09:52:53.492: INFO: Deleting ReplicationController affinity-nodeport took: 8.496239ms
Mar  3 09:52:53.692: INFO: Terminating ReplicationController affinity-nodeport pods took: 200.254739ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:53:07.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4090" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:24.666 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":205,"skipped":3646,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:53:07.911: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0303 09:53:14.263691      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0303 09:53:14.263716      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0303 09:53:14.263723      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  3 09:53:14.263: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:53:14.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6792" for this suite.

• [SLOW TEST:6.495 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":206,"skipped":3671,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:53:14.406: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-s8xr
STEP: Creating a pod to test atomic-volume-subpath
Mar  3 09:53:15.604: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-s8xr" in namespace "subpath-1583" to be "Succeeded or Failed"
Mar  3 09:53:15.674: INFO: Pod "pod-subpath-test-secret-s8xr": Phase="Pending", Reason="", readiness=false. Elapsed: 69.33644ms
Mar  3 09:53:17.679: INFO: Pod "pod-subpath-test-secret-s8xr": Phase="Running", Reason="", readiness=true. Elapsed: 2.074502775s
Mar  3 09:53:19.692: INFO: Pod "pod-subpath-test-secret-s8xr": Phase="Running", Reason="", readiness=true. Elapsed: 4.087986546s
Mar  3 09:53:21.700: INFO: Pod "pod-subpath-test-secret-s8xr": Phase="Running", Reason="", readiness=true. Elapsed: 6.095473705s
Mar  3 09:53:23.705: INFO: Pod "pod-subpath-test-secret-s8xr": Phase="Running", Reason="", readiness=true. Elapsed: 8.100856184s
Mar  3 09:53:25.715: INFO: Pod "pod-subpath-test-secret-s8xr": Phase="Running", Reason="", readiness=true. Elapsed: 10.110983491s
Mar  3 09:53:27.724: INFO: Pod "pod-subpath-test-secret-s8xr": Phase="Running", Reason="", readiness=true. Elapsed: 12.119298998s
Mar  3 09:53:29.731: INFO: Pod "pod-subpath-test-secret-s8xr": Phase="Running", Reason="", readiness=true. Elapsed: 14.126215635s
Mar  3 09:53:31.738: INFO: Pod "pod-subpath-test-secret-s8xr": Phase="Running", Reason="", readiness=true. Elapsed: 16.133166031s
Mar  3 09:53:33.746: INFO: Pod "pod-subpath-test-secret-s8xr": Phase="Running", Reason="", readiness=true. Elapsed: 18.14133861s
Mar  3 09:53:35.749: INFO: Pod "pod-subpath-test-secret-s8xr": Phase="Running", Reason="", readiness=true. Elapsed: 20.144541539s
Mar  3 09:53:37.754: INFO: Pod "pod-subpath-test-secret-s8xr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.14940652s
STEP: Saw pod success
Mar  3 09:53:37.754: INFO: Pod "pod-subpath-test-secret-s8xr" satisfied condition "Succeeded or Failed"
Mar  3 09:53:37.756: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-secret-s8xr container test-container-subpath-secret-s8xr: <nil>
STEP: delete the pod
Mar  3 09:53:37.781: INFO: Waiting for pod pod-subpath-test-secret-s8xr to disappear
Mar  3 09:53:37.785: INFO: Pod pod-subpath-test-secret-s8xr no longer exists
STEP: Deleting pod pod-subpath-test-secret-s8xr
Mar  3 09:53:37.785: INFO: Deleting pod "pod-subpath-test-secret-s8xr" in namespace "subpath-1583"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:53:37.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1583" for this suite.

• [SLOW TEST:23.392 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":207,"skipped":3672,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:53:37.798: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  3 09:53:37.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6139 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Mar  3 09:53:37.966: INFO: stderr: ""
Mar  3 09:53:37.966: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar  3 09:53:43.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6139 get pod e2e-test-httpd-pod -o json'
Mar  3 09:53:43.106: INFO: stderr: ""
Mar  3 09:53:43.106: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.244.226.123/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.226.123/32\"\n        },\n        \"creationTimestamp\": \"2021-03-03T09:53:37Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-03T09:53:37Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-03T09:53:38Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.244.226.123\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-03T09:53:39Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6139\",\n        \"resourceVersion\": \"17948\",\n        \"uid\": \"ac728c97-0af6-4a0d-8df8-fb1328913dd6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-5bkkk\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-5bkkk\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-5bkkk\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-03T09:53:37Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-03T09:53:39Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-03T09:53:39Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-03T09:53:37Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://ab34e4bf4983400932a6e06a213d0cf0d10e359d05f8d3fe91ffa59913aa5958\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-03-03T09:53:38Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.47.86\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.226.123\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.226.123\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-03-03T09:53:37Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar  3 09:53:43.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6139 replace -f -'
Mar  3 09:53:43.431: INFO: stderr: ""
Mar  3 09:53:43.431: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Mar  3 09:53:43.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-6139 delete pods e2e-test-httpd-pod'
Mar  3 09:53:47.698: INFO: stderr: ""
Mar  3 09:53:47.698: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:53:47.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6139" for this suite.

• [SLOW TEST:9.912 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":208,"skipped":3679,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:53:47.712: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  3 09:53:48.385: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 09:53:51.420: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:54:01.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9097" for this suite.
STEP: Destroying namespace "webhook-9097-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:14.068 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":209,"skipped":3698,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:54:01.780: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  3 09:54:02.499: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  3 09:54:04.511: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362042, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362042, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362042, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362042, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 09:54:07.530: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar  3 09:54:07.553: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:54:07.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8346" for this suite.
STEP: Destroying namespace "webhook-8346-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.885 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":210,"skipped":3700,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:54:07.664: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Mar  3 09:54:07.740: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:54:28.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7214" for this suite.

• [SLOW TEST:21.093 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":211,"skipped":3713,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:54:28.758: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:54:28.801: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:54:30.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8770" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":212,"skipped":3718,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:54:30.307: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-5744
STEP: creating replication controller nodeport-test in namespace services-5744
I0303 09:54:30.472385      24 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-5744, replica count: 2
Mar  3 09:54:33.523: INFO: Creating new exec pod
I0303 09:54:33.522976      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  3 09:54:36.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-5744 exec execpod27z5v -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar  3 09:54:36.753: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar  3 09:54:36.753: INFO: stdout: ""
Mar  3 09:54:36.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-5744 exec execpod27z5v -- /bin/sh -x -c nc -zv -t -w 2 10.110.234.63 80'
Mar  3 09:54:37.001: INFO: stderr: "+ nc -zv -t -w 2 10.110.234.63 80\nConnection to 10.110.234.63 80 port [tcp/http] succeeded!\n"
Mar  3 09:54:37.001: INFO: stdout: ""
Mar  3 09:54:37.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-5744 exec execpod27z5v -- /bin/sh -x -c nc -zv -t -w 2 10.0.47.86 30761'
Mar  3 09:54:37.202: INFO: stderr: "+ nc -zv -t -w 2 10.0.47.86 30761\nConnection to 10.0.47.86 30761 port [tcp/30761] succeeded!\n"
Mar  3 09:54:37.202: INFO: stdout: ""
Mar  3 09:54:37.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-5744 exec execpod27z5v -- /bin/sh -x -c nc -zv -t -w 2 10.0.49.5 30761'
Mar  3 09:54:37.390: INFO: stderr: "+ nc -zv -t -w 2 10.0.49.5 30761\nConnection to 10.0.49.5 30761 port [tcp/30761] succeeded!\n"
Mar  3 09:54:37.390: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:54:37.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5744" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:7.094 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":213,"skipped":3725,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:54:37.408: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-4161/configmap-test-937b7af9-f3e8-4020-80cf-7a30df108438
STEP: Creating a pod to test consume configMaps
Mar  3 09:54:37.459: INFO: Waiting up to 5m0s for pod "pod-configmaps-447224f7-6ce7-49a3-a303-16193000fbd7" in namespace "configmap-4161" to be "Succeeded or Failed"
Mar  3 09:54:37.475: INFO: Pod "pod-configmaps-447224f7-6ce7-49a3-a303-16193000fbd7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.66045ms
Mar  3 09:54:39.482: INFO: Pod "pod-configmaps-447224f7-6ce7-49a3-a303-16193000fbd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023583347s
STEP: Saw pod success
Mar  3 09:54:39.482: INFO: Pod "pod-configmaps-447224f7-6ce7-49a3-a303-16193000fbd7" satisfied condition "Succeeded or Failed"
Mar  3 09:54:39.486: INFO: Trying to get logs from node worker-1 pod pod-configmaps-447224f7-6ce7-49a3-a303-16193000fbd7 container env-test: <nil>
STEP: delete the pod
Mar  3 09:54:39.503: INFO: Waiting for pod pod-configmaps-447224f7-6ce7-49a3-a303-16193000fbd7 to disappear
Mar  3 09:54:39.508: INFO: Pod pod-configmaps-447224f7-6ce7-49a3-a303-16193000fbd7 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:54:39.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4161" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":214,"skipped":3740,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:54:39.518: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  3 09:54:39.559: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  3 09:54:39.566: INFO: Waiting for terminating namespaces to be deleted...
Mar  3 09:54:39.568: INFO: 
Logging pods the apiserver thinks is on node controller-0 before test
Mar  3 09:54:39.581: INFO: calico-kube-controllers-5f6546844f-gnl9f from kube-system started at 2021-03-03 08:47:09 +0000 UTC (1 container statuses recorded)
Mar  3 09:54:39.581: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  3 09:54:39.581: INFO: calico-node-468vb from kube-system started at 2021-03-03 08:46:49 +0000 UTC (1 container statuses recorded)
Mar  3 09:54:39.581: INFO: 	Container calico-node ready: true, restart count 0
Mar  3 09:54:39.581: INFO: coredns-5c98d7d4d8-96hpd from kube-system started at 2021-03-03 08:47:11 +0000 UTC (1 container statuses recorded)
Mar  3 09:54:39.581: INFO: 	Container coredns ready: true, restart count 0
Mar  3 09:54:39.581: INFO: konnectivity-agent-882qs from kube-system started at 2021-03-03 08:47:01 +0000 UTC (1 container statuses recorded)
Mar  3 09:54:39.581: INFO: 	Container konnectivity-agent ready: true, restart count 1
Mar  3 09:54:39.581: INFO: kube-proxy-snqpb from kube-system started at 2021-03-03 08:46:30 +0000 UTC (1 container statuses recorded)
Mar  3 09:54:39.581: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  3 09:54:39.581: INFO: metrics-server-6fbcd86f7b-r2c26 from kube-system started at 2021-03-03 08:47:09 +0000 UTC (1 container statuses recorded)
Mar  3 09:54:39.581: INFO: 	Container metrics-server ready: true, restart count 0
Mar  3 09:54:39.581: INFO: execpod27z5v from services-5744 started at 2021-03-03 09:54:33 +0000 UTC (1 container statuses recorded)
Mar  3 09:54:39.581: INFO: 	Container agnhost-container ready: true, restart count 0
Mar  3 09:54:39.581: INFO: sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-8fglb from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 09:54:39.581: INFO: 	Container sonobuoy-worker ready: false, restart count 3
Mar  3 09:54:39.581: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  3 09:54:39.581: INFO: 
Logging pods the apiserver thinks is on node worker-0 before test
Mar  3 09:54:39.589: INFO: calico-node-4vzkv from kube-system started at 2021-03-03 08:50:42 +0000 UTC (1 container statuses recorded)
Mar  3 09:54:39.589: INFO: 	Container calico-node ready: true, restart count 0
Mar  3 09:54:39.589: INFO: konnectivity-agent-hrn26 from kube-system started at 2021-03-03 08:51:02 +0000 UTC (1 container statuses recorded)
Mar  3 09:54:39.589: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  3 09:54:39.589: INFO: kube-proxy-xwt8k from kube-system started at 2021-03-03 08:50:42 +0000 UTC (1 container statuses recorded)
Mar  3 09:54:39.589: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  3 09:54:39.589: INFO: nodeport-test-m4whv from services-5744 started at 2021-03-03 09:54:30 +0000 UTC (1 container statuses recorded)
Mar  3 09:54:39.589: INFO: 	Container nodeport-test ready: true, restart count 0
Mar  3 09:54:39.589: INFO: sonobuoy-e2e-job-a123a876f42443b1 from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 09:54:39.589: INFO: 	Container e2e ready: true, restart count 0
Mar  3 09:54:39.589: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  3 09:54:39.589: INFO: sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-xjvxl from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 09:54:39.589: INFO: 	Container sonobuoy-worker ready: false, restart count 3
Mar  3 09:54:39.589: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  3 09:54:39.589: INFO: 
Logging pods the apiserver thinks is on node worker-1 before test
Mar  3 09:54:39.595: INFO: calico-node-d2bzg from kube-system started at 2021-03-03 08:52:07 +0000 UTC (1 container statuses recorded)
Mar  3 09:54:39.595: INFO: 	Container calico-node ready: true, restart count 0
Mar  3 09:54:39.595: INFO: konnectivity-agent-ssw2j from kube-system started at 2021-03-03 09:38:46 +0000 UTC (1 container statuses recorded)
Mar  3 09:54:39.595: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  3 09:54:39.595: INFO: kube-proxy-c7zpn from kube-system started at 2021-03-03 08:52:07 +0000 UTC (1 container statuses recorded)
Mar  3 09:54:39.595: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  3 09:54:39.595: INFO: nodeport-test-qqttc from services-5744 started at 2021-03-03 09:54:30 +0000 UTC (1 container statuses recorded)
Mar  3 09:54:39.595: INFO: 	Container nodeport-test ready: true, restart count 0
Mar  3 09:54:39.595: INFO: sonobuoy from sonobuoy started at 2021-03-03 08:53:20 +0000 UTC (1 container statuses recorded)
Mar  3 09:54:39.595: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  3 09:54:39.596: INFO: sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-x4pmv from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 09:54:39.597: INFO: 	Container sonobuoy-worker ready: false, restart count 3
Mar  3 09:54:39.597: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a4edb19a-a8d1-4658-a445-6e31c30c825e 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.0.47.86 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.0.47.86 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  3 09:54:49.778: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.47.86 http://127.0.0.1:54321/hostname] Namespace:sched-pred-8433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:54:49.778: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.47.86, port: 54321
Mar  3 09:54:49.864: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.47.86:54321/hostname] Namespace:sched-pred-8433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:54:49.864: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.47.86, port: 54321 UDP
Mar  3 09:54:49.931: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.47.86 54321] Namespace:sched-pred-8433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:54:49.931: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  3 09:54:54.995: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.47.86 http://127.0.0.1:54321/hostname] Namespace:sched-pred-8433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:54:54.995: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.47.86, port: 54321
Mar  3 09:54:55.066: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.47.86:54321/hostname] Namespace:sched-pred-8433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:54:55.066: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.47.86, port: 54321 UDP
Mar  3 09:54:55.138: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.47.86 54321] Namespace:sched-pred-8433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:54:55.139: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  3 09:55:00.210: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.47.86 http://127.0.0.1:54321/hostname] Namespace:sched-pred-8433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:55:00.210: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.47.86, port: 54321
Mar  3 09:55:00.284: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.47.86:54321/hostname] Namespace:sched-pred-8433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:55:00.284: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.47.86, port: 54321 UDP
Mar  3 09:55:00.361: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.47.86 54321] Namespace:sched-pred-8433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:55:00.361: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  3 09:55:05.430: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.47.86 http://127.0.0.1:54321/hostname] Namespace:sched-pred-8433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:55:05.430: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.47.86, port: 54321
Mar  3 09:55:05.499: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.47.86:54321/hostname] Namespace:sched-pred-8433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:55:05.499: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.47.86, port: 54321 UDP
Mar  3 09:55:05.569: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.47.86 54321] Namespace:sched-pred-8433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:55:05.569: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  3 09:55:10.646: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.47.86 http://127.0.0.1:54321/hostname] Namespace:sched-pred-8433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:55:10.647: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.47.86, port: 54321
Mar  3 09:55:10.712: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.47.86:54321/hostname] Namespace:sched-pred-8433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:55:10.712: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.47.86, port: 54321 UDP
Mar  3 09:55:10.793: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.47.86 54321] Namespace:sched-pred-8433 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 09:55:10.794: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: removing the label kubernetes.io/e2e-a4edb19a-a8d1-4658-a445-6e31c30c825e off the node worker-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a4edb19a-a8d1-4658-a445-6e31c30c825e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:55:15.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8433" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:36.378 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":215,"skipped":3801,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:55:15.896: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar  3 09:55:17.064: INFO: starting watch
STEP: patching
STEP: updating
Mar  3 09:55:17.074: INFO: waiting for watch events with expected annotations
Mar  3 09:55:17.074: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:55:17.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-6545" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":216,"skipped":3826,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:55:17.164: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Mar  3 09:55:17.233: INFO: created test-pod-1
Mar  3 09:55:17.246: INFO: created test-pod-2
Mar  3 09:55:17.257: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:55:17.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1699" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":217,"skipped":3845,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:55:17.381: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Mar  3 09:55:17.422: INFO: Waiting up to 5m0s for pod "downward-api-86fa1843-45d8-408d-aefd-fa162a7f9171" in namespace "downward-api-2045" to be "Succeeded or Failed"
Mar  3 09:55:17.429: INFO: Pod "downward-api-86fa1843-45d8-408d-aefd-fa162a7f9171": Phase="Pending", Reason="", readiness=false. Elapsed: 7.020907ms
Mar  3 09:55:19.435: INFO: Pod "downward-api-86fa1843-45d8-408d-aefd-fa162a7f9171": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013085229s
STEP: Saw pod success
Mar  3 09:55:19.435: INFO: Pod "downward-api-86fa1843-45d8-408d-aefd-fa162a7f9171" satisfied condition "Succeeded or Failed"
Mar  3 09:55:19.438: INFO: Trying to get logs from node worker-0 pod downward-api-86fa1843-45d8-408d-aefd-fa162a7f9171 container dapi-container: <nil>
STEP: delete the pod
Mar  3 09:55:19.486: INFO: Waiting for pod downward-api-86fa1843-45d8-408d-aefd-fa162a7f9171 to disappear
Mar  3 09:55:19.489: INFO: Pod downward-api-86fa1843-45d8-408d-aefd-fa162a7f9171 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:55:19.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2045" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":218,"skipped":3943,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:55:19.501: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-gtvj
STEP: Creating a pod to test atomic-volume-subpath
Mar  3 09:55:19.555: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-gtvj" in namespace "subpath-4360" to be "Succeeded or Failed"
Mar  3 09:55:19.567: INFO: Pod "pod-subpath-test-configmap-gtvj": Phase="Pending", Reason="", readiness=false. Elapsed: 11.40109ms
Mar  3 09:55:21.571: INFO: Pod "pod-subpath-test-configmap-gtvj": Phase="Running", Reason="", readiness=true. Elapsed: 2.015614405s
Mar  3 09:55:23.578: INFO: Pod "pod-subpath-test-configmap-gtvj": Phase="Running", Reason="", readiness=true. Elapsed: 4.022916614s
Mar  3 09:55:25.587: INFO: Pod "pod-subpath-test-configmap-gtvj": Phase="Running", Reason="", readiness=true. Elapsed: 6.031486604s
Mar  3 09:55:27.594: INFO: Pod "pod-subpath-test-configmap-gtvj": Phase="Running", Reason="", readiness=true. Elapsed: 8.039301488s
Mar  3 09:55:29.604: INFO: Pod "pod-subpath-test-configmap-gtvj": Phase="Running", Reason="", readiness=true. Elapsed: 10.048361757s
Mar  3 09:55:31.611: INFO: Pod "pod-subpath-test-configmap-gtvj": Phase="Running", Reason="", readiness=true. Elapsed: 12.055964289s
Mar  3 09:55:33.630: INFO: Pod "pod-subpath-test-configmap-gtvj": Phase="Running", Reason="", readiness=true. Elapsed: 14.074751187s
Mar  3 09:55:35.639: INFO: Pod "pod-subpath-test-configmap-gtvj": Phase="Running", Reason="", readiness=true. Elapsed: 16.083690203s
Mar  3 09:55:37.646: INFO: Pod "pod-subpath-test-configmap-gtvj": Phase="Running", Reason="", readiness=true. Elapsed: 18.090906144s
Mar  3 09:55:39.652: INFO: Pod "pod-subpath-test-configmap-gtvj": Phase="Running", Reason="", readiness=true. Elapsed: 20.096521652s
Mar  3 09:55:41.658: INFO: Pod "pod-subpath-test-configmap-gtvj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.1030547s
STEP: Saw pod success
Mar  3 09:55:41.658: INFO: Pod "pod-subpath-test-configmap-gtvj" satisfied condition "Succeeded or Failed"
Mar  3 09:55:41.661: INFO: Trying to get logs from node worker-0 pod pod-subpath-test-configmap-gtvj container test-container-subpath-configmap-gtvj: <nil>
STEP: delete the pod
Mar  3 09:55:41.679: INFO: Waiting for pod pod-subpath-test-configmap-gtvj to disappear
Mar  3 09:55:41.685: INFO: Pod pod-subpath-test-configmap-gtvj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-gtvj
Mar  3 09:55:41.685: INFO: Deleting pod "pod-subpath-test-configmap-gtvj" in namespace "subpath-4360"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:55:41.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4360" for this suite.

• [SLOW TEST:22.196 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":219,"skipped":3965,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:55:41.698: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4685.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4685.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4685.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4685.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4685.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4685.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4685.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4685.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4685.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4685.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4685.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 243.217.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.217.243_udp@PTR;check="$$(dig +tcp +noall +answer +search 243.217.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.217.243_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4685.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4685.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4685.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4685.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4685.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4685.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4685.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4685.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4685.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4685.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4685.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 243.217.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.217.243_udp@PTR;check="$$(dig +tcp +noall +answer +search 243.217.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.217.243_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  3 09:55:43.835: INFO: Unable to read wheezy_udp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:43.840: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:43.846: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:43.853: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:43.882: INFO: Unable to read jessie_udp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:43.885: INFO: Unable to read jessie_tcp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:43.889: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:43.893: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:43.914: INFO: Lookups using dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2 failed for: [wheezy_udp@dns-test-service.dns-4685.svc.cluster.local wheezy_tcp@dns-test-service.dns-4685.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local jessie_udp@dns-test-service.dns-4685.svc.cluster.local jessie_tcp@dns-test-service.dns-4685.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local]

Mar  3 09:55:48.919: INFO: Unable to read wheezy_udp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:48.928: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:48.932: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:48.935: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:48.958: INFO: Unable to read jessie_udp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:48.962: INFO: Unable to read jessie_tcp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:48.965: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:48.969: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:48.987: INFO: Lookups using dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2 failed for: [wheezy_udp@dns-test-service.dns-4685.svc.cluster.local wheezy_tcp@dns-test-service.dns-4685.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local jessie_udp@dns-test-service.dns-4685.svc.cluster.local jessie_tcp@dns-test-service.dns-4685.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local]

Mar  3 09:55:53.918: INFO: Unable to read wheezy_udp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:53.934: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:53.938: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:53.942: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:53.963: INFO: Unable to read jessie_udp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:53.966: INFO: Unable to read jessie_tcp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:53.970: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:53.973: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:53.992: INFO: Lookups using dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2 failed for: [wheezy_udp@dns-test-service.dns-4685.svc.cluster.local wheezy_tcp@dns-test-service.dns-4685.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local jessie_udp@dns-test-service.dns-4685.svc.cluster.local jessie_tcp@dns-test-service.dns-4685.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local]

Mar  3 09:55:58.918: INFO: Unable to read wheezy_udp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:58.927: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:58.931: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:58.934: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:58.955: INFO: Unable to read jessie_udp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:58.959: INFO: Unable to read jessie_tcp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:58.962: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:58.966: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:55:58.985: INFO: Lookups using dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2 failed for: [wheezy_udp@dns-test-service.dns-4685.svc.cluster.local wheezy_tcp@dns-test-service.dns-4685.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local jessie_udp@dns-test-service.dns-4685.svc.cluster.local jessie_tcp@dns-test-service.dns-4685.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local]

Mar  3 09:56:03.919: INFO: Unable to read wheezy_udp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:56:03.923: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:56:03.927: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:56:03.930: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:56:03.953: INFO: Unable to read jessie_udp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:56:03.956: INFO: Unable to read jessie_tcp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:56:03.960: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:56:03.963: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:56:03.982: INFO: Lookups using dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2 failed for: [wheezy_udp@dns-test-service.dns-4685.svc.cluster.local wheezy_tcp@dns-test-service.dns-4685.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local jessie_udp@dns-test-service.dns-4685.svc.cluster.local jessie_tcp@dns-test-service.dns-4685.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local]

Mar  3 09:56:08.919: INFO: Unable to read wheezy_udp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:56:08.936: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:56:08.942: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:56:08.945: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:56:08.969: INFO: Unable to read jessie_udp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:56:08.973: INFO: Unable to read jessie_tcp@dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:56:08.976: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:56:08.980: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local from pod dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2: the server could not find the requested resource (get pods dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2)
Mar  3 09:56:08.998: INFO: Lookups using dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2 failed for: [wheezy_udp@dns-test-service.dns-4685.svc.cluster.local wheezy_tcp@dns-test-service.dns-4685.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local jessie_udp@dns-test-service.dns-4685.svc.cluster.local jessie_tcp@dns-test-service.dns-4685.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4685.svc.cluster.local]

Mar  3 09:56:13.974: INFO: DNS probes using dns-4685/dns-test-e435e323-5797-46d3-b348-0b72f8f9eee2 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:56:14.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4685" for this suite.

• [SLOW TEST:32.483 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":220,"skipped":3973,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:56:14.183: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:56:18.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-933" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":221,"skipped":3996,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:56:18.275: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Mar  3 09:56:18.313: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:56:21.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5431" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":222,"skipped":4021,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:56:21.614: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:56:32.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1232" for this suite.

• [SLOW TEST:11.116 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":223,"skipped":4025,"failed":0}
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:56:32.730: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar  3 09:56:32.789: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3440  75efb474-fc1f-4b6b-8827-508c03643ac9 18996 0 2021-03-03 09:56:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-03 09:56:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  3 09:56:32.789: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3440  75efb474-fc1f-4b6b-8827-508c03643ac9 18997 0 2021-03-03 09:56:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-03 09:56:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  3 09:56:32.790: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3440  75efb474-fc1f-4b6b-8827-508c03643ac9 18998 0 2021-03-03 09:56:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-03 09:56:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar  3 09:56:42.823: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3440  75efb474-fc1f-4b6b-8827-508c03643ac9 19017 0 2021-03-03 09:56:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-03 09:56:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  3 09:56:42.823: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3440  75efb474-fc1f-4b6b-8827-508c03643ac9 19018 0 2021-03-03 09:56:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-03 09:56:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  3 09:56:42.823: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3440  75efb474-fc1f-4b6b-8827-508c03643ac9 19019 0 2021-03-03 09:56:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-03 09:56:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:56:42.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3440" for this suite.

• [SLOW TEST:10.102 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":224,"skipped":4025,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:56:42.832: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:56:42.956: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"a016c528-caa1-4bcd-9fb9-84bf0e4a76b2", Controller:(*bool)(0xc0033c223a), BlockOwnerDeletion:(*bool)(0xc0033c223b)}}
Mar  3 09:56:42.976: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"32df3369-22ab-41d6-832b-0aea40fe1ca7", Controller:(*bool)(0xc0042c8836), BlockOwnerDeletion:(*bool)(0xc0042c8837)}}
Mar  3 09:56:42.983: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"0ecb3a96-d662-477a-82b8-6f1249f8e496", Controller:(*bool)(0xc0042c8a96), BlockOwnerDeletion:(*bool)(0xc0042c8a97)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:56:47.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6539" for this suite.

• [SLOW TEST:5.186 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":225,"skipped":4027,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:56:48.022: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-20c2911c-8b10-461e-977d-4d50b9d08fb3
STEP: Creating a pod to test consume secrets
Mar  3 09:56:48.115: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-35ef2dff-59b2-4555-8140-b58f45510981" in namespace "projected-2970" to be "Succeeded or Failed"
Mar  3 09:56:48.122: INFO: Pod "pod-projected-secrets-35ef2dff-59b2-4555-8140-b58f45510981": Phase="Pending", Reason="", readiness=false. Elapsed: 6.419573ms
Mar  3 09:56:50.128: INFO: Pod "pod-projected-secrets-35ef2dff-59b2-4555-8140-b58f45510981": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013147026s
STEP: Saw pod success
Mar  3 09:56:50.128: INFO: Pod "pod-projected-secrets-35ef2dff-59b2-4555-8140-b58f45510981" satisfied condition "Succeeded or Failed"
Mar  3 09:56:50.131: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-35ef2dff-59b2-4555-8140-b58f45510981 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  3 09:56:50.162: INFO: Waiting for pod pod-projected-secrets-35ef2dff-59b2-4555-8140-b58f45510981 to disappear
Mar  3 09:56:50.164: INFO: Pod pod-projected-secrets-35ef2dff-59b2-4555-8140-b58f45510981 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:56:50.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2970" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":226,"skipped":4032,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:56:50.175: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:56:50.214: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Creating first CR 
Mar  3 09:56:50.810: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-03T09:56:50Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-03T09:56:50Z]] name:name1 resourceVersion:19113 uid:c9134adb-8fa5-45d5-825b-b595509bcfe3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar  3 09:57:00.822: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-03T09:57:00Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-03T09:57:00Z]] name:name2 resourceVersion:19146 uid:98d951cd-0fa1-40d9-97f1-ee3a8bfca27d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar  3 09:57:10.838: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-03T09:56:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-03T09:57:10Z]] name:name1 resourceVersion:19157 uid:c9134adb-8fa5-45d5-825b-b595509bcfe3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar  3 09:57:20.850: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-03T09:57:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-03T09:57:20Z]] name:name2 resourceVersion:19163 uid:98d951cd-0fa1-40d9-97f1-ee3a8bfca27d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar  3 09:57:30.861: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-03T09:56:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-03T09:57:10Z]] name:name1 resourceVersion:19168 uid:c9134adb-8fa5-45d5-825b-b595509bcfe3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar  3 09:57:40.876: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-03T09:57:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-03T09:57:20Z]] name:name2 resourceVersion:19175 uid:98d951cd-0fa1-40d9-97f1-ee3a8bfca27d] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:57:51.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5473" for this suite.

• [SLOW TEST:61.236 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":227,"skipped":4034,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:57:51.412: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Mar  3 09:57:51.492: INFO: Waiting up to 5m0s for pod "var-expansion-2ddb530a-3ad4-4718-b77d-aa885f434b8e" in namespace "var-expansion-3256" to be "Succeeded or Failed"
Mar  3 09:57:51.497: INFO: Pod "var-expansion-2ddb530a-3ad4-4718-b77d-aa885f434b8e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.184791ms
Mar  3 09:57:53.505: INFO: Pod "var-expansion-2ddb530a-3ad4-4718-b77d-aa885f434b8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012723972s
STEP: Saw pod success
Mar  3 09:57:53.505: INFO: Pod "var-expansion-2ddb530a-3ad4-4718-b77d-aa885f434b8e" satisfied condition "Succeeded or Failed"
Mar  3 09:57:53.507: INFO: Trying to get logs from node worker-1 pod var-expansion-2ddb530a-3ad4-4718-b77d-aa885f434b8e container dapi-container: <nil>
STEP: delete the pod
Mar  3 09:57:53.526: INFO: Waiting for pod var-expansion-2ddb530a-3ad4-4718-b77d-aa885f434b8e to disappear
Mar  3 09:57:53.529: INFO: Pod var-expansion-2ddb530a-3ad4-4718-b77d-aa885f434b8e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:57:53.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3256" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":228,"skipped":4078,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:57:53.545: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  3 09:57:54.000: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362273, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362273, loc:(*time.Location)(0x797de40)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-6bd9446d55\""}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 09:57:57.037: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 09:57:57.044: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4964-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:57:58.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8610" for this suite.
STEP: Destroying namespace "webhook-8610-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.094 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":229,"skipped":4086,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:57:58.639: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0303 09:58:09.048004      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0303 09:58:09.048130      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0303 09:58:09.048158      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  3 09:58:09.048: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar  3 09:58:09.048: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jmkz" in namespace "gc-5132"
Mar  3 09:58:09.099: INFO: Deleting pod "simpletest-rc-to-be-deleted-87snx" in namespace "gc-5132"
Mar  3 09:58:09.131: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ddv2" in namespace "gc-5132"
Mar  3 09:58:09.253: INFO: Deleting pod "simpletest-rc-to-be-deleted-h5zj2" in namespace "gc-5132"
Mar  3 09:58:09.399: INFO: Deleting pod "simpletest-rc-to-be-deleted-ldzbx" in namespace "gc-5132"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:58:09.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5132" for this suite.

• [SLOW TEST:10.884 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":230,"skipped":4102,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:58:09.524: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:58:11.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1185" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":231,"skipped":4112,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:58:11.897: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Mar  3 09:58:11.944: INFO: created test-podtemplate-1
Mar  3 09:58:11.947: INFO: created test-podtemplate-2
Mar  3 09:58:11.951: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Mar  3 09:58:11.955: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Mar  3 09:58:11.967: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:58:11.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-188" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":232,"skipped":4127,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:58:11.983: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-885659d1-f9fc-4f37-8212-40c803323f48
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-885659d1-f9fc-4f37-8212-40c803323f48
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:59:36.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9644" for this suite.

• [SLOW TEST:84.411 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":233,"skipped":4142,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:59:36.394: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-5755547e-e2ed-439c-b30a-059fd7fe7269
STEP: Creating a pod to test consume secrets
Mar  3 09:59:36.452: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ae95e447-ad58-4c6f-a120-5b04298a3945" in namespace "projected-1220" to be "Succeeded or Failed"
Mar  3 09:59:36.460: INFO: Pod "pod-projected-secrets-ae95e447-ad58-4c6f-a120-5b04298a3945": Phase="Pending", Reason="", readiness=false. Elapsed: 7.174821ms
Mar  3 09:59:38.466: INFO: Pod "pod-projected-secrets-ae95e447-ad58-4c6f-a120-5b04298a3945": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014030454s
STEP: Saw pod success
Mar  3 09:59:38.466: INFO: Pod "pod-projected-secrets-ae95e447-ad58-4c6f-a120-5b04298a3945" satisfied condition "Succeeded or Failed"
Mar  3 09:59:38.469: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-ae95e447-ad58-4c6f-a120-5b04298a3945 container secret-volume-test: <nil>
STEP: delete the pod
Mar  3 09:59:38.488: INFO: Waiting for pod pod-projected-secrets-ae95e447-ad58-4c6f-a120-5b04298a3945 to disappear
Mar  3 09:59:38.497: INFO: Pod pod-projected-secrets-ae95e447-ad58-4c6f-a120-5b04298a3945 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:59:38.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1220" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":234,"skipped":4150,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:59:38.506: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:59:38.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7712" for this suite.
STEP: Destroying namespace "nspatchtest-8e10bfa3-2052-4acb-a52b-8137b2c9fa92-9523" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":235,"skipped":4169,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:59:38.601: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Mar  3 09:59:38.637: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:59:40.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2242" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":236,"skipped":4188,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:59:40.935: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Mar  3 09:59:51.065: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:59:51.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0303 09:59:51.064960      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0303 09:59:51.064989      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0303 09:59:51.064996      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-2076" for this suite.

• [SLOW TEST:10.143 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":237,"skipped":4196,"failed":0}
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:59:51.079: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-150c8343-957c-4868-ac6d-658c6779a7df
STEP: Creating secret with name secret-projected-all-test-volume-c40ad485-f8c5-4e95-a7d5-6fd969ef7e05
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar  3 09:59:51.138: INFO: Waiting up to 5m0s for pod "projected-volume-8ec86830-0ce4-41b7-93c2-9acc2295d105" in namespace "projected-2255" to be "Succeeded or Failed"
Mar  3 09:59:51.150: INFO: Pod "projected-volume-8ec86830-0ce4-41b7-93c2-9acc2295d105": Phase="Pending", Reason="", readiness=false. Elapsed: 12.196805ms
Mar  3 09:59:53.159: INFO: Pod "projected-volume-8ec86830-0ce4-41b7-93c2-9acc2295d105": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020997369s
STEP: Saw pod success
Mar  3 09:59:53.159: INFO: Pod "projected-volume-8ec86830-0ce4-41b7-93c2-9acc2295d105" satisfied condition "Succeeded or Failed"
Mar  3 09:59:53.163: INFO: Trying to get logs from node worker-1 pod projected-volume-8ec86830-0ce4-41b7-93c2-9acc2295d105 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar  3 09:59:53.190: INFO: Waiting for pod projected-volume-8ec86830-0ce4-41b7-93c2-9acc2295d105 to disappear
Mar  3 09:59:53.193: INFO: Pod projected-volume-8ec86830-0ce4-41b7-93c2-9acc2295d105 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 09:59:53.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2255" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":238,"skipped":4197,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 09:59:53.205: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:00:21.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2152" for this suite.

• [SLOW TEST:28.131 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":239,"skipped":4220,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:00:21.336: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:00:24.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2075" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":240,"skipped":4225,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:00:24.449: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Mar  3 10:00:24.504: INFO: created test-event-1
Mar  3 10:00:24.507: INFO: created test-event-2
Mar  3 10:00:24.511: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Mar  3 10:00:24.514: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Mar  3 10:00:24.526: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:00:24.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-591" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":241,"skipped":4233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:00:24.539: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 10:00:24.606: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c8a77f9f-dd78-43cf-a3bf-20dac8d15b7f" in namespace "projected-8512" to be "Succeeded or Failed"
Mar  3 10:00:24.621: INFO: Pod "downwardapi-volume-c8a77f9f-dd78-43cf-a3bf-20dac8d15b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.953826ms
Mar  3 10:00:26.628: INFO: Pod "downwardapi-volume-c8a77f9f-dd78-43cf-a3bf-20dac8d15b7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021488866s
STEP: Saw pod success
Mar  3 10:00:26.628: INFO: Pod "downwardapi-volume-c8a77f9f-dd78-43cf-a3bf-20dac8d15b7f" satisfied condition "Succeeded or Failed"
Mar  3 10:00:26.631: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-c8a77f9f-dd78-43cf-a3bf-20dac8d15b7f container client-container: <nil>
STEP: delete the pod
Mar  3 10:00:26.652: INFO: Waiting for pod downwardapi-volume-c8a77f9f-dd78-43cf-a3bf-20dac8d15b7f to disappear
Mar  3 10:00:26.655: INFO: Pod downwardapi-volume-c8a77f9f-dd78-43cf-a3bf-20dac8d15b7f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:00:26.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8512" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":242,"skipped":4255,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:00:26.666: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar  3 10:00:28.805: INFO: &Pod{ObjectMeta:{send-events-f9206b74-17c4-4de1-a40b-aa4e9cb947bb  events-8348  cc920143-74a0-4fff-80fb-c93aa9221757 20103 0 2021-03-03 10:00:26 +0000 UTC <nil> <nil> map[name:foo time:768759297] map[cni.projectcalico.org/podIP:10.244.226.89/32 cni.projectcalico.org/podIPs:10.244.226.89/32] [] []  [{e2e.test Update v1 2021-03-03 10:00:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-03 10:00:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-03 10:00:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.226.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lqghg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lqghg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lqghg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 10:00:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 10:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 10:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 10:00:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.47.86,PodIP:10.244.226.89,StartTime:2021-03-03 10:00:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-03 10:00:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://4cfa565287be955e1222c35c0ebb490efc42a142784a3e2293cb1872a80dee0a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.226.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar  3 10:00:30.811: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar  3 10:00:32.819: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:00:32.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8348" for this suite.

• [SLOW TEST:6.177 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":243,"skipped":4258,"failed":0}
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:00:32.843: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-p2r7p in namespace proxy-8980
I0303 10:00:32.965922      24 runners.go:190] Created replication controller with name: proxy-service-p2r7p, namespace: proxy-8980, replica count: 1
I0303 10:00:34.016367      24 runners.go:190] proxy-service-p2r7p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0303 10:00:35.016861      24 runners.go:190] proxy-service-p2r7p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0303 10:00:36.017122      24 runners.go:190] proxy-service-p2r7p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0303 10:00:37.017412      24 runners.go:190] proxy-service-p2r7p Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  3 10:00:37.044: INFO: setup took 4.154839827s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar  3 10:00:37.091: INFO: (0) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 46.283559ms)
Mar  3 10:00:37.091: INFO: (0) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 46.121249ms)
Mar  3 10:00:37.100: INFO: (0) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 55.279134ms)
Mar  3 10:00:37.111: INFO: (0) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 66.222077ms)
Mar  3 10:00:37.118: INFO: (0) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 72.496473ms)
Mar  3 10:00:37.119: INFO: (0) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 73.865274ms)
Mar  3 10:00:37.118: INFO: (0) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 73.004633ms)
Mar  3 10:00:37.124: INFO: (0) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 78.230047ms)
Mar  3 10:00:37.129: INFO: (0) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 83.510731ms)
Mar  3 10:00:37.129: INFO: (0) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 84.349053ms)
Mar  3 10:00:37.138: INFO: (0) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 92.458796ms)
Mar  3 10:00:37.138: INFO: (0) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 92.621371ms)
Mar  3 10:00:37.138: INFO: (0) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 92.989767ms)
Mar  3 10:00:37.156: INFO: (0) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 109.952775ms)
Mar  3 10:00:37.157: INFO: (0) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 110.787056ms)
Mar  3 10:00:37.157: INFO: (0) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 112.192594ms)
Mar  3 10:00:37.193: INFO: (1) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 34.850975ms)
Mar  3 10:00:37.193: INFO: (1) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 34.334668ms)
Mar  3 10:00:37.194: INFO: (1) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 34.491144ms)
Mar  3 10:00:37.202: INFO: (1) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 43.315171ms)
Mar  3 10:00:37.202: INFO: (1) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 43.864523ms)
Mar  3 10:00:37.202: INFO: (1) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 44.128766ms)
Mar  3 10:00:37.202: INFO: (1) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 43.079188ms)
Mar  3 10:00:37.202: INFO: (1) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 43.276754ms)
Mar  3 10:00:37.208: INFO: (1) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 49.98166ms)
Mar  3 10:00:37.208: INFO: (1) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 49.191974ms)
Mar  3 10:00:37.208: INFO: (1) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 49.504531ms)
Mar  3 10:00:37.208: INFO: (1) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 49.115958ms)
Mar  3 10:00:37.208: INFO: (1) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 49.257523ms)
Mar  3 10:00:37.208: INFO: (1) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 49.332918ms)
Mar  3 10:00:37.208: INFO: (1) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 50.003039ms)
Mar  3 10:00:37.208: INFO: (1) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 50.485409ms)
Mar  3 10:00:37.259: INFO: (2) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 48.665122ms)
Mar  3 10:00:37.264: INFO: (2) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 53.245956ms)
Mar  3 10:00:37.264: INFO: (2) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 53.928485ms)
Mar  3 10:00:37.265: INFO: (2) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 54.270923ms)
Mar  3 10:00:37.265: INFO: (2) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 55.493449ms)
Mar  3 10:00:37.265: INFO: (2) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 54.192073ms)
Mar  3 10:00:37.265: INFO: (2) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 55.336941ms)
Mar  3 10:00:37.271: INFO: (2) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 61.848746ms)
Mar  3 10:00:37.279: INFO: (2) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 68.987333ms)
Mar  3 10:00:37.279: INFO: (2) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 68.953387ms)
Mar  3 10:00:37.279: INFO: (2) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 68.645584ms)
Mar  3 10:00:37.279: INFO: (2) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 68.821351ms)
Mar  3 10:00:37.279: INFO: (2) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 69.21341ms)
Mar  3 10:00:37.280: INFO: (2) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 69.724125ms)
Mar  3 10:00:37.292: INFO: (2) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 82.026717ms)
Mar  3 10:00:37.292: INFO: (2) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 82.313253ms)
Mar  3 10:00:37.304: INFO: (3) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 11.881462ms)
Mar  3 10:00:37.322: INFO: (3) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 28.929348ms)
Mar  3 10:00:37.325: INFO: (3) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 31.730969ms)
Mar  3 10:00:37.326: INFO: (3) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 31.569943ms)
Mar  3 10:00:37.326: INFO: (3) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 32.549426ms)
Mar  3 10:00:37.331: INFO: (3) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 37.024552ms)
Mar  3 10:00:37.331: INFO: (3) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 36.951774ms)
Mar  3 10:00:37.339: INFO: (3) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 45.191995ms)
Mar  3 10:00:37.339: INFO: (3) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 46.019966ms)
Mar  3 10:00:37.339: INFO: (3) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 44.888792ms)
Mar  3 10:00:37.345: INFO: (3) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 51.894992ms)
Mar  3 10:00:37.345: INFO: (3) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 51.211523ms)
Mar  3 10:00:37.345: INFO: (3) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 51.558488ms)
Mar  3 10:00:37.345: INFO: (3) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 51.874351ms)
Mar  3 10:00:37.345: INFO: (3) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 51.07237ms)
Mar  3 10:00:37.345: INFO: (3) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 52.526721ms)
Mar  3 10:00:37.366: INFO: (4) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 19.635724ms)
Mar  3 10:00:37.378: INFO: (4) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 31.924409ms)
Mar  3 10:00:37.380: INFO: (4) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 34.069157ms)
Mar  3 10:00:37.380: INFO: (4) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 34.146332ms)
Mar  3 10:00:37.380: INFO: (4) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 34.459823ms)
Mar  3 10:00:37.380: INFO: (4) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 34.695685ms)
Mar  3 10:00:37.380: INFO: (4) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 34.385776ms)
Mar  3 10:00:37.386: INFO: (4) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 40.131264ms)
Mar  3 10:00:37.389: INFO: (4) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 43.046805ms)
Mar  3 10:00:37.389: INFO: (4) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 43.135855ms)
Mar  3 10:00:37.389: INFO: (4) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 43.205809ms)
Mar  3 10:00:37.389: INFO: (4) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 42.853118ms)
Mar  3 10:00:37.389: INFO: (4) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 43.068227ms)
Mar  3 10:00:37.389: INFO: (4) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 43.443515ms)
Mar  3 10:00:37.389: INFO: (4) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 43.467142ms)
Mar  3 10:00:37.392: INFO: (4) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 45.980207ms)
Mar  3 10:00:37.411: INFO: (5) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 17.826973ms)
Mar  3 10:00:37.411: INFO: (5) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 18.202558ms)
Mar  3 10:00:37.412: INFO: (5) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 19.22401ms)
Mar  3 10:00:37.412: INFO: (5) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 18.471514ms)
Mar  3 10:00:37.414: INFO: (5) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 21.38486ms)
Mar  3 10:00:37.414: INFO: (5) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 20.945572ms)
Mar  3 10:00:37.414: INFO: (5) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 21.832245ms)
Mar  3 10:00:37.416: INFO: (5) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 22.666978ms)
Mar  3 10:00:37.421: INFO: (5) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 27.812528ms)
Mar  3 10:00:37.421: INFO: (5) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 28.511346ms)
Mar  3 10:00:37.422: INFO: (5) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 28.84135ms)
Mar  3 10:00:37.425: INFO: (5) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 31.486534ms)
Mar  3 10:00:37.426: INFO: (5) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 32.438046ms)
Mar  3 10:00:37.428: INFO: (5) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 35.194382ms)
Mar  3 10:00:37.428: INFO: (5) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 35.274222ms)
Mar  3 10:00:37.429: INFO: (5) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 36.364077ms)
Mar  3 10:00:37.438: INFO: (6) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 7.985081ms)
Mar  3 10:00:37.438: INFO: (6) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 8.817458ms)
Mar  3 10:00:37.441: INFO: (6) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 11.588824ms)
Mar  3 10:00:37.452: INFO: (6) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 21.709341ms)
Mar  3 10:00:37.469: INFO: (6) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 38.877311ms)
Mar  3 10:00:37.469: INFO: (6) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 38.907086ms)
Mar  3 10:00:37.469: INFO: (6) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 38.855093ms)
Mar  3 10:00:37.469: INFO: (6) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 39.133825ms)
Mar  3 10:00:37.469: INFO: (6) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 39.545399ms)
Mar  3 10:00:37.470: INFO: (6) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 39.866428ms)
Mar  3 10:00:37.470: INFO: (6) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 39.961797ms)
Mar  3 10:00:37.470: INFO: (6) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 39.86705ms)
Mar  3 10:00:37.470: INFO: (6) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 39.682575ms)
Mar  3 10:00:37.470: INFO: (6) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 40.122907ms)
Mar  3 10:00:37.472: INFO: (6) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 42.105017ms)
Mar  3 10:00:37.473: INFO: (6) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 42.797686ms)
Mar  3 10:00:37.480: INFO: (7) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 7.110367ms)
Mar  3 10:00:37.483: INFO: (7) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 9.629217ms)
Mar  3 10:00:37.484: INFO: (7) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 10.111423ms)
Mar  3 10:00:37.492: INFO: (7) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 17.802451ms)
Mar  3 10:00:37.492: INFO: (7) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 18.378751ms)
Mar  3 10:00:37.492: INFO: (7) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 17.972258ms)
Mar  3 10:00:37.494: INFO: (7) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 19.712775ms)
Mar  3 10:00:37.506: INFO: (7) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 31.845093ms)
Mar  3 10:00:37.506: INFO: (7) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 31.778238ms)
Mar  3 10:00:37.509: INFO: (7) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 34.943307ms)
Mar  3 10:00:37.509: INFO: (7) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 35.001115ms)
Mar  3 10:00:37.515: INFO: (7) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 41.116061ms)
Mar  3 10:00:37.515: INFO: (7) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 41.375966ms)
Mar  3 10:00:37.517: INFO: (7) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 42.220422ms)
Mar  3 10:00:37.517: INFO: (7) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 42.225659ms)
Mar  3 10:00:37.517: INFO: (7) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 42.311855ms)
Mar  3 10:00:37.533: INFO: (8) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 15.088031ms)
Mar  3 10:00:37.543: INFO: (8) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 25.702119ms)
Mar  3 10:00:37.543: INFO: (8) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 25.210887ms)
Mar  3 10:00:37.554: INFO: (8) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 35.683595ms)
Mar  3 10:00:37.554: INFO: (8) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 35.707056ms)
Mar  3 10:00:37.554: INFO: (8) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 36.238972ms)
Mar  3 10:00:37.557: INFO: (8) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 38.836877ms)
Mar  3 10:00:37.558: INFO: (8) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 40.297849ms)
Mar  3 10:00:37.558: INFO: (8) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 39.454665ms)
Mar  3 10:00:37.558: INFO: (8) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 39.498006ms)
Mar  3 10:00:37.558: INFO: (8) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 39.279824ms)
Mar  3 10:00:37.566: INFO: (8) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 47.461875ms)
Mar  3 10:00:37.568: INFO: (8) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 49.716098ms)
Mar  3 10:00:37.568: INFO: (8) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 50.11275ms)
Mar  3 10:00:37.568: INFO: (8) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 49.821871ms)
Mar  3 10:00:37.568: INFO: (8) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 50.022324ms)
Mar  3 10:00:37.596: INFO: (9) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 27.087224ms)
Mar  3 10:00:37.597: INFO: (9) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 27.661774ms)
Mar  3 10:00:37.601: INFO: (9) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 32.042135ms)
Mar  3 10:00:37.602: INFO: (9) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 31.919368ms)
Mar  3 10:00:37.602: INFO: (9) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 31.426882ms)
Mar  3 10:00:37.606: INFO: (9) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 36.484186ms)
Mar  3 10:00:37.606: INFO: (9) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 36.703572ms)
Mar  3 10:00:37.606: INFO: (9) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 37.833193ms)
Mar  3 10:00:37.607: INFO: (9) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 38.050204ms)
Mar  3 10:00:37.609: INFO: (9) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 38.728386ms)
Mar  3 10:00:37.609: INFO: (9) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 38.830648ms)
Mar  3 10:00:37.615: INFO: (9) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 45.546498ms)
Mar  3 10:00:37.615: INFO: (9) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 45.699241ms)
Mar  3 10:00:37.615: INFO: (9) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 46.173926ms)
Mar  3 10:00:37.619: INFO: (9) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 48.681095ms)
Mar  3 10:00:37.621: INFO: (9) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 50.924361ms)
Mar  3 10:00:37.666: INFO: (10) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 44.377677ms)
Mar  3 10:00:37.668: INFO: (10) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 47.299436ms)
Mar  3 10:00:37.668: INFO: (10) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 47.198887ms)
Mar  3 10:00:37.670: INFO: (10) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 48.689891ms)
Mar  3 10:00:37.670: INFO: (10) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 48.891826ms)
Mar  3 10:00:37.670: INFO: (10) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 49.147604ms)
Mar  3 10:00:37.670: INFO: (10) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 49.073107ms)
Mar  3 10:00:37.670: INFO: (10) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 48.956824ms)
Mar  3 10:00:37.670: INFO: (10) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 49.076974ms)
Mar  3 10:00:37.677: INFO: (10) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 55.671865ms)
Mar  3 10:00:37.678: INFO: (10) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 57.056096ms)
Mar  3 10:00:37.678: INFO: (10) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 56.795915ms)
Mar  3 10:00:37.678: INFO: (10) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 56.955067ms)
Mar  3 10:00:37.678: INFO: (10) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 56.930997ms)
Mar  3 10:00:37.682: INFO: (10) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 61.225971ms)
Mar  3 10:00:37.683: INFO: (10) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 62.653809ms)
Mar  3 10:00:37.709: INFO: (11) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 25.132822ms)
Mar  3 10:00:37.710: INFO: (11) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 26.205543ms)
Mar  3 10:00:37.712: INFO: (11) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 27.820734ms)
Mar  3 10:00:37.712: INFO: (11) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 27.867785ms)
Mar  3 10:00:37.712: INFO: (11) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 28.375945ms)
Mar  3 10:00:37.712: INFO: (11) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 28.131775ms)
Mar  3 10:00:37.730: INFO: (11) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 46.366581ms)
Mar  3 10:00:37.733: INFO: (11) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 48.473799ms)
Mar  3 10:00:37.733: INFO: (11) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 48.795545ms)
Mar  3 10:00:37.733: INFO: (11) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 48.457617ms)
Mar  3 10:00:37.733: INFO: (11) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 48.728873ms)
Mar  3 10:00:37.733: INFO: (11) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 48.573295ms)
Mar  3 10:00:37.733: INFO: (11) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 48.930143ms)
Mar  3 10:00:37.733: INFO: (11) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 48.972784ms)
Mar  3 10:00:37.734: INFO: (11) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 49.444391ms)
Mar  3 10:00:37.742: INFO: (11) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 57.939797ms)
Mar  3 10:00:37.771: INFO: (12) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 29.274098ms)
Mar  3 10:00:37.786: INFO: (12) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 43.108337ms)
Mar  3 10:00:37.787: INFO: (12) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 44.60667ms)
Mar  3 10:00:37.787: INFO: (12) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 44.971174ms)
Mar  3 10:00:37.787: INFO: (12) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 44.891237ms)
Mar  3 10:00:37.787: INFO: (12) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 44.874073ms)
Mar  3 10:00:37.792: INFO: (12) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 48.725156ms)
Mar  3 10:00:37.792: INFO: (12) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 48.989998ms)
Mar  3 10:00:37.792: INFO: (12) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 49.096255ms)
Mar  3 10:00:37.793: INFO: (12) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 50.001566ms)
Mar  3 10:00:37.793: INFO: (12) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 50.488044ms)
Mar  3 10:00:37.799: INFO: (12) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 56.161083ms)
Mar  3 10:00:37.799: INFO: (12) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 55.977574ms)
Mar  3 10:00:37.799: INFO: (12) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 55.98065ms)
Mar  3 10:00:37.803: INFO: (12) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 61.108681ms)
Mar  3 10:00:37.806: INFO: (12) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 64.121523ms)
Mar  3 10:00:37.827: INFO: (13) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 20.923918ms)
Mar  3 10:00:37.837: INFO: (13) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 30.493843ms)
Mar  3 10:00:37.853: INFO: (13) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 46.204283ms)
Mar  3 10:00:37.853: INFO: (13) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 46.231194ms)
Mar  3 10:00:37.853: INFO: (13) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 46.356567ms)
Mar  3 10:00:37.853: INFO: (13) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 46.238336ms)
Mar  3 10:00:37.853: INFO: (13) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 46.285089ms)
Mar  3 10:00:37.854: INFO: (13) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 47.368886ms)
Mar  3 10:00:37.854: INFO: (13) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 47.658687ms)
Mar  3 10:00:37.855: INFO: (13) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 48.806821ms)
Mar  3 10:00:37.855: INFO: (13) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 48.450031ms)
Mar  3 10:00:37.856: INFO: (13) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 49.057766ms)
Mar  3 10:00:37.862: INFO: (13) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 55.089709ms)
Mar  3 10:00:37.862: INFO: (13) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 55.755466ms)
Mar  3 10:00:37.864: INFO: (13) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 56.840275ms)
Mar  3 10:00:37.864: INFO: (13) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 57.313972ms)
Mar  3 10:00:37.897: INFO: (14) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 31.564224ms)
Mar  3 10:00:37.898: INFO: (14) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 32.29095ms)
Mar  3 10:00:37.899: INFO: (14) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 33.224744ms)
Mar  3 10:00:37.899: INFO: (14) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 33.366904ms)
Mar  3 10:00:37.901: INFO: (14) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 35.800814ms)
Mar  3 10:00:37.901: INFO: (14) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 36.524183ms)
Mar  3 10:00:37.901: INFO: (14) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 35.63068ms)
Mar  3 10:00:37.901: INFO: (14) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 35.341895ms)
Mar  3 10:00:37.920: INFO: (14) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 53.4307ms)
Mar  3 10:00:37.921: INFO: (14) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 54.616773ms)
Mar  3 10:00:37.921: INFO: (14) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 56.081705ms)
Mar  3 10:00:37.921: INFO: (14) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 55.169476ms)
Mar  3 10:00:37.922: INFO: (14) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 55.587032ms)
Mar  3 10:00:37.922: INFO: (14) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 56.108338ms)
Mar  3 10:00:37.922: INFO: (14) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 56.018967ms)
Mar  3 10:00:37.937: INFO: (14) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 71.798944ms)
Mar  3 10:00:37.969: INFO: (15) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 30.203195ms)
Mar  3 10:00:37.969: INFO: (15) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 31.331128ms)
Mar  3 10:00:37.969: INFO: (15) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 30.56738ms)
Mar  3 10:00:37.969: INFO: (15) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 31.900327ms)
Mar  3 10:00:37.978: INFO: (15) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 39.412746ms)
Mar  3 10:00:37.983: INFO: (15) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 44.455284ms)
Mar  3 10:00:37.983: INFO: (15) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 44.452044ms)
Mar  3 10:00:37.985: INFO: (15) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 46.644278ms)
Mar  3 10:00:37.987: INFO: (15) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 49.086477ms)
Mar  3 10:00:37.988: INFO: (15) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 50.771719ms)
Mar  3 10:00:37.996: INFO: (15) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 57.766379ms)
Mar  3 10:00:37.996: INFO: (15) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 57.847501ms)
Mar  3 10:00:37.998: INFO: (15) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 58.829828ms)
Mar  3 10:00:38.002: INFO: (15) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 62.969841ms)
Mar  3 10:00:38.002: INFO: (15) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 62.853772ms)
Mar  3 10:00:38.003: INFO: (15) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 64.698538ms)
Mar  3 10:00:38.018: INFO: (16) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 14.077055ms)
Mar  3 10:00:38.018: INFO: (16) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 13.797904ms)
Mar  3 10:00:38.029: INFO: (16) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 25.192372ms)
Mar  3 10:00:38.038: INFO: (16) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 33.104026ms)
Mar  3 10:00:38.038: INFO: (16) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 33.268269ms)
Mar  3 10:00:38.038: INFO: (16) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 34.006529ms)
Mar  3 10:00:38.043: INFO: (16) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 38.523051ms)
Mar  3 10:00:38.048: INFO: (16) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 43.228061ms)
Mar  3 10:00:38.048: INFO: (16) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 43.535707ms)
Mar  3 10:00:38.048: INFO: (16) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 43.170846ms)
Mar  3 10:00:38.049: INFO: (16) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 45.185033ms)
Mar  3 10:00:38.049: INFO: (16) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 44.227134ms)
Mar  3 10:00:38.050: INFO: (16) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 45.120558ms)
Mar  3 10:00:38.050: INFO: (16) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 45.387963ms)
Mar  3 10:00:38.055: INFO: (16) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 49.767641ms)
Mar  3 10:00:38.055: INFO: (16) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 49.905688ms)
Mar  3 10:00:38.096: INFO: (17) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 38.852844ms)
Mar  3 10:00:38.097: INFO: (17) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 40.193907ms)
Mar  3 10:00:38.100: INFO: (17) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 42.71975ms)
Mar  3 10:00:38.100: INFO: (17) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 43.132846ms)
Mar  3 10:00:38.100: INFO: (17) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 41.95795ms)
Mar  3 10:00:38.100: INFO: (17) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 42.667026ms)
Mar  3 10:00:38.102: INFO: (17) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 44.173757ms)
Mar  3 10:00:38.107: INFO: (17) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 49.369441ms)
Mar  3 10:00:38.117: INFO: (17) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 59.175422ms)
Mar  3 10:00:38.117: INFO: (17) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 60.522959ms)
Mar  3 10:00:38.117: INFO: (17) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 60.012089ms)
Mar  3 10:00:38.125: INFO: (17) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 67.398383ms)
Mar  3 10:00:38.125: INFO: (17) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 68.053612ms)
Mar  3 10:00:38.129: INFO: (17) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 71.619959ms)
Mar  3 10:00:38.131: INFO: (17) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 73.10198ms)
Mar  3 10:00:38.133: INFO: (17) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 74.823216ms)
Mar  3 10:00:38.163: INFO: (18) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 29.618889ms)
Mar  3 10:00:38.167: INFO: (18) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 33.734074ms)
Mar  3 10:00:38.167: INFO: (18) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 34.252225ms)
Mar  3 10:00:38.167: INFO: (18) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 34.201283ms)
Mar  3 10:00:38.167: INFO: (18) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 33.944035ms)
Mar  3 10:00:38.168: INFO: (18) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 35.003643ms)
Mar  3 10:00:38.168: INFO: (18) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 34.855383ms)
Mar  3 10:00:38.168: INFO: (18) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 35.00264ms)
Mar  3 10:00:38.169: INFO: (18) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 36.054147ms)
Mar  3 10:00:38.171: INFO: (18) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 37.932707ms)
Mar  3 10:00:38.172: INFO: (18) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 39.267715ms)
Mar  3 10:00:38.172: INFO: (18) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 39.343042ms)
Mar  3 10:00:38.193: INFO: (18) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 60.299499ms)
Mar  3 10:00:38.193: INFO: (18) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 60.576972ms)
Mar  3 10:00:38.194: INFO: (18) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 61.384032ms)
Mar  3 10:00:38.196: INFO: (18) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 62.763618ms)
Mar  3 10:00:38.233: INFO: (19) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname2/proxy/: bar (200; 36.729283ms)
Mar  3 10:00:38.233: INFO: (19) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:443/proxy/tlsrewritem... (200; 36.6761ms)
Mar  3 10:00:38.233: INFO: (19) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:462/proxy/: tls qux (200; 36.894528ms)
Mar  3 10:00:38.233: INFO: (19) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname2/proxy/: tls qux (200; 37.125166ms)
Mar  3 10:00:38.233: INFO: (19) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">... (200; 36.924767ms)
Mar  3 10:00:38.234: INFO: (19) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:1080/proxy/rewriteme">test<... (200; 37.464334ms)
Mar  3 10:00:38.234: INFO: (19) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname1/proxy/: foo (200; 37.8089ms)
Mar  3 10:00:38.237: INFO: (19) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 40.828038ms)
Mar  3 10:00:38.238: INFO: (19) /api/v1/namespaces/proxy-8980/services/http:proxy-service-p2r7p:portname2/proxy/: bar (200; 42.197288ms)
Mar  3 10:00:38.238: INFO: (19) /api/v1/namespaces/proxy-8980/pods/https:proxy-service-p2r7p-9q77x:460/proxy/: tls baz (200; 42.185537ms)
Mar  3 10:00:38.238: INFO: (19) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/: <a href="/api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x/proxy/rewriteme">test</a> (200; 42.022999ms)
Mar  3 10:00:38.238: INFO: (19) /api/v1/namespaces/proxy-8980/pods/proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 42.379311ms)
Mar  3 10:00:38.238: INFO: (19) /api/v1/namespaces/proxy-8980/services/https:proxy-service-p2r7p:tlsportname1/proxy/: tls baz (200; 42.006631ms)
Mar  3 10:00:38.238: INFO: (19) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:160/proxy/: foo (200; 41.994221ms)
Mar  3 10:00:38.242: INFO: (19) /api/v1/namespaces/proxy-8980/pods/http:proxy-service-p2r7p-9q77x:162/proxy/: bar (200; 45.252337ms)
Mar  3 10:00:38.248: INFO: (19) /api/v1/namespaces/proxy-8980/services/proxy-service-p2r7p:portname1/proxy/: foo (200; 51.737104ms)
STEP: deleting ReplicationController proxy-service-p2r7p in namespace proxy-8980, will wait for the garbage collector to delete the pods
Mar  3 10:00:38.323: INFO: Deleting ReplicationController proxy-service-p2r7p took: 14.286364ms
Mar  3 10:00:38.424: INFO: Terminating ReplicationController proxy-service-p2r7p pods took: 100.325165ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:00:51.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8980" for this suite.

• [SLOW TEST:18.195 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":244,"skipped":4265,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:00:51.038: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 10:00:51.084: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20d9d0fe-e890-4cef-bbdb-8e2046b2d5a7" in namespace "projected-3079" to be "Succeeded or Failed"
Mar  3 10:00:51.092: INFO: Pod "downwardapi-volume-20d9d0fe-e890-4cef-bbdb-8e2046b2d5a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.087325ms
Mar  3 10:00:53.098: INFO: Pod "downwardapi-volume-20d9d0fe-e890-4cef-bbdb-8e2046b2d5a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013734252s
STEP: Saw pod success
Mar  3 10:00:53.098: INFO: Pod "downwardapi-volume-20d9d0fe-e890-4cef-bbdb-8e2046b2d5a7" satisfied condition "Succeeded or Failed"
Mar  3 10:00:53.101: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-20d9d0fe-e890-4cef-bbdb-8e2046b2d5a7 container client-container: <nil>
STEP: delete the pod
Mar  3 10:00:53.120: INFO: Waiting for pod downwardapi-volume-20d9d0fe-e890-4cef-bbdb-8e2046b2d5a7 to disappear
Mar  3 10:00:53.125: INFO: Pod downwardapi-volume-20d9d0fe-e890-4cef-bbdb-8e2046b2d5a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:00:53.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3079" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":245,"skipped":4268,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:00:53.137: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar  3 10:00:53.182: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 10:00:57.558: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:01:12.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1231" for this suite.

• [SLOW TEST:19.855 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":246,"skipped":4303,"failed":0}
SSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:01:12.992: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar  3 10:01:13.064: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar  3 10:01:13.068: INFO: starting watch
STEP: patching
STEP: updating
Mar  3 10:01:13.082: INFO: waiting for watch events with expected annotations
Mar  3 10:01:13.083: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:01:13.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-5603" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":247,"skipped":4308,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:01:13.133: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Mar  3 10:01:15.736: INFO: Successfully updated pod "annotationupdate8d1aa1e6-685f-4c19-b70b-e041f70aa25e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:01:19.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-763" for this suite.

• [SLOW TEST:6.646 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":248,"skipped":4318,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:01:19.778: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:01:19.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2272" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":249,"skipped":4338,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:01:19.860: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 10:01:19.903: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a55e7c36-ee67-4ae2-b037-3d8054bec7f0" in namespace "downward-api-7053" to be "Succeeded or Failed"
Mar  3 10:01:19.911: INFO: Pod "downwardapi-volume-a55e7c36-ee67-4ae2-b037-3d8054bec7f0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.2836ms
Mar  3 10:01:21.919: INFO: Pod "downwardapi-volume-a55e7c36-ee67-4ae2-b037-3d8054bec7f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015870185s
STEP: Saw pod success
Mar  3 10:01:21.919: INFO: Pod "downwardapi-volume-a55e7c36-ee67-4ae2-b037-3d8054bec7f0" satisfied condition "Succeeded or Failed"
Mar  3 10:01:21.922: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-a55e7c36-ee67-4ae2-b037-3d8054bec7f0 container client-container: <nil>
STEP: delete the pod
Mar  3 10:01:21.952: INFO: Waiting for pod downwardapi-volume-a55e7c36-ee67-4ae2-b037-3d8054bec7f0 to disappear
Mar  3 10:01:21.956: INFO: Pod downwardapi-volume-a55e7c36-ee67-4ae2-b037-3d8054bec7f0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:01:21.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7053" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":250,"skipped":4340,"failed":0}
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:01:21.972: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Mar  3 10:01:22.082: INFO: PodSpec: initContainers in spec.initContainers
Mar  3 10:02:06.133: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-0cdcfd67-6b02-4fdd-8ff8-4a5fc6b91ec6", GenerateName:"", Namespace:"init-container-855", SelfLink:"", UID:"8041b5f9-ee15-43f0-9b2a-fb81a99c833e", ResourceVersion:"20482", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63750362482, loc:(*time.Location)(0x797de40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"82431148"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.244.192.82/32", "cni.projectcalico.org/podIPs":"10.244.192.82/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc005023c40), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005023c60)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc005023ca0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005023ce0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc005023d00), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005023d20)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-lctp6", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc007882340), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-lctp6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-lctp6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-lctp6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004bb1b08), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"controller-0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0036d0e70), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004bb1bf0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004bb1c10)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004bb1c18), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004bb1c1c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00409a8a0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362482, loc:(*time.Location)(0x797de40)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362482, loc:(*time.Location)(0x797de40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362482, loc:(*time.Location)(0x797de40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362482, loc:(*time.Location)(0x797de40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.49.5", PodIP:"10.244.192.82", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.192.82"}}, StartTime:(*v1.Time)(0xc005023d40), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0036d0f50)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0036d0fc0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://6584475cc7c53dbcc333521ddd19dd9a8fe27df98f904b90751a8d2bf1def078", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005023d80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005023d60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc004bb1ccf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:02:06.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-855" for this suite.

• [SLOW TEST:44.183 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":251,"skipped":4341,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:02:06.156: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 10:02:06.231: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar  3 10:02:06.245: INFO: Number of nodes with available pods: 0
Mar  3 10:02:06.245: INFO: Node controller-0 is running more than one daemon pod
Mar  3 10:02:07.255: INFO: Number of nodes with available pods: 1
Mar  3 10:02:07.255: INFO: Node controller-0 is running more than one daemon pod
Mar  3 10:02:08.252: INFO: Number of nodes with available pods: 1
Mar  3 10:02:08.252: INFO: Node controller-0 is running more than one daemon pod
Mar  3 10:02:09.255: INFO: Number of nodes with available pods: 3
Mar  3 10:02:09.255: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar  3 10:02:09.298: INFO: Wrong image for pod: daemon-set-bhdfv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:09.299: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:09.299: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:10.309: INFO: Wrong image for pod: daemon-set-bhdfv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:10.309: INFO: Pod daemon-set-bhdfv is not available
Mar  3 10:02:10.309: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:10.309: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:11.312: INFO: Wrong image for pod: daemon-set-bhdfv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:11.312: INFO: Pod daemon-set-bhdfv is not available
Mar  3 10:02:11.312: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:11.312: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:12.309: INFO: Wrong image for pod: daemon-set-bhdfv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:12.309: INFO: Pod daemon-set-bhdfv is not available
Mar  3 10:02:12.310: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:12.310: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:13.318: INFO: Wrong image for pod: daemon-set-bhdfv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:13.318: INFO: Pod daemon-set-bhdfv is not available
Mar  3 10:02:13.318: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:13.318: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:14.311: INFO: Wrong image for pod: daemon-set-bhdfv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:14.311: INFO: Pod daemon-set-bhdfv is not available
Mar  3 10:02:14.311: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:14.311: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:15.311: INFO: Wrong image for pod: daemon-set-bhdfv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:15.311: INFO: Pod daemon-set-bhdfv is not available
Mar  3 10:02:15.311: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:15.311: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:16.311: INFO: Wrong image for pod: daemon-set-bhdfv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:16.311: INFO: Pod daemon-set-bhdfv is not available
Mar  3 10:02:16.311: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:16.311: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:17.310: INFO: Wrong image for pod: daemon-set-bhdfv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:17.311: INFO: Pod daemon-set-bhdfv is not available
Mar  3 10:02:17.311: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:17.311: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:18.312: INFO: Pod daemon-set-dvgdh is not available
Mar  3 10:02:18.312: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:18.312: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:19.311: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:19.311: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:20.311: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:20.311: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:21.310: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:21.310: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:21.310: INFO: Pod daemon-set-xnxph is not available
Mar  3 10:02:22.310: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:22.310: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:22.310: INFO: Pod daemon-set-xnxph is not available
Mar  3 10:02:23.310: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:23.310: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:23.310: INFO: Pod daemon-set-xnxph is not available
Mar  3 10:02:24.310: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:24.310: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:24.310: INFO: Pod daemon-set-xnxph is not available
Mar  3 10:02:25.311: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:25.311: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:25.311: INFO: Pod daemon-set-xnxph is not available
Mar  3 10:02:26.309: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:26.310: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:26.310: INFO: Pod daemon-set-xnxph is not available
Mar  3 10:02:27.311: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:27.311: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:27.311: INFO: Pod daemon-set-xnxph is not available
Mar  3 10:02:28.311: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:28.311: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:28.311: INFO: Pod daemon-set-xnxph is not available
Mar  3 10:02:29.311: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:29.311: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:29.311: INFO: Pod daemon-set-xnxph is not available
Mar  3 10:02:30.311: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:30.311: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:30.311: INFO: Pod daemon-set-xnxph is not available
Mar  3 10:02:31.311: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:31.311: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:31.311: INFO: Pod daemon-set-xnxph is not available
Mar  3 10:02:32.310: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:32.311: INFO: Wrong image for pod: daemon-set-xnxph. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:32.311: INFO: Pod daemon-set-xnxph is not available
Mar  3 10:02:33.308: INFO: Pod daemon-set-f2pvv is not available
Mar  3 10:02:33.308: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:34.312: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:35.310: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:35.310: INFO: Pod daemon-set-kgtxz is not available
Mar  3 10:02:36.312: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:36.312: INFO: Pod daemon-set-kgtxz is not available
Mar  3 10:02:37.312: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:37.312: INFO: Pod daemon-set-kgtxz is not available
Mar  3 10:02:38.310: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:38.310: INFO: Pod daemon-set-kgtxz is not available
Mar  3 10:02:39.310: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:39.310: INFO: Pod daemon-set-kgtxz is not available
Mar  3 10:02:40.312: INFO: Wrong image for pod: daemon-set-kgtxz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  3 10:02:40.312: INFO: Pod daemon-set-kgtxz is not available
Mar  3 10:02:41.317: INFO: Pod daemon-set-n9449 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Mar  3 10:02:41.355: INFO: Number of nodes with available pods: 2
Mar  3 10:02:41.355: INFO: Node controller-0 is running more than one daemon pod
Mar  3 10:02:42.365: INFO: Number of nodes with available pods: 3
Mar  3 10:02:42.367: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-590, will wait for the garbage collector to delete the pods
Mar  3 10:02:42.442: INFO: Deleting DaemonSet.extensions daemon-set took: 6.215272ms
Mar  3 10:02:43.143: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.772113ms
Mar  3 10:02:52.449: INFO: Number of nodes with available pods: 0
Mar  3 10:02:52.450: INFO: Number of running nodes: 0, number of available pods: 0
Mar  3 10:02:52.452: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20681"},"items":null}

Mar  3 10:02:52.454: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20681"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:02:52.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-590" for this suite.

• [SLOW TEST:46.317 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":252,"skipped":4354,"failed":0}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:02:52.474: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  3 10:02:56.583: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  3 10:02:56.586: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  3 10:02:58.587: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  3 10:02:58.596: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  3 10:03:00.587: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  3 10:03:00.596: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  3 10:03:02.587: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  3 10:03:02.592: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:03:02.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6043" for this suite.

• [SLOW TEST:10.127 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":253,"skipped":4357,"failed":0}
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:03:02.601: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:03:08.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8029" for this suite.
STEP: Destroying namespace "nsdeletetest-309" for this suite.
Mar  3 10:03:08.772: INFO: Namespace nsdeletetest-309 was already deleted
STEP: Destroying namespace "nsdeletetest-5211" for this suite.

• [SLOW TEST:6.178 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":254,"skipped":4360,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:03:08.783: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  3 10:03:10.909: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:03:10.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2688" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":255,"skipped":4364,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:03:10.942: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Mar  3 10:03:10.997: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Mar  3 10:03:11.017: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:03:11.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4572" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":256,"skipped":4403,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:03:11.050: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 10:03:11.095: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar  3 10:03:15.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-8694 --namespace=crd-publish-openapi-8694 create -f -'
Mar  3 10:03:15.797: INFO: stderr: ""
Mar  3 10:03:15.797: INFO: stdout: "e2e-test-crd-publish-openapi-5888-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  3 10:03:15.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-8694 --namespace=crd-publish-openapi-8694 delete e2e-test-crd-publish-openapi-5888-crds test-foo'
Mar  3 10:03:15.881: INFO: stderr: ""
Mar  3 10:03:15.881: INFO: stdout: "e2e-test-crd-publish-openapi-5888-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar  3 10:03:15.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-8694 --namespace=crd-publish-openapi-8694 apply -f -'
Mar  3 10:03:16.180: INFO: stderr: ""
Mar  3 10:03:16.180: INFO: stdout: "e2e-test-crd-publish-openapi-5888-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  3 10:03:16.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-8694 --namespace=crd-publish-openapi-8694 delete e2e-test-crd-publish-openapi-5888-crds test-foo'
Mar  3 10:03:16.284: INFO: stderr: ""
Mar  3 10:03:16.284: INFO: stdout: "e2e-test-crd-publish-openapi-5888-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar  3 10:03:16.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-8694 --namespace=crd-publish-openapi-8694 create -f -'
Mar  3 10:03:16.513: INFO: rc: 1
Mar  3 10:03:16.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-8694 --namespace=crd-publish-openapi-8694 apply -f -'
Mar  3 10:03:16.727: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar  3 10:03:16.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-8694 --namespace=crd-publish-openapi-8694 create -f -'
Mar  3 10:03:16.949: INFO: rc: 1
Mar  3 10:03:16.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-8694 --namespace=crd-publish-openapi-8694 apply -f -'
Mar  3 10:03:17.162: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar  3 10:03:17.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-8694 explain e2e-test-crd-publish-openapi-5888-crds'
Mar  3 10:03:17.387: INFO: stderr: ""
Mar  3 10:03:17.388: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5888-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar  3 10:03:17.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-8694 explain e2e-test-crd-publish-openapi-5888-crds.metadata'
Mar  3 10:03:17.596: INFO: stderr: ""
Mar  3 10:03:17.596: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5888-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar  3 10:03:17.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-8694 explain e2e-test-crd-publish-openapi-5888-crds.spec'
Mar  3 10:03:17.811: INFO: stderr: ""
Mar  3 10:03:17.811: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5888-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar  3 10:03:17.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-8694 explain e2e-test-crd-publish-openapi-5888-crds.spec.bars'
Mar  3 10:03:18.049: INFO: stderr: ""
Mar  3 10:03:18.049: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5888-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar  3 10:03:18.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-8694 explain e2e-test-crd-publish-openapi-5888-crds.spec.bars2'
Mar  3 10:03:18.263: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:03:22.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8694" for this suite.

• [SLOW TEST:11.397 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":257,"skipped":4417,"failed":0}
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:03:22.446: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Mar  3 10:03:22.501: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:03:22.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3132" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":258,"skipped":4427,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:03:22.525: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5973.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5973.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5973.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5973.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5973.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5973.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  3 10:03:26.659: INFO: DNS probes using dns-5973/dns-test-c769db27-f90b-4afd-aa51-dc4ea2aa28d5 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:03:26.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5973" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":259,"skipped":4441,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:03:26.747: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:03:26.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-254" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":260,"skipped":4441,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:03:26.828: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Mar  3 10:05:27.424: INFO: Successfully updated pod "var-expansion-056d5164-889f-4fc8-978a-714180ff08a0"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Mar  3 10:05:29.443: INFO: Deleting pod "var-expansion-056d5164-889f-4fc8-978a-714180ff08a0" in namespace "var-expansion-3443"
Mar  3 10:05:29.452: INFO: Wait up to 5m0s for pod "var-expansion-056d5164-889f-4fc8-978a-714180ff08a0" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:06:09.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3443" for this suite.

• [SLOW TEST:162.652 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":261,"skipped":4461,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:06:09.481: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-528e23db-32c5-40b2-bf1e-c312d9b6639b
STEP: Creating a pod to test consume configMaps
Mar  3 10:06:09.552: INFO: Waiting up to 5m0s for pod "pod-configmaps-ee434821-fcf3-40e3-89c0-219aa25990a2" in namespace "configmap-3967" to be "Succeeded or Failed"
Mar  3 10:06:09.560: INFO: Pod "pod-configmaps-ee434821-fcf3-40e3-89c0-219aa25990a2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.815369ms
Mar  3 10:06:11.568: INFO: Pod "pod-configmaps-ee434821-fcf3-40e3-89c0-219aa25990a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015897011s
STEP: Saw pod success
Mar  3 10:06:11.568: INFO: Pod "pod-configmaps-ee434821-fcf3-40e3-89c0-219aa25990a2" satisfied condition "Succeeded or Failed"
Mar  3 10:06:11.571: INFO: Trying to get logs from node worker-1 pod pod-configmaps-ee434821-fcf3-40e3-89c0-219aa25990a2 container agnhost-container: <nil>
STEP: delete the pod
Mar  3 10:06:11.601: INFO: Waiting for pod pod-configmaps-ee434821-fcf3-40e3-89c0-219aa25990a2 to disappear
Mar  3 10:06:11.604: INFO: Pod pod-configmaps-ee434821-fcf3-40e3-89c0-219aa25990a2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:06:11.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3967" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":262,"skipped":4464,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:06:11.617: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-5006
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  3 10:06:11.656: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  3 10:06:11.691: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  3 10:06:13.696: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 10:06:15.699: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 10:06:17.701: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 10:06:19.701: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 10:06:21.701: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 10:06:23.696: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  3 10:06:23.706: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  3 10:06:25.713: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  3 10:06:25.717: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar  3 10:06:27.765: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  3 10:06:27.765: INFO: Going to poll 10.244.192.81 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Mar  3 10:06:27.767: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.192.81:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5006 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 10:06:27.767: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 10:06:27.864: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  3 10:06:27.864: INFO: Going to poll 10.244.43.28 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Mar  3 10:06:27.870: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.43.28:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5006 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 10:06:27.871: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 10:06:27.963: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  3 10:06:27.963: INFO: Going to poll 10.244.226.101 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Mar  3 10:06:27.967: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.226.101:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5006 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 10:06:27.967: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 10:06:28.042: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:06:28.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5006" for this suite.

• [SLOW TEST:16.439 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":263,"skipped":4469,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:06:28.056: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar  3 10:06:28.116: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6237  dd160072-19c8-4628-bdf5-5ea8c43dc58c 21279 0 2021-03-03 10:06:28 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-03-03 10:06:28 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-647d5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-647d5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-647d5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  3 10:06:28.126: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar  3 10:06:30.134: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Mar  3 10:06:30.134: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6237 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 10:06:30.134: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Verifying customized DNS server is configured on pod...
Mar  3 10:06:30.223: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6237 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 10:06:30.223: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 10:06:30.301: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:06:30.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6237" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":264,"skipped":4482,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:06:30.327: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Mar  3 10:07:10.420: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0303 10:07:10.420060      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0303 10:07:10.420089      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0303 10:07:10.420095      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  3 10:07:10.420: INFO: Deleting pod "simpletest.rc-5ndgp" in namespace "gc-9606"
Mar  3 10:07:10.502: INFO: Deleting pod "simpletest.rc-685gb" in namespace "gc-9606"
Mar  3 10:07:10.527: INFO: Deleting pod "simpletest.rc-6f4j2" in namespace "gc-9606"
Mar  3 10:07:10.608: INFO: Deleting pod "simpletest.rc-6pqvs" in namespace "gc-9606"
Mar  3 10:07:10.648: INFO: Deleting pod "simpletest.rc-gcg7n" in namespace "gc-9606"
Mar  3 10:07:10.675: INFO: Deleting pod "simpletest.rc-h6xtt" in namespace "gc-9606"
Mar  3 10:07:10.691: INFO: Deleting pod "simpletest.rc-ps9ng" in namespace "gc-9606"
Mar  3 10:07:10.706: INFO: Deleting pod "simpletest.rc-qv5zs" in namespace "gc-9606"
Mar  3 10:07:10.725: INFO: Deleting pod "simpletest.rc-x9v9n" in namespace "gc-9606"
Mar  3 10:07:10.834: INFO: Deleting pod "simpletest.rc-z4wjt" in namespace "gc-9606"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:07:10.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9606" for this suite.

• [SLOW TEST:40.637 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":265,"skipped":4486,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:07:10.964: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  3 10:07:11.121: INFO: Waiting up to 5m0s for pod "pod-c249fccf-fe4e-40d3-8c37-f7d8aea9b227" in namespace "emptydir-2009" to be "Succeeded or Failed"
Mar  3 10:07:11.142: INFO: Pod "pod-c249fccf-fe4e-40d3-8c37-f7d8aea9b227": Phase="Pending", Reason="", readiness=false. Elapsed: 20.529392ms
Mar  3 10:07:13.157: INFO: Pod "pod-c249fccf-fe4e-40d3-8c37-f7d8aea9b227": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036056317s
STEP: Saw pod success
Mar  3 10:07:13.157: INFO: Pod "pod-c249fccf-fe4e-40d3-8c37-f7d8aea9b227" satisfied condition "Succeeded or Failed"
Mar  3 10:07:13.162: INFO: Trying to get logs from node worker-1 pod pod-c249fccf-fe4e-40d3-8c37-f7d8aea9b227 container test-container: <nil>
STEP: delete the pod
Mar  3 10:07:13.237: INFO: Waiting for pod pod-c249fccf-fe4e-40d3-8c37-f7d8aea9b227 to disappear
Mar  3 10:07:13.245: INFO: Pod pod-c249fccf-fe4e-40d3-8c37-f7d8aea9b227 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:07:13.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2009" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":266,"skipped":4491,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:07:13.260: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  3 10:07:13.304: INFO: Waiting up to 5m0s for pod "pod-ca654b58-7359-46e3-9003-98e0e6092f1d" in namespace "emptydir-331" to be "Succeeded or Failed"
Mar  3 10:07:13.307: INFO: Pod "pod-ca654b58-7359-46e3-9003-98e0e6092f1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.505214ms
Mar  3 10:07:15.315: INFO: Pod "pod-ca654b58-7359-46e3-9003-98e0e6092f1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010314252s
STEP: Saw pod success
Mar  3 10:07:15.315: INFO: Pod "pod-ca654b58-7359-46e3-9003-98e0e6092f1d" satisfied condition "Succeeded or Failed"
Mar  3 10:07:15.318: INFO: Trying to get logs from node worker-1 pod pod-ca654b58-7359-46e3-9003-98e0e6092f1d container test-container: <nil>
STEP: delete the pod
Mar  3 10:07:15.336: INFO: Waiting for pod pod-ca654b58-7359-46e3-9003-98e0e6092f1d to disappear
Mar  3 10:07:15.340: INFO: Pod pod-ca654b58-7359-46e3-9003-98e0e6092f1d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:07:15.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-331" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":267,"skipped":4517,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:07:15.352: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-be041ac3-0630-4a77-9086-8b3ba0cebeef
STEP: Creating a pod to test consume configMaps
Mar  3 10:07:15.403: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3822910b-286b-4342-ad44-cdd487e300e3" in namespace "projected-1502" to be "Succeeded or Failed"
Mar  3 10:07:15.406: INFO: Pod "pod-projected-configmaps-3822910b-286b-4342-ad44-cdd487e300e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.931394ms
Mar  3 10:07:17.412: INFO: Pod "pod-projected-configmaps-3822910b-286b-4342-ad44-cdd487e300e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009080687s
STEP: Saw pod success
Mar  3 10:07:17.412: INFO: Pod "pod-projected-configmaps-3822910b-286b-4342-ad44-cdd487e300e3" satisfied condition "Succeeded or Failed"
Mar  3 10:07:17.415: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-3822910b-286b-4342-ad44-cdd487e300e3 container agnhost-container: <nil>
STEP: delete the pod
Mar  3 10:07:17.436: INFO: Waiting for pod pod-projected-configmaps-3822910b-286b-4342-ad44-cdd487e300e3 to disappear
Mar  3 10:07:17.438: INFO: Pod pod-projected-configmaps-3822910b-286b-4342-ad44-cdd487e300e3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:07:17.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1502" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":268,"skipped":4524,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:07:17.448: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Mar  3 10:07:17.491: INFO: Waiting up to 5m0s for pod "var-expansion-71c94d42-7231-4d3f-b397-31dda016c871" in namespace "var-expansion-1585" to be "Succeeded or Failed"
Mar  3 10:07:17.496: INFO: Pod "var-expansion-71c94d42-7231-4d3f-b397-31dda016c871": Phase="Pending", Reason="", readiness=false. Elapsed: 5.25929ms
Mar  3 10:07:19.504: INFO: Pod "var-expansion-71c94d42-7231-4d3f-b397-31dda016c871": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013272712s
STEP: Saw pod success
Mar  3 10:07:19.504: INFO: Pod "var-expansion-71c94d42-7231-4d3f-b397-31dda016c871" satisfied condition "Succeeded or Failed"
Mar  3 10:07:19.507: INFO: Trying to get logs from node worker-1 pod var-expansion-71c94d42-7231-4d3f-b397-31dda016c871 container dapi-container: <nil>
STEP: delete the pod
Mar  3 10:07:19.538: INFO: Waiting for pod var-expansion-71c94d42-7231-4d3f-b397-31dda016c871 to disappear
Mar  3 10:07:19.541: INFO: Pod var-expansion-71c94d42-7231-4d3f-b397-31dda016c871 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:07:19.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1585" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":269,"skipped":4525,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:07:19.555: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Mar  3 10:07:19.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-4311 create -f -'
Mar  3 10:07:19.875: INFO: stderr: ""
Mar  3 10:07:19.875: INFO: stdout: "pod/pause created\n"
Mar  3 10:07:19.875: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  3 10:07:19.875: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4311" to be "running and ready"
Mar  3 10:07:19.880: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.497603ms
Mar  3 10:07:21.887: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.011777383s
Mar  3 10:07:21.887: INFO: Pod "pause" satisfied condition "running and ready"
Mar  3 10:07:21.887: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Mar  3 10:07:21.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-4311 label pods pause testing-label=testing-label-value'
Mar  3 10:07:21.975: INFO: stderr: ""
Mar  3 10:07:21.975: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar  3 10:07:21.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-4311 get pod pause -L testing-label'
Mar  3 10:07:22.054: INFO: stderr: ""
Mar  3 10:07:22.054: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar  3 10:07:22.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-4311 label pods pause testing-label-'
Mar  3 10:07:22.196: INFO: stderr: ""
Mar  3 10:07:22.196: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar  3 10:07:22.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-4311 get pod pause -L testing-label'
Mar  3 10:07:22.276: INFO: stderr: ""
Mar  3 10:07:22.276: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Mar  3 10:07:22.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-4311 delete --grace-period=0 --force -f -'
Mar  3 10:07:22.379: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  3 10:07:22.379: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  3 10:07:22.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-4311 get rc,svc -l name=pause --no-headers'
Mar  3 10:07:22.481: INFO: stderr: "No resources found in kubectl-4311 namespace.\n"
Mar  3 10:07:22.481: INFO: stdout: ""
Mar  3 10:07:22.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-4311 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  3 10:07:22.560: INFO: stderr: ""
Mar  3 10:07:22.560: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:07:22.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4311" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":270,"skipped":4534,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:07:22.572: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Mar  3 10:07:23.157: INFO: created pod pod-service-account-defaultsa
Mar  3 10:07:23.157: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  3 10:07:23.171: INFO: created pod pod-service-account-mountsa
Mar  3 10:07:23.172: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  3 10:07:23.182: INFO: created pod pod-service-account-nomountsa
Mar  3 10:07:23.182: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  3 10:07:23.188: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  3 10:07:23.188: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  3 10:07:23.211: INFO: created pod pod-service-account-mountsa-mountspec
Mar  3 10:07:23.211: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  3 10:07:23.223: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  3 10:07:23.223: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  3 10:07:23.263: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  3 10:07:23.263: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  3 10:07:23.299: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  3 10:07:23.299: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  3 10:07:23.353: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  3 10:07:23.353: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:07:23.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2861" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":271,"skipped":4540,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:07:23.408: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-51b063c4-b94d-41f7-96ca-8eec444c82b6
STEP: Creating a pod to test consume configMaps
Mar  3 10:07:23.778: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4f538811-4a2d-47ab-96cd-1be46c352fc7" in namespace "projected-8197" to be "Succeeded or Failed"
Mar  3 10:07:23.813: INFO: Pod "pod-projected-configmaps-4f538811-4a2d-47ab-96cd-1be46c352fc7": Phase="Pending", Reason="", readiness=false. Elapsed: 34.433302ms
Mar  3 10:07:25.866: INFO: Pod "pod-projected-configmaps-4f538811-4a2d-47ab-96cd-1be46c352fc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087785564s
Mar  3 10:07:27.876: INFO: Pod "pod-projected-configmaps-4f538811-4a2d-47ab-96cd-1be46c352fc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.097578202s
STEP: Saw pod success
Mar  3 10:07:27.876: INFO: Pod "pod-projected-configmaps-4f538811-4a2d-47ab-96cd-1be46c352fc7" satisfied condition "Succeeded or Failed"
Mar  3 10:07:27.882: INFO: Trying to get logs from node controller-0 pod pod-projected-configmaps-4f538811-4a2d-47ab-96cd-1be46c352fc7 container agnhost-container: <nil>
STEP: delete the pod
Mar  3 10:07:27.916: INFO: Waiting for pod pod-projected-configmaps-4f538811-4a2d-47ab-96cd-1be46c352fc7 to disappear
Mar  3 10:07:27.919: INFO: Pod pod-projected-configmaps-4f538811-4a2d-47ab-96cd-1be46c352fc7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:07:27.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8197" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":272,"skipped":4580,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:07:27.927: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-6940
STEP: creating service affinity-clusterip in namespace services-6940
STEP: creating replication controller affinity-clusterip in namespace services-6940
I0303 10:07:27.991796      24 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-6940, replica count: 3
I0303 10:07:31.042267      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0303 10:07:34.042508      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  3 10:07:34.050: INFO: Creating new exec pod
Mar  3 10:07:37.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-6940 exec execpod-affinity7rhnm -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Mar  3 10:07:37.260: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar  3 10:07:37.260: INFO: stdout: ""
Mar  3 10:07:37.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-6940 exec execpod-affinity7rhnm -- /bin/sh -x -c nc -zv -t -w 2 10.96.253.185 80'
Mar  3 10:07:37.429: INFO: stderr: "+ nc -zv -t -w 2 10.96.253.185 80\nConnection to 10.96.253.185 80 port [tcp/http] succeeded!\n"
Mar  3 10:07:37.429: INFO: stdout: ""
Mar  3 10:07:37.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=services-6940 exec execpod-affinity7rhnm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.253.185:80/ ; done'
Mar  3 10:07:37.646: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.253.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.253.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.253.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.253.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.253.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.253.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.253.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.253.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.253.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.253.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.253.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.253.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.253.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.253.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.253.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.253.185:80/\n"
Mar  3 10:07:37.646: INFO: stdout: "\naffinity-clusterip-8w7t4\naffinity-clusterip-8w7t4\naffinity-clusterip-8w7t4\naffinity-clusterip-8w7t4\naffinity-clusterip-8w7t4\naffinity-clusterip-8w7t4\naffinity-clusterip-8w7t4\naffinity-clusterip-8w7t4\naffinity-clusterip-8w7t4\naffinity-clusterip-8w7t4\naffinity-clusterip-8w7t4\naffinity-clusterip-8w7t4\naffinity-clusterip-8w7t4\naffinity-clusterip-8w7t4\naffinity-clusterip-8w7t4\naffinity-clusterip-8w7t4"
Mar  3 10:07:37.646: INFO: Received response from host: affinity-clusterip-8w7t4
Mar  3 10:07:37.646: INFO: Received response from host: affinity-clusterip-8w7t4
Mar  3 10:07:37.646: INFO: Received response from host: affinity-clusterip-8w7t4
Mar  3 10:07:37.646: INFO: Received response from host: affinity-clusterip-8w7t4
Mar  3 10:07:37.646: INFO: Received response from host: affinity-clusterip-8w7t4
Mar  3 10:07:37.646: INFO: Received response from host: affinity-clusterip-8w7t4
Mar  3 10:07:37.646: INFO: Received response from host: affinity-clusterip-8w7t4
Mar  3 10:07:37.646: INFO: Received response from host: affinity-clusterip-8w7t4
Mar  3 10:07:37.646: INFO: Received response from host: affinity-clusterip-8w7t4
Mar  3 10:07:37.646: INFO: Received response from host: affinity-clusterip-8w7t4
Mar  3 10:07:37.646: INFO: Received response from host: affinity-clusterip-8w7t4
Mar  3 10:07:37.646: INFO: Received response from host: affinity-clusterip-8w7t4
Mar  3 10:07:37.646: INFO: Received response from host: affinity-clusterip-8w7t4
Mar  3 10:07:37.646: INFO: Received response from host: affinity-clusterip-8w7t4
Mar  3 10:07:37.646: INFO: Received response from host: affinity-clusterip-8w7t4
Mar  3 10:07:37.646: INFO: Received response from host: affinity-clusterip-8w7t4
Mar  3 10:07:37.646: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6940, will wait for the garbage collector to delete the pods
Mar  3 10:07:37.727: INFO: Deleting ReplicationController affinity-clusterip took: 6.511681ms
Mar  3 10:07:38.427: INFO: Terminating ReplicationController affinity-clusterip pods took: 700.235082ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:07:52.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6940" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:24.589 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":273,"skipped":4583,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:07:52.523: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:07:54.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3972" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":274,"skipped":4610,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:07:54.688: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  3 10:07:54.731: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  3 10:07:54.736: INFO: Waiting for terminating namespaces to be deleted...
Mar  3 10:07:54.739: INFO: 
Logging pods the apiserver thinks is on node controller-0 before test
Mar  3 10:07:54.744: INFO: calico-kube-controllers-5f6546844f-gnl9f from kube-system started at 2021-03-03 08:47:09 +0000 UTC (1 container statuses recorded)
Mar  3 10:07:54.744: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  3 10:07:54.744: INFO: calico-node-468vb from kube-system started at 2021-03-03 08:46:49 +0000 UTC (1 container statuses recorded)
Mar  3 10:07:54.744: INFO: 	Container calico-node ready: true, restart count 0
Mar  3 10:07:54.744: INFO: coredns-5c98d7d4d8-96hpd from kube-system started at 2021-03-03 08:47:11 +0000 UTC (1 container statuses recorded)
Mar  3 10:07:54.744: INFO: 	Container coredns ready: true, restart count 0
Mar  3 10:07:54.744: INFO: konnectivity-agent-882qs from kube-system started at 2021-03-03 08:47:01 +0000 UTC (1 container statuses recorded)
Mar  3 10:07:54.744: INFO: 	Container konnectivity-agent ready: true, restart count 1
Mar  3 10:07:54.744: INFO: kube-proxy-snqpb from kube-system started at 2021-03-03 08:46:30 +0000 UTC (1 container statuses recorded)
Mar  3 10:07:54.744: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  3 10:07:54.744: INFO: metrics-server-6fbcd86f7b-r2c26 from kube-system started at 2021-03-03 08:47:09 +0000 UTC (1 container statuses recorded)
Mar  3 10:07:54.744: INFO: 	Container metrics-server ready: true, restart count 0
Mar  3 10:07:54.744: INFO: sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-8fglb from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 10:07:54.744: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Mar  3 10:07:54.744: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  3 10:07:54.744: INFO: 
Logging pods the apiserver thinks is on node worker-0 before test
Mar  3 10:07:54.750: INFO: calico-node-4vzkv from kube-system started at 2021-03-03 08:50:42 +0000 UTC (1 container statuses recorded)
Mar  3 10:07:54.750: INFO: 	Container calico-node ready: true, restart count 0
Mar  3 10:07:54.750: INFO: konnectivity-agent-hrn26 from kube-system started at 2021-03-03 08:51:02 +0000 UTC (1 container statuses recorded)
Mar  3 10:07:54.750: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  3 10:07:54.750: INFO: kube-proxy-xwt8k from kube-system started at 2021-03-03 08:50:42 +0000 UTC (1 container statuses recorded)
Mar  3 10:07:54.750: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  3 10:07:54.750: INFO: sonobuoy-e2e-job-a123a876f42443b1 from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 10:07:54.750: INFO: 	Container e2e ready: true, restart count 0
Mar  3 10:07:54.750: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  3 10:07:54.750: INFO: sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-xjvxl from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 10:07:54.750: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Mar  3 10:07:54.750: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  3 10:07:54.750: INFO: 
Logging pods the apiserver thinks is on node worker-1 before test
Mar  3 10:07:54.759: INFO: calico-node-d2bzg from kube-system started at 2021-03-03 08:52:07 +0000 UTC (1 container statuses recorded)
Mar  3 10:07:54.759: INFO: 	Container calico-node ready: true, restart count 0
Mar  3 10:07:54.759: INFO: konnectivity-agent-ssw2j from kube-system started at 2021-03-03 09:38:46 +0000 UTC (1 container statuses recorded)
Mar  3 10:07:54.760: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  3 10:07:54.760: INFO: kube-proxy-c7zpn from kube-system started at 2021-03-03 08:52:07 +0000 UTC (1 container statuses recorded)
Mar  3 10:07:54.760: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  3 10:07:54.760: INFO: sonobuoy from sonobuoy started at 2021-03-03 08:53:20 +0000 UTC (1 container statuses recorded)
Mar  3 10:07:54.760: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  3 10:07:54.760: INFO: sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-x4pmv from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 10:07:54.760: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Mar  3 10:07:54.760: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node controller-0
STEP: verifying the node has the label node worker-0
STEP: verifying the node has the label node worker-1
Mar  3 10:07:54.855: INFO: Pod calico-kube-controllers-5f6546844f-gnl9f requesting resource cpu=0m on Node controller-0
Mar  3 10:07:54.855: INFO: Pod calico-node-468vb requesting resource cpu=250m on Node controller-0
Mar  3 10:07:54.856: INFO: Pod calico-node-4vzkv requesting resource cpu=250m on Node worker-0
Mar  3 10:07:54.856: INFO: Pod calico-node-d2bzg requesting resource cpu=250m on Node worker-1
Mar  3 10:07:54.856: INFO: Pod coredns-5c98d7d4d8-96hpd requesting resource cpu=100m on Node controller-0
Mar  3 10:07:54.856: INFO: Pod konnectivity-agent-882qs requesting resource cpu=0m on Node controller-0
Mar  3 10:07:54.856: INFO: Pod konnectivity-agent-hrn26 requesting resource cpu=0m on Node worker-0
Mar  3 10:07:54.856: INFO: Pod konnectivity-agent-ssw2j requesting resource cpu=0m on Node worker-1
Mar  3 10:07:54.856: INFO: Pod kube-proxy-c7zpn requesting resource cpu=0m on Node worker-1
Mar  3 10:07:54.856: INFO: Pod kube-proxy-snqpb requesting resource cpu=0m on Node controller-0
Mar  3 10:07:54.856: INFO: Pod kube-proxy-xwt8k requesting resource cpu=0m on Node worker-0
Mar  3 10:07:54.856: INFO: Pod metrics-server-6fbcd86f7b-r2c26 requesting resource cpu=10m on Node controller-0
Mar  3 10:07:54.856: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker-1
Mar  3 10:07:54.857: INFO: Pod sonobuoy-e2e-job-a123a876f42443b1 requesting resource cpu=0m on Node worker-0
Mar  3 10:07:54.857: INFO: Pod sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-8fglb requesting resource cpu=0m on Node controller-0
Mar  3 10:07:54.857: INFO: Pod sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-x4pmv requesting resource cpu=0m on Node worker-1
Mar  3 10:07:54.858: INFO: Pod sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-xjvxl requesting resource cpu=0m on Node worker-0
STEP: Starting Pods to consume most of the cluster CPU.
Mar  3 10:07:54.858: INFO: Creating a pod which consumes cpu=1148m on Node controller-0
Mar  3 10:07:54.867: INFO: Creating a pod which consumes cpu=1225m on Node worker-0
Mar  3 10:07:54.878: INFO: Creating a pod which consumes cpu=1225m on Node worker-1
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-283d70e0-572e-41ab-b312-7a821fd6e628.1668cd3090e740cf], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4326/filler-pod-283d70e0-572e-41ab-b312-7a821fd6e628 to controller-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-283d70e0-572e-41ab-b312-7a821fd6e628.1668cd30c4bc8321], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-283d70e0-572e-41ab-b312-7a821fd6e628.1668cd30ca656720], Reason = [Created], Message = [Created container filler-pod-283d70e0-572e-41ab-b312-7a821fd6e628]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-283d70e0-572e-41ab-b312-7a821fd6e628.1668cd30d59513da], Reason = [Started], Message = [Started container filler-pod-283d70e0-572e-41ab-b312-7a821fd6e628]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-40c34ba1-dff6-4bb3-bb5a-f008c193aab0.1668cd309590936c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4326/filler-pod-40c34ba1-dff6-4bb3-bb5a-f008c193aab0 to worker-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-40c34ba1-dff6-4bb3-bb5a-f008c193aab0.1668cd30dc9ea850], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-40c34ba1-dff6-4bb3-bb5a-f008c193aab0.1668cd30df4dff43], Reason = [Created], Message = [Created container filler-pod-40c34ba1-dff6-4bb3-bb5a-f008c193aab0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-40c34ba1-dff6-4bb3-bb5a-f008c193aab0.1668cd30e3ec6336], Reason = [Started], Message = [Started container filler-pod-40c34ba1-dff6-4bb3-bb5a-f008c193aab0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cab413a0-e25c-48fc-b4cd-b009ebf1e339.1668cd3091511c77], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4326/filler-pod-cab413a0-e25c-48fc-b4cd-b009ebf1e339 to worker-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cab413a0-e25c-48fc-b4cd-b009ebf1e339.1668cd30ba98866e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cab413a0-e25c-48fc-b4cd-b009ebf1e339.1668cd30bddc2e67], Reason = [Created], Message = [Created container filler-pod-cab413a0-e25c-48fc-b4cd-b009ebf1e339]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cab413a0-e25c-48fc-b4cd-b009ebf1e339.1668cd30c245a80d], Reason = [Started], Message = [Started container filler-pod-cab413a0-e25c-48fc-b4cd-b009ebf1e339]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1668cd310e5302b6], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node worker-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node controller-0
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:07:58.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4326" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":275,"skipped":4611,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:07:58.061: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-7701
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  3 10:07:58.097: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  3 10:07:58.172: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  3 10:08:00.181: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  3 10:08:02.179: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 10:08:04.176: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 10:08:06.180: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 10:08:08.180: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 10:08:10.180: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  3 10:08:12.178: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  3 10:08:12.184: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  3 10:08:14.190: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  3 10:08:14.196: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar  3 10:08:16.227: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  3 10:08:16.227: INFO: Breadth first check of 10.244.192.90 on host 10.0.49.5...
Mar  3 10:08:16.230: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.226.122:9080/dial?request=hostname&protocol=udp&host=10.244.192.90&port=8081&tries=1'] Namespace:pod-network-test-7701 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 10:08:16.230: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 10:08:16.310: INFO: Waiting for responses: map[]
Mar  3 10:08:16.310: INFO: reached 10.244.192.90 after 0/1 tries
Mar  3 10:08:16.310: INFO: Breadth first check of 10.244.43.36 on host 10.0.40.23...
Mar  3 10:08:16.315: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.226.122:9080/dial?request=hostname&protocol=udp&host=10.244.43.36&port=8081&tries=1'] Namespace:pod-network-test-7701 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 10:08:16.315: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 10:08:16.381: INFO: Waiting for responses: map[]
Mar  3 10:08:16.381: INFO: reached 10.244.43.36 after 0/1 tries
Mar  3 10:08:16.381: INFO: Breadth first check of 10.244.226.120 on host 10.0.47.86...
Mar  3 10:08:16.386: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.226.122:9080/dial?request=hostname&protocol=udp&host=10.244.226.120&port=8081&tries=1'] Namespace:pod-network-test-7701 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  3 10:08:16.386: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
Mar  3 10:08:16.461: INFO: Waiting for responses: map[]
Mar  3 10:08:16.461: INFO: reached 10.244.226.120 after 0/1 tries
Mar  3 10:08:16.461: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:08:16.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7701" for this suite.

• [SLOW TEST:18.415 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":276,"skipped":4615,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:08:16.476: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 10:08:16.524: INFO: Creating deployment "test-recreate-deployment"
Mar  3 10:08:16.528: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  3 10:08:16.546: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar  3 10:08:18.556: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  3 10:08:18.558: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  3 10:08:18.566: INFO: Updating deployment test-recreate-deployment
Mar  3 10:08:18.566: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  3 10:08:18.676: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5090  fe6648b9-16d6-47f8-b23d-e5fee0624eb4 22472 2 2021-03-03 10:08:16 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-03 10:08:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-03 10:08:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e81298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-03-03 10:08:18 +0000 UTC,LastTransitionTime:2021-03-03 10:08:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-03-03 10:08:18 +0000 UTC,LastTransitionTime:2021-03-03 10:08:16 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar  3 10:08:18.679: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-5090  420e8120-8fcd-47b1-a0de-e3c8657822d0 22469 1 2021-03-03 10:08:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment fe6648b9-16d6-47f8-b23d-e5fee0624eb4 0xc001e81700 0xc001e81701}] []  [{kube-controller-manager Update apps/v1 2021-03-03 10:08:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe6648b9-16d6-47f8-b23d-e5fee0624eb4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e81778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  3 10:08:18.679: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  3 10:08:18.679: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-5090  91da00a6-b661-477f-b927-3d428d9fb333 22459 2 2021-03-03 10:08:16 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment fe6648b9-16d6-47f8-b23d-e5fee0624eb4 0xc001e81607 0xc001e81608}] []  [{kube-controller-manager Update apps/v1 2021-03-03 10:08:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe6648b9-16d6-47f8-b23d-e5fee0624eb4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e81698 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  3 10:08:18.682: INFO: Pod "test-recreate-deployment-f79dd4667-vmvcn" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-vmvcn test-recreate-deployment-f79dd4667- deployment-5090  4d87fe6f-3f89-4686-9859-a6b5c5ee15bf 22471 0 2021-03-03 10:08:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 420e8120-8fcd-47b1-a0de-e3c8657822d0 0xc001e81d00 0xc001e81d01}] []  [{kube-controller-manager Update v1 2021-03-03 10:08:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"420e8120-8fcd-47b1-a0de-e3c8657822d0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-03 10:08:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p9cj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p9cj6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p9cj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 10:08:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 10:08:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 10:08:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-03 10:08:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.23,PodIP:,StartTime:2021-03-03 10:08:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:08:18.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5090" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":277,"skipped":4625,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:08:18.691: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7502
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7502
STEP: Creating statefulset with conflicting port in namespace statefulset-7502
STEP: Waiting until pod test-pod will start running in namespace statefulset-7502
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7502
Mar  3 10:08:20.788: INFO: Observed stateful pod in namespace: statefulset-7502, name: ss-0, uid: 338db7b0-b890-4061-b391-7ed4abaa7f8b, status phase: Pending. Waiting for statefulset controller to delete.
Mar  3 10:08:21.667: INFO: Observed stateful pod in namespace: statefulset-7502, name: ss-0, uid: 338db7b0-b890-4061-b391-7ed4abaa7f8b, status phase: Failed. Waiting for statefulset controller to delete.
Mar  3 10:08:21.685: INFO: Observed stateful pod in namespace: statefulset-7502, name: ss-0, uid: 338db7b0-b890-4061-b391-7ed4abaa7f8b, status phase: Failed. Waiting for statefulset controller to delete.
Mar  3 10:08:21.713: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7502
STEP: Removing pod with conflicting port in namespace statefulset-7502
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7502 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  3 10:08:25.830: INFO: Deleting all statefulset in ns statefulset-7502
Mar  3 10:08:25.834: INFO: Scaling statefulset ss to 0
Mar  3 10:08:35.871: INFO: Waiting for statefulset status.replicas updated to 0
Mar  3 10:08:35.873: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:08:35.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7502" for this suite.

• [SLOW TEST:17.229 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":278,"skipped":4648,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:08:35.922: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:08:47.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6786" for this suite.

• [SLOW TEST:11.255 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":279,"skipped":4663,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:08:47.178: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Mar  3 10:08:47.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-8020 create -f -'
Mar  3 10:08:47.450: INFO: stderr: ""
Mar  3 10:08:47.450: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar  3 10:08:48.456: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  3 10:08:48.456: INFO: Found 0 / 1
Mar  3 10:08:49.457: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  3 10:08:49.457: INFO: Found 1 / 1
Mar  3 10:08:49.457: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar  3 10:08:49.460: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  3 10:08:49.460: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  3 10:08:49.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-8020 patch pod agnhost-primary-485h7 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  3 10:08:49.550: INFO: stderr: ""
Mar  3 10:08:49.550: INFO: stdout: "pod/agnhost-primary-485h7 patched\n"
STEP: checking annotations
Mar  3 10:08:49.554: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  3 10:08:49.554: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:08:49.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8020" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":280,"skipped":4684,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:08:49.564: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  3 10:08:50.039: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  3 10:08:52.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362930, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362930, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362930, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362929, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 10:08:55.122: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:08:55.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6401" for this suite.
STEP: Destroying namespace "webhook-6401-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.784 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":281,"skipped":4686,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:08:55.349: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Mar  3 10:08:55.409: INFO: namespace kubectl-1116
Mar  3 10:08:55.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-1116 create -f -'
Mar  3 10:08:55.629: INFO: stderr: ""
Mar  3 10:08:55.629: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar  3 10:08:56.637: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  3 10:08:56.637: INFO: Found 0 / 1
Mar  3 10:08:57.634: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  3 10:08:57.634: INFO: Found 1 / 1
Mar  3 10:08:57.634: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  3 10:08:57.637: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  3 10:08:57.637: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  3 10:08:57.637: INFO: wait on agnhost-primary startup in kubectl-1116 
Mar  3 10:08:57.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-1116 logs agnhost-primary-gxw7b agnhost-primary'
Mar  3 10:08:57.746: INFO: stderr: ""
Mar  3 10:08:57.746: INFO: stdout: "Paused\n"
STEP: exposing RC
Mar  3 10:08:57.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-1116 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar  3 10:08:57.891: INFO: stderr: ""
Mar  3 10:08:57.891: INFO: stdout: "service/rm2 exposed\n"
Mar  3 10:08:57.921: INFO: Service rm2 in namespace kubectl-1116 found.
STEP: exposing service
Mar  3 10:08:59.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-1116 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar  3 10:09:00.056: INFO: stderr: ""
Mar  3 10:09:00.056: INFO: stdout: "service/rm3 exposed\n"
Mar  3 10:09:00.071: INFO: Service rm3 in namespace kubectl-1116 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:09:02.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1116" for this suite.

• [SLOW TEST:6.745 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":282,"skipped":4705,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:09:02.094: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-9b72d41e-6444-4433-8f4f-c3c650030eda
STEP: Creating a pod to test consume secrets
Mar  3 10:09:02.170: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5a16b9d6-6e29-4e56-970a-0e43e5c7fee8" in namespace "projected-6888" to be "Succeeded or Failed"
Mar  3 10:09:02.177: INFO: Pod "pod-projected-secrets-5a16b9d6-6e29-4e56-970a-0e43e5c7fee8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.900219ms
Mar  3 10:09:04.181: INFO: Pod "pod-projected-secrets-5a16b9d6-6e29-4e56-970a-0e43e5c7fee8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010813633s
STEP: Saw pod success
Mar  3 10:09:04.181: INFO: Pod "pod-projected-secrets-5a16b9d6-6e29-4e56-970a-0e43e5c7fee8" satisfied condition "Succeeded or Failed"
Mar  3 10:09:04.183: INFO: Trying to get logs from node controller-0 pod pod-projected-secrets-5a16b9d6-6e29-4e56-970a-0e43e5c7fee8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  3 10:09:04.235: INFO: Waiting for pod pod-projected-secrets-5a16b9d6-6e29-4e56-970a-0e43e5c7fee8 to disappear
Mar  3 10:09:04.239: INFO: Pod pod-projected-secrets-5a16b9d6-6e29-4e56-970a-0e43e5c7fee8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:09:04.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6888" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":283,"skipped":4705,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:09:04.253: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  3 10:09:04.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362944, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362944, loc:(*time.Location)(0x797de40)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-6bd9446d55\""}}, CollisionCount:(*int32)(nil)}
Mar  3 10:09:07.006: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362944, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362944, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362944, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750362944, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 10:09:10.021: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:09:10.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8382" for this suite.
STEP: Destroying namespace "webhook-8382-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.233 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":284,"skipped":4746,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:09:10.491: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Mar  3 10:09:10.626: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar  3 10:09:10.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-1284 create -f -'
Mar  3 10:09:10.883: INFO: stderr: ""
Mar  3 10:09:10.883: INFO: stdout: "service/agnhost-replica created\n"
Mar  3 10:09:10.884: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar  3 10:09:10.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-1284 create -f -'
Mar  3 10:09:11.104: INFO: stderr: ""
Mar  3 10:09:11.104: INFO: stdout: "service/agnhost-primary created\n"
Mar  3 10:09:11.105: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  3 10:09:11.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-1284 create -f -'
Mar  3 10:09:11.387: INFO: stderr: ""
Mar  3 10:09:11.387: INFO: stdout: "service/frontend created\n"
Mar  3 10:09:11.387: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar  3 10:09:11.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-1284 create -f -'
Mar  3 10:09:11.619: INFO: stderr: ""
Mar  3 10:09:11.619: INFO: stdout: "deployment.apps/frontend created\n"
Mar  3 10:09:11.619: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  3 10:09:11.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-1284 create -f -'
Mar  3 10:09:11.867: INFO: stderr: ""
Mar  3 10:09:11.868: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar  3 10:09:11.868: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  3 10:09:11.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-1284 create -f -'
Mar  3 10:09:12.167: INFO: stderr: ""
Mar  3 10:09:12.168: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Mar  3 10:09:12.168: INFO: Waiting for all frontend pods to be Running.
Mar  3 10:09:17.218: INFO: Waiting for frontend to serve content.
Mar  3 10:09:17.230: INFO: Trying to add a new entry to the guestbook.
Mar  3 10:09:17.244: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar  3 10:09:17.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-1284 delete --grace-period=0 --force -f -'
Mar  3 10:09:17.406: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  3 10:09:17.406: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Mar  3 10:09:17.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-1284 delete --grace-period=0 --force -f -'
Mar  3 10:09:17.561: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  3 10:09:17.561: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar  3 10:09:17.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-1284 delete --grace-period=0 --force -f -'
Mar  3 10:09:17.656: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  3 10:09:17.656: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  3 10:09:17.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-1284 delete --grace-period=0 --force -f -'
Mar  3 10:09:17.747: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  3 10:09:17.747: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  3 10:09:17.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-1284 delete --grace-period=0 --force -f -'
Mar  3 10:09:17.860: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  3 10:09:17.860: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar  3 10:09:17.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-1284 delete --grace-period=0 --force -f -'
Mar  3 10:09:18.040: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  3 10:09:18.040: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:09:18.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1284" for this suite.

• [SLOW TEST:7.586 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":285,"skipped":4782,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:09:18.077: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-244
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-244
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-244
Mar  3 10:09:18.335: INFO: Found 0 stateful pods, waiting for 1
Mar  3 10:09:28.349: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  3 10:09:28.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-244 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  3 10:09:28.553: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  3 10:09:28.553: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  3 10:09:28.553: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  3 10:09:28.558: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  3 10:09:38.570: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  3 10:09:38.571: INFO: Waiting for statefulset status.replicas updated to 0
Mar  3 10:09:38.590: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998482s
Mar  3 10:09:39.597: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995197952s
Mar  3 10:09:40.604: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988229241s
Mar  3 10:09:41.609: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981147347s
Mar  3 10:09:42.614: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.976428037s
Mar  3 10:09:43.620: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.970989478s
Mar  3 10:09:44.626: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.96536571s
Mar  3 10:09:45.632: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.959311031s
Mar  3 10:09:46.640: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.952902027s
Mar  3 10:09:47.646: INFO: Verifying statefulset ss doesn't scale past 1 for another 945.613643ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-244
Mar  3 10:09:48.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-244 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  3 10:09:48.822: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  3 10:09:48.822: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  3 10:09:48.822: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  3 10:09:48.827: INFO: Found 1 stateful pods, waiting for 3
Mar  3 10:09:58.838: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  3 10:09:58.838: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  3 10:09:58.838: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar  3 10:09:58.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-244 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  3 10:09:59.002: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  3 10:09:59.002: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  3 10:09:59.002: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  3 10:09:59.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-244 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  3 10:09:59.171: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  3 10:09:59.171: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  3 10:09:59.171: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  3 10:09:59.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-244 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  3 10:09:59.362: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  3 10:09:59.362: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  3 10:09:59.362: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  3 10:09:59.362: INFO: Waiting for statefulset status.replicas updated to 0
Mar  3 10:09:59.366: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar  3 10:10:09.381: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  3 10:10:09.381: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  3 10:10:09.381: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  3 10:10:09.394: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999842s
Mar  3 10:10:10.399: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996658329s
Mar  3 10:10:11.410: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991166179s
Mar  3 10:10:12.417: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979820189s
Mar  3 10:10:13.424: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972744643s
Mar  3 10:10:14.428: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.966492065s
Mar  3 10:10:15.433: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.961697221s
Mar  3 10:10:16.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957569165s
Mar  3 10:10:17.443: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.951807015s
Mar  3 10:10:18.450: INFO: Verifying statefulset ss doesn't scale past 3 for another 946.654094ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-244
Mar  3 10:10:19.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-244 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  3 10:10:19.618: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  3 10:10:19.618: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  3 10:10:19.618: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  3 10:10:19.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-244 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  3 10:10:19.782: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  3 10:10:19.782: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  3 10:10:19.782: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  3 10:10:19.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=statefulset-244 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  3 10:10:19.964: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  3 10:10:19.964: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  3 10:10:19.964: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  3 10:10:19.964: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  3 10:10:49.987: INFO: Deleting all statefulset in ns statefulset-244
Mar  3 10:10:49.990: INFO: Scaling statefulset ss to 0
Mar  3 10:10:50.004: INFO: Waiting for statefulset status.replicas updated to 0
Mar  3 10:10:50.007: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:10:50.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-244" for this suite.

• [SLOW TEST:91.971 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":286,"skipped":4792,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:10:50.048: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:11:03.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3266" for this suite.

• [SLOW TEST:13.171 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":287,"skipped":4792,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:11:03.224: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  3 10:11:03.314: INFO: Number of nodes with available pods: 0
Mar  3 10:11:03.314: INFO: Node controller-0 is running more than one daemon pod
Mar  3 10:11:04.325: INFO: Number of nodes with available pods: 0
Mar  3 10:11:04.325: INFO: Node controller-0 is running more than one daemon pod
Mar  3 10:11:05.322: INFO: Number of nodes with available pods: 3
Mar  3 10:11:05.322: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  3 10:11:05.346: INFO: Number of nodes with available pods: 3
Mar  3 10:11:05.346: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3158, will wait for the garbage collector to delete the pods
Mar  3 10:11:06.422: INFO: Deleting DaemonSet.extensions daemon-set took: 9.635165ms
Mar  3 10:11:07.123: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.291491ms
Mar  3 10:11:17.731: INFO: Number of nodes with available pods: 0
Mar  3 10:11:17.731: INFO: Number of running nodes: 0, number of available pods: 0
Mar  3 10:11:17.734: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23819"},"items":null}

Mar  3 10:11:17.737: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23819"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:11:17.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3158" for this suite.

• [SLOW TEST:14.529 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":288,"skipped":4859,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:11:17.753: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:11:17.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-408" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":289,"skipped":4865,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:11:17.836: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Mar  3 10:11:17.871: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-7814 proxy --unix-socket=/tmp/kubectl-proxy-unix692631437/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:11:17.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7814" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":290,"skipped":4898,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:11:17.963: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-a45ed59c-c650-44cc-93c1-281029a844d2
STEP: Creating a pod to test consume configMaps
Mar  3 10:11:18.010: INFO: Waiting up to 5m0s for pod "pod-configmaps-d0406164-3e39-4f3a-b908-f37db55295d6" in namespace "configmap-6151" to be "Succeeded or Failed"
Mar  3 10:11:18.028: INFO: Pod "pod-configmaps-d0406164-3e39-4f3a-b908-f37db55295d6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.224086ms
Mar  3 10:11:20.037: INFO: Pod "pod-configmaps-d0406164-3e39-4f3a-b908-f37db55295d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026570549s
STEP: Saw pod success
Mar  3 10:11:20.037: INFO: Pod "pod-configmaps-d0406164-3e39-4f3a-b908-f37db55295d6" satisfied condition "Succeeded or Failed"
Mar  3 10:11:20.040: INFO: Trying to get logs from node worker-1 pod pod-configmaps-d0406164-3e39-4f3a-b908-f37db55295d6 container agnhost-container: <nil>
STEP: delete the pod
Mar  3 10:11:20.075: INFO: Waiting for pod pod-configmaps-d0406164-3e39-4f3a-b908-f37db55295d6 to disappear
Mar  3 10:11:20.079: INFO: Pod pod-configmaps-d0406164-3e39-4f3a-b908-f37db55295d6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:11:20.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6151" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":291,"skipped":4924,"failed":0}
SS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:11:20.108: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 10:11:20.887: INFO: Checking APIGroup: apiregistration.k8s.io
Mar  3 10:11:20.888: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar  3 10:11:20.888: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Mar  3 10:11:20.889: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar  3 10:11:20.889: INFO: Checking APIGroup: apps
Mar  3 10:11:20.891: INFO: PreferredVersion.GroupVersion: apps/v1
Mar  3 10:11:20.891: INFO: Versions found [{apps/v1 v1}]
Mar  3 10:11:20.891: INFO: apps/v1 matches apps/v1
Mar  3 10:11:20.891: INFO: Checking APIGroup: events.k8s.io
Mar  3 10:11:20.892: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar  3 10:11:20.892: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Mar  3 10:11:20.893: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar  3 10:11:20.893: INFO: Checking APIGroup: authentication.k8s.io
Mar  3 10:11:20.895: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar  3 10:11:20.895: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Mar  3 10:11:20.895: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar  3 10:11:20.895: INFO: Checking APIGroup: authorization.k8s.io
Mar  3 10:11:20.896: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar  3 10:11:20.896: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Mar  3 10:11:20.896: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar  3 10:11:20.897: INFO: Checking APIGroup: autoscaling
Mar  3 10:11:20.898: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Mar  3 10:11:20.898: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Mar  3 10:11:20.898: INFO: autoscaling/v1 matches autoscaling/v1
Mar  3 10:11:20.898: INFO: Checking APIGroup: batch
Mar  3 10:11:20.900: INFO: PreferredVersion.GroupVersion: batch/v1
Mar  3 10:11:20.900: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Mar  3 10:11:20.900: INFO: batch/v1 matches batch/v1
Mar  3 10:11:20.900: INFO: Checking APIGroup: certificates.k8s.io
Mar  3 10:11:20.902: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar  3 10:11:20.902: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Mar  3 10:11:20.902: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar  3 10:11:20.902: INFO: Checking APIGroup: networking.k8s.io
Mar  3 10:11:20.903: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar  3 10:11:20.903: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Mar  3 10:11:20.903: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar  3 10:11:20.903: INFO: Checking APIGroup: extensions
Mar  3 10:11:20.904: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Mar  3 10:11:20.904: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Mar  3 10:11:20.904: INFO: extensions/v1beta1 matches extensions/v1beta1
Mar  3 10:11:20.904: INFO: Checking APIGroup: policy
Mar  3 10:11:20.906: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Mar  3 10:11:20.906: INFO: Versions found [{policy/v1beta1 v1beta1}]
Mar  3 10:11:20.906: INFO: policy/v1beta1 matches policy/v1beta1
Mar  3 10:11:20.906: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar  3 10:11:20.907: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar  3 10:11:20.907: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Mar  3 10:11:20.907: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar  3 10:11:20.907: INFO: Checking APIGroup: storage.k8s.io
Mar  3 10:11:20.909: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar  3 10:11:20.909: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar  3 10:11:20.909: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar  3 10:11:20.909: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar  3 10:11:20.910: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar  3 10:11:20.910: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Mar  3 10:11:20.910: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar  3 10:11:20.910: INFO: Checking APIGroup: apiextensions.k8s.io
Mar  3 10:11:20.912: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar  3 10:11:20.912: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Mar  3 10:11:20.912: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar  3 10:11:20.912: INFO: Checking APIGroup: scheduling.k8s.io
Mar  3 10:11:20.913: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar  3 10:11:20.913: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Mar  3 10:11:20.913: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar  3 10:11:20.913: INFO: Checking APIGroup: coordination.k8s.io
Mar  3 10:11:20.914: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar  3 10:11:20.915: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Mar  3 10:11:20.915: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar  3 10:11:20.915: INFO: Checking APIGroup: node.k8s.io
Mar  3 10:11:20.916: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar  3 10:11:20.916: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Mar  3 10:11:20.916: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar  3 10:11:20.916: INFO: Checking APIGroup: discovery.k8s.io
Mar  3 10:11:20.917: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Mar  3 10:11:20.917: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Mar  3 10:11:20.917: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Mar  3 10:11:20.917: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar  3 10:11:20.919: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Mar  3 10:11:20.919: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Mar  3 10:11:20.919: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Mar  3 10:11:20.919: INFO: Checking APIGroup: crd.projectcalico.org
Mar  3 10:11:20.926: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Mar  3 10:11:20.926: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Mar  3 10:11:20.926: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Mar  3 10:11:20.926: INFO: Checking APIGroup: helm.k0sproject.io
Mar  3 10:11:20.928: INFO: PreferredVersion.GroupVersion: helm.k0sproject.io/v1beta1
Mar  3 10:11:20.928: INFO: Versions found [{helm.k0sproject.io/v1beta1 v1beta1}]
Mar  3 10:11:20.928: INFO: helm.k0sproject.io/v1beta1 matches helm.k0sproject.io/v1beta1
Mar  3 10:11:20.928: INFO: Checking APIGroup: metrics.k8s.io
Mar  3 10:11:20.929: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Mar  3 10:11:20.929: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Mar  3 10:11:20.929: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:11:20.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-3480" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":292,"skipped":4926,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:11:20.939: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-4b915628-96e9-49da-b452-1f35d5c37d39
STEP: Creating configMap with name cm-test-opt-upd-c1511e1e-3083-43ac-919d-f038cea62452
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4b915628-96e9-49da-b452-1f35d5c37d39
STEP: Updating configmap cm-test-opt-upd-c1511e1e-3083-43ac-919d-f038cea62452
STEP: Creating configMap with name cm-test-opt-create-39030ddd-7c19-4922-a884-fed8ec8ada5b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:11:27.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3814" for this suite.

• [SLOW TEST:6.142 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":293,"skipped":4962,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:11:27.081: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 10:11:27.129: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e5003664-2131-4af8-924e-31c0d900c06b" in namespace "downward-api-3534" to be "Succeeded or Failed"
Mar  3 10:11:27.152: INFO: Pod "downwardapi-volume-e5003664-2131-4af8-924e-31c0d900c06b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.99191ms
Mar  3 10:11:29.157: INFO: Pod "downwardapi-volume-e5003664-2131-4af8-924e-31c0d900c06b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027946472s
STEP: Saw pod success
Mar  3 10:11:29.157: INFO: Pod "downwardapi-volume-e5003664-2131-4af8-924e-31c0d900c06b" satisfied condition "Succeeded or Failed"
Mar  3 10:11:29.161: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-e5003664-2131-4af8-924e-31c0d900c06b container client-container: <nil>
STEP: delete the pod
Mar  3 10:11:29.192: INFO: Waiting for pod downwardapi-volume-e5003664-2131-4af8-924e-31c0d900c06b to disappear
Mar  3 10:11:29.194: INFO: Pod downwardapi-volume-e5003664-2131-4af8-924e-31c0d900c06b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:11:29.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3534" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":294,"skipped":4963,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:11:29.203: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 10:11:29.253: INFO: Waiting up to 5m0s for pod "busybox-user-65534-87279524-c2d8-411b-a833-357210c9aed1" in namespace "security-context-test-6649" to be "Succeeded or Failed"
Mar  3 10:11:29.256: INFO: Pod "busybox-user-65534-87279524-c2d8-411b-a833-357210c9aed1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.727539ms
Mar  3 10:11:31.263: INFO: Pod "busybox-user-65534-87279524-c2d8-411b-a833-357210c9aed1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00978947s
Mar  3 10:11:31.263: INFO: Pod "busybox-user-65534-87279524-c2d8-411b-a833-357210c9aed1" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:11:31.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6649" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":295,"skipped":4981,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:11:31.272: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 10:11:31.337: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar  3 10:11:31.358: INFO: Number of nodes with available pods: 0
Mar  3 10:11:31.358: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar  3 10:11:31.394: INFO: Number of nodes with available pods: 0
Mar  3 10:11:31.394: INFO: Node worker-1 is running more than one daemon pod
Mar  3 10:11:32.399: INFO: Number of nodes with available pods: 0
Mar  3 10:11:32.399: INFO: Node worker-1 is running more than one daemon pod
Mar  3 10:11:33.400: INFO: Number of nodes with available pods: 1
Mar  3 10:11:33.400: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar  3 10:11:33.421: INFO: Number of nodes with available pods: 1
Mar  3 10:11:33.421: INFO: Number of running nodes: 0, number of available pods: 1
Mar  3 10:11:34.429: INFO: Number of nodes with available pods: 0
Mar  3 10:11:34.430: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar  3 10:11:34.444: INFO: Number of nodes with available pods: 0
Mar  3 10:11:34.444: INFO: Node worker-1 is running more than one daemon pod
Mar  3 10:11:35.449: INFO: Number of nodes with available pods: 0
Mar  3 10:11:35.449: INFO: Node worker-1 is running more than one daemon pod
Mar  3 10:11:36.450: INFO: Number of nodes with available pods: 0
Mar  3 10:11:36.450: INFO: Node worker-1 is running more than one daemon pod
Mar  3 10:11:37.450: INFO: Number of nodes with available pods: 0
Mar  3 10:11:37.450: INFO: Node worker-1 is running more than one daemon pod
Mar  3 10:11:38.450: INFO: Number of nodes with available pods: 0
Mar  3 10:11:38.450: INFO: Node worker-1 is running more than one daemon pod
Mar  3 10:11:39.451: INFO: Number of nodes with available pods: 0
Mar  3 10:11:39.451: INFO: Node worker-1 is running more than one daemon pod
Mar  3 10:11:40.451: INFO: Number of nodes with available pods: 0
Mar  3 10:11:40.451: INFO: Node worker-1 is running more than one daemon pod
Mar  3 10:11:41.455: INFO: Number of nodes with available pods: 0
Mar  3 10:11:41.455: INFO: Node worker-1 is running more than one daemon pod
Mar  3 10:11:42.451: INFO: Number of nodes with available pods: 0
Mar  3 10:11:42.451: INFO: Node worker-1 is running more than one daemon pod
Mar  3 10:11:43.450: INFO: Number of nodes with available pods: 0
Mar  3 10:11:43.450: INFO: Node worker-1 is running more than one daemon pod
Mar  3 10:11:44.449: INFO: Number of nodes with available pods: 0
Mar  3 10:11:44.449: INFO: Node worker-1 is running more than one daemon pod
Mar  3 10:11:45.450: INFO: Number of nodes with available pods: 0
Mar  3 10:11:45.450: INFO: Node worker-1 is running more than one daemon pod
Mar  3 10:11:46.451: INFO: Number of nodes with available pods: 0
Mar  3 10:11:46.451: INFO: Node worker-1 is running more than one daemon pod
Mar  3 10:11:47.453: INFO: Number of nodes with available pods: 0
Mar  3 10:11:47.453: INFO: Node worker-1 is running more than one daemon pod
Mar  3 10:11:48.449: INFO: Number of nodes with available pods: 0
Mar  3 10:11:48.449: INFO: Node worker-1 is running more than one daemon pod
Mar  3 10:11:49.449: INFO: Number of nodes with available pods: 1
Mar  3 10:11:49.449: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4117, will wait for the garbage collector to delete the pods
Mar  3 10:11:49.514: INFO: Deleting DaemonSet.extensions daemon-set took: 5.454193ms
Mar  3 10:11:50.214: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.260767ms
Mar  3 10:11:57.722: INFO: Number of nodes with available pods: 0
Mar  3 10:11:57.722: INFO: Number of running nodes: 0, number of available pods: 0
Mar  3 10:11:57.725: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24133"},"items":null}

Mar  3 10:11:57.729: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24133"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:11:57.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4117" for this suite.

• [SLOW TEST:26.487 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":296,"skipped":4990,"failed":0}
S
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:11:57.761: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:11:57.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4067" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":297,"skipped":4991,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:11:57.853: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 10:11:57.935: INFO: The status of Pod test-webserver-409611be-b800-4e72-a169-5040a1682297 is Pending, waiting for it to be Running (with Ready = true)
Mar  3 10:11:59.943: INFO: The status of Pod test-webserver-409611be-b800-4e72-a169-5040a1682297 is Pending, waiting for it to be Running (with Ready = true)
Mar  3 10:12:01.944: INFO: The status of Pod test-webserver-409611be-b800-4e72-a169-5040a1682297 is Running (Ready = false)
Mar  3 10:12:03.943: INFO: The status of Pod test-webserver-409611be-b800-4e72-a169-5040a1682297 is Running (Ready = false)
Mar  3 10:12:05.940: INFO: The status of Pod test-webserver-409611be-b800-4e72-a169-5040a1682297 is Running (Ready = false)
Mar  3 10:12:07.943: INFO: The status of Pod test-webserver-409611be-b800-4e72-a169-5040a1682297 is Running (Ready = false)
Mar  3 10:12:09.943: INFO: The status of Pod test-webserver-409611be-b800-4e72-a169-5040a1682297 is Running (Ready = false)
Mar  3 10:12:11.943: INFO: The status of Pod test-webserver-409611be-b800-4e72-a169-5040a1682297 is Running (Ready = false)
Mar  3 10:12:13.944: INFO: The status of Pod test-webserver-409611be-b800-4e72-a169-5040a1682297 is Running (Ready = false)
Mar  3 10:12:15.943: INFO: The status of Pod test-webserver-409611be-b800-4e72-a169-5040a1682297 is Running (Ready = false)
Mar  3 10:12:17.943: INFO: The status of Pod test-webserver-409611be-b800-4e72-a169-5040a1682297 is Running (Ready = false)
Mar  3 10:12:19.943: INFO: The status of Pod test-webserver-409611be-b800-4e72-a169-5040a1682297 is Running (Ready = true)
Mar  3 10:12:19.946: INFO: Container started at 2021-03-03 10:12:00 +0000 UTC, pod became ready at 2021-03-03 10:12:18 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:12:19.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-105" for this suite.

• [SLOW TEST:22.105 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":298,"skipped":5012,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:12:19.961: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 10:12:20.011: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  3 10:12:23.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-7069 --namespace=crd-publish-openapi-7069 create -f -'
Mar  3 10:12:24.239: INFO: stderr: ""
Mar  3 10:12:24.239: INFO: stdout: "e2e-test-crd-publish-openapi-7053-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  3 10:12:24.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-7069 --namespace=crd-publish-openapi-7069 delete e2e-test-crd-publish-openapi-7053-crds test-cr'
Mar  3 10:12:24.321: INFO: stderr: ""
Mar  3 10:12:24.322: INFO: stdout: "e2e-test-crd-publish-openapi-7053-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar  3 10:12:24.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-7069 --namespace=crd-publish-openapi-7069 apply -f -'
Mar  3 10:12:24.543: INFO: stderr: ""
Mar  3 10:12:24.543: INFO: stdout: "e2e-test-crd-publish-openapi-7053-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  3 10:12:24.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-7069 --namespace=crd-publish-openapi-7069 delete e2e-test-crd-publish-openapi-7053-crds test-cr'
Mar  3 10:12:24.629: INFO: stderr: ""
Mar  3 10:12:24.629: INFO: stdout: "e2e-test-crd-publish-openapi-7053-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar  3 10:12:24.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=crd-publish-openapi-7069 explain e2e-test-crd-publish-openapi-7053-crds'
Mar  3 10:12:24.851: INFO: stderr: ""
Mar  3 10:12:24.851: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7053-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:12:29.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7069" for this suite.

• [SLOW TEST:9.113 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":299,"skipped":5023,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:12:29.074: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  3 10:12:29.537: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 10:12:32.560: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:12:32.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-348" for this suite.
STEP: Destroying namespace "webhook-348-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":300,"skipped":5026,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:12:32.752: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar  3 10:12:33.471: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar  3 10:12:35.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750363153, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750363153, loc:(*time.Location)(0x797de40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750363153, loc:(*time.Location)(0x797de40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750363153, loc:(*time.Location)(0x797de40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  3 10:12:38.501: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 10:12:38.507: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:12:39.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5445" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.382 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":301,"skipped":5044,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:12:40.135: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Mar  3 10:12:40.225: INFO: observed Pod pod-test in namespace pods-9556 in phase Pending conditions []
Mar  3 10:12:40.231: INFO: observed Pod pod-test in namespace pods-9556 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 10:12:40 +0000 UTC  }]
Mar  3 10:12:40.257: INFO: observed Pod pod-test in namespace pods-9556 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 10:12:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 10:12:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 10:12:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 10:12:40 +0000 UTC  }]
Mar  3 10:12:40.787: INFO: observed Pod pod-test in namespace pods-9556 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 10:12:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 10:12:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-03 10:12:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-03 10:12:40 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Mar  3 10:12:41.145: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Mar  3 10:12:41.173: INFO: observed event type ADDED
Mar  3 10:12:41.173: INFO: observed event type MODIFIED
Mar  3 10:12:41.180: INFO: observed event type MODIFIED
Mar  3 10:12:41.180: INFO: observed event type MODIFIED
Mar  3 10:12:41.180: INFO: observed event type MODIFIED
Mar  3 10:12:41.180: INFO: observed event type MODIFIED
Mar  3 10:12:41.180: INFO: observed event type MODIFIED
Mar  3 10:12:41.180: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:12:41.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9556" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":302,"skipped":5046,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:12:41.189: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-e8efe71b-b40a-49d3-9342-23070abe9bba
STEP: Creating secret with name s-test-opt-upd-91665aa4-bd26-4ea0-bfd7-861d0cdd0c93
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-e8efe71b-b40a-49d3-9342-23070abe9bba
STEP: Updating secret s-test-opt-upd-91665aa4-bd26-4ea0-bfd7-861d0cdd0c93
STEP: Creating secret with name s-test-opt-create-0aa54513-6991-40e2-b2ca-d6be0a263b7c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:12:47.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5584" for this suite.

• [SLOW TEST:6.158 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":303,"skipped":5049,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:12:47.347: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  3 10:12:47.391: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  3 10:12:47.398: INFO: Waiting for terminating namespaces to be deleted...
Mar  3 10:12:47.400: INFO: 
Logging pods the apiserver thinks is on node controller-0 before test
Mar  3 10:12:47.406: INFO: calico-kube-controllers-5f6546844f-gnl9f from kube-system started at 2021-03-03 08:47:09 +0000 UTC (1 container statuses recorded)
Mar  3 10:12:47.406: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  3 10:12:47.406: INFO: calico-node-468vb from kube-system started at 2021-03-03 08:46:49 +0000 UTC (1 container statuses recorded)
Mar  3 10:12:47.406: INFO: 	Container calico-node ready: true, restart count 0
Mar  3 10:12:47.406: INFO: coredns-5c98d7d4d8-96hpd from kube-system started at 2021-03-03 08:47:11 +0000 UTC (1 container statuses recorded)
Mar  3 10:12:47.406: INFO: 	Container coredns ready: true, restart count 0
Mar  3 10:12:47.406: INFO: konnectivity-agent-882qs from kube-system started at 2021-03-03 08:47:01 +0000 UTC (1 container statuses recorded)
Mar  3 10:12:47.406: INFO: 	Container konnectivity-agent ready: true, restart count 1
Mar  3 10:12:47.406: INFO: kube-proxy-snqpb from kube-system started at 2021-03-03 08:46:30 +0000 UTC (1 container statuses recorded)
Mar  3 10:12:47.406: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  3 10:12:47.406: INFO: metrics-server-6fbcd86f7b-r2c26 from kube-system started at 2021-03-03 08:47:09 +0000 UTC (1 container statuses recorded)
Mar  3 10:12:47.406: INFO: 	Container metrics-server ready: true, restart count 0
Mar  3 10:12:47.406: INFO: sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-8fglb from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 10:12:47.406: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Mar  3 10:12:47.406: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  3 10:12:47.406: INFO: 
Logging pods the apiserver thinks is on node worker-0 before test
Mar  3 10:12:47.414: INFO: calico-node-4vzkv from kube-system started at 2021-03-03 08:50:42 +0000 UTC (1 container statuses recorded)
Mar  3 10:12:47.414: INFO: 	Container calico-node ready: true, restart count 0
Mar  3 10:12:47.414: INFO: konnectivity-agent-hrn26 from kube-system started at 2021-03-03 08:51:02 +0000 UTC (1 container statuses recorded)
Mar  3 10:12:47.414: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  3 10:12:47.414: INFO: kube-proxy-xwt8k from kube-system started at 2021-03-03 08:50:42 +0000 UTC (1 container statuses recorded)
Mar  3 10:12:47.414: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  3 10:12:47.414: INFO: sonobuoy-e2e-job-a123a876f42443b1 from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 10:12:47.414: INFO: 	Container e2e ready: true, restart count 0
Mar  3 10:12:47.414: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  3 10:12:47.414: INFO: sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-xjvxl from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 10:12:47.414: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Mar  3 10:12:47.414: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  3 10:12:47.414: INFO: 
Logging pods the apiserver thinks is on node worker-1 before test
Mar  3 10:12:47.421: INFO: calico-node-d2bzg from kube-system started at 2021-03-03 08:52:07 +0000 UTC (1 container statuses recorded)
Mar  3 10:12:47.421: INFO: 	Container calico-node ready: true, restart count 0
Mar  3 10:12:47.421: INFO: konnectivity-agent-ssw2j from kube-system started at 2021-03-03 09:38:46 +0000 UTC (1 container statuses recorded)
Mar  3 10:12:47.421: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  3 10:12:47.421: INFO: kube-proxy-c7zpn from kube-system started at 2021-03-03 08:52:07 +0000 UTC (1 container statuses recorded)
Mar  3 10:12:47.421: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  3 10:12:47.421: INFO: pod-projected-secrets-65de566c-b07b-48ac-bfb4-3b58d74f63ba from projected-5584 started at 2021-03-03 10:12:41 +0000 UTC (3 container statuses recorded)
Mar  3 10:12:47.421: INFO: 	Container creates-volume-test ready: true, restart count 0
Mar  3 10:12:47.421: INFO: 	Container dels-volume-test ready: true, restart count 0
Mar  3 10:12:47.421: INFO: 	Container upds-volume-test ready: true, restart count 0
Mar  3 10:12:47.421: INFO: sonobuoy from sonobuoy started at 2021-03-03 08:53:20 +0000 UTC (1 container statuses recorded)
Mar  3 10:12:47.421: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  3 10:12:47.421: INFO: sonobuoy-systemd-logs-daemon-set-3b846f5aae804927-x4pmv from sonobuoy started at 2021-03-03 08:53:27 +0000 UTC (2 container statuses recorded)
Mar  3 10:12:47.421: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Mar  3 10:12:47.421: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-1de26fcd-e1c0-465d-b15b-807dcd3c442b 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.49.5 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-1de26fcd-e1c0-465d-b15b-807dcd3c442b off the node controller-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1de26fcd-e1c0-465d-b15b-807dcd3c442b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:17:51.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2838" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:304.305 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":304,"skipped":5101,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:17:51.657: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Mar  3 10:17:51.711: INFO: Waiting up to 5m0s for pod "var-expansion-af885676-f443-43f5-a586-b96f75ba513c" in namespace "var-expansion-7721" to be "Succeeded or Failed"
Mar  3 10:17:51.728: INFO: Pod "var-expansion-af885676-f443-43f5-a586-b96f75ba513c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.703237ms
Mar  3 10:17:53.735: INFO: Pod "var-expansion-af885676-f443-43f5-a586-b96f75ba513c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024066618s
STEP: Saw pod success
Mar  3 10:17:53.735: INFO: Pod "var-expansion-af885676-f443-43f5-a586-b96f75ba513c" satisfied condition "Succeeded or Failed"
Mar  3 10:17:53.738: INFO: Trying to get logs from node worker-1 pod var-expansion-af885676-f443-43f5-a586-b96f75ba513c container dapi-container: <nil>
STEP: delete the pod
Mar  3 10:17:53.770: INFO: Waiting for pod var-expansion-af885676-f443-43f5-a586-b96f75ba513c to disappear
Mar  3 10:17:53.772: INFO: Pod var-expansion-af885676-f443-43f5-a586-b96f75ba513c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:17:53.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7721" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":305,"skipped":5131,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:17:53.787: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Mar  3 10:17:53.836: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2542b94c-4fb9-4fdb-baa8-1438012e6490" in namespace "projected-3739" to be "Succeeded or Failed"
Mar  3 10:17:53.855: INFO: Pod "downwardapi-volume-2542b94c-4fb9-4fdb-baa8-1438012e6490": Phase="Pending", Reason="", readiness=false. Elapsed: 18.566294ms
Mar  3 10:17:55.861: INFO: Pod "downwardapi-volume-2542b94c-4fb9-4fdb-baa8-1438012e6490": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025389242s
STEP: Saw pod success
Mar  3 10:17:55.861: INFO: Pod "downwardapi-volume-2542b94c-4fb9-4fdb-baa8-1438012e6490" satisfied condition "Succeeded or Failed"
Mar  3 10:17:55.864: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-2542b94c-4fb9-4fdb-baa8-1438012e6490 container client-container: <nil>
STEP: delete the pod
Mar  3 10:17:55.883: INFO: Waiting for pod downwardapi-volume-2542b94c-4fb9-4fdb-baa8-1438012e6490 to disappear
Mar  3 10:17:55.885: INFO: Pod downwardapi-volume-2542b94c-4fb9-4fdb-baa8-1438012e6490 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:17:55.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3739" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":306,"skipped":5222,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:17:55.894: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Mar  3 10:17:55.940: INFO: Waiting up to 5m0s for pod "test-pod-f27f2456-6077-43b2-94a3-38888e2196a8" in namespace "svcaccounts-9272" to be "Succeeded or Failed"
Mar  3 10:17:55.946: INFO: Pod "test-pod-f27f2456-6077-43b2-94a3-38888e2196a8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.274396ms
Mar  3 10:17:57.951: INFO: Pod "test-pod-f27f2456-6077-43b2-94a3-38888e2196a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011166835s
STEP: Saw pod success
Mar  3 10:17:57.951: INFO: Pod "test-pod-f27f2456-6077-43b2-94a3-38888e2196a8" satisfied condition "Succeeded or Failed"
Mar  3 10:17:57.954: INFO: Trying to get logs from node worker-1 pod test-pod-f27f2456-6077-43b2-94a3-38888e2196a8 container agnhost-container: <nil>
STEP: delete the pod
Mar  3 10:17:57.969: INFO: Waiting for pod test-pod-f27f2456-6077-43b2-94a3-38888e2196a8 to disappear
Mar  3 10:17:57.975: INFO: Pod test-pod-f27f2456-6077-43b2-94a3-38888e2196a8 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:17:57.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9272" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":307,"skipped":5283,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:17:57.985: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  3 10:18:00.152: INFO: Waiting up to 5m0s for pod "client-envvars-f357f121-f20d-49d9-a3ab-361f48015380" in namespace "pods-172" to be "Succeeded or Failed"
Mar  3 10:18:00.171: INFO: Pod "client-envvars-f357f121-f20d-49d9-a3ab-361f48015380": Phase="Pending", Reason="", readiness=false. Elapsed: 19.435684ms
Mar  3 10:18:02.179: INFO: Pod "client-envvars-f357f121-f20d-49d9-a3ab-361f48015380": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027422729s
STEP: Saw pod success
Mar  3 10:18:02.179: INFO: Pod "client-envvars-f357f121-f20d-49d9-a3ab-361f48015380" satisfied condition "Succeeded or Failed"
Mar  3 10:18:02.182: INFO: Trying to get logs from node worker-0 pod client-envvars-f357f121-f20d-49d9-a3ab-361f48015380 container env3cont: <nil>
STEP: delete the pod
Mar  3 10:18:02.282: INFO: Waiting for pod client-envvars-f357f121-f20d-49d9-a3ab-361f48015380 to disappear
Mar  3 10:18:02.289: INFO: Pod client-envvars-f357f121-f20d-49d9-a3ab-361f48015380 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:18:02.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-172" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":308,"skipped":5291,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:18:02.328: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-d03fecb2-fb28-4c62-a348-ae7648a6b34d
STEP: Creating a pod to test consume configMaps
Mar  3 10:18:02.390: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a4792b6a-5c85-476c-a0c3-db04c7ab7b64" in namespace "projected-5596" to be "Succeeded or Failed"
Mar  3 10:18:02.396: INFO: Pod "pod-projected-configmaps-a4792b6a-5c85-476c-a0c3-db04c7ab7b64": Phase="Pending", Reason="", readiness=false. Elapsed: 6.371136ms
Mar  3 10:18:04.404: INFO: Pod "pod-projected-configmaps-a4792b6a-5c85-476c-a0c3-db04c7ab7b64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014004835s
STEP: Saw pod success
Mar  3 10:18:04.404: INFO: Pod "pod-projected-configmaps-a4792b6a-5c85-476c-a0c3-db04c7ab7b64" satisfied condition "Succeeded or Failed"
Mar  3 10:18:04.406: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-a4792b6a-5c85-476c-a0c3-db04c7ab7b64 container agnhost-container: <nil>
STEP: delete the pod
Mar  3 10:18:04.426: INFO: Waiting for pod pod-projected-configmaps-a4792b6a-5c85-476c-a0c3-db04c7ab7b64 to disappear
Mar  3 10:18:04.429: INFO: Pod pod-projected-configmaps-a4792b6a-5c85-476c-a0c3-db04c7ab7b64 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:18:04.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5596" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":309,"skipped":5298,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:18:04.441: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  3 10:18:04.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3727 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Mar  3 10:18:04.584: INFO: stderr: ""
Mar  3 10:18:04.584: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Mar  3 10:18:04.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3727 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Mar  3 10:18:04.872: INFO: stderr: ""
Mar  3 10:18:04.872: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Mar  3 10:18:04.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-613311166 --namespace=kubectl-3727 delete pods e2e-test-httpd-pod'
Mar  3 10:18:07.102: INFO: stderr: ""
Mar  3 10:18:07.102: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:18:07.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3727" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":310,"skipped":5317,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar  3 10:18:07.190: INFO: >>> kubeConfig: /tmp/kubeconfig-613311166
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  3 10:18:11.312: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  3 10:18:11.320: INFO: Pod pod-with-prestop-http-hook still exists
Mar  3 10:18:13.320: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  3 10:18:13.328: INFO: Pod pod-with-prestop-http-hook still exists
Mar  3 10:18:15.320: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  3 10:18:15.327: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  3 10:18:15.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7384" for this suite.

• [SLOW TEST:8.156 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":311,"skipped":5347,"failed":0}
SSSSSSSSSMar  3 10:18:15.346: INFO: Running AfterSuite actions on all nodes
Mar  3 10:18:15.346: INFO: Running AfterSuite actions on node 1
Mar  3 10:18:15.346: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 5069.804 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
PASS

Ginkgo ran 1 suite in 1h24m31.363751642s
Test Suite Passed
