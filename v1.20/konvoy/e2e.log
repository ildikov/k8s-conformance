I0422 20:39:34.444047      24 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-002341601
I0422 20:39:34.444070      24 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0422 20:39:34.444194      24 e2e.go:129] Starting e2e run "3c32a897-c486-4ee7-bcc8-1f01dd37d2e8" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1619123973 - Will randomize all specs
Will run 311 of 5667 specs

Apr 22 20:39:34.457: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
E0422 20:39:34.457987      24 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Apr 22 20:39:34.458: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr 22 20:39:34.476: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr 22 20:39:34.506: INFO: 29 / 29 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr 22 20:39:34.506: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Apr 22 20:39:34.506: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr 22 20:39:34.513: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Apr 22 20:39:34.513: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Apr 22 20:39:34.513: INFO: e2e test version: v1.20.6
Apr 22 20:39:34.514: INFO: kube-apiserver version: v1.20.6
Apr 22 20:39:34.514: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 20:39:34.517: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:39:34.518: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename configmap
Apr 22 20:39:34.553: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:39:34.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9305" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":1,"skipped":35,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:39:34.587: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename downward-api
E0422 20:39:34.589338      24 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 20:39:34.618: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9eddbb36-f8b2-4a3c-874a-82dd584255f9" in namespace "downward-api-8211" to be "Succeeded or Failed"
Apr 22 20:39:34.620: INFO: Pod "downwardapi-volume-9eddbb36-f8b2-4a3c-874a-82dd584255f9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.975253ms
Apr 22 20:39:36.626: INFO: Pod "downwardapi-volume-9eddbb36-f8b2-4a3c-874a-82dd584255f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007804518s
Apr 22 20:39:38.631: INFO: Pod "downwardapi-volume-9eddbb36-f8b2-4a3c-874a-82dd584255f9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013481988s
Apr 22 20:39:40.637: INFO: Pod "downwardapi-volume-9eddbb36-f8b2-4a3c-874a-82dd584255f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019393462s
STEP: Saw pod success
Apr 22 20:39:40.637: INFO: Pod "downwardapi-volume-9eddbb36-f8b2-4a3c-874a-82dd584255f9" satisfied condition "Succeeded or Failed"
Apr 22 20:39:40.639: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod downwardapi-volume-9eddbb36-f8b2-4a3c-874a-82dd584255f9 container client-container: <nil>
STEP: delete the pod
Apr 22 20:39:40.663: INFO: Waiting for pod downwardapi-volume-9eddbb36-f8b2-4a3c-874a-82dd584255f9 to disappear
Apr 22 20:39:40.665: INFO: Pod downwardapi-volume-9eddbb36-f8b2-4a3c-874a-82dd584255f9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:39:40.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8211" for this suite.

• [SLOW TEST:6.085 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":2,"skipped":45,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:39:40.672: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:39:49.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1131" for this suite.

• [SLOW TEST:8.725 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":3,"skipped":65,"failed":0}
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:39:49.397: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:40:11.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3479" for this suite.
STEP: Destroying namespace "nsdeletetest-6754" for this suite.
Apr 22 20:40:11.502: INFO: Namespace nsdeletetest-6754 was already deleted
STEP: Destroying namespace "nsdeletetest-9330" for this suite.

• [SLOW TEST:22.110 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":4,"skipped":67,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:40:11.507: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-7f7a22bb-5be6-48da-9af7-a59f4513c42f
STEP: Creating a pod to test consume secrets
Apr 22 20:40:11.544: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d8ac420a-e579-465e-b3df-40c5f76ddc20" in namespace "projected-8001" to be "Succeeded or Failed"
Apr 22 20:40:11.546: INFO: Pod "pod-projected-secrets-d8ac420a-e579-465e-b3df-40c5f76ddc20": Phase="Pending", Reason="", readiness=false. Elapsed: 1.932582ms
Apr 22 20:40:13.551: INFO: Pod "pod-projected-secrets-d8ac420a-e579-465e-b3df-40c5f76ddc20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007364243s
Apr 22 20:40:15.557: INFO: Pod "pod-projected-secrets-d8ac420a-e579-465e-b3df-40c5f76ddc20": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013316735s
Apr 22 20:40:17.563: INFO: Pod "pod-projected-secrets-d8ac420a-e579-465e-b3df-40c5f76ddc20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019256122s
STEP: Saw pod success
Apr 22 20:40:17.563: INFO: Pod "pod-projected-secrets-d8ac420a-e579-465e-b3df-40c5f76ddc20" satisfied condition "Succeeded or Failed"
Apr 22 20:40:17.565: INFO: Trying to get logs from node ip-10-0-130-230.us-west-2.compute.internal pod pod-projected-secrets-d8ac420a-e579-465e-b3df-40c5f76ddc20 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 22 20:40:17.599: INFO: Waiting for pod pod-projected-secrets-d8ac420a-e579-465e-b3df-40c5f76ddc20 to disappear
Apr 22 20:40:17.601: INFO: Pod pod-projected-secrets-d8ac420a-e579-465e-b3df-40c5f76ddc20 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:40:17.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8001" for this suite.

• [SLOW TEST:6.101 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":5,"skipped":100,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:40:17.608: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:40:17.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6592" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":6,"skipped":132,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:40:17.642: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 22 20:40:17.953: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 20:40:20.973: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 20:40:20.978: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2252-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:40:22.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9672" for this suite.
STEP: Destroying namespace "webhook-9672-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":7,"skipped":136,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:40:22.213: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 22 20:40:22.254: INFO: Waiting up to 5m0s for pod "downward-api-750d4e6e-cdb8-4c1d-ae81-d071c208350b" in namespace "downward-api-140" to be "Succeeded or Failed"
Apr 22 20:40:22.259: INFO: Pod "downward-api-750d4e6e-cdb8-4c1d-ae81-d071c208350b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.271734ms
Apr 22 20:40:24.264: INFO: Pod "downward-api-750d4e6e-cdb8-4c1d-ae81-d071c208350b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009933303s
Apr 22 20:40:26.270: INFO: Pod "downward-api-750d4e6e-cdb8-4c1d-ae81-d071c208350b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015577577s
STEP: Saw pod success
Apr 22 20:40:26.270: INFO: Pod "downward-api-750d4e6e-cdb8-4c1d-ae81-d071c208350b" satisfied condition "Succeeded or Failed"
Apr 22 20:40:26.272: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod downward-api-750d4e6e-cdb8-4c1d-ae81-d071c208350b container dapi-container: <nil>
STEP: delete the pod
Apr 22 20:40:26.285: INFO: Waiting for pod downward-api-750d4e6e-cdb8-4c1d-ae81-d071c208350b to disappear
Apr 22 20:40:26.287: INFO: Pod downward-api-750d4e6e-cdb8-4c1d-ae81-d071c208350b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:40:26.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-140" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":8,"skipped":140,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:40:26.294: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-b732f77c-abc5-4ee6-8527-b3683c0e1479
STEP: Creating secret with name s-test-opt-upd-2a221401-2435-4349-be8a-f3c32cb1664e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-b732f77c-abc5-4ee6-8527-b3683c0e1479
STEP: Updating secret s-test-opt-upd-2a221401-2435-4349-be8a-f3c32cb1664e
STEP: Creating secret with name s-test-opt-create-79b1aaff-d78c-47c7-9d06-ca924f36d08d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:40:36.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5368" for this suite.

• [SLOW TEST:10.169 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":9,"skipped":141,"failed":0}
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:40:36.463: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Apr 22 20:40:39.032: INFO: Successfully updated pod "labelsupdatebe5956da-0754-42a5-a4d0-da05e94cc11e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:40:43.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3977" for this suite.

• [SLOW TEST:6.593 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":10,"skipped":141,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:40:43.057: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Apr 22 20:40:43.081: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-8238 proxy --unix-socket=/tmp/kubectl-proxy-unix891138752/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:40:43.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8238" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":11,"skipped":148,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:40:43.133: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 22 20:40:43.569: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 22 20:40:45.580: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720843, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720843, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720843, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720843, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 20:40:48.598: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:40:48.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2039" for this suite.
STEP: Destroying namespace "webhook-2039-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.524 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":12,"skipped":153,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:40:48.657: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Apr 22 20:40:48.695: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1662  6eb5741a-b911-45fa-bc95-56a799fadc97 2514 0 2021-04-22 20:40:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-22 20:40:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 22 20:40:48.695: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1662  6eb5741a-b911-45fa-bc95-56a799fadc97 2515 0 2021-04-22 20:40:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-22 20:40:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 22 20:40:48.696: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1662  6eb5741a-b911-45fa-bc95-56a799fadc97 2516 0 2021-04-22 20:40:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-22 20:40:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Apr 22 20:40:58.737: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1662  6eb5741a-b911-45fa-bc95-56a799fadc97 2569 0 2021-04-22 20:40:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-22 20:40:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 22 20:40:58.737: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1662  6eb5741a-b911-45fa-bc95-56a799fadc97 2570 0 2021-04-22 20:40:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-22 20:40:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 22 20:40:58.737: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1662  6eb5741a-b911-45fa-bc95-56a799fadc97 2571 0 2021-04-22 20:40:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-22 20:40:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:40:58.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1662" for this suite.

• [SLOW TEST:10.087 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":13,"skipped":169,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:40:58.744: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Apr 22 20:40:58.778: INFO: Pod name pod-release: Found 0 pods out of 1
Apr 22 20:41:03.783: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:41:04.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7754" for this suite.

• [SLOW TEST:6.065 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":14,"skipped":191,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:41:04.811: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Apr 22 20:41:04.837: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Apr 22 20:41:05.408: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Apr 22 20:41:07.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720865, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720865, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720865, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720865, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 22 20:41:09.448: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720865, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720865, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720865, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720865, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 22 20:41:11.448: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720865, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720865, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720865, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720865, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 22 20:41:13.447: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720865, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720865, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720865, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754720865, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 22 20:41:16.775: INFO: Waited 1.315155985s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:41:17.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-177" for this suite.

• [SLOW TEST:12.857 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":15,"skipped":281,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:41:17.668: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:41:45.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-340" for this suite.

• [SLOW TEST:28.257 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":16,"skipped":285,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:41:45.925: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Apr 22 20:41:45.959: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6563  329019cb-8525-40c5-b808-7368550772f8 2918 0 2021-04-22 20:41:45 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-04-22 20:41:45 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f622f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f622f,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f622f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:41:45.961: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Apr 22 20:41:47.967: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Apr 22 20:41:47.967: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6563 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 20:41:47.967: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Verifying customized DNS server is configured on pod...
Apr 22 20:41:48.070: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6563 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 20:41:48.070: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 20:41:48.132: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:41:48.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6563" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":17,"skipped":305,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:41:48.151: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-ad1598e8-2681-49b0-9eb9-493501233dae
STEP: Creating a pod to test consume secrets
Apr 22 20:41:48.197: INFO: Waiting up to 5m0s for pod "pod-secrets-1ffcd599-64f1-4627-8a35-5bd9d9ca335e" in namespace "secrets-2178" to be "Succeeded or Failed"
Apr 22 20:41:48.201: INFO: Pod "pod-secrets-1ffcd599-64f1-4627-8a35-5bd9d9ca335e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.389229ms
Apr 22 20:41:50.207: INFO: Pod "pod-secrets-1ffcd599-64f1-4627-8a35-5bd9d9ca335e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009972355s
STEP: Saw pod success
Apr 22 20:41:50.207: INFO: Pod "pod-secrets-1ffcd599-64f1-4627-8a35-5bd9d9ca335e" satisfied condition "Succeeded or Failed"
Apr 22 20:41:50.209: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod pod-secrets-1ffcd599-64f1-4627-8a35-5bd9d9ca335e container secret-volume-test: <nil>
STEP: delete the pod
Apr 22 20:41:50.221: INFO: Waiting for pod pod-secrets-1ffcd599-64f1-4627-8a35-5bd9d9ca335e to disappear
Apr 22 20:41:50.223: INFO: Pod pod-secrets-1ffcd599-64f1-4627-8a35-5bd9d9ca335e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:41:50.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2178" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":18,"skipped":311,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:41:50.230: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:41:54.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4567" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":19,"skipped":324,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:41:54.283: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Apr 22 20:41:54.312: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 22 20:42:54.344: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 20:42:54.347: INFO: Starting informer...
STEP: Starting pod...
Apr 22 20:42:54.561: INFO: Pod is running on ip-10-0-131-2.us-west-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Apr 22 20:42:54.579: INFO: Pod wasn't evicted. Proceeding
Apr 22 20:42:54.579: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Apr 22 20:44:09.604: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:44:09.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-2427" for this suite.

• [SLOW TEST:135.350 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":20,"skipped":346,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:44:09.634: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-9aa3ad87-f964-44d8-b6e3-d2526260c241
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:44:13.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3115" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":21,"skipped":351,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:44:13.711: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-dce81c1d-6c95-4fce-a901-10f08cc9ae06
STEP: Creating a pod to test consume configMaps
Apr 22 20:44:13.754: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-552d03a1-642a-4d8b-a987-ac83063fc9b5" in namespace "projected-722" to be "Succeeded or Failed"
Apr 22 20:44:13.756: INFO: Pod "pod-projected-configmaps-552d03a1-642a-4d8b-a987-ac83063fc9b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.965334ms
Apr 22 20:44:15.762: INFO: Pod "pod-projected-configmaps-552d03a1-642a-4d8b-a987-ac83063fc9b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008121814s
STEP: Saw pod success
Apr 22 20:44:15.762: INFO: Pod "pod-projected-configmaps-552d03a1-642a-4d8b-a987-ac83063fc9b5" satisfied condition "Succeeded or Failed"
Apr 22 20:44:15.764: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-projected-configmaps-552d03a1-642a-4d8b-a987-ac83063fc9b5 container agnhost-container: <nil>
STEP: delete the pod
Apr 22 20:44:15.783: INFO: Waiting for pod pod-projected-configmaps-552d03a1-642a-4d8b-a987-ac83063fc9b5 to disappear
Apr 22 20:44:15.784: INFO: Pod pod-projected-configmaps-552d03a1-642a-4d8b-a987-ac83063fc9b5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:44:15.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-722" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":22,"skipped":366,"failed":0}
SSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:44:15.791: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:44:15.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-871" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":23,"skipped":369,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:44:15.873: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 22 20:44:15.902: INFO: Waiting up to 5m0s for pod "downward-api-7149ce31-ca91-4b07-af27-6b51d4d4691e" in namespace "downward-api-8462" to be "Succeeded or Failed"
Apr 22 20:44:15.907: INFO: Pod "downward-api-7149ce31-ca91-4b07-af27-6b51d4d4691e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.655574ms
Apr 22 20:44:17.913: INFO: Pod "downward-api-7149ce31-ca91-4b07-af27-6b51d4d4691e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010738007s
Apr 22 20:44:19.919: INFO: Pod "downward-api-7149ce31-ca91-4b07-af27-6b51d4d4691e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017118896s
STEP: Saw pod success
Apr 22 20:44:19.919: INFO: Pod "downward-api-7149ce31-ca91-4b07-af27-6b51d4d4691e" satisfied condition "Succeeded or Failed"
Apr 22 20:44:19.921: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod downward-api-7149ce31-ca91-4b07-af27-6b51d4d4691e container dapi-container: <nil>
STEP: delete the pod
Apr 22 20:44:19.935: INFO: Waiting for pod downward-api-7149ce31-ca91-4b07-af27-6b51d4d4691e to disappear
Apr 22 20:44:19.938: INFO: Pod downward-api-7149ce31-ca91-4b07-af27-6b51d4d4691e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:44:19.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8462" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":24,"skipped":408,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:44:19.946: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 20:44:19.978: INFO: Waiting up to 5m0s for pod "busybox-user-65534-2bf3b16e-0eee-4fcc-8b50-1187ff60d187" in namespace "security-context-test-8463" to be "Succeeded or Failed"
Apr 22 20:44:19.980: INFO: Pod "busybox-user-65534-2bf3b16e-0eee-4fcc-8b50-1187ff60d187": Phase="Pending", Reason="", readiness=false. Elapsed: 1.986636ms
Apr 22 20:44:21.987: INFO: Pod "busybox-user-65534-2bf3b16e-0eee-4fcc-8b50-1187ff60d187": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008454908s
Apr 22 20:44:23.992: INFO: Pod "busybox-user-65534-2bf3b16e-0eee-4fcc-8b50-1187ff60d187": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013814735s
Apr 22 20:44:23.992: INFO: Pod "busybox-user-65534-2bf3b16e-0eee-4fcc-8b50-1187ff60d187" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:44:23.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8463" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":25,"skipped":425,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:44:24.012: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Apr 22 20:44:24.053: INFO: Waiting up to 5m0s for pod "test-pod-b461b7fa-b87c-4279-85bb-dafcb3bb755c" in namespace "svcaccounts-7483" to be "Succeeded or Failed"
Apr 22 20:44:24.055: INFO: Pod "test-pod-b461b7fa-b87c-4279-85bb-dafcb3bb755c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.162841ms
Apr 22 20:44:26.061: INFO: Pod "test-pod-b461b7fa-b87c-4279-85bb-dafcb3bb755c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008211861s
STEP: Saw pod success
Apr 22 20:44:26.061: INFO: Pod "test-pod-b461b7fa-b87c-4279-85bb-dafcb3bb755c" satisfied condition "Succeeded or Failed"
Apr 22 20:44:26.064: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod test-pod-b461b7fa-b87c-4279-85bb-dafcb3bb755c container agnhost-container: <nil>
STEP: delete the pod
Apr 22 20:44:26.083: INFO: Waiting for pod test-pod-b461b7fa-b87c-4279-85bb-dafcb3bb755c to disappear
Apr 22 20:44:26.084: INFO: Pod test-pod-b461b7fa-b87c-4279-85bb-dafcb3bb755c no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:44:26.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7483" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":26,"skipped":477,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:44:26.094: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 20:44:26.127: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-5a49fb1a-8bab-4255-ad67-4e6b25d30729" in namespace "security-context-test-1767" to be "Succeeded or Failed"
Apr 22 20:44:26.129: INFO: Pod "busybox-readonly-false-5a49fb1a-8bab-4255-ad67-4e6b25d30729": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014926ms
Apr 22 20:44:28.134: INFO: Pod "busybox-readonly-false-5a49fb1a-8bab-4255-ad67-4e6b25d30729": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007409706s
Apr 22 20:44:28.134: INFO: Pod "busybox-readonly-false-5a49fb1a-8bab-4255-ad67-4e6b25d30729" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:44:28.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1767" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":27,"skipped":487,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:44:28.144: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Apr 22 20:44:30.193: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9789 PodName:var-expansion-62ada4eb-1903-4f02-9db5-4543c89107ba ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 20:44:30.193: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: test for file in mounted path
Apr 22 20:44:30.245: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9789 PodName:var-expansion-62ada4eb-1903-4f02-9db5-4543c89107ba ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 20:44:30.245: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: updating the annotation value
Apr 22 20:44:30.809: INFO: Successfully updated pod "var-expansion-62ada4eb-1903-4f02-9db5-4543c89107ba"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Apr 22 20:44:30.812: INFO: Deleting pod "var-expansion-62ada4eb-1903-4f02-9db5-4543c89107ba" in namespace "var-expansion-9789"
Apr 22 20:44:30.817: INFO: Wait up to 5m0s for pod "var-expansion-62ada4eb-1903-4f02-9db5-4543c89107ba" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:45:08.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9789" for this suite.

• [SLOW TEST:40.699 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":28,"skipped":488,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:45:08.843: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3188
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Apr 22 20:45:08.880: INFO: Found 0 stateful pods, waiting for 3
Apr 22 20:45:18.891: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 22 20:45:18.891: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 22 20:45:18.891: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Apr 22 20:45:28.891: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 22 20:45:28.891: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 22 20:45:28.891: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr 22 20:45:28.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-3188 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 22 20:45:29.207: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 22 20:45:29.207: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 22 20:45:29.207: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Apr 22 20:45:39.253: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Apr 22 20:45:49.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-3188 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 22 20:45:49.387: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 22 20:45:49.387: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 22 20:45:49.387: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 22 20:45:59.410: INFO: Waiting for StatefulSet statefulset-3188/ss2 to complete update
Apr 22 20:45:59.410: INFO: Waiting for Pod statefulset-3188/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 22 20:45:59.410: INFO: Waiting for Pod statefulset-3188/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 22 20:46:09.424: INFO: Waiting for StatefulSet statefulset-3188/ss2 to complete update
Apr 22 20:46:09.424: INFO: Waiting for Pod statefulset-3188/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 22 20:46:09.424: INFO: Waiting for Pod statefulset-3188/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 22 20:46:19.423: INFO: Waiting for StatefulSet statefulset-3188/ss2 to complete update
Apr 22 20:46:19.424: INFO: Waiting for Pod statefulset-3188/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 22 20:46:29.424: INFO: Waiting for StatefulSet statefulset-3188/ss2 to complete update
Apr 22 20:46:29.424: INFO: Waiting for Pod statefulset-3188/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 22 20:46:39.424: INFO: Waiting for StatefulSet statefulset-3188/ss2 to complete update
STEP: Rolling back to a previous revision
Apr 22 20:46:49.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-3188 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 22 20:46:49.544: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 22 20:46:49.544: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 22 20:46:49.545: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 22 20:46:59.592: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Apr 22 20:47:09.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-3188 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 22 20:47:09.727: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 22 20:47:09.727: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 22 20:47:09.727: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 22 20:47:19.751: INFO: Waiting for StatefulSet statefulset-3188/ss2 to complete update
Apr 22 20:47:19.751: INFO: Waiting for Pod statefulset-3188/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Apr 22 20:47:19.751: INFO: Waiting for Pod statefulset-3188/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Apr 22 20:47:29.765: INFO: Waiting for StatefulSet statefulset-3188/ss2 to complete update
Apr 22 20:47:29.765: INFO: Waiting for Pod statefulset-3188/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Apr 22 20:47:39.764: INFO: Waiting for StatefulSet statefulset-3188/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 22 20:47:49.764: INFO: Deleting all statefulset in ns statefulset-3188
Apr 22 20:47:49.767: INFO: Scaling statefulset ss2 to 0
Apr 22 20:48:09.796: INFO: Waiting for statefulset status.replicas updated to 0
Apr 22 20:48:09.799: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:48:09.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3188" for this suite.

• [SLOW TEST:180.984 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":29,"skipped":543,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:48:09.828: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 22 20:48:09.860: INFO: Waiting up to 5m0s for pod "pod-83672463-16ec-4286-93b6-cd4067b8fc68" in namespace "emptydir-2230" to be "Succeeded or Failed"
Apr 22 20:48:09.863: INFO: Pod "pod-83672463-16ec-4286-93b6-cd4067b8fc68": Phase="Pending", Reason="", readiness=false. Elapsed: 3.612309ms
Apr 22 20:48:11.869: INFO: Pod "pod-83672463-16ec-4286-93b6-cd4067b8fc68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009914081s
STEP: Saw pod success
Apr 22 20:48:11.869: INFO: Pod "pod-83672463-16ec-4286-93b6-cd4067b8fc68" satisfied condition "Succeeded or Failed"
Apr 22 20:48:11.872: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod pod-83672463-16ec-4286-93b6-cd4067b8fc68 container test-container: <nil>
STEP: delete the pod
Apr 22 20:48:11.893: INFO: Waiting for pod pod-83672463-16ec-4286-93b6-cd4067b8fc68 to disappear
Apr 22 20:48:11.894: INFO: Pod pod-83672463-16ec-4286-93b6-cd4067b8fc68 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:48:11.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2230" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":30,"skipped":553,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:48:11.904: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-6bc5ac94-b093-4df9-aa8e-23321ca0e4de
Apr 22 20:48:11.935: INFO: Pod name my-hostname-basic-6bc5ac94-b093-4df9-aa8e-23321ca0e4de: Found 0 pods out of 1
Apr 22 20:48:16.942: INFO: Pod name my-hostname-basic-6bc5ac94-b093-4df9-aa8e-23321ca0e4de: Found 1 pods out of 1
Apr 22 20:48:16.942: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6bc5ac94-b093-4df9-aa8e-23321ca0e4de" are running
Apr 22 20:48:16.944: INFO: Pod "my-hostname-basic-6bc5ac94-b093-4df9-aa8e-23321ca0e4de-wkjzc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-22 20:48:11 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-22 20:48:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-22 20:48:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-22 20:48:11 +0000 UTC Reason: Message:}])
Apr 22 20:48:16.944: INFO: Trying to dial the pod
Apr 22 20:48:21.960: INFO: Controller my-hostname-basic-6bc5ac94-b093-4df9-aa8e-23321ca0e4de: Got expected result from replica 1 [my-hostname-basic-6bc5ac94-b093-4df9-aa8e-23321ca0e4de-wkjzc]: "my-hostname-basic-6bc5ac94-b093-4df9-aa8e-23321ca0e4de-wkjzc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:48:21.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3603" for this suite.

• [SLOW TEST:10.065 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":31,"skipped":563,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:48:21.969: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 20:48:22.005: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e94fc0a-36a0-47f4-8c78-732f7909207c" in namespace "downward-api-5960" to be "Succeeded or Failed"
Apr 22 20:48:22.008: INFO: Pod "downwardapi-volume-3e94fc0a-36a0-47f4-8c78-732f7909207c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.415304ms
Apr 22 20:48:24.011: INFO: Pod "downwardapi-volume-3e94fc0a-36a0-47f4-8c78-732f7909207c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006070358s
STEP: Saw pod success
Apr 22 20:48:24.011: INFO: Pod "downwardapi-volume-3e94fc0a-36a0-47f4-8c78-732f7909207c" satisfied condition "Succeeded or Failed"
Apr 22 20:48:24.013: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod downwardapi-volume-3e94fc0a-36a0-47f4-8c78-732f7909207c container client-container: <nil>
STEP: delete the pod
Apr 22 20:48:24.026: INFO: Waiting for pod downwardapi-volume-3e94fc0a-36a0-47f4-8c78-732f7909207c to disappear
Apr 22 20:48:24.028: INFO: Pod downwardapi-volume-3e94fc0a-36a0-47f4-8c78-732f7909207c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:48:24.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5960" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":32,"skipped":576,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:48:24.036: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 20:48:24.334: INFO: Checking APIGroup: apiregistration.k8s.io
Apr 22 20:48:24.334: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Apr 22 20:48:24.334: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Apr 22 20:48:24.335: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Apr 22 20:48:24.335: INFO: Checking APIGroup: apps
Apr 22 20:48:24.335: INFO: PreferredVersion.GroupVersion: apps/v1
Apr 22 20:48:24.335: INFO: Versions found [{apps/v1 v1}]
Apr 22 20:48:24.335: INFO: apps/v1 matches apps/v1
Apr 22 20:48:24.335: INFO: Checking APIGroup: events.k8s.io
Apr 22 20:48:24.336: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Apr 22 20:48:24.336: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Apr 22 20:48:24.336: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Apr 22 20:48:24.336: INFO: Checking APIGroup: authentication.k8s.io
Apr 22 20:48:24.337: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Apr 22 20:48:24.337: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Apr 22 20:48:24.337: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Apr 22 20:48:24.337: INFO: Checking APIGroup: authorization.k8s.io
Apr 22 20:48:24.337: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Apr 22 20:48:24.337: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Apr 22 20:48:24.337: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Apr 22 20:48:24.337: INFO: Checking APIGroup: autoscaling
Apr 22 20:48:24.338: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Apr 22 20:48:24.338: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Apr 22 20:48:24.338: INFO: autoscaling/v1 matches autoscaling/v1
Apr 22 20:48:24.338: INFO: Checking APIGroup: batch
Apr 22 20:48:24.339: INFO: PreferredVersion.GroupVersion: batch/v1
Apr 22 20:48:24.339: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Apr 22 20:48:24.339: INFO: batch/v1 matches batch/v1
Apr 22 20:48:24.339: INFO: Checking APIGroup: certificates.k8s.io
Apr 22 20:48:24.339: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Apr 22 20:48:24.339: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Apr 22 20:48:24.339: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Apr 22 20:48:24.339: INFO: Checking APIGroup: networking.k8s.io
Apr 22 20:48:24.340: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Apr 22 20:48:24.340: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Apr 22 20:48:24.340: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Apr 22 20:48:24.340: INFO: Checking APIGroup: extensions
Apr 22 20:48:24.340: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Apr 22 20:48:24.340: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Apr 22 20:48:24.340: INFO: extensions/v1beta1 matches extensions/v1beta1
Apr 22 20:48:24.340: INFO: Checking APIGroup: policy
Apr 22 20:48:24.341: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Apr 22 20:48:24.341: INFO: Versions found [{policy/v1beta1 v1beta1}]
Apr 22 20:48:24.341: INFO: policy/v1beta1 matches policy/v1beta1
Apr 22 20:48:24.341: INFO: Checking APIGroup: rbac.authorization.k8s.io
Apr 22 20:48:24.342: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Apr 22 20:48:24.342: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Apr 22 20:48:24.342: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Apr 22 20:48:24.342: INFO: Checking APIGroup: storage.k8s.io
Apr 22 20:48:24.342: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Apr 22 20:48:24.342: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Apr 22 20:48:24.342: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Apr 22 20:48:24.342: INFO: Checking APIGroup: admissionregistration.k8s.io
Apr 22 20:48:24.343: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Apr 22 20:48:24.343: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Apr 22 20:48:24.343: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Apr 22 20:48:24.343: INFO: Checking APIGroup: apiextensions.k8s.io
Apr 22 20:48:24.344: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Apr 22 20:48:24.344: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Apr 22 20:48:24.344: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Apr 22 20:48:24.344: INFO: Checking APIGroup: scheduling.k8s.io
Apr 22 20:48:24.344: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Apr 22 20:48:24.344: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Apr 22 20:48:24.344: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Apr 22 20:48:24.344: INFO: Checking APIGroup: coordination.k8s.io
Apr 22 20:48:24.345: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Apr 22 20:48:24.345: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Apr 22 20:48:24.345: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Apr 22 20:48:24.345: INFO: Checking APIGroup: node.k8s.io
Apr 22 20:48:24.345: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Apr 22 20:48:24.345: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Apr 22 20:48:24.345: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Apr 22 20:48:24.345: INFO: Checking APIGroup: discovery.k8s.io
Apr 22 20:48:24.346: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Apr 22 20:48:24.346: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Apr 22 20:48:24.346: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Apr 22 20:48:24.346: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Apr 22 20:48:24.347: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Apr 22 20:48:24.347: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Apr 22 20:48:24.347: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Apr 22 20:48:24.347: INFO: Checking APIGroup: crd.projectcalico.org
Apr 22 20:48:24.347: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Apr 22 20:48:24.347: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Apr 22 20:48:24.347: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Apr 22 20:48:24.347: INFO: Checking APIGroup: kubeaddons.mesosphere.io
Apr 22 20:48:24.348: INFO: PreferredVersion.GroupVersion: kubeaddons.mesosphere.io/v1beta2
Apr 22 20:48:24.348: INFO: Versions found [{kubeaddons.mesosphere.io/v1beta2 v1beta2} {kubeaddons.mesosphere.io/v1beta1 v1beta1}]
Apr 22 20:48:24.348: INFO: kubeaddons.mesosphere.io/v1beta2 matches kubeaddons.mesosphere.io/v1beta2
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:48:24.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-7437" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":33,"skipped":590,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:48:24.356: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3733
STEP: creating service affinity-nodeport in namespace services-3733
STEP: creating replication controller affinity-nodeport in namespace services-3733
I0422 20:48:24.397765      24 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-3733, replica count: 3
I0422 20:48:27.447995      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0422 20:48:30.448143      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 22 20:48:30.464: INFO: Creating new exec pod
Apr 22 20:48:33.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-3733 exec execpod-affinitypltqj -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Apr 22 20:48:33.601: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Apr 22 20:48:33.601: INFO: stdout: ""
Apr 22 20:48:33.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-3733 exec execpod-affinitypltqj -- /bin/sh -x -c nc -zv -t -w 2 10.0.26.36 80'
Apr 22 20:48:33.707: INFO: stderr: "+ nc -zv -t -w 2 10.0.26.36 80\nConnection to 10.0.26.36 80 port [tcp/http] succeeded!\n"
Apr 22 20:48:33.707: INFO: stdout: ""
Apr 22 20:48:33.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-3733 exec execpod-affinitypltqj -- /bin/sh -x -c nc -zv -t -w 2 10.0.130.171 32228'
Apr 22 20:48:33.817: INFO: stderr: "+ nc -zv -t -w 2 10.0.130.171 32228\nConnection to 10.0.130.171 32228 port [tcp/32228] succeeded!\n"
Apr 22 20:48:33.817: INFO: stdout: ""
Apr 22 20:48:33.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-3733 exec execpod-affinitypltqj -- /bin/sh -x -c nc -zv -t -w 2 10.0.131.122 32228'
Apr 22 20:48:33.938: INFO: stderr: "+ nc -zv -t -w 2 10.0.131.122 32228\nConnection to 10.0.131.122 32228 port [tcp/32228] succeeded!\n"
Apr 22 20:48:33.938: INFO: stdout: ""
Apr 22 20:48:33.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-3733 exec execpod-affinitypltqj -- /bin/sh -x -c nc -zv -t -w 2 34.208.188.48 32228'
Apr 22 20:48:34.050: INFO: stderr: "+ nc -zv -t -w 2 34.208.188.48 32228\nConnection to 34.208.188.48 32228 port [tcp/32228] succeeded!\n"
Apr 22 20:48:34.050: INFO: stdout: ""
Apr 22 20:48:34.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-3733 exec execpod-affinitypltqj -- /bin/sh -x -c nc -zv -t -w 2 52.88.123.209 32228'
Apr 22 20:48:34.160: INFO: stderr: "+ nc -zv -t -w 2 52.88.123.209 32228\nConnection to 52.88.123.209 32228 port [tcp/32228] succeeded!\n"
Apr 22 20:48:34.160: INFO: stdout: ""
Apr 22 20:48:34.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-3733 exec execpod-affinitypltqj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.130.171:32228/ ; done'
Apr 22 20:48:34.341: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:32228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:32228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:32228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:32228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:32228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:32228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:32228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:32228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:32228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:32228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:32228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:32228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:32228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:32228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:32228/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:32228/\n"
Apr 22 20:48:34.341: INFO: stdout: "\naffinity-nodeport-xbcx5\naffinity-nodeport-xbcx5\naffinity-nodeport-xbcx5\naffinity-nodeport-xbcx5\naffinity-nodeport-xbcx5\naffinity-nodeport-xbcx5\naffinity-nodeport-xbcx5\naffinity-nodeport-xbcx5\naffinity-nodeport-xbcx5\naffinity-nodeport-xbcx5\naffinity-nodeport-xbcx5\naffinity-nodeport-xbcx5\naffinity-nodeport-xbcx5\naffinity-nodeport-xbcx5\naffinity-nodeport-xbcx5\naffinity-nodeport-xbcx5"
Apr 22 20:48:34.341: INFO: Received response from host: affinity-nodeport-xbcx5
Apr 22 20:48:34.341: INFO: Received response from host: affinity-nodeport-xbcx5
Apr 22 20:48:34.341: INFO: Received response from host: affinity-nodeport-xbcx5
Apr 22 20:48:34.341: INFO: Received response from host: affinity-nodeport-xbcx5
Apr 22 20:48:34.341: INFO: Received response from host: affinity-nodeport-xbcx5
Apr 22 20:48:34.341: INFO: Received response from host: affinity-nodeport-xbcx5
Apr 22 20:48:34.341: INFO: Received response from host: affinity-nodeport-xbcx5
Apr 22 20:48:34.341: INFO: Received response from host: affinity-nodeport-xbcx5
Apr 22 20:48:34.341: INFO: Received response from host: affinity-nodeport-xbcx5
Apr 22 20:48:34.341: INFO: Received response from host: affinity-nodeport-xbcx5
Apr 22 20:48:34.341: INFO: Received response from host: affinity-nodeport-xbcx5
Apr 22 20:48:34.341: INFO: Received response from host: affinity-nodeport-xbcx5
Apr 22 20:48:34.341: INFO: Received response from host: affinity-nodeport-xbcx5
Apr 22 20:48:34.341: INFO: Received response from host: affinity-nodeport-xbcx5
Apr 22 20:48:34.341: INFO: Received response from host: affinity-nodeport-xbcx5
Apr 22 20:48:34.341: INFO: Received response from host: affinity-nodeport-xbcx5
Apr 22 20:48:34.341: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-3733, will wait for the garbage collector to delete the pods
Apr 22 20:48:34.413: INFO: Deleting ReplicationController affinity-nodeport took: 5.8848ms
Apr 22 20:48:35.114: INFO: Terminating ReplicationController affinity-nodeport pods took: 700.152871ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:48:48.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3733" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:24.401 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":34,"skipped":593,"failed":0}
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:48:48.757: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-e63ac5f0-7bf8-4bf8-be8d-f144c26bce11
STEP: Creating a pod to test consume secrets
Apr 22 20:48:48.796: INFO: Waiting up to 5m0s for pod "pod-secrets-2eb88787-77d0-47ff-a601-711d5d315b56" in namespace "secrets-8233" to be "Succeeded or Failed"
Apr 22 20:48:48.798: INFO: Pod "pod-secrets-2eb88787-77d0-47ff-a601-711d5d315b56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009457ms
Apr 22 20:48:50.804: INFO: Pod "pod-secrets-2eb88787-77d0-47ff-a601-711d5d315b56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008025283s
STEP: Saw pod success
Apr 22 20:48:50.804: INFO: Pod "pod-secrets-2eb88787-77d0-47ff-a601-711d5d315b56" satisfied condition "Succeeded or Failed"
Apr 22 20:48:50.806: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod pod-secrets-2eb88787-77d0-47ff-a601-711d5d315b56 container secret-volume-test: <nil>
STEP: delete the pod
Apr 22 20:48:50.820: INFO: Waiting for pod pod-secrets-2eb88787-77d0-47ff-a601-711d5d315b56 to disappear
Apr 22 20:48:50.822: INFO: Pod pod-secrets-2eb88787-77d0-47ff-a601-711d5d315b56 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:48:50.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8233" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":35,"skipped":593,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:48:50.830: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 22 20:48:51.278: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 20:48:54.297: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:48:54.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3841" for this suite.
STEP: Destroying namespace "webhook-3841-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":36,"skipped":640,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:48:54.409: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:49:00.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-762" for this suite.
STEP: Destroying namespace "nsdeletetest-1220" for this suite.
Apr 22 20:49:00.512: INFO: Namespace nsdeletetest-1220 was already deleted
STEP: Destroying namespace "nsdeletetest-7194" for this suite.

• [SLOW TEST:6.107 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":37,"skipped":654,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:49:00.517: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Apr 22 20:49:00.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-5999 api-versions'
Apr 22 20:49:00.600: INFO: stderr: ""
Apr 22 20:49:00.600: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nkubeaddons.mesosphere.io/v1beta1\nkubeaddons.mesosphere.io/v1beta2\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:49:00.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5999" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":38,"skipped":668,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:49:00.609: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 22 20:49:04.675: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 22 20:49:04.677: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 22 20:49:06.677: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 22 20:49:06.683: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 22 20:49:08.677: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 22 20:49:08.682: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:49:08.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4813" for this suite.

• [SLOW TEST:8.108 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":39,"skipped":695,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:49:08.717: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9547
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-9547
I0422 20:49:08.763506      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-9547, replica count: 2
I0422 20:49:11.813757      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 22 20:49:11.813: INFO: Creating new exec pod
Apr 22 20:49:14.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-9547 exec execpodmvq64 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Apr 22 20:49:14.958: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 22 20:49:14.958: INFO: stdout: ""
Apr 22 20:49:14.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-9547 exec execpodmvq64 -- /bin/sh -x -c nc -zv -t -w 2 10.0.42.112 80'
Apr 22 20:49:15.081: INFO: stderr: "+ nc -zv -t -w 2 10.0.42.112 80\nConnection to 10.0.42.112 80 port [tcp/http] succeeded!\n"
Apr 22 20:49:15.081: INFO: stdout: ""
Apr 22 20:49:15.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-9547 exec execpodmvq64 -- /bin/sh -x -c nc -zv -t -w 2 10.0.131.122 30809'
Apr 22 20:49:15.194: INFO: stderr: "+ nc -zv -t -w 2 10.0.131.122 30809\nConnection to 10.0.131.122 30809 port [tcp/30809] succeeded!\n"
Apr 22 20:49:15.194: INFO: stdout: ""
Apr 22 20:49:15.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-9547 exec execpodmvq64 -- /bin/sh -x -c nc -zv -t -w 2 10.0.131.2 30809'
Apr 22 20:49:15.299: INFO: stderr: "+ nc -zv -t -w 2 10.0.131.2 30809\nConnection to 10.0.131.2 30809 port [tcp/30809] succeeded!\n"
Apr 22 20:49:15.299: INFO: stdout: ""
Apr 22 20:49:15.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-9547 exec execpodmvq64 -- /bin/sh -x -c nc -zv -t -w 2 52.88.123.209 30809'
Apr 22 20:49:15.411: INFO: stderr: "+ nc -zv -t -w 2 52.88.123.209 30809\nConnection to 52.88.123.209 30809 port [tcp/30809] succeeded!\n"
Apr 22 20:49:15.411: INFO: stdout: ""
Apr 22 20:49:15.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-9547 exec execpodmvq64 -- /bin/sh -x -c nc -zv -t -w 2 54.189.106.97 30809'
Apr 22 20:49:15.523: INFO: stderr: "+ nc -zv -t -w 2 54.189.106.97 30809\nConnection to 54.189.106.97 30809 port [tcp/30809] succeeded!\n"
Apr 22 20:49:15.523: INFO: stdout: ""
Apr 22 20:49:15.523: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:49:15.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9547" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.839 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":40,"skipped":722,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:49:15.556: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Apr 22 20:49:15.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-5163 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Apr 22 20:49:15.645: INFO: stderr: ""
Apr 22 20:49:15.646: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Apr 22 20:49:15.646: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Apr 22 20:49:15.646: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5163" to be "running and ready, or succeeded"
Apr 22 20:49:15.648: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.49496ms
Apr 22 20:49:17.653: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.007584055s
Apr 22 20:49:17.653: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Apr 22 20:49:17.653: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Apr 22 20:49:17.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-5163 logs logs-generator logs-generator'
Apr 22 20:49:17.725: INFO: stderr: ""
Apr 22 20:49:17.725: INFO: stdout: "I0422 20:49:16.581784       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/kzl 549\nI0422 20:49:16.781882       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/7wx2 310\nI0422 20:49:16.981875       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/rgvg 340\nI0422 20:49:17.181874       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/l9q 524\nI0422 20:49:17.381880       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/ggz2 460\nI0422 20:49:17.581880       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/8f9 228\n"
Apr 22 20:49:19.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-5163 logs logs-generator logs-generator'
Apr 22 20:49:19.792: INFO: stderr: ""
Apr 22 20:49:19.792: INFO: stdout: "I0422 20:49:16.581784       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/kzl 549\nI0422 20:49:16.781882       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/7wx2 310\nI0422 20:49:16.981875       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/rgvg 340\nI0422 20:49:17.181874       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/l9q 524\nI0422 20:49:17.381880       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/ggz2 460\nI0422 20:49:17.581880       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/8f9 228\nI0422 20:49:17.781872       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/nmll 349\nI0422 20:49:17.981881       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/mvgx 544\nI0422 20:49:18.181881       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/w776 247\nI0422 20:49:18.381865       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/g9cg 252\nI0422 20:49:18.581879       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/lgs 541\nI0422 20:49:18.781884       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/b8hq 257\nI0422 20:49:18.981871       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/nm54 337\nI0422 20:49:19.181881       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/qzq5 337\nI0422 20:49:19.381883       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/4bxf 490\nI0422 20:49:19.581866       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/zjk7 578\nI0422 20:49:19.781882       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/npj 313\n"
STEP: limiting log lines
Apr 22 20:49:19.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-5163 logs logs-generator logs-generator --tail=1'
Apr 22 20:49:19.864: INFO: stderr: ""
Apr 22 20:49:19.864: INFO: stdout: "I0422 20:49:19.781882       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/npj 313\n"
Apr 22 20:49:19.864: INFO: got output "I0422 20:49:19.781882       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/npj 313\n"
STEP: limiting log bytes
Apr 22 20:49:19.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-5163 logs logs-generator logs-generator --limit-bytes=1'
Apr 22 20:49:19.926: INFO: stderr: ""
Apr 22 20:49:19.926: INFO: stdout: "I"
Apr 22 20:49:19.926: INFO: got output "I"
STEP: exposing timestamps
Apr 22 20:49:19.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-5163 logs logs-generator logs-generator --tail=1 --timestamps'
Apr 22 20:49:19.986: INFO: stderr: ""
Apr 22 20:49:19.986: INFO: stdout: "2021-04-22T20:49:19.981983747Z I0422 20:49:19.981884       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/sl96 307\n"
Apr 22 20:49:19.986: INFO: got output "2021-04-22T20:49:19.981983747Z I0422 20:49:19.981884       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/sl96 307\n"
STEP: restricting to a time range
Apr 22 20:49:22.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-5163 logs logs-generator logs-generator --since=1s'
Apr 22 20:49:22.553: INFO: stderr: ""
Apr 22 20:49:22.553: INFO: stdout: "I0422 20:49:21.581880       1 logs_generator.go:76] 25 GET /api/v1/namespaces/ns/pods/nzp 203\nI0422 20:49:21.781886       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/b2qj 211\nI0422 20:49:21.981861       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/z5cz 597\nI0422 20:49:22.181890       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/6ln 588\nI0422 20:49:22.381890       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/2zd 348\n"
Apr 22 20:49:22.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-5163 logs logs-generator logs-generator --since=24h'
Apr 22 20:49:22.624: INFO: stderr: ""
Apr 22 20:49:22.624: INFO: stdout: "I0422 20:49:16.581784       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/kzl 549\nI0422 20:49:16.781882       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/7wx2 310\nI0422 20:49:16.981875       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/rgvg 340\nI0422 20:49:17.181874       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/l9q 524\nI0422 20:49:17.381880       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/ggz2 460\nI0422 20:49:17.581880       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/8f9 228\nI0422 20:49:17.781872       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/nmll 349\nI0422 20:49:17.981881       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/mvgx 544\nI0422 20:49:18.181881       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/w776 247\nI0422 20:49:18.381865       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/g9cg 252\nI0422 20:49:18.581879       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/lgs 541\nI0422 20:49:18.781884       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/b8hq 257\nI0422 20:49:18.981871       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/nm54 337\nI0422 20:49:19.181881       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/qzq5 337\nI0422 20:49:19.381883       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/4bxf 490\nI0422 20:49:19.581866       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/zjk7 578\nI0422 20:49:19.781882       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/npj 313\nI0422 20:49:19.981884       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/sl96 307\nI0422 20:49:20.181879       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/nkcg 507\nI0422 20:49:20.381878       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/mhxn 553\nI0422 20:49:20.581879       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/kqk5 595\nI0422 20:49:20.781882       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/frf6 533\nI0422 20:49:20.981882       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/qxh 344\nI0422 20:49:21.181883       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/d6w6 304\nI0422 20:49:21.381882       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/kw7p 586\nI0422 20:49:21.581880       1 logs_generator.go:76] 25 GET /api/v1/namespaces/ns/pods/nzp 203\nI0422 20:49:21.781886       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/b2qj 211\nI0422 20:49:21.981861       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/z5cz 597\nI0422 20:49:22.181890       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/6ln 588\nI0422 20:49:22.381890       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/kube-system/pods/2zd 348\nI0422 20:49:22.581884       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/sqh7 443\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Apr 22 20:49:22.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-5163 delete pod logs-generator'
Apr 22 20:49:28.626: INFO: stderr: ""
Apr 22 20:49:28.626: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:49:28.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5163" for this suite.

• [SLOW TEST:13.089 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":41,"skipped":736,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:49:28.645: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:49:28.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5685" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":42,"skipped":756,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:49:28.701: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 20:49:28.724: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:49:30.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5827" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":43,"skipped":772,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:49:30.799: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8628.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8628.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8628.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8628.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8628.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8628.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 22 20:49:38.869: INFO: DNS probes using dns-8628/dns-test-388bbb40-01b1-4402-94d9-0ba48f4f5d64 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:49:38.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8628" for this suite.

• [SLOW TEST:8.096 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":44,"skipped":778,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:49:38.895: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 20:49:38.925: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr 22 20:49:38.931: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 22 20:49:43.942: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 22 20:49:43.942: INFO: Creating deployment "test-rolling-update-deployment"
Apr 22 20:49:43.954: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr 22 20:49:43.959: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr 22 20:49:45.967: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr 22 20:49:45.969: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 22 20:49:45.975: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6218  7df127fb-ea1e-47c5-9884-93b71541e4fd 5747 1 2021-04-22 20:49:43 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-04-22 20:49:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-22 20:49:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002eab728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-04-22 20:49:43 +0000 UTC,LastTransitionTime:2021-04-22 20:49:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-04-22 20:49:45 +0000 UTC,LastTransitionTime:2021-04-22 20:49:43 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 22 20:49:45.977: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-6218  81e68a31-0908-4683-a88c-ac2cbfa9056c 5736 1 2021-04-22 20:49:43 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 7df127fb-ea1e-47c5-9884-93b71541e4fd 0xc002eabbb7 0xc002eabbb8}] []  [{kube-controller-manager Update apps/v1 2021-04-22 20:49:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7df127fb-ea1e-47c5-9884-93b71541e4fd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002eabc48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 22 20:49:45.977: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr 22 20:49:45.977: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6218  359491c6-86ed-4612-a01b-37ffeeed4242 5746 2 2021-04-22 20:49:38 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 7df127fb-ea1e-47c5-9884-93b71541e4fd 0xc002eabaaf 0xc002eabac0}] []  [{e2e.test Update apps/v1 2021-04-22 20:49:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-22 20:49:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7df127fb-ea1e-47c5-9884-93b71541e4fd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002eabb58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 22 20:49:45.979: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-2k74w" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-2k74w test-rolling-update-deployment-6b6bf9df46- deployment-6218  d3c7841c-8db3-4f18-aa34-16701e3e05f1 5735 0 2021-04-22 20:49:43 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[cni.projectcalico.org/podIP:192.168.224.71/32 cni.projectcalico.org/podIPs:192.168.224.71/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 81e68a31-0908-4683-a88c-ac2cbfa9056c 0xc002586057 0xc002586058}] []  [{kube-controller-manager Update v1 2021-04-22 20:49:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81e68a31-0908-4683-a88c-ac2cbfa9056c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-22 20:49:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-22 20:49:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.224.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nfttc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nfttc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nfttc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-171.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:49:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:49:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:49:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:49:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.171,PodIP:192.168.224.71,StartTime:2021-04-22 20:49:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-22 20:49:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://7d89d60e8b1d1300fd694f9d7c90dd852d46a1abf2669aeea0ea558bc46d8eb0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.224.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:49:45.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6218" for this suite.

• [SLOW TEST:7.094 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":45,"skipped":797,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:49:45.989: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0422 20:50:26.063824      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 22 20:55:26.068: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Apr 22 20:55:26.068: INFO: Deleting pod "simpletest.rc-2jnj4" in namespace "gc-9433"
Apr 22 20:55:26.082: INFO: Deleting pod "simpletest.rc-55fqq" in namespace "gc-9433"
Apr 22 20:55:26.092: INFO: Deleting pod "simpletest.rc-6l4wh" in namespace "gc-9433"
Apr 22 20:55:26.102: INFO: Deleting pod "simpletest.rc-8rs96" in namespace "gc-9433"
Apr 22 20:55:26.112: INFO: Deleting pod "simpletest.rc-ggc5v" in namespace "gc-9433"
Apr 22 20:55:26.125: INFO: Deleting pod "simpletest.rc-pqg9z" in namespace "gc-9433"
Apr 22 20:55:26.133: INFO: Deleting pod "simpletest.rc-q2pzm" in namespace "gc-9433"
Apr 22 20:55:26.144: INFO: Deleting pod "simpletest.rc-sd2pb" in namespace "gc-9433"
Apr 22 20:55:26.152: INFO: Deleting pod "simpletest.rc-wxhs5" in namespace "gc-9433"
Apr 22 20:55:26.161: INFO: Deleting pod "simpletest.rc-xxz4p" in namespace "gc-9433"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:55:26.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9433" for this suite.

• [SLOW TEST:340.190 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":46,"skipped":809,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:55:26.179: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 20:55:26.211: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a92ce07d-a657-47a9-a26f-ee45b7f57c07" in namespace "downward-api-7205" to be "Succeeded or Failed"
Apr 22 20:55:26.216: INFO: Pod "downwardapi-volume-a92ce07d-a657-47a9-a26f-ee45b7f57c07": Phase="Pending", Reason="", readiness=false. Elapsed: 5.514945ms
Apr 22 20:55:28.221: INFO: Pod "downwardapi-volume-a92ce07d-a657-47a9-a26f-ee45b7f57c07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010898586s
STEP: Saw pod success
Apr 22 20:55:28.222: INFO: Pod "downwardapi-volume-a92ce07d-a657-47a9-a26f-ee45b7f57c07" satisfied condition "Succeeded or Failed"
Apr 22 20:55:28.224: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod downwardapi-volume-a92ce07d-a657-47a9-a26f-ee45b7f57c07 container client-container: <nil>
STEP: delete the pod
Apr 22 20:55:28.247: INFO: Waiting for pod downwardapi-volume-a92ce07d-a657-47a9-a26f-ee45b7f57c07 to disappear
Apr 22 20:55:28.249: INFO: Pod downwardapi-volume-a92ce07d-a657-47a9-a26f-ee45b7f57c07 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:55:28.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7205" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":47,"skipped":815,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:55:28.256: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-8bfff9ed-2cab-4f18-8980-b0a40ff5b2d7
STEP: Creating a pod to test consume configMaps
Apr 22 20:55:28.288: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-70d8934d-9998-4c69-a7e1-ab9de86761b5" in namespace "projected-5032" to be "Succeeded or Failed"
Apr 22 20:55:28.290: INFO: Pod "pod-projected-configmaps-70d8934d-9998-4c69-a7e1-ab9de86761b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026014ms
Apr 22 20:55:30.296: INFO: Pod "pod-projected-configmaps-70d8934d-9998-4c69-a7e1-ab9de86761b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007842422s
STEP: Saw pod success
Apr 22 20:55:30.296: INFO: Pod "pod-projected-configmaps-70d8934d-9998-4c69-a7e1-ab9de86761b5" satisfied condition "Succeeded or Failed"
Apr 22 20:55:30.298: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-projected-configmaps-70d8934d-9998-4c69-a7e1-ab9de86761b5 container agnhost-container: <nil>
STEP: delete the pod
Apr 22 20:55:30.320: INFO: Waiting for pod pod-projected-configmaps-70d8934d-9998-4c69-a7e1-ab9de86761b5 to disappear
Apr 22 20:55:30.322: INFO: Pod pod-projected-configmaps-70d8934d-9998-4c69-a7e1-ab9de86761b5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:55:30.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5032" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":48,"skipped":819,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:55:30.330: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Apr 22 20:55:30.358: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:55:34.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4187" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":49,"skipped":852,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:55:34.379: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 22 20:55:34.920: INFO: starting watch
STEP: patching
STEP: updating
Apr 22 20:55:34.929: INFO: waiting for watch events with expected annotations
Apr 22 20:55:34.929: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:55:34.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-3488" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":50,"skipped":872,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:55:34.986: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Apr 22 20:55:35.011: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-57 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:55:35.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-57" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":51,"skipped":921,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:55:35.078: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Apr 22 20:55:35.114: INFO: observed Pod pod-test in namespace pods-2531 in phase Pending conditions []
Apr 22 20:55:35.115: INFO: observed Pod pod-test in namespace pods-2531 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 20:55:35 +0000 UTC  }]
Apr 22 20:55:35.132: INFO: observed Pod pod-test in namespace pods-2531 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 20:55:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 20:55:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 20:55:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 20:55:35 +0000 UTC  }]
Apr 22 20:55:35.605: INFO: observed Pod pod-test in namespace pods-2531 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 20:55:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 20:55:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 20:55:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 20:55:35 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Apr 22 20:55:36.660: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Apr 22 20:55:36.680: INFO: observed event type ADDED
Apr 22 20:55:36.680: INFO: observed event type MODIFIED
Apr 22 20:55:36.681: INFO: observed event type MODIFIED
Apr 22 20:55:36.681: INFO: observed event type MODIFIED
Apr 22 20:55:36.681: INFO: observed event type MODIFIED
Apr 22 20:55:36.681: INFO: observed event type MODIFIED
Apr 22 20:55:36.681: INFO: observed event type MODIFIED
Apr 22 20:55:36.681: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:55:36.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2531" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":52,"skipped":925,"failed":0}
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:55:36.688: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:55:38.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4182" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":53,"skipped":931,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:55:38.743: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 20:55:38.769: INFO: Creating deployment "webserver-deployment"
Apr 22 20:55:38.773: INFO: Waiting for observed generation 1
Apr 22 20:55:40.780: INFO: Waiting for all required pods to come up
Apr 22 20:55:40.783: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Apr 22 20:55:46.793: INFO: Waiting for deployment "webserver-deployment" to complete
Apr 22 20:55:46.797: INFO: Updating deployment "webserver-deployment" with a non-existent image
Apr 22 20:55:46.810: INFO: Updating deployment webserver-deployment
Apr 22 20:55:46.810: INFO: Waiting for observed generation 2
Apr 22 20:55:48.818: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr 22 20:55:48.820: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr 22 20:55:48.822: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 22 20:55:48.828: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr 22 20:55:48.828: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr 22 20:55:48.829: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 22 20:55:48.833: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Apr 22 20:55:48.833: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Apr 22 20:55:48.842: INFO: Updating deployment webserver-deployment
Apr 22 20:55:48.842: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Apr 22 20:55:48.845: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr 22 20:55:48.847: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 22 20:55:50.858: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9597  7b247277-3527-4cce-ac2e-40f03405ca3d 7586 3 2021-04-22 20:55:38 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-04-22 20:55:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-22 20:55:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002842268 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:9,UnavailableReplicas:24,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-04-22 20:55:48 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-04-22 20:55:50 +0000 UTC,LastTransitionTime:2021-04-22 20:55:38 +0000 UTC,},},ReadyReplicas:9,CollisionCount:nil,},}

Apr 22 20:55:50.860: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-9597  3ae7a478-f660-4890-ad9b-bf74ae5e6ecc 7408 3 2021-04-22 20:55:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 7b247277-3527-4cce-ac2e-40f03405ca3d 0xc002842f87 0xc002842f88}] []  [{kube-controller-manager Update apps/v1 2021-04-22 20:55:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b247277-3527-4cce-ac2e-40f03405ca3d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002843008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 22 20:55:50.860: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Apr 22 20:55:50.860: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-9597  508c1d09-ed72-499c-8aa5-83644d9f4433 7585 3 2021-04-22 20:55:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 7b247277-3527-4cce-ac2e-40f03405ca3d 0xc002843067 0xc002843068}] []  [{kube-controller-manager Update apps/v1 2021-04-22 20:55:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b247277-3527-4cce-ac2e-40f03405ca3d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028430d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[]ReplicaSetCondition{},},}
Apr 22 20:55:50.865: INFO: Pod "webserver-deployment-795d758f88-456ck" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-456ck webserver-deployment-795d758f88- deployment-9597  da26e4a9-5b73-499b-9560-51264439693d 7492 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.255.223/32 cni.projectcalico.org/podIPs:192.168.255.223/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3ae7a478-f660-4890-ad9b-bf74ae5e6ecc 0xc0015e8760 0xc0015e8761}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ae7a478-f660-4890-ad9b-bf74ae5e6ecc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-122.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.122,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.866: INFO: Pod "webserver-deployment-795d758f88-5xh7k" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5xh7k webserver-deployment-795d758f88- deployment-9597  745918c8-dc1e-4d15-9ac8-fbb9d3a85679 7489 0 2021-04-22 20:55:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.156.208/32 cni.projectcalico.org/podIPs:192.168.156.208/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3ae7a478-f660-4890-ad9b-bf74ae5e6ecc 0xc0015e8907 0xc0015e8908}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ae7a478-f660-4890-ad9b-bf74ae5e6ecc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-22 20:55:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.156.208\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-230.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.230,PodIP:192.168.156.208,StartTime:2021-04-22 20:55:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.156.208,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.866: INFO: Pod "webserver-deployment-795d758f88-7krgb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7krgb webserver-deployment-795d758f88- deployment-9597  36f0cceb-bec1-4fdb-9e7f-e122140a6c90 7581 0 2021-04-22 20:55:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.126.32/32 cni.projectcalico.org/podIPs:192.168.126.32/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3ae7a478-f660-4890-ad9b-bf74ae5e6ecc 0xc0015e8ae7 0xc0015e8ae8}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ae7a478-f660-4890-ad9b-bf74ae5e6ecc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-22 20:55:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-22 20:55:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.126.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-2.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.2,PodIP:192.168.126.32,StartTime:2021-04-22 20:55:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.126.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.866: INFO: Pod "webserver-deployment-795d758f88-85484" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-85484 webserver-deployment-795d758f88- deployment-9597  492c8ba7-64b1-4a83-bcef-f42edccdd25f 7516 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.156.212/32 cni.projectcalico.org/podIPs:192.168.156.212/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3ae7a478-f660-4890-ad9b-bf74ae5e6ecc 0xc0015e8cc0 0xc0015e8cc1}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ae7a478-f660-4890-ad9b-bf74ae5e6ecc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-230.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.230,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.866: INFO: Pod "webserver-deployment-795d758f88-945md" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-945md webserver-deployment-795d758f88- deployment-9597  83a49358-d99a-45e2-9197-5c10fd0c6789 7485 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.224.79/32 cni.projectcalico.org/podIPs:192.168.224.79/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3ae7a478-f660-4890-ad9b-bf74ae5e6ecc 0xc0015e8e87 0xc0015e8e88}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ae7a478-f660-4890-ad9b-bf74ae5e6ecc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-171.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.171,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.866: INFO: Pod "webserver-deployment-795d758f88-bg2np" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bg2np webserver-deployment-795d758f88- deployment-9597  c81bd919-086e-4017-8193-e6008feef5dd 7473 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.255.222/32 cni.projectcalico.org/podIPs:192.168.255.222/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3ae7a478-f660-4890-ad9b-bf74ae5e6ecc 0xc0015e9037 0xc0015e9038}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ae7a478-f660-4890-ad9b-bf74ae5e6ecc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-122.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.122,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.866: INFO: Pod "webserver-deployment-795d758f88-hkqv6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hkqv6 webserver-deployment-795d758f88- deployment-9597  4807505c-96e4-4750-92ed-85ae5bc6b287 7569 0 2021-04-22 20:55:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.255.221/32 cni.projectcalico.org/podIPs:192.168.255.221/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3ae7a478-f660-4890-ad9b-bf74ae5e6ecc 0xc0015e91e7 0xc0015e91e8}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ae7a478-f660-4890-ad9b-bf74ae5e6ecc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-22 20:55:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.255.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-122.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.122,PodIP:192.168.255.221,StartTime:2021-04-22 20:55:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.255.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.866: INFO: Pod "webserver-deployment-795d758f88-k97v9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-k97v9 webserver-deployment-795d758f88- deployment-9597  93651a57-da4b-4e2f-8a12-4f9fa669741b 7547 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.126.37/32 cni.projectcalico.org/podIPs:192.168.126.37/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3ae7a478-f660-4890-ad9b-bf74ae5e6ecc 0xc0015e9427 0xc0015e9428}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ae7a478-f660-4890-ad9b-bf74ae5e6ecc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-2.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.2,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.867: INFO: Pod "webserver-deployment-795d758f88-lcl9d" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lcl9d webserver-deployment-795d758f88- deployment-9597  184e9aa7-cda4-4183-adfa-9596cc8c6693 7477 0 2021-04-22 20:55:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.126.31/32 cni.projectcalico.org/podIPs:192.168.126.31/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3ae7a478-f660-4890-ad9b-bf74ae5e6ecc 0xc0015e9810 0xc0015e9811}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ae7a478-f660-4890-ad9b-bf74ae5e6ecc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-22 20:55:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.126.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-2.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.2,PodIP:192.168.126.31,StartTime:2021-04-22 20:55:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.126.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.867: INFO: Pod "webserver-deployment-795d758f88-psccg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-psccg webserver-deployment-795d758f88- deployment-9597  fd942d20-081d-473b-81d0-eb5ffa3c9ea4 7481 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.156.211/32 cni.projectcalico.org/podIPs:192.168.156.211/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3ae7a478-f660-4890-ad9b-bf74ae5e6ecc 0xc0015e9ad0 0xc0015e9ad1}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ae7a478-f660-4890-ad9b-bf74ae5e6ecc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-230.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.230,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.867: INFO: Pod "webserver-deployment-795d758f88-svzh2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-svzh2 webserver-deployment-795d758f88- deployment-9597  968779f1-0af4-4278-966f-672a05931e05 7544 0 2021-04-22 20:55:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.224.77/32 cni.projectcalico.org/podIPs:192.168.224.77/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3ae7a478-f660-4890-ad9b-bf74ae5e6ecc 0xc0015e9e07 0xc0015e9e08}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ae7a478-f660-4890-ad9b-bf74ae5e6ecc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-22 20:55:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.224.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-171.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.171,PodIP:192.168.224.77,StartTime:2021-04-22 20:55:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.224.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.867: INFO: Pod "webserver-deployment-795d758f88-vl85m" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vl85m webserver-deployment-795d758f88- deployment-9597  91f4a44f-6767-41c4-858a-9f431565469f 7528 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.224.80/32 cni.projectcalico.org/podIPs:192.168.224.80/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3ae7a478-f660-4890-ad9b-bf74ae5e6ecc 0xc0023c20b7 0xc0023c20b8}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ae7a478-f660-4890-ad9b-bf74ae5e6ecc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-171.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.171,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.867: INFO: Pod "webserver-deployment-795d758f88-zlkv2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zlkv2 webserver-deployment-795d758f88- deployment-9597  5166a38d-02d6-4662-8028-49176ebca93d 7538 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:192.168.126.36/32 cni.projectcalico.org/podIPs:192.168.126.36/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 3ae7a478-f660-4890-ad9b-bf74ae5e6ecc 0xc0023c2287 0xc0023c2288}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ae7a478-f660-4890-ad9b-bf74ae5e6ecc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-2.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.2,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.867: INFO: Pod "webserver-deployment-dd94f59b7-2wlch" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2wlch webserver-deployment-dd94f59b7- deployment-9597  eb64d881-70f8-4841-82bb-226b42b06c9d 7552 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.224.81/32 cni.projectcalico.org/podIPs:192.168.224.81/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc0023c2430 0xc0023c2431}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-171.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.171,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.867: INFO: Pod "webserver-deployment-dd94f59b7-46ns9" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-46ns9 webserver-deployment-dd94f59b7- deployment-9597  244d6e34-344b-4c0d-8ae6-732ecac20641 7498 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.126.35/32 cni.projectcalico.org/podIPs:192.168.126.35/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc0023c25b0 0xc0023c25b1}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-2.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.2,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.868: INFO: Pod "webserver-deployment-dd94f59b7-4xfbp" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4xfbp webserver-deployment-dd94f59b7- deployment-9597  2a084a39-4c51-468b-9e73-bc40ed4e3a8b 7461 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.156.210/32 cni.projectcalico.org/podIPs:192.168.156.210/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc0023c2730 0xc0023c2731}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-230.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.230,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.868: INFO: Pod "webserver-deployment-dd94f59b7-5nqfk" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5nqfk webserver-deployment-dd94f59b7- deployment-9597  dd4c8391-111f-4749-afd6-8736b3e72d53 7536 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.255.225/32 cni.projectcalico.org/podIPs:192.168.255.225/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc0023c28b0 0xc0023c28b1}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-122.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.122,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.868: INFO: Pod "webserver-deployment-dd94f59b7-8dqqb" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8dqqb webserver-deployment-dd94f59b7- deployment-9597  a098456a-8d2a-4491-8ae0-79fc46622e96 7549 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.255.226/32 cni.projectcalico.org/podIPs:192.168.255.226/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc0023c2a30 0xc0023c2a31}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-122.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.122,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.868: INFO: Pod "webserver-deployment-dd94f59b7-9cts4" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9cts4 webserver-deployment-dd94f59b7- deployment-9597  c50a6f9d-50a6-4b3f-b9c8-78c5c80fdecc 7584 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.224.78/32 cni.projectcalico.org/podIPs:192.168.224.78/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc0023c2bb0 0xc0023c2bb1}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-22 20:55:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.224.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-171.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.171,PodIP:192.168.224.78,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-22 20:55:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://7df31cf25d133a7eda3a43660e02726cd802d92ab86d2824d8e3ebc3c932e4d2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.224.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.868: INFO: Pod "webserver-deployment-dd94f59b7-b89pc" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-b89pc webserver-deployment-dd94f59b7- deployment-9597  637d0808-4810-4dd1-b0b2-8928d84eda0b 7168 0 2021-04-22 20:55:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.126.30/32 cni.projectcalico.org/podIPs:192.168.126.30/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc0023c2d50 0xc0023c2d51}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-22 20:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-22 20:55:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.126.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-2.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.2,PodIP:192.168.126.30,StartTime:2021-04-22 20:55:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-22 20:55:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://cc0eb0e97717476e944e2cf9396d92ec4b0753d7b628209faf17d6135c9db024,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.126.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.868: INFO: Pod "webserver-deployment-dd94f59b7-c5l2g" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-c5l2g webserver-deployment-dd94f59b7- deployment-9597  cabfa5d8-c4d4-4031-9c88-b245ace766c6 7177 0 2021-04-22 20:55:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.255.220/32 cni.projectcalico.org/podIPs:192.168.255.220/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc0023c2ef0 0xc0023c2ef1}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-22 20:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-22 20:55:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.255.220\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-122.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.122,PodIP:192.168.255.220,StartTime:2021-04-22 20:55:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-22 20:55:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://88e6df5fde72c130477538baab6fcd496bbd5f04d646dfd3e52e3682e0d31054,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.255.220,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.869: INFO: Pod "webserver-deployment-dd94f59b7-dg9mw" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dg9mw webserver-deployment-dd94f59b7- deployment-9597  523b957b-55fb-4540-b4aa-b9355c530950 7534 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.156.213/32 cni.projectcalico.org/podIPs:192.168.156.213/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc0023c3090 0xc0023c3091}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-230.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.230,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.869: INFO: Pod "webserver-deployment-dd94f59b7-dq8gk" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dq8gk webserver-deployment-dd94f59b7- deployment-9597  5261a0ba-c2ea-4a51-bae9-ee2d169843f8 7174 0 2021-04-22 20:55:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.224.75/32 cni.projectcalico.org/podIPs:192.168.224.75/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc0023c3210 0xc0023c3211}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-22 20:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-22 20:55:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.224.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-171.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.171,PodIP:192.168.224.75,StartTime:2021-04-22 20:55:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-22 20:55:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://2994e29ca075f392dc5358bbdf5342e51844b5508f4b4c22987554c88e43ce3e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.224.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.869: INFO: Pod "webserver-deployment-dd94f59b7-fhhbk" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fhhbk webserver-deployment-dd94f59b7- deployment-9597  8e37f699-e97d-4544-ad62-a6177c7fb4b8 7521 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.255.224/32 cni.projectcalico.org/podIPs:192.168.255.224/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc0023c3440 0xc0023c3441}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-122.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.122,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.869: INFO: Pod "webserver-deployment-dd94f59b7-fnn79" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fnn79 webserver-deployment-dd94f59b7- deployment-9597  c01b2057-9768-4ddb-8ccd-310437955262 7229 0 2021-04-22 20:55:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.156.206/32 cni.projectcalico.org/podIPs:192.168.156.206/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc0023c36e0 0xc0023c36e1}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-22 20:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-22 20:55:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.156.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-230.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.230,PodIP:192.168.156.206,StartTime:2021-04-22 20:55:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-22 20:55:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://007c3af352e5d1bafb77c850fe3417fa4dfce9c547052109497b1a75cb7fb5aa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.156.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.869: INFO: Pod "webserver-deployment-dd94f59b7-mc76s" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mc76s webserver-deployment-dd94f59b7- deployment-9597  13009690-311b-4131-b697-249e158dd85c 7551 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.156.214/32 cni.projectcalico.org/podIPs:192.168.156.214/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc0023c38a0 0xc0023c38a1}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-230.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.230,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.869: INFO: Pod "webserver-deployment-dd94f59b7-qjpp7" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qjpp7 webserver-deployment-dd94f59b7- deployment-9597  d4ef66cf-bf12-4bd0-b53b-934104528ee8 7154 0 2021-04-22 20:55:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.255.219/32 cni.projectcalico.org/podIPs:192.168.255.219/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc0023c3a20 0xc0023c3a21}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-22 20:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-22 20:55:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.255.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-122.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.122,PodIP:192.168.255.219,StartTime:2021-04-22 20:55:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-22 20:55:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://cf4dee9bb178d9d66893c6be8e7f0a9067320a88f074e717e99cb7cbc79cbe9f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.255.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.869: INFO: Pod "webserver-deployment-dd94f59b7-qkkd7" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qkkd7 webserver-deployment-dd94f59b7- deployment-9597  fbc61c32-6939-4b41-9cf0-793d76bc87ea 7446 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.156.209/32 cni.projectcalico.org/podIPs:192.168.156.209/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc0023c3c00 0xc0023c3c01}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-230.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.230,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.869: INFO: Pod "webserver-deployment-dd94f59b7-qswwm" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qswwm webserver-deployment-dd94f59b7- deployment-9597  55920ced-d0d0-47c4-8838-dea42e519b79 7449 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.126.33/32 cni.projectcalico.org/podIPs:192.168.126.33/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc0023c3e70 0xc0023c3e71}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-2.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.2,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.870: INFO: Pod "webserver-deployment-dd94f59b7-sg4g4" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-sg4g4 webserver-deployment-dd94f59b7- deployment-9597  d7fe15d4-005e-40ae-b417-4257aa6f83d3 7196 0 2021-04-22 20:55:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.224.76/32 cni.projectcalico.org/podIPs:192.168.224.76/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc0023c3ff0 0xc0023c3ff1}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-22 20:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-22 20:55:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.224.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-171.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.171,PodIP:192.168.224.76,StartTime:2021-04-22 20:55:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-22 20:55:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://ddfc1842f07c71ca36f123094f8f9f4bc05c736d4d17cd3bff37b377b5cb6470,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.224.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.870: INFO: Pod "webserver-deployment-dd94f59b7-tww4k" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-tww4k webserver-deployment-dd94f59b7- deployment-9597  0e0e4234-1e46-4b47-aa25-74ae3551b560 7147 0 2021-04-22 20:55:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.126.29/32 cni.projectcalico.org/podIPs:192.168.126.29/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc001cd4190 0xc001cd4191}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-22 20:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-22 20:55:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.126.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-2.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.2,PodIP:192.168.126.29,StartTime:2021-04-22 20:55:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-22 20:55:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://2c778911f39e5179a006c9f35394a0de42d1994220f4e3055f4dd4516d36adc9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.126.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.870: INFO: Pod "webserver-deployment-dd94f59b7-wbprw" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wbprw webserver-deployment-dd94f59b7- deployment-9597  8e5a159e-a5a4-4c20-9d49-3675ced796b0 7151 0 2021-04-22 20:55:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.224.74/32 cni.projectcalico.org/podIPs:192.168.224.74/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc001cd4330 0xc001cd4331}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-22 20:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-22 20:55:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.224.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-171.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.171,PodIP:192.168.224.74,StartTime:2021-04-22 20:55:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-22 20:55:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://40dc9fe01b050bea8a45de0e650835c2c83f1d38d125933d96df4d9808fb4822,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.224.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 22 20:55:50.870: INFO: Pod "webserver-deployment-dd94f59b7-x4pmd" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-x4pmd webserver-deployment-dd94f59b7- deployment-9597  7d3618d3-2ac7-4568-8e81-ade4f57e7052 7468 0 2021-04-22 20:55:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:192.168.126.34/32 cni.projectcalico.org/podIPs:192.168.126.34/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 508c1d09-ed72-499c-8aa5-83644d9f4433 0xc001cd44d0 0xc001cd44d1}] []  [{kube-controller-manager Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508c1d09-ed72-499c-8aa5-83644d9f4433\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 20:55:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-04-22 20:55:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f5sgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f5sgg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f5sgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-2.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 20:55:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.2,PodIP:,StartTime:2021-04-22 20:55:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:55:50.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9597" for this suite.

• [SLOW TEST:12.136 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":54,"skipped":948,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:55:50.880: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 22 20:55:50.913: INFO: Waiting up to 5m0s for pod "pod-d2d19845-d04e-4ba3-96fb-0a50d4faae6e" in namespace "emptydir-9274" to be "Succeeded or Failed"
Apr 22 20:55:50.915: INFO: Pod "pod-d2d19845-d04e-4ba3-96fb-0a50d4faae6e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.973351ms
Apr 22 20:55:52.920: INFO: Pod "pod-d2d19845-d04e-4ba3-96fb-0a50d4faae6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007003832s
Apr 22 20:55:54.927: INFO: Pod "pod-d2d19845-d04e-4ba3-96fb-0a50d4faae6e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014129298s
Apr 22 20:55:56.931: INFO: Pod "pod-d2d19845-d04e-4ba3-96fb-0a50d4faae6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018678018s
STEP: Saw pod success
Apr 22 20:55:56.931: INFO: Pod "pod-d2d19845-d04e-4ba3-96fb-0a50d4faae6e" satisfied condition "Succeeded or Failed"
Apr 22 20:55:56.933: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-d2d19845-d04e-4ba3-96fb-0a50d4faae6e container test-container: <nil>
STEP: delete the pod
Apr 22 20:55:56.946: INFO: Waiting for pod pod-d2d19845-d04e-4ba3-96fb-0a50d4faae6e to disappear
Apr 22 20:55:56.948: INFO: Pod pod-d2d19845-d04e-4ba3-96fb-0a50d4faae6e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:55:56.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9274" for this suite.

• [SLOW TEST:6.076 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":55,"skipped":964,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:55:56.956: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:55:57.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7629" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":56,"skipped":975,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:55:57.011: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:56:07.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8292" for this suite.

• [SLOW TEST:10.042 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":57,"skipped":982,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:56:07.053: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 22 20:56:10.104: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:56:10.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9226" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":58,"skipped":1004,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:56:10.131: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-4d9a710a-c8c8-4f7c-8a26-8222e50062f3
STEP: Creating configMap with name cm-test-opt-upd-67c50224-0e47-4283-bb84-55b368d6cc74
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4d9a710a-c8c8-4f7c-8a26-8222e50062f3
STEP: Updating configmap cm-test-opt-upd-67c50224-0e47-4283-bb84-55b368d6cc74
STEP: Creating configMap with name cm-test-opt-create-834e2c15-2121-4566-ac72-92afb6f02a3f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:57:18.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6326" for this suite.

• [SLOW TEST:68.278 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":59,"skipped":1006,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:57:18.410: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 20:57:18.444: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0bb53a00-5dc8-4b46-bd2b-0400df265b40" in namespace "downward-api-9924" to be "Succeeded or Failed"
Apr 22 20:57:18.446: INFO: Pod "downwardapi-volume-0bb53a00-5dc8-4b46-bd2b-0400df265b40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026035ms
Apr 22 20:57:20.452: INFO: Pod "downwardapi-volume-0bb53a00-5dc8-4b46-bd2b-0400df265b40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007825493s
STEP: Saw pod success
Apr 22 20:57:20.452: INFO: Pod "downwardapi-volume-0bb53a00-5dc8-4b46-bd2b-0400df265b40" satisfied condition "Succeeded or Failed"
Apr 22 20:57:20.454: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod downwardapi-volume-0bb53a00-5dc8-4b46-bd2b-0400df265b40 container client-container: <nil>
STEP: delete the pod
Apr 22 20:57:20.467: INFO: Waiting for pod downwardapi-volume-0bb53a00-5dc8-4b46-bd2b-0400df265b40 to disappear
Apr 22 20:57:20.469: INFO: Pod downwardapi-volume-0bb53a00-5dc8-4b46-bd2b-0400df265b40 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:57:20.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9924" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":60,"skipped":1008,"failed":0}
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:57:20.476: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-3fcc4787-5ac6-40e0-bf26-a8a19aaec2fe
STEP: Creating secret with name secret-projected-all-test-volume-77d5788c-2b8a-4198-9c65-9e5c2d981c13
STEP: Creating a pod to test Check all projections for projected volume plugin
Apr 22 20:57:20.515: INFO: Waiting up to 5m0s for pod "projected-volume-fd33f304-add9-4deb-96eb-3d4b002e4d4f" in namespace "projected-1499" to be "Succeeded or Failed"
Apr 22 20:57:20.520: INFO: Pod "projected-volume-fd33f304-add9-4deb-96eb-3d4b002e4d4f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.787262ms
Apr 22 20:57:22.526: INFO: Pod "projected-volume-fd33f304-add9-4deb-96eb-3d4b002e4d4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010672113s
Apr 22 20:57:24.532: INFO: Pod "projected-volume-fd33f304-add9-4deb-96eb-3d4b002e4d4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016524522s
STEP: Saw pod success
Apr 22 20:57:24.532: INFO: Pod "projected-volume-fd33f304-add9-4deb-96eb-3d4b002e4d4f" satisfied condition "Succeeded or Failed"
Apr 22 20:57:24.534: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod projected-volume-fd33f304-add9-4deb-96eb-3d4b002e4d4f container projected-all-volume-test: <nil>
STEP: delete the pod
Apr 22 20:57:24.553: INFO: Waiting for pod projected-volume-fd33f304-add9-4deb-96eb-3d4b002e4d4f to disappear
Apr 22 20:57:24.556: INFO: Pod projected-volume-fd33f304-add9-4deb-96eb-3d4b002e4d4f no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:57:24.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1499" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":61,"skipped":1009,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:57:24.566: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 22 20:57:24.600: INFO: Waiting up to 5m0s for pod "pod-c5993f05-2222-494d-b44f-13a6270eb7c1" in namespace "emptydir-1227" to be "Succeeded or Failed"
Apr 22 20:57:24.606: INFO: Pod "pod-c5993f05-2222-494d-b44f-13a6270eb7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.921404ms
Apr 22 20:57:26.610: INFO: Pod "pod-c5993f05-2222-494d-b44f-13a6270eb7c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010342986s
STEP: Saw pod success
Apr 22 20:57:26.610: INFO: Pod "pod-c5993f05-2222-494d-b44f-13a6270eb7c1" satisfied condition "Succeeded or Failed"
Apr 22 20:57:26.613: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-c5993f05-2222-494d-b44f-13a6270eb7c1 container test-container: <nil>
STEP: delete the pod
Apr 22 20:57:26.626: INFO: Waiting for pod pod-c5993f05-2222-494d-b44f-13a6270eb7c1 to disappear
Apr 22 20:57:26.628: INFO: Pod pod-c5993f05-2222-494d-b44f-13a6270eb7c1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:57:26.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1227" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":62,"skipped":1028,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:57:26.635: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:57:26.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7307" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":63,"skipped":1039,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:57:26.720: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 22 20:57:26.750: INFO: Waiting up to 5m0s for pod "downward-api-736c2d25-9df9-4a93-acb6-8acc6fd914c1" in namespace "downward-api-4446" to be "Succeeded or Failed"
Apr 22 20:57:26.751: INFO: Pod "downward-api-736c2d25-9df9-4a93-acb6-8acc6fd914c1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.90404ms
Apr 22 20:57:28.756: INFO: Pod "downward-api-736c2d25-9df9-4a93-acb6-8acc6fd914c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006870389s
STEP: Saw pod success
Apr 22 20:57:28.756: INFO: Pod "downward-api-736c2d25-9df9-4a93-acb6-8acc6fd914c1" satisfied condition "Succeeded or Failed"
Apr 22 20:57:28.759: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod downward-api-736c2d25-9df9-4a93-acb6-8acc6fd914c1 container dapi-container: <nil>
STEP: delete the pod
Apr 22 20:57:28.771: INFO: Waiting for pod downward-api-736c2d25-9df9-4a93-acb6-8acc6fd914c1 to disappear
Apr 22 20:57:28.773: INFO: Pod downward-api-736c2d25-9df9-4a93-acb6-8acc6fd914c1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:57:28.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4446" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":64,"skipped":1041,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:57:28.780: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Apr 22 20:57:28.808: INFO: Waiting up to 5m0s for pod "client-containers-8d895ee0-08a2-4878-8c96-46b1c8e5df23" in namespace "containers-306" to be "Succeeded or Failed"
Apr 22 20:57:28.813: INFO: Pod "client-containers-8d895ee0-08a2-4878-8c96-46b1c8e5df23": Phase="Pending", Reason="", readiness=false. Elapsed: 5.036463ms
Apr 22 20:57:30.820: INFO: Pod "client-containers-8d895ee0-08a2-4878-8c96-46b1c8e5df23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01144251s
STEP: Saw pod success
Apr 22 20:57:30.820: INFO: Pod "client-containers-8d895ee0-08a2-4878-8c96-46b1c8e5df23" satisfied condition "Succeeded or Failed"
Apr 22 20:57:30.822: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod client-containers-8d895ee0-08a2-4878-8c96-46b1c8e5df23 container agnhost-container: <nil>
STEP: delete the pod
Apr 22 20:57:30.837: INFO: Waiting for pod client-containers-8d895ee0-08a2-4878-8c96-46b1c8e5df23 to disappear
Apr 22 20:57:30.839: INFO: Pod client-containers-8d895ee0-08a2-4878-8c96-46b1c8e5df23 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:57:30.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-306" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":65,"skipped":1066,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:57:30.846: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-c0aed6b4-919f-4b64-a961-a32cb41608e7
STEP: Creating a pod to test consume secrets
Apr 22 20:57:30.885: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6e56f4ff-f769-4960-a913-261dc903c3fc" in namespace "projected-4168" to be "Succeeded or Failed"
Apr 22 20:57:30.887: INFO: Pod "pod-projected-secrets-6e56f4ff-f769-4960-a913-261dc903c3fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022925ms
Apr 22 20:57:32.893: INFO: Pod "pod-projected-secrets-6e56f4ff-f769-4960-a913-261dc903c3fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008090084s
STEP: Saw pod success
Apr 22 20:57:32.893: INFO: Pod "pod-projected-secrets-6e56f4ff-f769-4960-a913-261dc903c3fc" satisfied condition "Succeeded or Failed"
Apr 22 20:57:32.895: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-projected-secrets-6e56f4ff-f769-4960-a913-261dc903c3fc container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 22 20:57:32.910: INFO: Waiting for pod pod-projected-secrets-6e56f4ff-f769-4960-a913-261dc903c3fc to disappear
Apr 22 20:57:32.912: INFO: Pod pod-projected-secrets-6e56f4ff-f769-4960-a913-261dc903c3fc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:57:32.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4168" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":66,"skipped":1082,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:57:32.919: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:57:39.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5906" for this suite.

• [SLOW TEST:7.063 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":67,"skipped":1105,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:57:39.983: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 22 20:57:40.031: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 22 20:58:40.064: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Apr 22 20:58:40.091: INFO: Created pod: pod0-sched-preemption-low-priority
Apr 22 20:58:40.103: INFO: Created pod: pod1-sched-preemption-medium-priority
Apr 22 20:58:40.120: INFO: Created pod: pod2-sched-preemption-medium-priority
Apr 22 20:58:40.136: INFO: Created pod: pod3-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:59:10.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4448" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:90.245 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":68,"skipped":1117,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:59:10.229: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 20:59:10.262: INFO: Waiting up to 5m0s for pod "downwardapi-volume-30fe1094-f368-4cc0-b0c4-99a172f18e89" in namespace "downward-api-9126" to be "Succeeded or Failed"
Apr 22 20:59:10.264: INFO: Pod "downwardapi-volume-30fe1094-f368-4cc0-b0c4-99a172f18e89": Phase="Pending", Reason="", readiness=false. Elapsed: 1.920633ms
Apr 22 20:59:12.270: INFO: Pod "downwardapi-volume-30fe1094-f368-4cc0-b0c4-99a172f18e89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007505422s
STEP: Saw pod success
Apr 22 20:59:12.270: INFO: Pod "downwardapi-volume-30fe1094-f368-4cc0-b0c4-99a172f18e89" satisfied condition "Succeeded or Failed"
Apr 22 20:59:12.272: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod downwardapi-volume-30fe1094-f368-4cc0-b0c4-99a172f18e89 container client-container: <nil>
STEP: delete the pod
Apr 22 20:59:12.291: INFO: Waiting for pod downwardapi-volume-30fe1094-f368-4cc0-b0c4-99a172f18e89 to disappear
Apr 22 20:59:12.293: INFO: Pod downwardapi-volume-30fe1094-f368-4cc0-b0c4-99a172f18e89 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:59:12.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9126" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":69,"skipped":1332,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:59:12.301: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 22 20:59:12.327: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 22 20:59:12.334: INFO: Waiting for terminating namespaces to be deleted...
Apr 22 20:59:12.336: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-130-171.us-west-2.compute.internal before test
Apr 22 20:59:12.348: INFO: calico-node-zstxl from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 20:59:12.348: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 20:59:12.348: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 20:59:12.348: INFO: kube-proxy-246ms from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 20:59:12.348: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 20:59:12.348: INFO: preemptor-pod from sched-preemption-4448 started at 2021-04-22 20:59:08 +0000 UTC (1 container statuses recorded)
Apr 22 20:59:12.348: INFO: 	Container preemptor-pod ready: true, restart count 0
Apr 22 20:59:12.348: INFO: sonobuoy-e2e-job-f05335387339495e from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 20:59:12.348: INFO: 	Container e2e ready: true, restart count 0
Apr 22 20:59:12.348: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 22 20:59:12.348: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-x9h8q from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 20:59:12.348: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 22 20:59:12.348: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 22 20:59:12.348: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-130-230.us-west-2.compute.internal before test
Apr 22 20:59:12.353: INFO: calico-node-92wx7 from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 20:59:12.353: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 20:59:12.353: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 20:59:12.353: INFO: coredns-74ff55c5b-b2ghx from kube-system started at 2021-04-22 20:42:54 +0000 UTC (1 container statuses recorded)
Apr 22 20:59:12.353: INFO: 	Container coredns ready: true, restart count 0
Apr 22 20:59:12.353: INFO: kube-proxy-z7cgr from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 20:59:12.353: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 20:59:12.353: INFO: pod1-sched-preemption-medium-priority from sched-preemption-4448 started at 2021-04-22 20:58:48 +0000 UTC (1 container statuses recorded)
Apr 22 20:59:12.354: INFO: 	Container pod1-sched-preemption-medium-priority ready: true, restart count 0
Apr 22 20:59:12.354: INFO: sonobuoy from sonobuoy started at 2021-04-22 20:39:15 +0000 UTC (1 container statuses recorded)
Apr 22 20:59:12.354: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 22 20:59:12.354: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-f46mv from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 20:59:12.354: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 22 20:59:12.354: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 22 20:59:12.354: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-131-122.us-west-2.compute.internal before test
Apr 22 20:59:12.360: INFO: calico-node-w7mgb from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 20:59:12.360: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 20:59:12.360: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 20:59:12.360: INFO: coredns-74ff55c5b-l4x4h from kube-system started at 2021-04-22 20:38:22 +0000 UTC (1 container statuses recorded)
Apr 22 20:59:12.360: INFO: 	Container coredns ready: true, restart count 0
Apr 22 20:59:12.360: INFO: kube-proxy-d5r2q from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 20:59:12.360: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 20:59:12.360: INFO: pod2-sched-preemption-medium-priority from sched-preemption-4448 started at 2021-04-22 20:58:53 +0000 UTC (1 container statuses recorded)
Apr 22 20:59:12.360: INFO: 	Container pod2-sched-preemption-medium-priority ready: true, restart count 0
Apr 22 20:59:12.360: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-c9ssg from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 20:59:12.360: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 22 20:59:12.360: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 22 20:59:12.360: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-131-2.us-west-2.compute.internal before test
Apr 22 20:59:12.365: INFO: calico-node-bbjxk from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 20:59:12.365: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 20:59:12.365: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 20:59:12.365: INFO: coredns-74ff55c5b-kj2n4 from kube-system started at 2021-04-22 20:38:22 +0000 UTC (1 container statuses recorded)
Apr 22 20:59:12.365: INFO: 	Container coredns ready: false, restart count 0
Apr 22 20:59:12.365: INFO: kube-proxy-b7qsd from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 20:59:12.365: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 20:59:12.365: INFO: pod3-sched-preemption-medium-priority from sched-preemption-4448 started at 2021-04-22 20:58:53 +0000 UTC (1 container statuses recorded)
Apr 22 20:59:12.365: INFO: 	Container pod3-sched-preemption-medium-priority ready: true, restart count 0
Apr 22 20:59:12.365: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-4m6tb from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 20:59:12.365: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 22 20:59:12.365: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-04ad9d5f-4443-45ed-acaa-05f17100a4ef 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-04ad9d5f-4443-45ed-acaa-05f17100a4ef off the node ip-10-0-131-2.us-west-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-04ad9d5f-4443-45ed-acaa-05f17100a4ef
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:59:16.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2940" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":70,"skipped":1334,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:59:16.429: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6245
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6245
STEP: Creating statefulset with conflicting port in namespace statefulset-6245
STEP: Waiting until pod test-pod will start running in namespace statefulset-6245
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6245
Apr 22 20:59:20.482: INFO: Observed stateful pod in namespace: statefulset-6245, name: ss-0, uid: 09a49c38-f7cc-491a-ac86-e4158173ae21, status phase: Pending. Waiting for statefulset controller to delete.
Apr 22 20:59:20.541: INFO: Observed stateful pod in namespace: statefulset-6245, name: ss-0, uid: 09a49c38-f7cc-491a-ac86-e4158173ae21, status phase: Failed. Waiting for statefulset controller to delete.
Apr 22 20:59:20.545: INFO: Observed stateful pod in namespace: statefulset-6245, name: ss-0, uid: 09a49c38-f7cc-491a-ac86-e4158173ae21, status phase: Failed. Waiting for statefulset controller to delete.
Apr 22 20:59:20.549: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6245
STEP: Removing pod with conflicting port in namespace statefulset-6245
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6245 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 22 20:59:24.576: INFO: Deleting all statefulset in ns statefulset-6245
Apr 22 20:59:24.578: INFO: Scaling statefulset ss to 0
Apr 22 20:59:34.602: INFO: Waiting for statefulset status.replicas updated to 0
Apr 22 20:59:34.604: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:59:34.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6245" for this suite.

• [SLOW TEST:18.196 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":71,"skipped":1348,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:59:34.626: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 20:59:51.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1362" for this suite.

• [SLOW TEST:17.089 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":72,"skipped":1348,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 20:59:51.715: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-9903
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 22 20:59:51.747: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 22 20:59:51.776: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 22 20:59:53.782: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 20:59:55.782: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 20:59:57.780: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 20:59:59.782: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 21:00:01.782: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 21:00:03.782: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 21:00:05.782: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 22 21:00:05.786: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 22 21:00:07.791: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 22 21:00:09.792: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 22 21:00:11.792: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr 22 21:00:11.796: INFO: The status of Pod netserver-2 is Running (Ready = true)
Apr 22 21:00:11.800: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Apr 22 21:00:13.815: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Apr 22 21:00:13.815: INFO: Breadth first check of 192.168.224.88 on host 10.0.130.171...
Apr 22 21:00:13.817: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.126.48:9080/dial?request=hostname&protocol=http&host=192.168.224.88&port=8080&tries=1'] Namespace:pod-network-test-9903 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:00:13.817: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 21:00:13.876: INFO: Waiting for responses: map[]
Apr 22 21:00:13.876: INFO: reached 192.168.224.88 after 0/1 tries
Apr 22 21:00:13.876: INFO: Breadth first check of 192.168.156.216 on host 10.0.130.230...
Apr 22 21:00:13.879: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.126.48:9080/dial?request=hostname&protocol=http&host=192.168.156.216&port=8080&tries=1'] Namespace:pod-network-test-9903 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:00:13.879: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 21:00:13.934: INFO: Waiting for responses: map[]
Apr 22 21:00:13.934: INFO: reached 192.168.156.216 after 0/1 tries
Apr 22 21:00:13.934: INFO: Breadth first check of 192.168.255.234 on host 10.0.131.122...
Apr 22 21:00:13.937: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.126.48:9080/dial?request=hostname&protocol=http&host=192.168.255.234&port=8080&tries=1'] Namespace:pod-network-test-9903 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:00:13.937: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 21:00:13.988: INFO: Waiting for responses: map[]
Apr 22 21:00:13.988: INFO: reached 192.168.255.234 after 0/1 tries
Apr 22 21:00:13.988: INFO: Breadth first check of 192.168.126.47 on host 10.0.131.2...
Apr 22 21:00:13.991: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.126.48:9080/dial?request=hostname&protocol=http&host=192.168.126.47&port=8080&tries=1'] Namespace:pod-network-test-9903 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:00:13.991: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 21:00:14.046: INFO: Waiting for responses: map[]
Apr 22 21:00:14.046: INFO: reached 192.168.126.47 after 0/1 tries
Apr 22 21:00:14.046: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:00:14.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9903" for this suite.

• [SLOW TEST:22.342 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":73,"skipped":1352,"failed":0}
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:00:14.057: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 22 21:00:14.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-6150 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Apr 22 21:00:14.309: INFO: stderr: ""
Apr 22 21:00:14.309: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Apr 22 21:00:14.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-6150 delete pods e2e-test-httpd-pod'
Apr 22 21:00:28.466: INFO: stderr: ""
Apr 22 21:00:28.466: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:00:28.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6150" for this suite.

• [SLOW TEST:14.429 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":74,"skipped":1352,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:00:28.486: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:00:28.535: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Apr 22 21:00:28.541: INFO: Number of nodes with available pods: 0
Apr 22 21:00:28.541: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Apr 22 21:00:28.558: INFO: Number of nodes with available pods: 0
Apr 22 21:00:28.558: INFO: Node ip-10-0-131-2.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:00:29.563: INFO: Number of nodes with available pods: 0
Apr 22 21:00:29.563: INFO: Node ip-10-0-131-2.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:00:30.563: INFO: Number of nodes with available pods: 0
Apr 22 21:00:30.563: INFO: Node ip-10-0-131-2.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:00:31.563: INFO: Number of nodes with available pods: 1
Apr 22 21:00:31.563: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Apr 22 21:00:31.578: INFO: Number of nodes with available pods: 1
Apr 22 21:00:31.578: INFO: Number of running nodes: 0, number of available pods: 1
Apr 22 21:00:32.583: INFO: Number of nodes with available pods: 0
Apr 22 21:00:32.583: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Apr 22 21:00:32.591: INFO: Number of nodes with available pods: 0
Apr 22 21:00:32.591: INFO: Node ip-10-0-131-2.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:00:33.596: INFO: Number of nodes with available pods: 0
Apr 22 21:00:33.596: INFO: Node ip-10-0-131-2.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:00:34.596: INFO: Number of nodes with available pods: 0
Apr 22 21:00:34.596: INFO: Node ip-10-0-131-2.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:00:35.596: INFO: Number of nodes with available pods: 0
Apr 22 21:00:35.596: INFO: Node ip-10-0-131-2.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:00:36.596: INFO: Number of nodes with available pods: 0
Apr 22 21:00:36.596: INFO: Node ip-10-0-131-2.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:00:37.596: INFO: Number of nodes with available pods: 1
Apr 22 21:00:37.596: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6866, will wait for the garbage collector to delete the pods
Apr 22 21:00:37.661: INFO: Deleting DaemonSet.extensions daemon-set took: 9.558646ms
Apr 22 21:00:38.362: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.119681ms
Apr 22 21:00:41.068: INFO: Number of nodes with available pods: 0
Apr 22 21:00:41.068: INFO: Number of running nodes: 0, number of available pods: 0
Apr 22 21:00:41.070: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9941"},"items":null}

Apr 22 21:00:41.072: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9941"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:00:41.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6866" for this suite.

• [SLOW TEST:12.614 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":75,"skipped":1353,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:00:41.100: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Apr 22 21:00:41.128: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 21:00:44.009: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:00:55.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6319" for this suite.

• [SLOW TEST:14.336 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":76,"skipped":1370,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:00:55.436: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:00:55.478: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:00:56.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9568" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":77,"skipped":1380,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:00:56.514: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Apr 22 21:00:56.561: INFO: Waiting up to 5m0s for pod "var-expansion-a0555fe7-4846-44df-86a5-4c44da06aed2" in namespace "var-expansion-6534" to be "Succeeded or Failed"
Apr 22 21:00:56.564: INFO: Pod "var-expansion-a0555fe7-4846-44df-86a5-4c44da06aed2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.936234ms
Apr 22 21:00:58.569: INFO: Pod "var-expansion-a0555fe7-4846-44df-86a5-4c44da06aed2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008238082s
Apr 22 21:01:00.575: INFO: Pod "var-expansion-a0555fe7-4846-44df-86a5-4c44da06aed2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01405707s
STEP: Saw pod success
Apr 22 21:01:00.575: INFO: Pod "var-expansion-a0555fe7-4846-44df-86a5-4c44da06aed2" satisfied condition "Succeeded or Failed"
Apr 22 21:01:00.577: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod var-expansion-a0555fe7-4846-44df-86a5-4c44da06aed2 container dapi-container: <nil>
STEP: delete the pod
Apr 22 21:01:00.604: INFO: Waiting for pod var-expansion-a0555fe7-4846-44df-86a5-4c44da06aed2 to disappear
Apr 22 21:01:00.606: INFO: Pod var-expansion-a0555fe7-4846-44df-86a5-4c44da06aed2 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:01:00.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6534" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":78,"skipped":1394,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:01:00.613: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:01:16.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7044" for this suite.

• [SLOW TEST:16.140 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":79,"skipped":1411,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:01:16.753: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Apr 22 21:01:16.825: INFO: Waiting up to 5m0s for pod "var-expansion-8003d25f-8237-4bde-bbba-7cce45cc8e1f" in namespace "var-expansion-8653" to be "Succeeded or Failed"
Apr 22 21:01:16.827: INFO: Pod "var-expansion-8003d25f-8237-4bde-bbba-7cce45cc8e1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.943775ms
Apr 22 21:01:18.833: INFO: Pod "var-expansion-8003d25f-8237-4bde-bbba-7cce45cc8e1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007675175s
Apr 22 21:01:20.838: INFO: Pod "var-expansion-8003d25f-8237-4bde-bbba-7cce45cc8e1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013257865s
STEP: Saw pod success
Apr 22 21:01:20.838: INFO: Pod "var-expansion-8003d25f-8237-4bde-bbba-7cce45cc8e1f" satisfied condition "Succeeded or Failed"
Apr 22 21:01:20.840: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod var-expansion-8003d25f-8237-4bde-bbba-7cce45cc8e1f container dapi-container: <nil>
STEP: delete the pod
Apr 22 21:01:20.854: INFO: Waiting for pod var-expansion-8003d25f-8237-4bde-bbba-7cce45cc8e1f to disappear
Apr 22 21:01:20.856: INFO: Pod var-expansion-8003d25f-8237-4bde-bbba-7cce45cc8e1f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:01:20.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8653" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":80,"skipped":1432,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:01:20.863: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Apr 22 21:01:20.896: INFO: Waiting up to 5m0s for pod "pod-a7174df2-30a9-4e9c-80a3-cb10e143c922" in namespace "emptydir-9423" to be "Succeeded or Failed"
Apr 22 21:01:20.900: INFO: Pod "pod-a7174df2-30a9-4e9c-80a3-cb10e143c922": Phase="Pending", Reason="", readiness=false. Elapsed: 4.219795ms
Apr 22 21:01:22.906: INFO: Pod "pod-a7174df2-30a9-4e9c-80a3-cb10e143c922": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010756259s
STEP: Saw pod success
Apr 22 21:01:22.906: INFO: Pod "pod-a7174df2-30a9-4e9c-80a3-cb10e143c922" satisfied condition "Succeeded or Failed"
Apr 22 21:01:22.908: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-a7174df2-30a9-4e9c-80a3-cb10e143c922 container test-container: <nil>
STEP: delete the pod
Apr 22 21:01:22.922: INFO: Waiting for pod pod-a7174df2-30a9-4e9c-80a3-cb10e143c922 to disappear
Apr 22 21:01:22.924: INFO: Pod pod-a7174df2-30a9-4e9c-80a3-cb10e143c922 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:01:22.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9423" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":81,"skipped":1459,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:01:22.931: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-e0064ab1-fe47-4a76-8dcc-da89b9fcedf4
STEP: Creating a pod to test consume secrets
Apr 22 21:01:22.967: INFO: Waiting up to 5m0s for pod "pod-secrets-2ec85232-c01b-496f-8f4a-1eb9125316a8" in namespace "secrets-5133" to be "Succeeded or Failed"
Apr 22 21:01:22.969: INFO: Pod "pod-secrets-2ec85232-c01b-496f-8f4a-1eb9125316a8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.99476ms
Apr 22 21:01:24.976: INFO: Pod "pod-secrets-2ec85232-c01b-496f-8f4a-1eb9125316a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008374378s
Apr 22 21:01:26.979: INFO: Pod "pod-secrets-2ec85232-c01b-496f-8f4a-1eb9125316a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011784794s
STEP: Saw pod success
Apr 22 21:01:26.979: INFO: Pod "pod-secrets-2ec85232-c01b-496f-8f4a-1eb9125316a8" satisfied condition "Succeeded or Failed"
Apr 22 21:01:26.981: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-secrets-2ec85232-c01b-496f-8f4a-1eb9125316a8 container secret-env-test: <nil>
STEP: delete the pod
Apr 22 21:01:26.998: INFO: Waiting for pod pod-secrets-2ec85232-c01b-496f-8f4a-1eb9125316a8 to disappear
Apr 22 21:01:27.000: INFO: Pod pod-secrets-2ec85232-c01b-496f-8f4a-1eb9125316a8 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:01:27.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5133" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":82,"skipped":1469,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:01:27.007: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2011
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2011
STEP: creating replication controller externalsvc in namespace services-2011
I0422 21:01:27.057924      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-2011, replica count: 2
I0422 21:01:30.108184      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Apr 22 21:01:30.132: INFO: Creating new exec pod
Apr 22 21:01:32.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-2011 exec execpodmv4zv -- /bin/sh -x -c nslookup nodeport-service.services-2011.svc.cluster.local'
Apr 22 21:01:32.327: INFO: stderr: "+ nslookup nodeport-service.services-2011.svc.cluster.local\n"
Apr 22 21:01:32.327: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nnodeport-service.services-2011.svc.cluster.local\tcanonical name = externalsvc.services-2011.svc.cluster.local.\nName:\texternalsvc.services-2011.svc.cluster.local\nAddress: 10.0.19.82\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2011, will wait for the garbage collector to delete the pods
Apr 22 21:01:32.387: INFO: Deleting ReplicationController externalsvc took: 6.851319ms
Apr 22 21:01:33.087: INFO: Terminating ReplicationController externalsvc pods took: 700.134668ms
Apr 22 21:01:37.206: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:01:37.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2011" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:10.220 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":83,"skipped":1472,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:01:37.228: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:01:37.254: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Creating first CR 
Apr 22 21:01:37.802: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-22T21:01:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-22T21:01:37Z]] name:name1 resourceVersion:10424 uid:75ff4ccc-2e79-4991-b059-78714c4baa9a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Apr 22 21:01:47.815: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-22T21:01:47Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-22T21:01:47Z]] name:name2 resourceVersion:10475 uid:7e05cc8c-559b-4048-b237-bea5e2a29daa] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Apr 22 21:01:57.829: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-22T21:01:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-22T21:01:57Z]] name:name1 resourceVersion:10496 uid:75ff4ccc-2e79-4991-b059-78714c4baa9a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Apr 22 21:02:07.843: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-22T21:01:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-22T21:02:07Z]] name:name2 resourceVersion:10517 uid:7e05cc8c-559b-4048-b237-bea5e2a29daa] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Apr 22 21:02:17.858: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-22T21:01:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-22T21:01:57Z]] name:name1 resourceVersion:10538 uid:75ff4ccc-2e79-4991-b059-78714c4baa9a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Apr 22 21:02:27.874: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-22T21:01:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-22T21:02:07Z]] name:name2 resourceVersion:10559 uid:7e05cc8c-559b-4048-b237-bea5e2a29daa] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:02:38.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6451" for this suite.

• [SLOW TEST:61.174 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":84,"skipped":1473,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:02:38.402: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:02:38.438: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8617
I0422 21:02:38.449935      24 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8617, replica count: 1
I0422 21:02:39.500165      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0422 21:02:40.500295      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 22 21:02:40.610: INFO: Created: latency-svc-6ftm7
Apr 22 21:02:40.618: INFO: Got endpoints: latency-svc-6ftm7 [17.670985ms]
Apr 22 21:02:40.630: INFO: Created: latency-svc-5558m
Apr 22 21:02:40.634: INFO: Got endpoints: latency-svc-5558m [15.835195ms]
Apr 22 21:02:40.636: INFO: Created: latency-svc-5l456
Apr 22 21:02:40.640: INFO: Got endpoints: latency-svc-5l456 [22.029678ms]
Apr 22 21:02:40.642: INFO: Created: latency-svc-fpn58
Apr 22 21:02:40.646: INFO: Got endpoints: latency-svc-fpn58 [28.402584ms]
Apr 22 21:02:40.649: INFO: Created: latency-svc-vqqhl
Apr 22 21:02:40.653: INFO: Got endpoints: latency-svc-vqqhl [35.07853ms]
Apr 22 21:02:40.655: INFO: Created: latency-svc-tvkm8
Apr 22 21:02:40.660: INFO: Got endpoints: latency-svc-tvkm8 [41.912438ms]
Apr 22 21:02:40.662: INFO: Created: latency-svc-768s4
Apr 22 21:02:40.665: INFO: Got endpoints: latency-svc-768s4 [47.085402ms]
Apr 22 21:02:40.668: INFO: Created: latency-svc-6rxvn
Apr 22 21:02:40.673: INFO: Got endpoints: latency-svc-6rxvn [54.21208ms]
Apr 22 21:02:40.674: INFO: Created: latency-svc-j5jhs
Apr 22 21:02:40.679: INFO: Got endpoints: latency-svc-j5jhs [60.484095ms]
Apr 22 21:02:40.681: INFO: Created: latency-svc-xdbsg
Apr 22 21:02:40.686: INFO: Got endpoints: latency-svc-xdbsg [67.863497ms]
Apr 22 21:02:40.688: INFO: Created: latency-svc-q922r
Apr 22 21:02:40.694: INFO: Got endpoints: latency-svc-q922r [75.389213ms]
Apr 22 21:02:40.694: INFO: Created: latency-svc-8v768
Apr 22 21:02:40.698: INFO: Got endpoints: latency-svc-8v768 [79.615952ms]
Apr 22 21:02:40.700: INFO: Created: latency-svc-9dbrt
Apr 22 21:02:40.703: INFO: Got endpoints: latency-svc-9dbrt [84.33629ms]
Apr 22 21:02:40.706: INFO: Created: latency-svc-9blhc
Apr 22 21:02:40.713: INFO: Got endpoints: latency-svc-9blhc [94.615769ms]
Apr 22 21:02:40.718: INFO: Created: latency-svc-ksk2t
Apr 22 21:02:40.718: INFO: Got endpoints: latency-svc-ksk2t [99.532992ms]
Apr 22 21:02:40.723: INFO: Created: latency-svc-2b59z
Apr 22 21:02:40.727: INFO: Got endpoints: latency-svc-2b59z [108.298786ms]
Apr 22 21:02:40.728: INFO: Created: latency-svc-r9c2x
Apr 22 21:02:40.732: INFO: Got endpoints: latency-svc-r9c2x [98.063807ms]
Apr 22 21:02:40.735: INFO: Created: latency-svc-25x7q
Apr 22 21:02:40.739: INFO: Got endpoints: latency-svc-25x7q [98.461455ms]
Apr 22 21:02:40.740: INFO: Created: latency-svc-brq84
Apr 22 21:02:40.744: INFO: Got endpoints: latency-svc-brq84 [97.34451ms]
Apr 22 21:02:40.745: INFO: Created: latency-svc-kw6fw
Apr 22 21:02:40.749: INFO: Got endpoints: latency-svc-kw6fw [95.228606ms]
Apr 22 21:02:40.751: INFO: Created: latency-svc-bfzhl
Apr 22 21:02:40.755: INFO: Got endpoints: latency-svc-bfzhl [94.633533ms]
Apr 22 21:02:40.757: INFO: Created: latency-svc-kt2d9
Apr 22 21:02:40.760: INFO: Got endpoints: latency-svc-kt2d9 [95.031461ms]
Apr 22 21:02:40.762: INFO: Created: latency-svc-zcgxf
Apr 22 21:02:40.766: INFO: Got endpoints: latency-svc-zcgxf [93.205413ms]
Apr 22 21:02:40.768: INFO: Created: latency-svc-f8l62
Apr 22 21:02:40.772: INFO: Got endpoints: latency-svc-f8l62 [93.12001ms]
Apr 22 21:02:40.775: INFO: Created: latency-svc-cv2lk
Apr 22 21:02:40.779: INFO: Got endpoints: latency-svc-cv2lk [92.723326ms]
Apr 22 21:02:40.782: INFO: Created: latency-svc-2rd4q
Apr 22 21:02:40.785: INFO: Got endpoints: latency-svc-2rd4q [90.930492ms]
Apr 22 21:02:40.787: INFO: Created: latency-svc-mvdrl
Apr 22 21:02:40.791: INFO: Got endpoints: latency-svc-mvdrl [92.708364ms]
Apr 22 21:02:40.793: INFO: Created: latency-svc-wsrgx
Apr 22 21:02:40.798: INFO: Got endpoints: latency-svc-wsrgx [94.543509ms]
Apr 22 21:02:40.799: INFO: Created: latency-svc-6x6x5
Apr 22 21:02:40.805: INFO: Got endpoints: latency-svc-6x6x5 [91.172731ms]
Apr 22 21:02:40.807: INFO: Created: latency-svc-cxxxn
Apr 22 21:02:40.812: INFO: Got endpoints: latency-svc-cxxxn [93.139411ms]
Apr 22 21:02:40.814: INFO: Created: latency-svc-wgjvv
Apr 22 21:02:40.818: INFO: Got endpoints: latency-svc-wgjvv [90.755558ms]
Apr 22 21:02:40.819: INFO: Created: latency-svc-6hkq5
Apr 22 21:02:40.824: INFO: Got endpoints: latency-svc-6hkq5 [91.976398ms]
Apr 22 21:02:40.826: INFO: Created: latency-svc-p2lh9
Apr 22 21:02:40.829: INFO: Got endpoints: latency-svc-p2lh9 [90.725779ms]
Apr 22 21:02:40.831: INFO: Created: latency-svc-wmdp8
Apr 22 21:02:40.836: INFO: Got endpoints: latency-svc-wmdp8 [92.295455ms]
Apr 22 21:02:40.837: INFO: Created: latency-svc-xxbvm
Apr 22 21:02:40.841: INFO: Created: latency-svc-mpx44
Apr 22 21:02:40.846: INFO: Created: latency-svc-9lpcd
Apr 22 21:02:40.850: INFO: Created: latency-svc-bzlpc
Apr 22 21:02:40.856: INFO: Created: latency-svc-zt8qz
Apr 22 21:02:40.860: INFO: Created: latency-svc-r9z7h
Apr 22 21:02:40.864: INFO: Got endpoints: latency-svc-xxbvm [115.86836ms]
Apr 22 21:02:40.866: INFO: Created: latency-svc-jhz58
Apr 22 21:02:40.870: INFO: Created: latency-svc-jxmx4
Apr 22 21:02:40.874: INFO: Created: latency-svc-sz2pj
Apr 22 21:02:40.878: INFO: Created: latency-svc-zqq5s
Apr 22 21:02:40.882: INFO: Created: latency-svc-tbfth
Apr 22 21:02:40.886: INFO: Created: latency-svc-lqrdb
Apr 22 21:02:40.891: INFO: Created: latency-svc-xnvcx
Apr 22 21:02:40.895: INFO: Created: latency-svc-9mg5m
Apr 22 21:02:40.898: INFO: Created: latency-svc-7jc7b
Apr 22 21:02:40.903: INFO: Created: latency-svc-97p6x
Apr 22 21:02:40.914: INFO: Got endpoints: latency-svc-mpx44 [159.854193ms]
Apr 22 21:02:40.921: INFO: Created: latency-svc-r59x8
Apr 22 21:02:40.964: INFO: Got endpoints: latency-svc-9lpcd [203.97034ms]
Apr 22 21:02:40.972: INFO: Created: latency-svc-dxdp4
Apr 22 21:02:41.015: INFO: Got endpoints: latency-svc-bzlpc [249.39434ms]
Apr 22 21:02:41.023: INFO: Created: latency-svc-jng77
Apr 22 21:02:41.065: INFO: Got endpoints: latency-svc-zt8qz [292.912692ms]
Apr 22 21:02:41.073: INFO: Created: latency-svc-fxp7r
Apr 22 21:02:41.114: INFO: Got endpoints: latency-svc-r9z7h [335.137465ms]
Apr 22 21:02:41.122: INFO: Created: latency-svc-l4k69
Apr 22 21:02:41.164: INFO: Got endpoints: latency-svc-jhz58 [379.307769ms]
Apr 22 21:02:41.173: INFO: Created: latency-svc-fbg5c
Apr 22 21:02:41.215: INFO: Got endpoints: latency-svc-jxmx4 [424.354213ms]
Apr 22 21:02:41.223: INFO: Created: latency-svc-xqmtn
Apr 22 21:02:41.264: INFO: Got endpoints: latency-svc-sz2pj [466.665322ms]
Apr 22 21:02:41.272: INFO: Created: latency-svc-gwh6l
Apr 22 21:02:41.315: INFO: Got endpoints: latency-svc-zqq5s [510.12715ms]
Apr 22 21:02:41.322: INFO: Created: latency-svc-77xhd
Apr 22 21:02:41.366: INFO: Got endpoints: latency-svc-tbfth [554.033869ms]
Apr 22 21:02:41.374: INFO: Created: latency-svc-v54bz
Apr 22 21:02:41.415: INFO: Got endpoints: latency-svc-lqrdb [596.630268ms]
Apr 22 21:02:41.422: INFO: Created: latency-svc-bzgh9
Apr 22 21:02:41.466: INFO: Got endpoints: latency-svc-xnvcx [642.007654ms]
Apr 22 21:02:41.473: INFO: Created: latency-svc-mgrkk
Apr 22 21:02:41.521: INFO: Got endpoints: latency-svc-9mg5m [691.782803ms]
Apr 22 21:02:41.529: INFO: Created: latency-svc-cn9lk
Apr 22 21:02:41.565: INFO: Got endpoints: latency-svc-7jc7b [729.505366ms]
Apr 22 21:02:41.573: INFO: Created: latency-svc-cn8ht
Apr 22 21:02:41.615: INFO: Got endpoints: latency-svc-97p6x [750.449568ms]
Apr 22 21:02:41.623: INFO: Created: latency-svc-x5xzb
Apr 22 21:02:41.665: INFO: Got endpoints: latency-svc-r59x8 [750.647204ms]
Apr 22 21:02:41.675: INFO: Created: latency-svc-8zffk
Apr 22 21:02:41.715: INFO: Got endpoints: latency-svc-dxdp4 [750.381623ms]
Apr 22 21:02:41.723: INFO: Created: latency-svc-n4nc5
Apr 22 21:02:41.765: INFO: Got endpoints: latency-svc-jng77 [750.092407ms]
Apr 22 21:02:41.772: INFO: Created: latency-svc-bmhkk
Apr 22 21:02:41.821: INFO: Got endpoints: latency-svc-fxp7r [756.433738ms]
Apr 22 21:02:41.829: INFO: Created: latency-svc-whn4l
Apr 22 21:02:41.866: INFO: Got endpoints: latency-svc-l4k69 [751.251369ms]
Apr 22 21:02:41.873: INFO: Created: latency-svc-f6l2n
Apr 22 21:02:41.915: INFO: Got endpoints: latency-svc-fbg5c [750.368466ms]
Apr 22 21:02:41.922: INFO: Created: latency-svc-4xgdk
Apr 22 21:02:41.965: INFO: Got endpoints: latency-svc-xqmtn [749.65933ms]
Apr 22 21:02:41.973: INFO: Created: latency-svc-6tb7v
Apr 22 21:02:42.015: INFO: Got endpoints: latency-svc-gwh6l [750.611417ms]
Apr 22 21:02:42.023: INFO: Created: latency-svc-cgl4l
Apr 22 21:02:42.065: INFO: Got endpoints: latency-svc-77xhd [749.922337ms]
Apr 22 21:02:42.072: INFO: Created: latency-svc-5snxb
Apr 22 21:02:42.115: INFO: Got endpoints: latency-svc-v54bz [749.351217ms]
Apr 22 21:02:42.123: INFO: Created: latency-svc-756vj
Apr 22 21:02:42.165: INFO: Got endpoints: latency-svc-bzgh9 [750.177813ms]
Apr 22 21:02:42.180: INFO: Created: latency-svc-5nkfm
Apr 22 21:02:42.215: INFO: Got endpoints: latency-svc-mgrkk [749.059509ms]
Apr 22 21:02:42.223: INFO: Created: latency-svc-9xzj9
Apr 22 21:02:42.265: INFO: Got endpoints: latency-svc-cn9lk [744.116827ms]
Apr 22 21:02:42.273: INFO: Created: latency-svc-xz2gm
Apr 22 21:02:42.315: INFO: Got endpoints: latency-svc-cn8ht [750.083626ms]
Apr 22 21:02:42.324: INFO: Created: latency-svc-xbqk5
Apr 22 21:02:42.365: INFO: Got endpoints: latency-svc-x5xzb [749.74912ms]
Apr 22 21:02:42.373: INFO: Created: latency-svc-tclgh
Apr 22 21:02:42.415: INFO: Got endpoints: latency-svc-8zffk [749.905441ms]
Apr 22 21:02:42.425: INFO: Created: latency-svc-bjmdr
Apr 22 21:02:42.465: INFO: Got endpoints: latency-svc-n4nc5 [750.347987ms]
Apr 22 21:02:42.473: INFO: Created: latency-svc-7lgzm
Apr 22 21:02:42.515: INFO: Got endpoints: latency-svc-bmhkk [749.215032ms]
Apr 22 21:02:42.522: INFO: Created: latency-svc-qkdpw
Apr 22 21:02:42.565: INFO: Got endpoints: latency-svc-whn4l [743.778232ms]
Apr 22 21:02:42.573: INFO: Created: latency-svc-njqbh
Apr 22 21:02:42.616: INFO: Got endpoints: latency-svc-f6l2n [750.293361ms]
Apr 22 21:02:42.626: INFO: Created: latency-svc-vdqqn
Apr 22 21:02:42.665: INFO: Got endpoints: latency-svc-4xgdk [750.44588ms]
Apr 22 21:02:42.676: INFO: Created: latency-svc-7984j
Apr 22 21:02:42.715: INFO: Got endpoints: latency-svc-6tb7v [749.948678ms]
Apr 22 21:02:42.723: INFO: Created: latency-svc-2xq8b
Apr 22 21:02:42.765: INFO: Got endpoints: latency-svc-cgl4l [749.900168ms]
Apr 22 21:02:42.773: INFO: Created: latency-svc-bxsxf
Apr 22 21:02:42.814: INFO: Got endpoints: latency-svc-5snxb [749.677225ms]
Apr 22 21:02:42.822: INFO: Created: latency-svc-km2tl
Apr 22 21:02:42.865: INFO: Got endpoints: latency-svc-756vj [749.963309ms]
Apr 22 21:02:42.872: INFO: Created: latency-svc-z5w8d
Apr 22 21:02:42.915: INFO: Got endpoints: latency-svc-5nkfm [750.463493ms]
Apr 22 21:02:42.924: INFO: Created: latency-svc-54gfb
Apr 22 21:02:42.965: INFO: Got endpoints: latency-svc-9xzj9 [750.321342ms]
Apr 22 21:02:42.973: INFO: Created: latency-svc-vjk7t
Apr 22 21:02:43.015: INFO: Got endpoints: latency-svc-xz2gm [749.9377ms]
Apr 22 21:02:43.023: INFO: Created: latency-svc-ltwjb
Apr 22 21:02:43.064: INFO: Got endpoints: latency-svc-xbqk5 [748.813309ms]
Apr 22 21:02:43.072: INFO: Created: latency-svc-r8dmq
Apr 22 21:02:43.116: INFO: Got endpoints: latency-svc-tclgh [751.041799ms]
Apr 22 21:02:43.123: INFO: Created: latency-svc-c5q69
Apr 22 21:02:43.164: INFO: Got endpoints: latency-svc-bjmdr [749.256609ms]
Apr 22 21:02:43.174: INFO: Created: latency-svc-8pppc
Apr 22 21:02:43.215: INFO: Got endpoints: latency-svc-7lgzm [750.002528ms]
Apr 22 21:02:43.222: INFO: Created: latency-svc-rbbcf
Apr 22 21:02:43.265: INFO: Got endpoints: latency-svc-qkdpw [750.508138ms]
Apr 22 21:02:43.272: INFO: Created: latency-svc-dnpmx
Apr 22 21:02:43.314: INFO: Got endpoints: latency-svc-njqbh [748.889641ms]
Apr 22 21:02:43.322: INFO: Created: latency-svc-bjdfz
Apr 22 21:02:43.367: INFO: Got endpoints: latency-svc-vdqqn [750.812245ms]
Apr 22 21:02:43.374: INFO: Created: latency-svc-f97dt
Apr 22 21:02:43.416: INFO: Got endpoints: latency-svc-7984j [750.944873ms]
Apr 22 21:02:43.424: INFO: Created: latency-svc-q5smd
Apr 22 21:02:43.470: INFO: Got endpoints: latency-svc-2xq8b [754.814517ms]
Apr 22 21:02:43.478: INFO: Created: latency-svc-pd2f2
Apr 22 21:02:43.515: INFO: Got endpoints: latency-svc-bxsxf [749.789629ms]
Apr 22 21:02:43.522: INFO: Created: latency-svc-kjx8f
Apr 22 21:02:43.564: INFO: Got endpoints: latency-svc-km2tl [749.913671ms]
Apr 22 21:02:43.572: INFO: Created: latency-svc-7wssr
Apr 22 21:02:43.615: INFO: Got endpoints: latency-svc-z5w8d [749.780623ms]
Apr 22 21:02:43.623: INFO: Created: latency-svc-gbh95
Apr 22 21:02:43.665: INFO: Got endpoints: latency-svc-54gfb [749.626557ms]
Apr 22 21:02:43.674: INFO: Created: latency-svc-8l8fx
Apr 22 21:02:43.715: INFO: Got endpoints: latency-svc-vjk7t [749.636536ms]
Apr 22 21:02:43.722: INFO: Created: latency-svc-q4bnl
Apr 22 21:02:43.764: INFO: Got endpoints: latency-svc-ltwjb [748.805336ms]
Apr 22 21:02:43.771: INFO: Created: latency-svc-qvrnp
Apr 22 21:02:43.815: INFO: Got endpoints: latency-svc-r8dmq [750.682385ms]
Apr 22 21:02:43.822: INFO: Created: latency-svc-rzw7x
Apr 22 21:02:43.865: INFO: Got endpoints: latency-svc-c5q69 [749.394818ms]
Apr 22 21:02:43.874: INFO: Created: latency-svc-q5ggr
Apr 22 21:02:43.915: INFO: Got endpoints: latency-svc-8pppc [750.206904ms]
Apr 22 21:02:43.922: INFO: Created: latency-svc-q2fq2
Apr 22 21:02:43.965: INFO: Got endpoints: latency-svc-rbbcf [749.825181ms]
Apr 22 21:02:43.972: INFO: Created: latency-svc-xjp59
Apr 22 21:02:44.015: INFO: Got endpoints: latency-svc-dnpmx [750.142419ms]
Apr 22 21:02:44.023: INFO: Created: latency-svc-hf8nv
Apr 22 21:02:44.065: INFO: Got endpoints: latency-svc-bjdfz [750.634856ms]
Apr 22 21:02:44.073: INFO: Created: latency-svc-ndsgs
Apr 22 21:02:44.116: INFO: Got endpoints: latency-svc-f97dt [749.014789ms]
Apr 22 21:02:44.124: INFO: Created: latency-svc-mhkh2
Apr 22 21:02:44.165: INFO: Got endpoints: latency-svc-q5smd [748.948105ms]
Apr 22 21:02:44.178: INFO: Created: latency-svc-hpppd
Apr 22 21:02:44.214: INFO: Got endpoints: latency-svc-pd2f2 [744.330345ms]
Apr 22 21:02:44.222: INFO: Created: latency-svc-zj622
Apr 22 21:02:44.265: INFO: Got endpoints: latency-svc-kjx8f [750.462096ms]
Apr 22 21:02:44.273: INFO: Created: latency-svc-nrx8t
Apr 22 21:02:44.317: INFO: Got endpoints: latency-svc-7wssr [752.56582ms]
Apr 22 21:02:44.328: INFO: Created: latency-svc-xsxgj
Apr 22 21:02:44.364: INFO: Got endpoints: latency-svc-gbh95 [749.144073ms]
Apr 22 21:02:44.371: INFO: Created: latency-svc-zdgnq
Apr 22 21:02:44.415: INFO: Got endpoints: latency-svc-8l8fx [749.816132ms]
Apr 22 21:02:44.421: INFO: Created: latency-svc-bfcx2
Apr 22 21:02:44.465: INFO: Got endpoints: latency-svc-q4bnl [749.981612ms]
Apr 22 21:02:44.472: INFO: Created: latency-svc-v9q27
Apr 22 21:02:44.514: INFO: Got endpoints: latency-svc-qvrnp [750.221459ms]
Apr 22 21:02:44.522: INFO: Created: latency-svc-qm7rh
Apr 22 21:02:44.566: INFO: Got endpoints: latency-svc-rzw7x [750.655628ms]
Apr 22 21:02:44.573: INFO: Created: latency-svc-p5mr9
Apr 22 21:02:44.615: INFO: Got endpoints: latency-svc-q5ggr [749.925666ms]
Apr 22 21:02:44.623: INFO: Created: latency-svc-qgnkh
Apr 22 21:02:44.664: INFO: Got endpoints: latency-svc-q2fq2 [749.416868ms]
Apr 22 21:02:44.671: INFO: Created: latency-svc-8l9c5
Apr 22 21:02:44.714: INFO: Got endpoints: latency-svc-xjp59 [748.82049ms]
Apr 22 21:02:44.721: INFO: Created: latency-svc-kxnqx
Apr 22 21:02:44.766: INFO: Got endpoints: latency-svc-hf8nv [750.506122ms]
Apr 22 21:02:44.778: INFO: Created: latency-svc-b8wvl
Apr 22 21:02:44.815: INFO: Got endpoints: latency-svc-ndsgs [749.91251ms]
Apr 22 21:02:44.822: INFO: Created: latency-svc-7m4sn
Apr 22 21:02:44.864: INFO: Got endpoints: latency-svc-mhkh2 [748.677105ms]
Apr 22 21:02:44.871: INFO: Created: latency-svc-dm2nj
Apr 22 21:02:44.916: INFO: Got endpoints: latency-svc-hpppd [750.634838ms]
Apr 22 21:02:44.923: INFO: Created: latency-svc-vqpzn
Apr 22 21:02:44.965: INFO: Got endpoints: latency-svc-zj622 [750.450748ms]
Apr 22 21:02:44.972: INFO: Created: latency-svc-qlmvj
Apr 22 21:02:45.015: INFO: Got endpoints: latency-svc-nrx8t [749.285067ms]
Apr 22 21:02:45.022: INFO: Created: latency-svc-6f775
Apr 22 21:02:45.066: INFO: Got endpoints: latency-svc-xsxgj [748.804408ms]
Apr 22 21:02:45.073: INFO: Created: latency-svc-jm55k
Apr 22 21:02:45.114: INFO: Got endpoints: latency-svc-zdgnq [750.333103ms]
Apr 22 21:02:45.122: INFO: Created: latency-svc-w4khm
Apr 22 21:02:45.164: INFO: Got endpoints: latency-svc-bfcx2 [749.617287ms]
Apr 22 21:02:45.174: INFO: Created: latency-svc-9b754
Apr 22 21:02:45.215: INFO: Got endpoints: latency-svc-v9q27 [750.256137ms]
Apr 22 21:02:45.224: INFO: Created: latency-svc-lplzr
Apr 22 21:02:45.265: INFO: Got endpoints: latency-svc-qm7rh [750.169463ms]
Apr 22 21:02:45.272: INFO: Created: latency-svc-bjqq8
Apr 22 21:02:45.314: INFO: Got endpoints: latency-svc-p5mr9 [748.412885ms]
Apr 22 21:02:45.322: INFO: Created: latency-svc-8xdgw
Apr 22 21:02:45.365: INFO: Got endpoints: latency-svc-qgnkh [749.6402ms]
Apr 22 21:02:45.372: INFO: Created: latency-svc-4dk5h
Apr 22 21:02:45.414: INFO: Got endpoints: latency-svc-8l9c5 [749.863727ms]
Apr 22 21:02:45.421: INFO: Created: latency-svc-24mrd
Apr 22 21:02:45.466: INFO: Got endpoints: latency-svc-kxnqx [751.587698ms]
Apr 22 21:02:45.473: INFO: Created: latency-svc-9dsq7
Apr 22 21:02:45.514: INFO: Got endpoints: latency-svc-b8wvl [748.191335ms]
Apr 22 21:02:45.522: INFO: Created: latency-svc-f27wr
Apr 22 21:02:45.565: INFO: Got endpoints: latency-svc-7m4sn [749.90885ms]
Apr 22 21:02:45.572: INFO: Created: latency-svc-4pvh4
Apr 22 21:02:45.615: INFO: Got endpoints: latency-svc-dm2nj [750.407195ms]
Apr 22 21:02:45.622: INFO: Created: latency-svc-vpjpv
Apr 22 21:02:45.664: INFO: Got endpoints: latency-svc-vqpzn [748.700467ms]
Apr 22 21:02:45.672: INFO: Created: latency-svc-t7zr4
Apr 22 21:02:45.715: INFO: Got endpoints: latency-svc-qlmvj [750.126917ms]
Apr 22 21:02:45.722: INFO: Created: latency-svc-ch6dk
Apr 22 21:02:45.765: INFO: Got endpoints: latency-svc-6f775 [750.347316ms]
Apr 22 21:02:45.772: INFO: Created: latency-svc-pwhjl
Apr 22 21:02:45.814: INFO: Got endpoints: latency-svc-jm55k [748.570938ms]
Apr 22 21:02:45.821: INFO: Created: latency-svc-jhprv
Apr 22 21:02:45.864: INFO: Got endpoints: latency-svc-w4khm [750.156722ms]
Apr 22 21:02:45.872: INFO: Created: latency-svc-p6crw
Apr 22 21:02:45.915: INFO: Got endpoints: latency-svc-9b754 [750.545345ms]
Apr 22 21:02:45.922: INFO: Created: latency-svc-bgpvg
Apr 22 21:02:45.965: INFO: Got endpoints: latency-svc-lplzr [749.610637ms]
Apr 22 21:02:45.972: INFO: Created: latency-svc-sptrx
Apr 22 21:02:46.014: INFO: Got endpoints: latency-svc-bjqq8 [749.873472ms]
Apr 22 21:02:46.022: INFO: Created: latency-svc-2rkcm
Apr 22 21:02:46.065: INFO: Got endpoints: latency-svc-8xdgw [750.814101ms]
Apr 22 21:02:46.072: INFO: Created: latency-svc-zfm56
Apr 22 21:02:46.115: INFO: Got endpoints: latency-svc-4dk5h [749.784317ms]
Apr 22 21:02:46.122: INFO: Created: latency-svc-98vkn
Apr 22 21:02:46.165: INFO: Got endpoints: latency-svc-24mrd [750.60115ms]
Apr 22 21:02:46.174: INFO: Created: latency-svc-fvns2
Apr 22 21:02:46.215: INFO: Got endpoints: latency-svc-9dsq7 [749.085841ms]
Apr 22 21:02:46.221: INFO: Created: latency-svc-w65tt
Apr 22 21:02:46.265: INFO: Got endpoints: latency-svc-f27wr [750.669102ms]
Apr 22 21:02:46.272: INFO: Created: latency-svc-wh5hn
Apr 22 21:02:46.314: INFO: Got endpoints: latency-svc-4pvh4 [749.582719ms]
Apr 22 21:02:46.321: INFO: Created: latency-svc-8p76z
Apr 22 21:02:46.365: INFO: Got endpoints: latency-svc-vpjpv [750.274367ms]
Apr 22 21:02:46.372: INFO: Created: latency-svc-vrdzm
Apr 22 21:02:46.416: INFO: Got endpoints: latency-svc-t7zr4 [751.031026ms]
Apr 22 21:02:46.423: INFO: Created: latency-svc-j9m8z
Apr 22 21:02:46.464: INFO: Got endpoints: latency-svc-ch6dk [749.492853ms]
Apr 22 21:02:46.472: INFO: Created: latency-svc-tntb9
Apr 22 21:02:46.517: INFO: Got endpoints: latency-svc-pwhjl [752.114708ms]
Apr 22 21:02:46.536: INFO: Created: latency-svc-dr2q9
Apr 22 21:02:46.565: INFO: Got endpoints: latency-svc-jhprv [750.880492ms]
Apr 22 21:02:46.572: INFO: Created: latency-svc-djgzz
Apr 22 21:02:46.614: INFO: Got endpoints: latency-svc-p6crw [749.429715ms]
Apr 22 21:02:46.623: INFO: Created: latency-svc-8tv6v
Apr 22 21:02:46.666: INFO: Got endpoints: latency-svc-bgpvg [750.617481ms]
Apr 22 21:02:46.673: INFO: Created: latency-svc-kl882
Apr 22 21:02:46.715: INFO: Got endpoints: latency-svc-sptrx [749.958097ms]
Apr 22 21:02:46.724: INFO: Created: latency-svc-t9cfn
Apr 22 21:02:46.764: INFO: Got endpoints: latency-svc-2rkcm [749.952122ms]
Apr 22 21:02:46.771: INFO: Created: latency-svc-kgn8x
Apr 22 21:02:46.815: INFO: Got endpoints: latency-svc-zfm56 [750.312707ms]
Apr 22 21:02:46.823: INFO: Created: latency-svc-42v5s
Apr 22 21:02:46.865: INFO: Got endpoints: latency-svc-98vkn [750.583149ms]
Apr 22 21:02:46.874: INFO: Created: latency-svc-54xkm
Apr 22 21:02:46.914: INFO: Got endpoints: latency-svc-fvns2 [749.620404ms]
Apr 22 21:02:46.921: INFO: Created: latency-svc-mxgfx
Apr 22 21:02:46.964: INFO: Got endpoints: latency-svc-w65tt [749.537305ms]
Apr 22 21:02:46.971: INFO: Created: latency-svc-qp6zk
Apr 22 21:02:47.015: INFO: Got endpoints: latency-svc-wh5hn [750.322648ms]
Apr 22 21:02:47.022: INFO: Created: latency-svc-qqrmm
Apr 22 21:02:47.064: INFO: Got endpoints: latency-svc-8p76z [750.103802ms]
Apr 22 21:02:47.072: INFO: Created: latency-svc-gg6d5
Apr 22 21:02:47.115: INFO: Got endpoints: latency-svc-vrdzm [750.033449ms]
Apr 22 21:02:47.123: INFO: Created: latency-svc-wzt48
Apr 22 21:02:47.165: INFO: Got endpoints: latency-svc-j9m8z [749.850943ms]
Apr 22 21:02:47.174: INFO: Created: latency-svc-qdxhz
Apr 22 21:02:47.215: INFO: Got endpoints: latency-svc-tntb9 [750.166245ms]
Apr 22 21:02:47.224: INFO: Created: latency-svc-tbs7z
Apr 22 21:02:47.264: INFO: Got endpoints: latency-svc-dr2q9 [747.0395ms]
Apr 22 21:02:47.271: INFO: Created: latency-svc-m4jfx
Apr 22 21:02:47.315: INFO: Got endpoints: latency-svc-djgzz [749.818921ms]
Apr 22 21:02:47.322: INFO: Created: latency-svc-dq7jc
Apr 22 21:02:47.364: INFO: Got endpoints: latency-svc-8tv6v [750.585563ms]
Apr 22 21:02:47.372: INFO: Created: latency-svc-s4dsv
Apr 22 21:02:47.416: INFO: Got endpoints: latency-svc-kl882 [750.622121ms]
Apr 22 21:02:47.430: INFO: Created: latency-svc-86g6d
Apr 22 21:02:47.465: INFO: Got endpoints: latency-svc-t9cfn [749.673309ms]
Apr 22 21:02:47.472: INFO: Created: latency-svc-c7zdf
Apr 22 21:02:47.515: INFO: Got endpoints: latency-svc-kgn8x [750.998643ms]
Apr 22 21:02:47.523: INFO: Created: latency-svc-96txz
Apr 22 21:02:47.565: INFO: Got endpoints: latency-svc-42v5s [749.531809ms]
Apr 22 21:02:47.572: INFO: Created: latency-svc-rswkt
Apr 22 21:02:47.614: INFO: Got endpoints: latency-svc-54xkm [749.073564ms]
Apr 22 21:02:47.622: INFO: Created: latency-svc-dqrhn
Apr 22 21:02:47.665: INFO: Got endpoints: latency-svc-mxgfx [750.82123ms]
Apr 22 21:02:47.673: INFO: Created: latency-svc-9hhqm
Apr 22 21:02:47.715: INFO: Got endpoints: latency-svc-qp6zk [750.650242ms]
Apr 22 21:02:47.723: INFO: Created: latency-svc-4zjlt
Apr 22 21:02:47.765: INFO: Got endpoints: latency-svc-qqrmm [749.789979ms]
Apr 22 21:02:47.773: INFO: Created: latency-svc-dhjr7
Apr 22 21:02:47.814: INFO: Got endpoints: latency-svc-gg6d5 [749.540188ms]
Apr 22 21:02:47.821: INFO: Created: latency-svc-xjb4g
Apr 22 21:02:47.865: INFO: Got endpoints: latency-svc-wzt48 [749.304071ms]
Apr 22 21:02:47.872: INFO: Created: latency-svc-9pfbg
Apr 22 21:02:47.915: INFO: Got endpoints: latency-svc-qdxhz [749.389588ms]
Apr 22 21:02:47.922: INFO: Created: latency-svc-msw4k
Apr 22 21:02:47.964: INFO: Got endpoints: latency-svc-tbs7z [749.63029ms]
Apr 22 21:02:47.972: INFO: Created: latency-svc-dsr85
Apr 22 21:02:48.015: INFO: Got endpoints: latency-svc-m4jfx [751.27897ms]
Apr 22 21:02:48.023: INFO: Created: latency-svc-dnbkf
Apr 22 21:02:48.065: INFO: Got endpoints: latency-svc-dq7jc [750.355016ms]
Apr 22 21:02:48.073: INFO: Created: latency-svc-kt4kc
Apr 22 21:02:48.116: INFO: Got endpoints: latency-svc-s4dsv [751.446211ms]
Apr 22 21:02:48.124: INFO: Created: latency-svc-slbtt
Apr 22 21:02:48.165: INFO: Got endpoints: latency-svc-86g6d [749.201578ms]
Apr 22 21:02:48.173: INFO: Created: latency-svc-7rtmz
Apr 22 21:02:48.215: INFO: Got endpoints: latency-svc-c7zdf [750.801256ms]
Apr 22 21:02:48.223: INFO: Created: latency-svc-xq69k
Apr 22 21:02:48.266: INFO: Got endpoints: latency-svc-96txz [750.227719ms]
Apr 22 21:02:48.274: INFO: Created: latency-svc-kmclp
Apr 22 21:02:48.316: INFO: Got endpoints: latency-svc-rswkt [750.620093ms]
Apr 22 21:02:48.323: INFO: Created: latency-svc-wbqr4
Apr 22 21:02:48.364: INFO: Got endpoints: latency-svc-dqrhn [750.098607ms]
Apr 22 21:02:48.372: INFO: Created: latency-svc-h9vsq
Apr 22 21:02:48.416: INFO: Got endpoints: latency-svc-9hhqm [750.616863ms]
Apr 22 21:02:48.424: INFO: Created: latency-svc-vsd6n
Apr 22 21:02:48.466: INFO: Got endpoints: latency-svc-4zjlt [750.790967ms]
Apr 22 21:02:48.515: INFO: Got endpoints: latency-svc-dhjr7 [750.084424ms]
Apr 22 21:02:48.565: INFO: Got endpoints: latency-svc-xjb4g [750.747618ms]
Apr 22 21:02:48.615: INFO: Got endpoints: latency-svc-9pfbg [750.056539ms]
Apr 22 21:02:48.665: INFO: Got endpoints: latency-svc-msw4k [750.340229ms]
Apr 22 21:02:48.715: INFO: Got endpoints: latency-svc-dsr85 [750.703596ms]
Apr 22 21:02:48.765: INFO: Got endpoints: latency-svc-dnbkf [749.546827ms]
Apr 22 21:02:48.815: INFO: Got endpoints: latency-svc-kt4kc [749.996598ms]
Apr 22 21:02:48.865: INFO: Got endpoints: latency-svc-slbtt [748.80382ms]
Apr 22 21:02:48.915: INFO: Got endpoints: latency-svc-7rtmz [749.082529ms]
Apr 22 21:02:48.966: INFO: Got endpoints: latency-svc-xq69k [750.850944ms]
Apr 22 21:02:49.015: INFO: Got endpoints: latency-svc-kmclp [749.342135ms]
Apr 22 21:02:49.065: INFO: Got endpoints: latency-svc-wbqr4 [749.71138ms]
Apr 22 21:02:49.115: INFO: Got endpoints: latency-svc-h9vsq [750.533633ms]
Apr 22 21:02:49.165: INFO: Got endpoints: latency-svc-vsd6n [749.397596ms]
Apr 22 21:02:49.165: INFO: Latencies: [15.835195ms 22.029678ms 28.402584ms 35.07853ms 41.912438ms 47.085402ms 54.21208ms 60.484095ms 67.863497ms 75.389213ms 79.615952ms 84.33629ms 90.725779ms 90.755558ms 90.930492ms 91.172731ms 91.976398ms 92.295455ms 92.708364ms 92.723326ms 93.12001ms 93.139411ms 93.205413ms 94.543509ms 94.615769ms 94.633533ms 95.031461ms 95.228606ms 97.34451ms 98.063807ms 98.461455ms 99.532992ms 108.298786ms 115.86836ms 159.854193ms 203.97034ms 249.39434ms 292.912692ms 335.137465ms 379.307769ms 424.354213ms 466.665322ms 510.12715ms 554.033869ms 596.630268ms 642.007654ms 691.782803ms 729.505366ms 743.778232ms 744.116827ms 744.330345ms 747.0395ms 748.191335ms 748.412885ms 748.570938ms 748.677105ms 748.700467ms 748.80382ms 748.804408ms 748.805336ms 748.813309ms 748.82049ms 748.889641ms 748.948105ms 749.014789ms 749.059509ms 749.073564ms 749.082529ms 749.085841ms 749.144073ms 749.201578ms 749.215032ms 749.256609ms 749.285067ms 749.304071ms 749.342135ms 749.351217ms 749.389588ms 749.394818ms 749.397596ms 749.416868ms 749.429715ms 749.492853ms 749.531809ms 749.537305ms 749.540188ms 749.546827ms 749.582719ms 749.610637ms 749.617287ms 749.620404ms 749.626557ms 749.63029ms 749.636536ms 749.6402ms 749.65933ms 749.673309ms 749.677225ms 749.71138ms 749.74912ms 749.780623ms 749.784317ms 749.789629ms 749.789979ms 749.816132ms 749.818921ms 749.825181ms 749.850943ms 749.863727ms 749.873472ms 749.900168ms 749.905441ms 749.90885ms 749.91251ms 749.913671ms 749.922337ms 749.925666ms 749.9377ms 749.948678ms 749.952122ms 749.958097ms 749.963309ms 749.981612ms 749.996598ms 750.002528ms 750.033449ms 750.056539ms 750.083626ms 750.084424ms 750.092407ms 750.098607ms 750.103802ms 750.126917ms 750.142419ms 750.156722ms 750.166245ms 750.169463ms 750.177813ms 750.206904ms 750.221459ms 750.227719ms 750.256137ms 750.274367ms 750.293361ms 750.312707ms 750.321342ms 750.322648ms 750.333103ms 750.340229ms 750.347316ms 750.347987ms 750.355016ms 750.368466ms 750.381623ms 750.407195ms 750.44588ms 750.449568ms 750.450748ms 750.462096ms 750.463493ms 750.506122ms 750.508138ms 750.533633ms 750.545345ms 750.583149ms 750.585563ms 750.60115ms 750.611417ms 750.616863ms 750.617481ms 750.620093ms 750.622121ms 750.634838ms 750.634856ms 750.647204ms 750.650242ms 750.655628ms 750.669102ms 750.682385ms 750.703596ms 750.747618ms 750.790967ms 750.801256ms 750.812245ms 750.814101ms 750.82123ms 750.850944ms 750.880492ms 750.944873ms 750.998643ms 751.031026ms 751.041799ms 751.251369ms 751.27897ms 751.446211ms 751.587698ms 752.114708ms 752.56582ms 754.814517ms 756.433738ms]
Apr 22 21:02:49.165: INFO: 50 %ile: 749.780623ms
Apr 22 21:02:49.165: INFO: 90 %ile: 750.747618ms
Apr 22 21:02:49.165: INFO: 99 %ile: 754.814517ms
Apr 22 21:02:49.165: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:02:49.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8617" for this suite.

• [SLOW TEST:10.777 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":85,"skipped":1485,"failed":0}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:02:49.179: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4095
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-4095
Apr 22 21:02:49.215: INFO: Found 0 stateful pods, waiting for 1
Apr 22 21:02:59.221: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 22 21:02:59.241: INFO: Deleting all statefulset in ns statefulset-4095
Apr 22 21:02:59.244: INFO: Scaling statefulset ss to 0
Apr 22 21:03:19.264: INFO: Waiting for statefulset status.replicas updated to 0
Apr 22 21:03:19.266: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:03:19.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4095" for this suite.

• [SLOW TEST:30.113 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":86,"skipped":1485,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:03:19.293: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-ad096f45-b884-4911-ba4d-61ffcb9fedcc in namespace container-probe-9134
Apr 22 21:03:21.331: INFO: Started pod liveness-ad096f45-b884-4911-ba4d-61ffcb9fedcc in namespace container-probe-9134
STEP: checking the pod's current state and verifying that restartCount is present
Apr 22 21:03:21.333: INFO: Initial restart count of pod liveness-ad096f45-b884-4911-ba4d-61ffcb9fedcc is 0
Apr 22 21:03:43.398: INFO: Restart count of pod container-probe-9134/liveness-ad096f45-b884-4911-ba4d-61ffcb9fedcc is now 1 (22.065472682s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:03:43.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9134" for this suite.

• [SLOW TEST:24.125 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":87,"skipped":1503,"failed":0}
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:03:43.418: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Apr 22 21:03:43.452: INFO: Waiting up to 5m0s for pod "client-containers-667ba362-16ea-42b3-b4a3-f28e4058f2a8" in namespace "containers-1717" to be "Succeeded or Failed"
Apr 22 21:03:43.454: INFO: Pod "client-containers-667ba362-16ea-42b3-b4a3-f28e4058f2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.912199ms
Apr 22 21:03:45.460: INFO: Pod "client-containers-667ba362-16ea-42b3-b4a3-f28e4058f2a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007755825s
STEP: Saw pod success
Apr 22 21:03:45.460: INFO: Pod "client-containers-667ba362-16ea-42b3-b4a3-f28e4058f2a8" satisfied condition "Succeeded or Failed"
Apr 22 21:03:45.462: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod client-containers-667ba362-16ea-42b3-b4a3-f28e4058f2a8 container agnhost-container: <nil>
STEP: delete the pod
Apr 22 21:03:45.485: INFO: Waiting for pod client-containers-667ba362-16ea-42b3-b4a3-f28e4058f2a8 to disappear
Apr 22 21:03:45.487: INFO: Pod client-containers-667ba362-16ea-42b3-b4a3-f28e4058f2a8 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:03:45.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1717" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":88,"skipped":1508,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:03:45.495: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Apr 22 21:03:45.521: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Apr 22 21:03:45.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2653 create -f -'
Apr 22 21:03:45.751: INFO: stderr: ""
Apr 22 21:03:45.751: INFO: stdout: "service/agnhost-replica created\n"
Apr 22 21:03:45.751: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Apr 22 21:03:45.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2653 create -f -'
Apr 22 21:03:45.973: INFO: stderr: ""
Apr 22 21:03:45.973: INFO: stdout: "service/agnhost-primary created\n"
Apr 22 21:03:45.973: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr 22 21:03:45.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2653 create -f -'
Apr 22 21:03:46.132: INFO: stderr: ""
Apr 22 21:03:46.132: INFO: stdout: "service/frontend created\n"
Apr 22 21:03:46.133: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Apr 22 21:03:46.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2653 create -f -'
Apr 22 21:03:46.311: INFO: stderr: ""
Apr 22 21:03:46.311: INFO: stdout: "deployment.apps/frontend created\n"
Apr 22 21:03:46.311: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 22 21:03:46.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2653 create -f -'
Apr 22 21:03:46.471: INFO: stderr: ""
Apr 22 21:03:46.471: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Apr 22 21:03:46.471: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 22 21:03:46.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2653 create -f -'
Apr 22 21:03:46.687: INFO: stderr: ""
Apr 22 21:03:46.687: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Apr 22 21:03:46.687: INFO: Waiting for all frontend pods to be Running.
Apr 22 21:03:51.737: INFO: Waiting for frontend to serve content.
Apr 22 21:03:51.745: INFO: Trying to add a new entry to the guestbook.
Apr 22 21:03:51.753: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Apr 22 21:03:51.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2653 delete --grace-period=0 --force -f -'
Apr 22 21:03:51.838: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 22 21:03:51.838: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Apr 22 21:03:51.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2653 delete --grace-period=0 --force -f -'
Apr 22 21:03:51.913: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 22 21:03:51.913: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Apr 22 21:03:51.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2653 delete --grace-period=0 --force -f -'
Apr 22 21:03:51.981: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 22 21:03:51.981: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 22 21:03:51.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2653 delete --grace-period=0 --force -f -'
Apr 22 21:03:52.041: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 22 21:03:52.041: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 22 21:03:52.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2653 delete --grace-period=0 --force -f -'
Apr 22 21:03:52.102: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 22 21:03:52.102: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Apr 22 21:03:52.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2653 delete --grace-period=0 --force -f -'
Apr 22 21:03:52.161: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 22 21:03:52.161: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:03:52.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2653" for this suite.

• [SLOW TEST:6.674 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":89,"skipped":1516,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:03:52.169: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-3cebce61-8c6d-4053-a6c0-c2b150b62703
STEP: Creating a pod to test consume configMaps
Apr 22 21:03:52.202: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1192d6ab-990d-4a70-9630-a8a6d67968e8" in namespace "projected-4725" to be "Succeeded or Failed"
Apr 22 21:03:52.204: INFO: Pod "pod-projected-configmaps-1192d6ab-990d-4a70-9630-a8a6d67968e8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.92583ms
Apr 22 21:03:54.209: INFO: Pod "pod-projected-configmaps-1192d6ab-990d-4a70-9630-a8a6d67968e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007182293s
STEP: Saw pod success
Apr 22 21:03:54.209: INFO: Pod "pod-projected-configmaps-1192d6ab-990d-4a70-9630-a8a6d67968e8" satisfied condition "Succeeded or Failed"
Apr 22 21:03:54.211: INFO: Trying to get logs from node ip-10-0-130-171.us-west-2.compute.internal pod pod-projected-configmaps-1192d6ab-990d-4a70-9630-a8a6d67968e8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 22 21:03:54.231: INFO: Waiting for pod pod-projected-configmaps-1192d6ab-990d-4a70-9630-a8a6d67968e8 to disappear
Apr 22 21:03:54.233: INFO: Pod pod-projected-configmaps-1192d6ab-990d-4a70-9630-a8a6d67968e8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:03:54.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4725" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":90,"skipped":1518,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:03:54.241: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-aeb990a4-e790-4000-ac46-cc6724609c27
STEP: Creating a pod to test consume configMaps
Apr 22 21:03:54.277: INFO: Waiting up to 5m0s for pod "pod-configmaps-97c4985f-e4a7-4825-a841-c58911979897" in namespace "configmap-7684" to be "Succeeded or Failed"
Apr 22 21:03:54.279: INFO: Pod "pod-configmaps-97c4985f-e4a7-4825-a841-c58911979897": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01225ms
Apr 22 21:03:56.285: INFO: Pod "pod-configmaps-97c4985f-e4a7-4825-a841-c58911979897": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007926647s
STEP: Saw pod success
Apr 22 21:03:56.285: INFO: Pod "pod-configmaps-97c4985f-e4a7-4825-a841-c58911979897" satisfied condition "Succeeded or Failed"
Apr 22 21:03:56.288: INFO: Trying to get logs from node ip-10-0-130-171.us-west-2.compute.internal pod pod-configmaps-97c4985f-e4a7-4825-a841-c58911979897 container agnhost-container: <nil>
STEP: delete the pod
Apr 22 21:03:56.302: INFO: Waiting for pod pod-configmaps-97c4985f-e4a7-4825-a841-c58911979897 to disappear
Apr 22 21:03:56.304: INFO: Pod pod-configmaps-97c4985f-e4a7-4825-a841-c58911979897 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:03:56.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7684" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":91,"skipped":1524,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:03:56.313: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 22 21:03:58.865: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7eb32cff-d83b-42fb-b5e5-e5a1a7aee32d"
Apr 22 21:03:58.865: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7eb32cff-d83b-42fb-b5e5-e5a1a7aee32d" in namespace "pods-3814" to be "terminated due to deadline exceeded"
Apr 22 21:03:58.867: INFO: Pod "pod-update-activedeadlineseconds-7eb32cff-d83b-42fb-b5e5-e5a1a7aee32d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006377ms
Apr 22 21:04:00.873: INFO: Pod "pod-update-activedeadlineseconds-7eb32cff-d83b-42fb-b5e5-e5a1a7aee32d": Phase="Running", Reason="", readiness=true. Elapsed: 2.00826497s
Apr 22 21:04:02.880: INFO: Pod "pod-update-activedeadlineseconds-7eb32cff-d83b-42fb-b5e5-e5a1a7aee32d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.014769809s
Apr 22 21:04:02.880: INFO: Pod "pod-update-activedeadlineseconds-7eb32cff-d83b-42fb-b5e5-e5a1a7aee32d" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:04:02.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3814" for this suite.

• [SLOW TEST:6.576 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":92,"skipped":1525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:04:02.889: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-mbgg
STEP: Creating a pod to test atomic-volume-subpath
Apr 22 21:04:02.929: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mbgg" in namespace "subpath-165" to be "Succeeded or Failed"
Apr 22 21:04:02.931: INFO: Pod "pod-subpath-test-configmap-mbgg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080687ms
Apr 22 21:04:04.938: INFO: Pod "pod-subpath-test-configmap-mbgg": Phase="Running", Reason="", readiness=true. Elapsed: 2.008703493s
Apr 22 21:04:06.944: INFO: Pod "pod-subpath-test-configmap-mbgg": Phase="Running", Reason="", readiness=true. Elapsed: 4.014880995s
Apr 22 21:04:08.950: INFO: Pod "pod-subpath-test-configmap-mbgg": Phase="Running", Reason="", readiness=true. Elapsed: 6.02052441s
Apr 22 21:04:10.956: INFO: Pod "pod-subpath-test-configmap-mbgg": Phase="Running", Reason="", readiness=true. Elapsed: 8.026849423s
Apr 22 21:04:12.962: INFO: Pod "pod-subpath-test-configmap-mbgg": Phase="Running", Reason="", readiness=true. Elapsed: 10.032622601s
Apr 22 21:04:14.967: INFO: Pod "pod-subpath-test-configmap-mbgg": Phase="Running", Reason="", readiness=true. Elapsed: 12.038213957s
Apr 22 21:04:16.974: INFO: Pod "pod-subpath-test-configmap-mbgg": Phase="Running", Reason="", readiness=true. Elapsed: 14.044546116s
Apr 22 21:04:18.979: INFO: Pod "pod-subpath-test-configmap-mbgg": Phase="Running", Reason="", readiness=true. Elapsed: 16.049492915s
Apr 22 21:04:20.984: INFO: Pod "pod-subpath-test-configmap-mbgg": Phase="Running", Reason="", readiness=true. Elapsed: 18.055104187s
Apr 22 21:04:22.991: INFO: Pod "pod-subpath-test-configmap-mbgg": Phase="Running", Reason="", readiness=true. Elapsed: 20.061929433s
Apr 22 21:04:24.997: INFO: Pod "pod-subpath-test-configmap-mbgg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.067371159s
STEP: Saw pod success
Apr 22 21:04:24.997: INFO: Pod "pod-subpath-test-configmap-mbgg" satisfied condition "Succeeded or Failed"
Apr 22 21:04:24.999: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-subpath-test-configmap-mbgg container test-container-subpath-configmap-mbgg: <nil>
STEP: delete the pod
Apr 22 21:04:25.017: INFO: Waiting for pod pod-subpath-test-configmap-mbgg to disappear
Apr 22 21:04:25.019: INFO: Pod pod-subpath-test-configmap-mbgg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mbgg
Apr 22 21:04:25.019: INFO: Deleting pod "pod-subpath-test-configmap-mbgg" in namespace "subpath-165"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:04:25.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-165" for this suite.

• [SLOW TEST:22.139 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":93,"skipped":1568,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:04:25.028: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr 22 21:04:25.718: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Apr 22 21:04:27.726: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754722265, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754722265, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754722265, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754722265, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 21:04:30.743: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:04:30.748: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:04:31.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4074" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.910 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":94,"skipped":1579,"failed":0}
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:04:31.938: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-8484/secret-test-d96185bc-d6ea-4852-b084-4f3ef9600ff7
STEP: Creating a pod to test consume secrets
Apr 22 21:04:31.974: INFO: Waiting up to 5m0s for pod "pod-configmaps-31c1305a-0cb0-4b7a-9728-d7507760b144" in namespace "secrets-8484" to be "Succeeded or Failed"
Apr 22 21:04:31.976: INFO: Pod "pod-configmaps-31c1305a-0cb0-4b7a-9728-d7507760b144": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022229ms
Apr 22 21:04:33.982: INFO: Pod "pod-configmaps-31c1305a-0cb0-4b7a-9728-d7507760b144": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008333886s
STEP: Saw pod success
Apr 22 21:04:33.982: INFO: Pod "pod-configmaps-31c1305a-0cb0-4b7a-9728-d7507760b144" satisfied condition "Succeeded or Failed"
Apr 22 21:04:33.984: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod pod-configmaps-31c1305a-0cb0-4b7a-9728-d7507760b144 container env-test: <nil>
STEP: delete the pod
Apr 22 21:04:33.997: INFO: Waiting for pod pod-configmaps-31c1305a-0cb0-4b7a-9728-d7507760b144 to disappear
Apr 22 21:04:33.999: INFO: Pod pod-configmaps-31c1305a-0cb0-4b7a-9728-d7507760b144 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:04:33.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8484" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":95,"skipped":1579,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:04:34.006: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr 22 21:04:34.451: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 21:04:37.471: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:04:37.475: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:04:38.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8621" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":96,"skipped":1596,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:04:38.648: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-97254899-3c01-4b19-8699-9374123462cb
STEP: Creating a pod to test consume secrets
Apr 22 21:04:38.693: INFO: Waiting up to 5m0s for pod "pod-secrets-75ee7d41-64a3-4bbc-b31a-cf8ae8191d5f" in namespace "secrets-2271" to be "Succeeded or Failed"
Apr 22 21:04:38.700: INFO: Pod "pod-secrets-75ee7d41-64a3-4bbc-b31a-cf8ae8191d5f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.38313ms
Apr 22 21:04:40.706: INFO: Pod "pod-secrets-75ee7d41-64a3-4bbc-b31a-cf8ae8191d5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013365118s
STEP: Saw pod success
Apr 22 21:04:40.706: INFO: Pod "pod-secrets-75ee7d41-64a3-4bbc-b31a-cf8ae8191d5f" satisfied condition "Succeeded or Failed"
Apr 22 21:04:40.708: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod pod-secrets-75ee7d41-64a3-4bbc-b31a-cf8ae8191d5f container secret-volume-test: <nil>
STEP: delete the pod
Apr 22 21:04:40.727: INFO: Waiting for pod pod-secrets-75ee7d41-64a3-4bbc-b31a-cf8ae8191d5f to disappear
Apr 22 21:04:40.729: INFO: Pod pod-secrets-75ee7d41-64a3-4bbc-b31a-cf8ae8191d5f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:04:40.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2271" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":97,"skipped":1597,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:04:40.737: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 22 21:04:44.802: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 22 21:04:44.804: INFO: Pod pod-with-prestop-http-hook still exists
Apr 22 21:04:46.805: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 22 21:04:46.811: INFO: Pod pod-with-prestop-http-hook still exists
Apr 22 21:04:48.804: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 22 21:04:48.810: INFO: Pod pod-with-prestop-http-hook still exists
Apr 22 21:04:50.804: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 22 21:04:50.811: INFO: Pod pod-with-prestop-http-hook still exists
Apr 22 21:04:52.805: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 22 21:04:52.811: INFO: Pod pod-with-prestop-http-hook still exists
Apr 22 21:04:54.805: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 22 21:04:54.811: INFO: Pod pod-with-prestop-http-hook still exists
Apr 22 21:04:56.805: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 22 21:04:56.811: INFO: Pod pod-with-prestop-http-hook still exists
Apr 22 21:04:58.805: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 22 21:04:58.810: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:04:58.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4487" for this suite.

• [SLOW TEST:18.094 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":98,"skipped":1602,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:04:58.831: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-d1ff4ced-ac60-4e31-9ce1-570956004876
STEP: Creating a pod to test consume configMaps
Apr 22 21:04:58.867: INFO: Waiting up to 5m0s for pod "pod-configmaps-de1fe721-b342-4aa8-8272-0f6449265bfd" in namespace "configmap-1120" to be "Succeeded or Failed"
Apr 22 21:04:58.869: INFO: Pod "pod-configmaps-de1fe721-b342-4aa8-8272-0f6449265bfd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.839316ms
Apr 22 21:05:00.875: INFO: Pod "pod-configmaps-de1fe721-b342-4aa8-8272-0f6449265bfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007973536s
STEP: Saw pod success
Apr 22 21:05:00.875: INFO: Pod "pod-configmaps-de1fe721-b342-4aa8-8272-0f6449265bfd" satisfied condition "Succeeded or Failed"
Apr 22 21:05:00.877: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod pod-configmaps-de1fe721-b342-4aa8-8272-0f6449265bfd container agnhost-container: <nil>
STEP: delete the pod
Apr 22 21:05:00.892: INFO: Waiting for pod pod-configmaps-de1fe721-b342-4aa8-8272-0f6449265bfd to disappear
Apr 22 21:05:00.894: INFO: Pod pod-configmaps-de1fe721-b342-4aa8-8272-0f6449265bfd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:05:00.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1120" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":99,"skipped":1603,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:05:00.901: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:05:00.941: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Apr 22 21:05:00.949: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:00.949: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:00.949: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:00.951: INFO: Number of nodes with available pods: 0
Apr 22 21:05:00.951: INFO: Node ip-10-0-130-171.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:05:01.957: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:01.957: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:01.957: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:01.959: INFO: Number of nodes with available pods: 0
Apr 22 21:05:01.959: INFO: Node ip-10-0-130-171.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:05:02.959: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:02.959: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:02.959: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:02.963: INFO: Number of nodes with available pods: 2
Apr 22 21:05:02.963: INFO: Node ip-10-0-130-171.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:05:03.957: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:03.957: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:03.957: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:03.960: INFO: Number of nodes with available pods: 4
Apr 22 21:05:03.960: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Apr 22 21:05:03.981: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:03.981: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:03.981: INFO: Wrong image for pod: daemon-set-xtsr5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:03.982: INFO: Wrong image for pod: daemon-set-zxk8t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:03.986: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:03.986: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:03.986: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:04.991: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:04.991: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:04.991: INFO: Wrong image for pod: daemon-set-xtsr5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:04.991: INFO: Wrong image for pod: daemon-set-zxk8t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:04.995: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:04.995: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:04.995: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:05.991: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:05.991: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:05.991: INFO: Wrong image for pod: daemon-set-xtsr5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:05.991: INFO: Pod daemon-set-xtsr5 is not available
Apr 22 21:05:05.991: INFO: Wrong image for pod: daemon-set-zxk8t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:05.995: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:05.995: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:05.995: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:06.990: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:06.991: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:06.991: INFO: Pod daemon-set-qj9n8 is not available
Apr 22 21:05:06.991: INFO: Wrong image for pod: daemon-set-zxk8t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:06.994: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:06.994: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:06.994: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:07.990: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:07.990: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:07.990: INFO: Wrong image for pod: daemon-set-zxk8t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:07.993: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:07.993: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:07.993: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:08.991: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:08.991: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:08.991: INFO: Wrong image for pod: daemon-set-zxk8t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:08.994: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:08.994: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:08.994: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:09.991: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:09.991: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:09.991: INFO: Wrong image for pod: daemon-set-zxk8t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:09.991: INFO: Pod daemon-set-zxk8t is not available
Apr 22 21:05:09.994: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:09.994: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:09.994: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:10.991: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:10.991: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:10.991: INFO: Wrong image for pod: daemon-set-zxk8t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:10.991: INFO: Pod daemon-set-zxk8t is not available
Apr 22 21:05:10.994: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:10.994: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:10.994: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:11.991: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:11.991: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:11.991: INFO: Wrong image for pod: daemon-set-zxk8t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:11.991: INFO: Pod daemon-set-zxk8t is not available
Apr 22 21:05:11.995: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:11.995: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:11.995: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:12.991: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:12.991: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:12.991: INFO: Wrong image for pod: daemon-set-zxk8t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:12.991: INFO: Pod daemon-set-zxk8t is not available
Apr 22 21:05:12.994: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:12.994: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:12.994: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:13.991: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:13.991: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:13.991: INFO: Wrong image for pod: daemon-set-zxk8t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:13.991: INFO: Pod daemon-set-zxk8t is not available
Apr 22 21:05:13.994: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:13.994: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:13.995: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:14.990: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:14.990: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:14.990: INFO: Wrong image for pod: daemon-set-zxk8t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:14.990: INFO: Pod daemon-set-zxk8t is not available
Apr 22 21:05:14.994: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:14.994: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:14.994: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:15.990: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:15.990: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:15.990: INFO: Wrong image for pod: daemon-set-zxk8t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:15.990: INFO: Pod daemon-set-zxk8t is not available
Apr 22 21:05:15.994: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:15.994: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:15.994: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:16.992: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:16.992: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:16.992: INFO: Wrong image for pod: daemon-set-zxk8t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:16.992: INFO: Pod daemon-set-zxk8t is not available
Apr 22 21:05:16.995: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:16.995: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:16.995: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:17.990: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:17.990: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:17.990: INFO: Wrong image for pod: daemon-set-zxk8t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:17.990: INFO: Pod daemon-set-zxk8t is not available
Apr 22 21:05:17.993: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:17.993: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:17.994: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:18.991: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:18.991: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:18.991: INFO: Pod daemon-set-sstn4 is not available
Apr 22 21:05:18.995: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:18.995: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:18.995: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:19.990: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:19.991: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:19.991: INFO: Pod daemon-set-sstn4 is not available
Apr 22 21:05:19.994: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:19.994: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:19.994: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:20.990: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:20.990: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:20.990: INFO: Pod daemon-set-b47r6 is not available
Apr 22 21:05:20.994: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:20.994: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:20.994: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:21.990: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:21.990: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:21.990: INFO: Pod daemon-set-b47r6 is not available
Apr 22 21:05:21.994: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:21.994: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:21.994: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:22.991: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:22.991: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:22.991: INFO: Pod daemon-set-b47r6 is not available
Apr 22 21:05:22.994: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:22.994: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:22.994: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:23.991: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:23.991: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:23.991: INFO: Pod daemon-set-b47r6 is not available
Apr 22 21:05:23.995: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:23.995: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:23.995: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:24.991: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:24.991: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:24.991: INFO: Pod daemon-set-b47r6 is not available
Apr 22 21:05:24.995: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:24.995: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:24.995: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:25.991: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:25.991: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:25.991: INFO: Pod daemon-set-b47r6 is not available
Apr 22 21:05:25.994: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:25.994: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:25.994: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:26.991: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:26.991: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:26.991: INFO: Pod daemon-set-b47r6 is not available
Apr 22 21:05:26.994: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:26.994: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:26.994: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:27.993: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:27.993: INFO: Wrong image for pod: daemon-set-b47r6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:27.993: INFO: Pod daemon-set-b47r6 is not available
Apr 22 21:05:27.999: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:27.999: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:27.999: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:28.991: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:28.991: INFO: Pod daemon-set-kl2tv is not available
Apr 22 21:05:28.994: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:28.994: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:28.994: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:29.991: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:29.995: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:29.995: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:29.995: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:30.990: INFO: Wrong image for pod: daemon-set-77z6b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 22 21:05:30.990: INFO: Pod daemon-set-77z6b is not available
Apr 22 21:05:30.993: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:30.993: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:30.993: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:31.991: INFO: Pod daemon-set-wr9z6 is not available
Apr 22 21:05:31.994: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:31.994: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:31.994: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Apr 22 21:05:31.998: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:31.998: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:31.998: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:32.000: INFO: Number of nodes with available pods: 3
Apr 22 21:05:32.000: INFO: Node ip-10-0-130-171.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:05:33.006: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:33.006: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:33.006: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:05:33.009: INFO: Number of nodes with available pods: 4
Apr 22 21:05:33.009: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4957, will wait for the garbage collector to delete the pods
Apr 22 21:05:33.079: INFO: Deleting DaemonSet.extensions daemon-set took: 7.571177ms
Apr 22 21:05:33.779: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.105229ms
Apr 22 21:05:38.683: INFO: Number of nodes with available pods: 0
Apr 22 21:05:38.683: INFO: Number of running nodes: 0, number of available pods: 0
Apr 22 21:05:38.685: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13746"},"items":null}

Apr 22 21:05:38.687: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13746"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:05:38.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4957" for this suite.

• [SLOW TEST:37.809 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":100,"skipped":1617,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:05:38.710: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-dde490ba-1667-4c24-b340-dd80cdbf665c
STEP: Creating a pod to test consume configMaps
Apr 22 21:05:38.751: INFO: Waiting up to 5m0s for pod "pod-configmaps-11dbd065-c0f6-494f-aa90-147dddb57850" in namespace "configmap-8007" to be "Succeeded or Failed"
Apr 22 21:05:38.753: INFO: Pod "pod-configmaps-11dbd065-c0f6-494f-aa90-147dddb57850": Phase="Pending", Reason="", readiness=false. Elapsed: 1.920757ms
Apr 22 21:05:40.758: INFO: Pod "pod-configmaps-11dbd065-c0f6-494f-aa90-147dddb57850": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006921366s
STEP: Saw pod success
Apr 22 21:05:40.758: INFO: Pod "pod-configmaps-11dbd065-c0f6-494f-aa90-147dddb57850" satisfied condition "Succeeded or Failed"
Apr 22 21:05:40.760: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-configmaps-11dbd065-c0f6-494f-aa90-147dddb57850 container agnhost-container: <nil>
STEP: delete the pod
Apr 22 21:05:40.782: INFO: Waiting for pod pod-configmaps-11dbd065-c0f6-494f-aa90-147dddb57850 to disappear
Apr 22 21:05:40.784: INFO: Pod pod-configmaps-11dbd065-c0f6-494f-aa90-147dddb57850 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:05:40.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8007" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":101,"skipped":1619,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:05:40.791: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Apr 22 21:05:43.351: INFO: Successfully updated pod "annotationupdate45d29559-b629-4ae8-a3c7-558f7b100984"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:05:45.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8386" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":102,"skipped":1623,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:05:45.374: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 22 21:05:45.930: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 22 21:05:47.938: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754722345, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754722345, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754722345, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754722345, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 21:05:50.957: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Apr 22 21:05:50.974: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:05:50.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1066" for this suite.
STEP: Destroying namespace "webhook-1066-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.660 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":103,"skipped":1628,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:05:51.034: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Apr 22 21:05:51.062: INFO: created test-podtemplate-1
Apr 22 21:05:51.066: INFO: created test-podtemplate-2
Apr 22 21:05:51.068: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Apr 22 21:05:51.070: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Apr 22 21:05:51.084: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:05:51.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8060" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":104,"skipped":1696,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:05:51.101: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-9aa75544-f393-4f69-a7ad-8047c5a73d89 in namespace container-probe-2351
Apr 22 21:05:53.135: INFO: Started pod liveness-9aa75544-f393-4f69-a7ad-8047c5a73d89 in namespace container-probe-2351
STEP: checking the pod's current state and verifying that restartCount is present
Apr 22 21:05:53.139: INFO: Initial restart count of pod liveness-9aa75544-f393-4f69-a7ad-8047c5a73d89 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:09:53.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2351" for this suite.

• [SLOW TEST:242.753 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":105,"skipped":1698,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:09:53.853: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:10:53.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8619" for this suite.

• [SLOW TEST:60.049 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":106,"skipped":1729,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:10:53.902: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 21:10:53.935: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2f9e26d8-2530-4920-ac82-c8f44fe11ef6" in namespace "projected-463" to be "Succeeded or Failed"
Apr 22 21:10:53.937: INFO: Pod "downwardapi-volume-2f9e26d8-2530-4920-ac82-c8f44fe11ef6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.993909ms
Apr 22 21:10:55.943: INFO: Pod "downwardapi-volume-2f9e26d8-2530-4920-ac82-c8f44fe11ef6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008195558s
STEP: Saw pod success
Apr 22 21:10:55.943: INFO: Pod "downwardapi-volume-2f9e26d8-2530-4920-ac82-c8f44fe11ef6" satisfied condition "Succeeded or Failed"
Apr 22 21:10:55.946: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod downwardapi-volume-2f9e26d8-2530-4920-ac82-c8f44fe11ef6 container client-container: <nil>
STEP: delete the pod
Apr 22 21:10:55.969: INFO: Waiting for pod downwardapi-volume-2f9e26d8-2530-4920-ac82-c8f44fe11ef6 to disappear
Apr 22 21:10:55.971: INFO: Pod downwardapi-volume-2f9e26d8-2530-4920-ac82-c8f44fe11ef6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:10:55.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-463" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":107,"skipped":1741,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:10:55.979: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 22 21:10:56.681: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 22 21:10:58.691: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754722656, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754722656, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754722656, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754722656, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 21:11:01.705: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:11:01.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7649" for this suite.
STEP: Destroying namespace "webhook-7649-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.823 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":108,"skipped":1744,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:11:01.802: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-fef37b9d-24b1-4d4d-b86c-581b6d21d533
STEP: Creating a pod to test consume configMaps
Apr 22 21:11:01.839: INFO: Waiting up to 5m0s for pod "pod-configmaps-3f34bef8-1d36-4bf3-88a0-cf32b5197c09" in namespace "configmap-3349" to be "Succeeded or Failed"
Apr 22 21:11:01.841: INFO: Pod "pod-configmaps-3f34bef8-1d36-4bf3-88a0-cf32b5197c09": Phase="Pending", Reason="", readiness=false. Elapsed: 1.997028ms
Apr 22 21:11:03.847: INFO: Pod "pod-configmaps-3f34bef8-1d36-4bf3-88a0-cf32b5197c09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008346054s
STEP: Saw pod success
Apr 22 21:11:03.847: INFO: Pod "pod-configmaps-3f34bef8-1d36-4bf3-88a0-cf32b5197c09" satisfied condition "Succeeded or Failed"
Apr 22 21:11:03.849: INFO: Trying to get logs from node ip-10-0-130-171.us-west-2.compute.internal pod pod-configmaps-3f34bef8-1d36-4bf3-88a0-cf32b5197c09 container agnhost-container: <nil>
STEP: delete the pod
Apr 22 21:11:03.871: INFO: Waiting for pod pod-configmaps-3f34bef8-1d36-4bf3-88a0-cf32b5197c09 to disappear
Apr 22 21:11:03.873: INFO: Pod pod-configmaps-3f34bef8-1d36-4bf3-88a0-cf32b5197c09 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:11:03.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3349" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":109,"skipped":1752,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:11:03.881: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:11:03.907: INFO: Creating ReplicaSet my-hostname-basic-7b453ae7-1cfb-4585-a222-363451af9828
Apr 22 21:11:03.913: INFO: Pod name my-hostname-basic-7b453ae7-1cfb-4585-a222-363451af9828: Found 0 pods out of 1
Apr 22 21:11:08.923: INFO: Pod name my-hostname-basic-7b453ae7-1cfb-4585-a222-363451af9828: Found 1 pods out of 1
Apr 22 21:11:08.923: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-7b453ae7-1cfb-4585-a222-363451af9828" is running
Apr 22 21:11:08.925: INFO: Pod "my-hostname-basic-7b453ae7-1cfb-4585-a222-363451af9828-z96dd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-22 21:11:03 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-22 21:11:05 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-22 21:11:05 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-22 21:11:03 +0000 UTC Reason: Message:}])
Apr 22 21:11:08.925: INFO: Trying to dial the pod
Apr 22 21:11:13.940: INFO: Controller my-hostname-basic-7b453ae7-1cfb-4585-a222-363451af9828: Got expected result from replica 1 [my-hostname-basic-7b453ae7-1cfb-4585-a222-363451af9828-z96dd]: "my-hostname-basic-7b453ae7-1cfb-4585-a222-363451af9828-z96dd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:11:13.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8926" for this suite.

• [SLOW TEST:10.073 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":110,"skipped":1768,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:11:13.954: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Apr 22 21:11:13.982: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 21:11:16.879: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:11:28.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1386" for this suite.

• [SLOW TEST:14.356 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":111,"skipped":1775,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:11:28.310: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 22 21:11:28.878: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 22 21:11:30.889: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754722688, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754722688, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754722688, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754722688, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 21:11:33.905: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:11:34.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8061" for this suite.
STEP: Destroying namespace "webhook-8061-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.778 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":112,"skipped":1782,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:11:34.087: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-ddzk
STEP: Creating a pod to test atomic-volume-subpath
Apr 22 21:11:34.123: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ddzk" in namespace "subpath-8186" to be "Succeeded or Failed"
Apr 22 21:11:34.125: INFO: Pod "pod-subpath-test-configmap-ddzk": Phase="Pending", Reason="", readiness=false. Elapsed: 1.981326ms
Apr 22 21:11:36.132: INFO: Pod "pod-subpath-test-configmap-ddzk": Phase="Running", Reason="", readiness=true. Elapsed: 2.008911453s
Apr 22 21:11:38.139: INFO: Pod "pod-subpath-test-configmap-ddzk": Phase="Running", Reason="", readiness=true. Elapsed: 4.015390952s
Apr 22 21:11:40.144: INFO: Pod "pod-subpath-test-configmap-ddzk": Phase="Running", Reason="", readiness=true. Elapsed: 6.020421335s
Apr 22 21:11:42.150: INFO: Pod "pod-subpath-test-configmap-ddzk": Phase="Running", Reason="", readiness=true. Elapsed: 8.026447249s
Apr 22 21:11:44.156: INFO: Pod "pod-subpath-test-configmap-ddzk": Phase="Running", Reason="", readiness=true. Elapsed: 10.033027651s
Apr 22 21:11:46.162: INFO: Pod "pod-subpath-test-configmap-ddzk": Phase="Running", Reason="", readiness=true. Elapsed: 12.038954818s
Apr 22 21:11:48.169: INFO: Pod "pod-subpath-test-configmap-ddzk": Phase="Running", Reason="", readiness=true. Elapsed: 14.045280759s
Apr 22 21:11:50.175: INFO: Pod "pod-subpath-test-configmap-ddzk": Phase="Running", Reason="", readiness=true. Elapsed: 16.051370422s
Apr 22 21:11:52.180: INFO: Pod "pod-subpath-test-configmap-ddzk": Phase="Running", Reason="", readiness=true. Elapsed: 18.056683174s
Apr 22 21:11:54.185: INFO: Pod "pod-subpath-test-configmap-ddzk": Phase="Running", Reason="", readiness=true. Elapsed: 20.061945524s
Apr 22 21:11:56.191: INFO: Pod "pod-subpath-test-configmap-ddzk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.067366491s
STEP: Saw pod success
Apr 22 21:11:56.191: INFO: Pod "pod-subpath-test-configmap-ddzk" satisfied condition "Succeeded or Failed"
Apr 22 21:11:56.193: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-subpath-test-configmap-ddzk container test-container-subpath-configmap-ddzk: <nil>
STEP: delete the pod
Apr 22 21:11:56.221: INFO: Waiting for pod pod-subpath-test-configmap-ddzk to disappear
Apr 22 21:11:56.223: INFO: Pod pod-subpath-test-configmap-ddzk no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ddzk
Apr 22 21:11:56.223: INFO: Deleting pod "pod-subpath-test-configmap-ddzk" in namespace "subpath-8186"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:11:56.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8186" for this suite.

• [SLOW TEST:22.145 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":113,"skipped":1809,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:11:56.233: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 22 21:11:56.266: INFO: Waiting up to 5m0s for pod "pod-d5109b5d-7209-41f1-9885-400086217332" in namespace "emptydir-6117" to be "Succeeded or Failed"
Apr 22 21:11:56.268: INFO: Pod "pod-d5109b5d-7209-41f1-9885-400086217332": Phase="Pending", Reason="", readiness=false. Elapsed: 2.856955ms
Apr 22 21:11:58.274: INFO: Pod "pod-d5109b5d-7209-41f1-9885-400086217332": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008525825s
STEP: Saw pod success
Apr 22 21:11:58.274: INFO: Pod "pod-d5109b5d-7209-41f1-9885-400086217332" satisfied condition "Succeeded or Failed"
Apr 22 21:11:58.276: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod pod-d5109b5d-7209-41f1-9885-400086217332 container test-container: <nil>
STEP: delete the pod
Apr 22 21:11:58.290: INFO: Waiting for pod pod-d5109b5d-7209-41f1-9885-400086217332 to disappear
Apr 22 21:11:58.292: INFO: Pod pod-d5109b5d-7209-41f1-9885-400086217332 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:11:58.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6117" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":114,"skipped":1824,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:11:58.300: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:11:58.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5121" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":115,"skipped":1845,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:11:58.364: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:12:09.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9762" for this suite.

• [SLOW TEST:11.099 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":116,"skipped":1849,"failed":0}
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:12:09.463: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:12:09.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6464" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":117,"skipped":1849,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:12:09.502: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Apr 22 21:12:12.059: INFO: Successfully updated pod "annotationupdate933943d9-c683-4132-8ea9-133d23b7c1f4"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:12:16.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-283" for this suite.

• [SLOW TEST:6.589 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":118,"skipped":1851,"failed":0}
SSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:12:16.091: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Apr 22 21:12:16.134: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:12:16.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2888" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":119,"skipped":1855,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:12:16.157: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:12:20.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5962" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":120,"skipped":1907,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:12:20.214: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 22 21:12:20.258: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Apr 22 21:12:20.261: INFO: starting watch
STEP: patching
STEP: updating
Apr 22 21:12:20.271: INFO: waiting for watch events with expected annotations
Apr 22 21:12:20.271: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:12:20.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-7887" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":121,"skipped":1946,"failed":0}

------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:12:20.309: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 22 21:12:20.333: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 22 21:12:20.339: INFO: Waiting for terminating namespaces to be deleted...
Apr 22 21:12:20.341: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-130-171.us-west-2.compute.internal before test
Apr 22 21:12:20.346: INFO: calico-node-zstxl from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 21:12:20.346: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 21:12:20.346: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 21:12:20.346: INFO: kube-proxy-246ms from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 21:12:20.346: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 21:12:20.346: INFO: sonobuoy-e2e-job-f05335387339495e from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 21:12:20.346: INFO: 	Container e2e ready: true, restart count 0
Apr 22 21:12:20.346: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 22 21:12:20.346: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-x9h8q from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 21:12:20.346: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 22 21:12:20.346: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 22 21:12:20.346: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-130-230.us-west-2.compute.internal before test
Apr 22 21:12:20.351: INFO: calico-node-92wx7 from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 21:12:20.351: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 21:12:20.351: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 21:12:20.351: INFO: coredns-74ff55c5b-b2ghx from kube-system started at 2021-04-22 20:42:54 +0000 UTC (1 container statuses recorded)
Apr 22 21:12:20.351: INFO: 	Container coredns ready: true, restart count 0
Apr 22 21:12:20.351: INFO: kube-proxy-z7cgr from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 21:12:20.351: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 21:12:20.351: INFO: sonobuoy from sonobuoy started at 2021-04-22 20:39:15 +0000 UTC (1 container statuses recorded)
Apr 22 21:12:20.351: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 22 21:12:20.351: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-f46mv from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 21:12:20.351: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 22 21:12:20.351: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 22 21:12:20.351: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-131-122.us-west-2.compute.internal before test
Apr 22 21:12:20.356: INFO: annotationupdate933943d9-c683-4132-8ea9-133d23b7c1f4 from downward-api-283 started at 2021-04-22 21:12:09 +0000 UTC (1 container statuses recorded)
Apr 22 21:12:20.356: INFO: 	Container client-container ready: true, restart count 0
Apr 22 21:12:20.356: INFO: calico-node-w7mgb from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 21:12:20.356: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 21:12:20.356: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 21:12:20.356: INFO: coredns-74ff55c5b-l4x4h from kube-system started at 2021-04-22 20:38:22 +0000 UTC (1 container statuses recorded)
Apr 22 21:12:20.356: INFO: 	Container coredns ready: true, restart count 0
Apr 22 21:12:20.356: INFO: kube-proxy-d5r2q from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 21:12:20.356: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 21:12:20.356: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-c9ssg from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 21:12:20.356: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 22 21:12:20.356: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 22 21:12:20.356: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-131-2.us-west-2.compute.internal before test
Apr 22 21:12:20.368: INFO: calico-node-bbjxk from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 21:12:20.368: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 21:12:20.368: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 21:12:20.368: INFO: coredns-74ff55c5b-kj2n4 from kube-system started at 2021-04-22 20:38:22 +0000 UTC (1 container statuses recorded)
Apr 22 21:12:20.368: INFO: 	Container coredns ready: false, restart count 0
Apr 22 21:12:20.368: INFO: kube-proxy-b7qsd from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 21:12:20.368: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 21:12:20.368: INFO: busybox-readonly-fsd8816e77-26e1-4e60-b81a-a509a0452a8e from kubelet-test-5962 started at 2021-04-22 21:12:16 +0000 UTC (1 container statuses recorded)
Apr 22 21:12:20.368: INFO: 	Container busybox-readonly-fsd8816e77-26e1-4e60-b81a-a509a0452a8e ready: true, restart count 0
Apr 22 21:12:20.368: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-4m6tb from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 21:12:20.368: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 22 21:12:20.368: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-30068e16-5743-4d7b-a9bb-e310bc1a68cc 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.0.130.171 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.0.130.171 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 22 21:12:30.465: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.130.171 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6364 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:12:30.465: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.130.171, port: 54321
Apr 22 21:12:30.524: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.130.171:54321/hostname] Namespace:sched-pred-6364 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:12:30.524: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.130.171, port: 54321 UDP
Apr 22 21:12:30.573: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.130.171 54321] Namespace:sched-pred-6364 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:12:30.573: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 22 21:12:35.620: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.130.171 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6364 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:12:35.620: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.130.171, port: 54321
Apr 22 21:12:35.673: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.130.171:54321/hostname] Namespace:sched-pred-6364 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:12:35.673: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.130.171, port: 54321 UDP
Apr 22 21:12:35.726: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.130.171 54321] Namespace:sched-pred-6364 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:12:35.726: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 22 21:12:40.773: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.130.171 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6364 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:12:40.773: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.130.171, port: 54321
Apr 22 21:12:40.832: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.130.171:54321/hostname] Namespace:sched-pred-6364 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:12:40.832: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.130.171, port: 54321 UDP
Apr 22 21:12:40.881: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.130.171 54321] Namespace:sched-pred-6364 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:12:40.881: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 22 21:12:45.935: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.130.171 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6364 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:12:45.935: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.130.171, port: 54321
Apr 22 21:12:45.988: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.130.171:54321/hostname] Namespace:sched-pred-6364 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:12:45.988: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.130.171, port: 54321 UDP
Apr 22 21:12:46.038: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.130.171 54321] Namespace:sched-pred-6364 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:12:46.038: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 22 21:12:51.086: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.130.171 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6364 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:12:51.086: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.130.171, port: 54321
Apr 22 21:12:51.143: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.130.171:54321/hostname] Namespace:sched-pred-6364 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:12:51.143: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.130.171, port: 54321 UDP
Apr 22 21:12:51.196: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.130.171 54321] Namespace:sched-pred-6364 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:12:51.196: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: removing the label kubernetes.io/e2e-30068e16-5743-4d7b-a9bb-e310bc1a68cc off the node ip-10-0-130-171.us-west-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-30068e16-5743-4d7b-a9bb-e310bc1a68cc
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:12:56.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6364" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:35.973 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":122,"skipped":1946,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:12:56.282: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:13:07.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2121" for this suite.

• [SLOW TEST:11.087 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":123,"skipped":2031,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:13:07.369: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 22 21:13:07.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-8894 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Apr 22 21:13:07.616: INFO: stderr: ""
Apr 22 21:13:07.616: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Apr 22 21:13:07.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-8894 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Apr 22 21:13:07.849: INFO: stderr: ""
Apr 22 21:13:07.849: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Apr 22 21:13:07.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-8894 delete pods e2e-test-httpd-pod'
Apr 22 21:13:09.488: INFO: stderr: ""
Apr 22 21:13:09.488: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:13:09.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8894" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":124,"skipped":2064,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:13:09.501: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Apr 22 21:13:09.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-4400 create -f -'
Apr 22 21:13:09.766: INFO: stderr: ""
Apr 22 21:13:09.766: INFO: stdout: "pod/pause created\n"
Apr 22 21:13:09.766: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr 22 21:13:09.766: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4400" to be "running and ready"
Apr 22 21:13:09.769: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.48699ms
Apr 22 21:13:11.775: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009365347s
Apr 22 21:13:11.775: INFO: Pod "pause" satisfied condition "running and ready"
Apr 22 21:13:11.775: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Apr 22 21:13:11.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-4400 label pods pause testing-label=testing-label-value'
Apr 22 21:13:11.838: INFO: stderr: ""
Apr 22 21:13:11.838: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Apr 22 21:13:11.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-4400 get pod pause -L testing-label'
Apr 22 21:13:11.891: INFO: stderr: ""
Apr 22 21:13:11.891: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Apr 22 21:13:11.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-4400 label pods pause testing-label-'
Apr 22 21:13:11.957: INFO: stderr: ""
Apr 22 21:13:11.957: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Apr 22 21:13:11.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-4400 get pod pause -L testing-label'
Apr 22 21:13:12.011: INFO: stderr: ""
Apr 22 21:13:12.011: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Apr 22 21:13:12.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-4400 delete --grace-period=0 --force -f -'
Apr 22 21:13:12.074: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 22 21:13:12.074: INFO: stdout: "pod \"pause\" force deleted\n"
Apr 22 21:13:12.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-4400 get rc,svc -l name=pause --no-headers'
Apr 22 21:13:12.134: INFO: stderr: "No resources found in kubectl-4400 namespace.\n"
Apr 22 21:13:12.135: INFO: stdout: ""
Apr 22 21:13:12.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-4400 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 22 21:13:12.191: INFO: stderr: ""
Apr 22 21:13:12.191: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:13:12.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4400" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":125,"skipped":2066,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:13:12.201: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 21:13:12.230: INFO: Waiting up to 5m0s for pod "downwardapi-volume-28238038-047b-40ff-8ce5-7758ebffb18c" in namespace "projected-9806" to be "Succeeded or Failed"
Apr 22 21:13:12.236: INFO: Pod "downwardapi-volume-28238038-047b-40ff-8ce5-7758ebffb18c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.643577ms
Apr 22 21:13:14.241: INFO: Pod "downwardapi-volume-28238038-047b-40ff-8ce5-7758ebffb18c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011120178s
STEP: Saw pod success
Apr 22 21:13:14.241: INFO: Pod "downwardapi-volume-28238038-047b-40ff-8ce5-7758ebffb18c" satisfied condition "Succeeded or Failed"
Apr 22 21:13:14.244: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod downwardapi-volume-28238038-047b-40ff-8ce5-7758ebffb18c container client-container: <nil>
STEP: delete the pod
Apr 22 21:13:14.259: INFO: Waiting for pod downwardapi-volume-28238038-047b-40ff-8ce5-7758ebffb18c to disappear
Apr 22 21:13:14.261: INFO: Pod downwardapi-volume-28238038-047b-40ff-8ce5-7758ebffb18c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:13:14.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9806" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":126,"skipped":2099,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:13:14.268: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:13:30.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4660" for this suite.

• [SLOW TEST:16.136 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":127,"skipped":2113,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:13:30.404: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 22 21:13:30.448: INFO: Waiting up to 5m0s for pod "pod-d0d80842-6945-403f-bf01-5c3b55a932d7" in namespace "emptydir-9909" to be "Succeeded or Failed"
Apr 22 21:13:30.450: INFO: Pod "pod-d0d80842-6945-403f-bf01-5c3b55a932d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034335ms
Apr 22 21:13:32.456: INFO: Pod "pod-d0d80842-6945-403f-bf01-5c3b55a932d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008184744s
STEP: Saw pod success
Apr 22 21:13:32.456: INFO: Pod "pod-d0d80842-6945-403f-bf01-5c3b55a932d7" satisfied condition "Succeeded or Failed"
Apr 22 21:13:32.458: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-d0d80842-6945-403f-bf01-5c3b55a932d7 container test-container: <nil>
STEP: delete the pod
Apr 22 21:13:32.472: INFO: Waiting for pod pod-d0d80842-6945-403f-bf01-5c3b55a932d7 to disappear
Apr 22 21:13:32.474: INFO: Pod pod-d0d80842-6945-403f-bf01-5c3b55a932d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:13:32.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9909" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":128,"skipped":2123,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:13:32.481: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Apr 22 21:13:36.523: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3098 PodName:pod-sharedvolume-9f5afba6-a9ea-4413-855b-ec5f991b923b ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:13:36.523: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 21:13:36.579: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:13:36.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3098" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":129,"skipped":2123,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:13:36.593: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:13:36.617: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 22 21:13:39.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-5574 --namespace=crd-publish-openapi-5574 create -f -'
Apr 22 21:13:39.892: INFO: stderr: ""
Apr 22 21:13:39.892: INFO: stdout: "e2e-test-crd-publish-openapi-3838-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 22 21:13:39.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-5574 --namespace=crd-publish-openapi-5574 delete e2e-test-crd-publish-openapi-3838-crds test-cr'
Apr 22 21:13:39.953: INFO: stderr: ""
Apr 22 21:13:39.953: INFO: stdout: "e2e-test-crd-publish-openapi-3838-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Apr 22 21:13:39.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-5574 --namespace=crd-publish-openapi-5574 apply -f -'
Apr 22 21:13:40.205: INFO: stderr: ""
Apr 22 21:13:40.205: INFO: stdout: "e2e-test-crd-publish-openapi-3838-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 22 21:13:40.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-5574 --namespace=crd-publish-openapi-5574 delete e2e-test-crd-publish-openapi-3838-crds test-cr'
Apr 22 21:13:40.266: INFO: stderr: ""
Apr 22 21:13:40.266: INFO: stdout: "e2e-test-crd-publish-openapi-3838-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr 22 21:13:40.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-5574 explain e2e-test-crd-publish-openapi-3838-crds'
Apr 22 21:13:40.467: INFO: stderr: ""
Apr 22 21:13:40.467: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3838-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:13:43.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5574" for this suite.

• [SLOW TEST:6.763 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":130,"skipped":2163,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:13:43.356: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-3e05e657-21c4-4e67-bebd-4eb4b6b7c2b7
STEP: Creating a pod to test consume configMaps
Apr 22 21:13:43.396: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-343b5ec5-ead2-4263-8186-6a64712aad04" in namespace "projected-1065" to be "Succeeded or Failed"
Apr 22 21:13:43.399: INFO: Pod "pod-projected-configmaps-343b5ec5-ead2-4263-8186-6a64712aad04": Phase="Pending", Reason="", readiness=false. Elapsed: 3.155334ms
Apr 22 21:13:45.405: INFO: Pod "pod-projected-configmaps-343b5ec5-ead2-4263-8186-6a64712aad04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009195195s
STEP: Saw pod success
Apr 22 21:13:45.405: INFO: Pod "pod-projected-configmaps-343b5ec5-ead2-4263-8186-6a64712aad04" satisfied condition "Succeeded or Failed"
Apr 22 21:13:45.407: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod pod-projected-configmaps-343b5ec5-ead2-4263-8186-6a64712aad04 container agnhost-container: <nil>
STEP: delete the pod
Apr 22 21:13:45.420: INFO: Waiting for pod pod-projected-configmaps-343b5ec5-ead2-4263-8186-6a64712aad04 to disappear
Apr 22 21:13:45.422: INFO: Pod pod-projected-configmaps-343b5ec5-ead2-4263-8186-6a64712aad04 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:13:45.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1065" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":131,"skipped":2174,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:13:45.430: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 22 21:13:45.461: INFO: Waiting up to 5m0s for pod "pod-f375258a-44a5-4671-af79-825458742a23" in namespace "emptydir-6136" to be "Succeeded or Failed"
Apr 22 21:13:45.464: INFO: Pod "pod-f375258a-44a5-4671-af79-825458742a23": Phase="Pending", Reason="", readiness=false. Elapsed: 3.493045ms
Apr 22 21:13:47.470: INFO: Pod "pod-f375258a-44a5-4671-af79-825458742a23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009297126s
STEP: Saw pod success
Apr 22 21:13:47.470: INFO: Pod "pod-f375258a-44a5-4671-af79-825458742a23" satisfied condition "Succeeded or Failed"
Apr 22 21:13:47.472: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-f375258a-44a5-4671-af79-825458742a23 container test-container: <nil>
STEP: delete the pod
Apr 22 21:13:47.488: INFO: Waiting for pod pod-f375258a-44a5-4671-af79-825458742a23 to disappear
Apr 22 21:13:47.490: INFO: Pod pod-f375258a-44a5-4671-af79-825458742a23 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:13:47.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6136" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":132,"skipped":2206,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:13:47.497: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-8248
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 22 21:13:47.523: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 22 21:13:47.562: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 22 21:13:49.568: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 21:13:51.568: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 21:13:53.568: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 21:13:55.568: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 21:13:57.567: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 21:13:59.568: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 21:14:01.568: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 22 21:14:01.572: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 22 21:14:03.578: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr 22 21:14:03.582: INFO: The status of Pod netserver-2 is Running (Ready = false)
Apr 22 21:14:05.589: INFO: The status of Pod netserver-2 is Running (Ready = false)
Apr 22 21:14:07.588: INFO: The status of Pod netserver-2 is Running (Ready = true)
Apr 22 21:14:07.592: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Apr 22 21:14:09.615: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Apr 22 21:14:09.615: INFO: Breadth first check of 192.168.224.100 on host 10.0.130.171...
Apr 22 21:14:09.617: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.126.12:9080/dial?request=hostname&protocol=udp&host=192.168.224.100&port=8081&tries=1'] Namespace:pod-network-test-8248 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:14:09.617: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 21:14:09.673: INFO: Waiting for responses: map[]
Apr 22 21:14:09.673: INFO: reached 192.168.224.100 after 0/1 tries
Apr 22 21:14:09.673: INFO: Breadth first check of 192.168.156.223 on host 10.0.130.230...
Apr 22 21:14:09.676: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.126.12:9080/dial?request=hostname&protocol=udp&host=192.168.156.223&port=8081&tries=1'] Namespace:pod-network-test-8248 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:14:09.676: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 21:14:09.727: INFO: Waiting for responses: map[]
Apr 22 21:14:09.727: INFO: reached 192.168.156.223 after 0/1 tries
Apr 22 21:14:09.727: INFO: Breadth first check of 192.168.255.195 on host 10.0.131.122...
Apr 22 21:14:09.748: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.126.12:9080/dial?request=hostname&protocol=udp&host=192.168.255.195&port=8081&tries=1'] Namespace:pod-network-test-8248 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:14:09.748: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 21:14:09.800: INFO: Waiting for responses: map[]
Apr 22 21:14:09.800: INFO: reached 192.168.255.195 after 0/1 tries
Apr 22 21:14:09.800: INFO: Breadth first check of 192.168.126.11 on host 10.0.131.2...
Apr 22 21:14:09.803: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.126.12:9080/dial?request=hostname&protocol=udp&host=192.168.126.11&port=8081&tries=1'] Namespace:pod-network-test-8248 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:14:09.803: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 21:14:09.856: INFO: Waiting for responses: map[]
Apr 22 21:14:09.856: INFO: reached 192.168.126.11 after 0/1 tries
Apr 22 21:14:09.856: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:14:09.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8248" for this suite.

• [SLOW TEST:22.370 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":133,"skipped":2212,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:14:09.866: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 21:14:09.902: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4465e707-f884-4d8f-b603-a33a2e781c46" in namespace "projected-9085" to be "Succeeded or Failed"
Apr 22 21:14:09.904: INFO: Pod "downwardapi-volume-4465e707-f884-4d8f-b603-a33a2e781c46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.104515ms
Apr 22 21:14:11.910: INFO: Pod "downwardapi-volume-4465e707-f884-4d8f-b603-a33a2e781c46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007902234s
STEP: Saw pod success
Apr 22 21:14:11.910: INFO: Pod "downwardapi-volume-4465e707-f884-4d8f-b603-a33a2e781c46" satisfied condition "Succeeded or Failed"
Apr 22 21:14:11.912: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod downwardapi-volume-4465e707-f884-4d8f-b603-a33a2e781c46 container client-container: <nil>
STEP: delete the pod
Apr 22 21:14:11.924: INFO: Waiting for pod downwardapi-volume-4465e707-f884-4d8f-b603-a33a2e781c46 to disappear
Apr 22 21:14:11.926: INFO: Pod downwardapi-volume-4465e707-f884-4d8f-b603-a33a2e781c46 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:14:11.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9085" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":134,"skipped":2231,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:14:11.933: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Apr 22 21:14:11.958: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 22 21:15:11.987: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:15:11.990: INFO: Starting informer...
STEP: Starting pods...
Apr 22 21:15:12.204: INFO: Pod1 is running on ip-10-0-131-122.us-west-2.compute.internal. Tainting Node
Apr 22 21:15:14.421: INFO: Pod2 is running on ip-10-0-131-122.us-west-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Apr 22 21:15:20.789: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Apr 22 21:15:40.827: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:15:40.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8979" for this suite.

• [SLOW TEST:88.918 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":135,"skipped":2233,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:15:40.851: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 21:15:40.888: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb6853b0-ffe2-4ce2-a144-63f44c152953" in namespace "downward-api-8694" to be "Succeeded or Failed"
Apr 22 21:15:40.891: INFO: Pod "downwardapi-volume-eb6853b0-ffe2-4ce2-a144-63f44c152953": Phase="Pending", Reason="", readiness=false. Elapsed: 3.177124ms
Apr 22 21:15:42.900: INFO: Pod "downwardapi-volume-eb6853b0-ffe2-4ce2-a144-63f44c152953": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011651402s
STEP: Saw pod success
Apr 22 21:15:42.900: INFO: Pod "downwardapi-volume-eb6853b0-ffe2-4ce2-a144-63f44c152953" satisfied condition "Succeeded or Failed"
Apr 22 21:15:42.902: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod downwardapi-volume-eb6853b0-ffe2-4ce2-a144-63f44c152953 container client-container: <nil>
STEP: delete the pod
Apr 22 21:15:42.924: INFO: Waiting for pod downwardapi-volume-eb6853b0-ffe2-4ce2-a144-63f44c152953 to disappear
Apr 22 21:15:42.928: INFO: Pod downwardapi-volume-eb6853b0-ffe2-4ce2-a144-63f44c152953 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:15:42.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8694" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":136,"skipped":2240,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:15:42.936: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-6910
STEP: creating service affinity-clusterip in namespace services-6910
STEP: creating replication controller affinity-clusterip in namespace services-6910
I0422 21:15:42.976276      24 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-6910, replica count: 3
I0422 21:15:46.026976      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 22 21:15:46.035: INFO: Creating new exec pod
Apr 22 21:15:49.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-6910 exec execpod-affinityp2hfl -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Apr 22 21:15:49.174: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Apr 22 21:15:49.174: INFO: stdout: ""
Apr 22 21:15:49.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-6910 exec execpod-affinityp2hfl -- /bin/sh -x -c nc -zv -t -w 2 10.0.35.93 80'
Apr 22 21:15:49.288: INFO: stderr: "+ nc -zv -t -w 2 10.0.35.93 80\nConnection to 10.0.35.93 80 port [tcp/http] succeeded!\n"
Apr 22 21:15:49.288: INFO: stdout: ""
Apr 22 21:15:49.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-6910 exec execpod-affinityp2hfl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.35.93:80/ ; done'
Apr 22 21:15:49.453: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.35.93:80/\n"
Apr 22 21:15:49.453: INFO: stdout: "\naffinity-clusterip-5qd88\naffinity-clusterip-5qd88\naffinity-clusterip-5qd88\naffinity-clusterip-5qd88\naffinity-clusterip-5qd88\naffinity-clusterip-5qd88\naffinity-clusterip-5qd88\naffinity-clusterip-5qd88\naffinity-clusterip-5qd88\naffinity-clusterip-5qd88\naffinity-clusterip-5qd88\naffinity-clusterip-5qd88\naffinity-clusterip-5qd88\naffinity-clusterip-5qd88\naffinity-clusterip-5qd88\naffinity-clusterip-5qd88"
Apr 22 21:15:49.453: INFO: Received response from host: affinity-clusterip-5qd88
Apr 22 21:15:49.453: INFO: Received response from host: affinity-clusterip-5qd88
Apr 22 21:15:49.453: INFO: Received response from host: affinity-clusterip-5qd88
Apr 22 21:15:49.453: INFO: Received response from host: affinity-clusterip-5qd88
Apr 22 21:15:49.453: INFO: Received response from host: affinity-clusterip-5qd88
Apr 22 21:15:49.453: INFO: Received response from host: affinity-clusterip-5qd88
Apr 22 21:15:49.453: INFO: Received response from host: affinity-clusterip-5qd88
Apr 22 21:15:49.453: INFO: Received response from host: affinity-clusterip-5qd88
Apr 22 21:15:49.453: INFO: Received response from host: affinity-clusterip-5qd88
Apr 22 21:15:49.453: INFO: Received response from host: affinity-clusterip-5qd88
Apr 22 21:15:49.453: INFO: Received response from host: affinity-clusterip-5qd88
Apr 22 21:15:49.453: INFO: Received response from host: affinity-clusterip-5qd88
Apr 22 21:15:49.453: INFO: Received response from host: affinity-clusterip-5qd88
Apr 22 21:15:49.453: INFO: Received response from host: affinity-clusterip-5qd88
Apr 22 21:15:49.453: INFO: Received response from host: affinity-clusterip-5qd88
Apr 22 21:15:49.453: INFO: Received response from host: affinity-clusterip-5qd88
Apr 22 21:15:49.453: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6910, will wait for the garbage collector to delete the pods
Apr 22 21:15:49.523: INFO: Deleting ReplicationController affinity-clusterip took: 5.979627ms
Apr 22 21:15:50.224: INFO: Terminating ReplicationController affinity-clusterip pods took: 700.133094ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:15:58.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6910" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:15.831 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":137,"skipped":2259,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:15:58.767: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Apr 22 21:15:59.092: INFO: Pod name wrapped-volume-race-20dc3d93-93aa-4629-b3e2-bd802fa6847d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-20dc3d93-93aa-4629-b3e2-bd802fa6847d in namespace emptydir-wrapper-3811, will wait for the garbage collector to delete the pods
Apr 22 21:16:03.201: INFO: Deleting ReplicationController wrapped-volume-race-20dc3d93-93aa-4629-b3e2-bd802fa6847d took: 9.354552ms
Apr 22 21:16:03.901: INFO: Terminating ReplicationController wrapped-volume-race-20dc3d93-93aa-4629-b3e2-bd802fa6847d pods took: 700.118697ms
STEP: Creating RC which spawns configmap-volume pods
Apr 22 21:16:18.735: INFO: Pod name wrapped-volume-race-d5945d3c-1798-4a15-b03e-613c1e26460c: Found 0 pods out of 5
Apr 22 21:16:23.744: INFO: Pod name wrapped-volume-race-d5945d3c-1798-4a15-b03e-613c1e26460c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d5945d3c-1798-4a15-b03e-613c1e26460c in namespace emptydir-wrapper-3811, will wait for the garbage collector to delete the pods
Apr 22 21:16:23.819: INFO: Deleting ReplicationController wrapped-volume-race-d5945d3c-1798-4a15-b03e-613c1e26460c took: 9.324099ms
Apr 22 21:16:23.919: INFO: Terminating ReplicationController wrapped-volume-race-d5945d3c-1798-4a15-b03e-613c1e26460c pods took: 100.057152ms
STEP: Creating RC which spawns configmap-volume pods
Apr 22 21:16:38.655: INFO: Pod name wrapped-volume-race-b5c5bae7-efe6-4b98-b565-bea4be2b19a8: Found 0 pods out of 5
Apr 22 21:16:43.665: INFO: Pod name wrapped-volume-race-b5c5bae7-efe6-4b98-b565-bea4be2b19a8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b5c5bae7-efe6-4b98-b565-bea4be2b19a8 in namespace emptydir-wrapper-3811, will wait for the garbage collector to delete the pods
Apr 22 21:16:45.746: INFO: Deleting ReplicationController wrapped-volume-race-b5c5bae7-efe6-4b98-b565-bea4be2b19a8 took: 9.428943ms
Apr 22 21:16:46.446: INFO: Terminating ReplicationController wrapped-volume-race-b5c5bae7-efe6-4b98-b565-bea4be2b19a8 pods took: 700.125851ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:16:58.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3811" for this suite.

• [SLOW TEST:60.038 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":138,"skipped":2263,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:16:58.806: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 22 21:16:58.840: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 22 21:17:58.873: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:17:58.875: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:17:58.920: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Apr 22 21:17:58.923: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:17:58.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6890" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:17:58.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3770" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.199 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":139,"skipped":2273,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:17:59.005: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:18:04.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2892" for this suite.

• [SLOW TEST:5.062 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":140,"skipped":2285,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:18:04.067: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Apr 22 21:18:04.092: INFO: PodSpec: initContainers in spec.initContainers
Apr 22 21:18:50.171: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-522da6f1-a762-4335-aa5b-f1f410c29828", GenerateName:"", Namespace:"init-container-983", SelfLink:"", UID:"cee52e97-b0dd-4aa8-a2ef-d2b80aa0bdb2", ResourceVersion:"18105", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63754723084, loc:(*time.Location)(0x7975ee0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"92551373"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.126.20/32", "cni.projectcalico.org/podIPs":"192.168.126.20/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001a3ec20), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001a3ec40)}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001a3ec60), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001a3ec80)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001a3eca0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001a3ed00)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-tbxvx", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc006ad4940), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-tbxvx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-tbxvx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-tbxvx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003f3dea8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-131-2.us-west-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000a8b810), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003f3df20)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003f3df40)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003f3df48), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003f3df4c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00114c580), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754723084, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754723084, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754723084, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754723084, loc:(*time.Location)(0x7975ee0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.131.2", PodIP:"192.168.126.20", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.126.20"}}, StartTime:(*v1.Time)(0xc001a3ed20), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000a8b8f0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000a8b9d0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://a448ddc6221ca47568819448c9f41772afc06705bc870ed585bd87b85da57155", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001a3ed60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001a3ed40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc003f3dfcf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:18:50.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-983" for this suite.

• [SLOW TEST:46.136 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":141,"skipped":2330,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:18:50.203: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:18:50.253: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"0c4662db-6747-4965-a046-fa0e39cb285c", Controller:(*bool)(0xc005c7f786), BlockOwnerDeletion:(*bool)(0xc005c7f787)}}
Apr 22 21:18:50.257: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"d0b58b56-482e-479e-aa27-b9aaa21f72c9", Controller:(*bool)(0xc005a1a4ae), BlockOwnerDeletion:(*bool)(0xc005a1a4af)}}
Apr 22 21:18:50.261: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"615871ab-c5d9-4402-819d-874793be3629", Controller:(*bool)(0xc003d1729e), BlockOwnerDeletion:(*bool)(0xc003d1729f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:18:55.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5977" for this suite.

• [SLOW TEST:5.081 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":142,"skipped":2366,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:18:55.285: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:18:57.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5277" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":143,"skipped":2368,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:18:57.367: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-65310b79-4dcc-4140-865b-47608a55894d
STEP: Creating a pod to test consume secrets
Apr 22 21:18:57.402: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8c6591b9-d033-4df0-b3f3-c5702fafde09" in namespace "projected-5187" to be "Succeeded or Failed"
Apr 22 21:18:57.406: INFO: Pod "pod-projected-secrets-8c6591b9-d033-4df0-b3f3-c5702fafde09": Phase="Pending", Reason="", readiness=false. Elapsed: 3.983509ms
Apr 22 21:18:59.412: INFO: Pod "pod-projected-secrets-8c6591b9-d033-4df0-b3f3-c5702fafde09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009624586s
STEP: Saw pod success
Apr 22 21:18:59.412: INFO: Pod "pod-projected-secrets-8c6591b9-d033-4df0-b3f3-c5702fafde09" satisfied condition "Succeeded or Failed"
Apr 22 21:18:59.414: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-projected-secrets-8c6591b9-d033-4df0-b3f3-c5702fafde09 container secret-volume-test: <nil>
STEP: delete the pod
Apr 22 21:18:59.437: INFO: Waiting for pod pod-projected-secrets-8c6591b9-d033-4df0-b3f3-c5702fafde09 to disappear
Apr 22 21:18:59.439: INFO: Pod pod-projected-secrets-8c6591b9-d033-4df0-b3f3-c5702fafde09 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:18:59.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5187" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":144,"skipped":2374,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:18:59.447: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-e02b48f3-7fe2-465d-98c3-9640de13a138
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:18:59.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5870" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":145,"skipped":2393,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:18:59.480: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 22 21:18:59.515: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 22 21:19:59.549: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:19:59.551: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Apr 22 21:20:01.621: INFO: found a healthy node: ip-10-0-131-122.us-west-2.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:20:17.683: INFO: pods created so far: [1 1 1]
Apr 22 21:20:17.683: INFO: length of pods created so far: 3
Apr 22 21:20:21.693: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:20:28.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4141" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:20:28.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4013" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:89.305 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":146,"skipped":2395,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:20:28.786: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Apr 22 21:20:30.828: INFO: &Pod{ObjectMeta:{send-events-d6b809e1-2301-4cce-99d2-eea3daa731d3  events-914  a78bf99a-dd58-4cb2-9f7c-3c3700054eeb 18674 0 2021-04-22 21:20:28 +0000 UTC <nil> <nil> map[name:foo time:810809951] map[cni.projectcalico.org/podIP:192.168.224.103/32 cni.projectcalico.org/podIPs:192.168.224.103/32] [] []  [{e2e.test Update v1 2021-04-22 21:20:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-22 21:20:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-22 21:20:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.224.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-knmgw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-knmgw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-knmgw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-171.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 21:20:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 21:20:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 21:20:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 21:20:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.171,PodIP:192.168.224.103,StartTime:2021-04-22 21:20:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-22 21:20:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://69ac81f8d3ade3ab6e6ebc9e649a4b2bc58f2313c04af810f568f1fb6fc3fd52,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.224.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Apr 22 21:20:32.834: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Apr 22 21:20:34.840: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:20:34.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-914" for this suite.

• [SLOW TEST:6.069 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":147,"skipped":2429,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:20:34.855: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 22 21:20:34.887: INFO: Waiting up to 5m0s for pod "pod-56537fd1-b568-4942-87ec-a0a54571db60" in namespace "emptydir-6487" to be "Succeeded or Failed"
Apr 22 21:20:34.889: INFO: Pod "pod-56537fd1-b568-4942-87ec-a0a54571db60": Phase="Pending", Reason="", readiness=false. Elapsed: 1.893909ms
Apr 22 21:20:36.894: INFO: Pod "pod-56537fd1-b568-4942-87ec-a0a54571db60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007664003s
STEP: Saw pod success
Apr 22 21:20:36.894: INFO: Pod "pod-56537fd1-b568-4942-87ec-a0a54571db60" satisfied condition "Succeeded or Failed"
Apr 22 21:20:36.897: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod pod-56537fd1-b568-4942-87ec-a0a54571db60 container test-container: <nil>
STEP: delete the pod
Apr 22 21:20:36.920: INFO: Waiting for pod pod-56537fd1-b568-4942-87ec-a0a54571db60 to disappear
Apr 22 21:20:36.922: INFO: Pod pod-56537fd1-b568-4942-87ec-a0a54571db60 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:20:36.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6487" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":148,"skipped":2456,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:20:36.929: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Apr 22 21:20:36.959: INFO: Waiting up to 5m0s for pod "var-expansion-31a02997-289d-419a-9225-b02745646259" in namespace "var-expansion-3250" to be "Succeeded or Failed"
Apr 22 21:20:36.961: INFO: Pod "var-expansion-31a02997-289d-419a-9225-b02745646259": Phase="Pending", Reason="", readiness=false. Elapsed: 1.926262ms
Apr 22 21:20:38.967: INFO: Pod "var-expansion-31a02997-289d-419a-9225-b02745646259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008549481s
Apr 22 21:20:40.974: INFO: Pod "var-expansion-31a02997-289d-419a-9225-b02745646259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014868474s
STEP: Saw pod success
Apr 22 21:20:40.974: INFO: Pod "var-expansion-31a02997-289d-419a-9225-b02745646259" satisfied condition "Succeeded or Failed"
Apr 22 21:20:40.976: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod var-expansion-31a02997-289d-419a-9225-b02745646259 container dapi-container: <nil>
STEP: delete the pod
Apr 22 21:20:40.992: INFO: Waiting for pod var-expansion-31a02997-289d-419a-9225-b02745646259 to disappear
Apr 22 21:20:40.995: INFO: Pod var-expansion-31a02997-289d-419a-9225-b02745646259 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:20:40.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3250" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":149,"skipped":2491,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:20:41.003: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-8e4489ae-7e23-4c2d-8758-f7292faccdfb
STEP: Creating a pod to test consume secrets
Apr 22 21:20:41.061: INFO: Waiting up to 5m0s for pod "pod-secrets-74953b27-a892-443f-9ced-2e5a520e991e" in namespace "secrets-7273" to be "Succeeded or Failed"
Apr 22 21:20:41.063: INFO: Pod "pod-secrets-74953b27-a892-443f-9ced-2e5a520e991e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.993145ms
Apr 22 21:20:43.069: INFO: Pod "pod-secrets-74953b27-a892-443f-9ced-2e5a520e991e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00778269s
STEP: Saw pod success
Apr 22 21:20:43.069: INFO: Pod "pod-secrets-74953b27-a892-443f-9ced-2e5a520e991e" satisfied condition "Succeeded or Failed"
Apr 22 21:20:43.071: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-secrets-74953b27-a892-443f-9ced-2e5a520e991e container secret-volume-test: <nil>
STEP: delete the pod
Apr 22 21:20:43.088: INFO: Waiting for pod pod-secrets-74953b27-a892-443f-9ced-2e5a520e991e to disappear
Apr 22 21:20:43.090: INFO: Pod pod-secrets-74953b27-a892-443f-9ced-2e5a520e991e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:20:43.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7273" for this suite.
STEP: Destroying namespace "secret-namespace-7231" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":150,"skipped":2511,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:20:43.102: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6387, will wait for the garbage collector to delete the pods
Apr 22 21:20:47.201: INFO: Deleting Job.batch foo took: 9.623271ms
Apr 22 21:20:47.901: INFO: Terminating Job.batch foo pods took: 700.12002ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:21:28.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6387" for this suite.

• [SLOW TEST:45.627 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":151,"skipped":2518,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:21:28.730: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Apr 22 21:23:29.292: INFO: Successfully updated pod "var-expansion-2a744983-2e51-497c-aad7-a051b8a43a42"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Apr 22 21:23:31.299: INFO: Deleting pod "var-expansion-2a744983-2e51-497c-aad7-a051b8a43a42" in namespace "var-expansion-5180"
Apr 22 21:23:31.307: INFO: Wait up to 5m0s for pod "var-expansion-2a744983-2e51-497c-aad7-a051b8a43a42" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:24:09.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5180" for this suite.

• [SLOW TEST:160.604 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":152,"skipped":2529,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:24:09.334: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-bc507829-0ad8-4285-9d6f-c6088afc3d65
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-bc507829-0ad8-4285-9d6f-c6088afc3d65
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:24:13.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9476" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":153,"skipped":2562,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:24:13.439: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-5354/configmap-test-8cb34d64-0895-4a15-84a0-9fef7919f2b0
STEP: Creating a pod to test consume configMaps
Apr 22 21:24:13.480: INFO: Waiting up to 5m0s for pod "pod-configmaps-e0ae08b8-2272-40d8-92ef-30837d087a4a" in namespace "configmap-5354" to be "Succeeded or Failed"
Apr 22 21:24:13.482: INFO: Pod "pod-configmaps-e0ae08b8-2272-40d8-92ef-30837d087a4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.104172ms
Apr 22 21:24:15.488: INFO: Pod "pod-configmaps-e0ae08b8-2272-40d8-92ef-30837d087a4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008101443s
Apr 22 21:24:17.494: INFO: Pod "pod-configmaps-e0ae08b8-2272-40d8-92ef-30837d087a4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013879849s
STEP: Saw pod success
Apr 22 21:24:17.494: INFO: Pod "pod-configmaps-e0ae08b8-2272-40d8-92ef-30837d087a4a" satisfied condition "Succeeded or Failed"
Apr 22 21:24:17.496: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod pod-configmaps-e0ae08b8-2272-40d8-92ef-30837d087a4a container env-test: <nil>
STEP: delete the pod
Apr 22 21:24:17.517: INFO: Waiting for pod pod-configmaps-e0ae08b8-2272-40d8-92ef-30837d087a4a to disappear
Apr 22 21:24:17.519: INFO: Pod pod-configmaps-e0ae08b8-2272-40d8-92ef-30837d087a4a no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:24:17.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5354" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":154,"skipped":2564,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:24:17.526: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:24:17.553: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 22 21:24:20.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-2661 --namespace=crd-publish-openapi-2661 create -f -'
Apr 22 21:24:20.853: INFO: stderr: ""
Apr 22 21:24:20.853: INFO: stdout: "e2e-test-crd-publish-openapi-9633-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 22 21:24:20.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-2661 --namespace=crd-publish-openapi-2661 delete e2e-test-crd-publish-openapi-9633-crds test-cr'
Apr 22 21:24:20.925: INFO: stderr: ""
Apr 22 21:24:20.925: INFO: stdout: "e2e-test-crd-publish-openapi-9633-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Apr 22 21:24:20.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-2661 --namespace=crd-publish-openapi-2661 apply -f -'
Apr 22 21:24:21.180: INFO: stderr: ""
Apr 22 21:24:21.180: INFO: stdout: "e2e-test-crd-publish-openapi-9633-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 22 21:24:21.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-2661 --namespace=crd-publish-openapi-2661 delete e2e-test-crd-publish-openapi-9633-crds test-cr'
Apr 22 21:24:21.242: INFO: stderr: ""
Apr 22 21:24:21.242: INFO: stdout: "e2e-test-crd-publish-openapi-9633-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr 22 21:24:21.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-2661 explain e2e-test-crd-publish-openapi-9633-crds'
Apr 22 21:24:21.404: INFO: stderr: ""
Apr 22 21:24:21.404: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9633-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:24:24.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2661" for this suite.

• [SLOW TEST:6.765 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":155,"skipped":2573,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:24:24.292: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-f14a345c-425b-4139-9839-7555eb32cd73
STEP: Creating secret with name s-test-opt-upd-6a6b195f-51f2-4c2a-bebd-c129d5c743e6
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-f14a345c-425b-4139-9839-7555eb32cd73
STEP: Updating secret s-test-opt-upd-6a6b195f-51f2-4c2a-bebd-c129d5c743e6
STEP: Creating secret with name s-test-opt-create-e2271df3-ed82-460f-be3c-be932c8dc1ae
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:25:40.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-438" for this suite.

• [SLOW TEST:76.310 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":156,"skipped":2581,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:25:40.602: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Apr 22 21:25:40.634: INFO: created test-event-1
Apr 22 21:25:40.637: INFO: created test-event-2
Apr 22 21:25:40.641: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Apr 22 21:25:40.643: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Apr 22 21:25:40.656: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:25:40.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5231" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":157,"skipped":2604,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:25:40.665: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-5464
STEP: Creating a pod to test atomic-volume-subpath
Apr 22 21:25:40.699: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-5464" in namespace "subpath-7725" to be "Succeeded or Failed"
Apr 22 21:25:40.700: INFO: Pod "pod-subpath-test-secret-5464": Phase="Pending", Reason="", readiness=false. Elapsed: 1.917523ms
Apr 22 21:25:42.704: INFO: Pod "pod-subpath-test-secret-5464": Phase="Running", Reason="", readiness=true. Elapsed: 2.005868136s
Apr 22 21:25:44.711: INFO: Pod "pod-subpath-test-secret-5464": Phase="Running", Reason="", readiness=true. Elapsed: 4.011985225s
Apr 22 21:25:46.717: INFO: Pod "pod-subpath-test-secret-5464": Phase="Running", Reason="", readiness=true. Elapsed: 6.018125004s
Apr 22 21:25:48.723: INFO: Pod "pod-subpath-test-secret-5464": Phase="Running", Reason="", readiness=true. Elapsed: 8.024178553s
Apr 22 21:25:50.729: INFO: Pod "pod-subpath-test-secret-5464": Phase="Running", Reason="", readiness=true. Elapsed: 10.030517263s
Apr 22 21:25:52.733: INFO: Pod "pod-subpath-test-secret-5464": Phase="Running", Reason="", readiness=true. Elapsed: 12.034293186s
Apr 22 21:25:54.739: INFO: Pod "pod-subpath-test-secret-5464": Phase="Running", Reason="", readiness=true. Elapsed: 14.040398881s
Apr 22 21:25:56.745: INFO: Pod "pod-subpath-test-secret-5464": Phase="Running", Reason="", readiness=true. Elapsed: 16.04650104s
Apr 22 21:25:58.751: INFO: Pod "pod-subpath-test-secret-5464": Phase="Running", Reason="", readiness=true. Elapsed: 18.052274637s
Apr 22 21:26:00.757: INFO: Pod "pod-subpath-test-secret-5464": Phase="Running", Reason="", readiness=true. Elapsed: 20.058261359s
Apr 22 21:26:02.761: INFO: Pod "pod-subpath-test-secret-5464": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.062260528s
STEP: Saw pod success
Apr 22 21:26:02.761: INFO: Pod "pod-subpath-test-secret-5464" satisfied condition "Succeeded or Failed"
Apr 22 21:26:02.763: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod pod-subpath-test-secret-5464 container test-container-subpath-secret-5464: <nil>
STEP: delete the pod
Apr 22 21:26:02.785: INFO: Waiting for pod pod-subpath-test-secret-5464 to disappear
Apr 22 21:26:02.787: INFO: Pod pod-subpath-test-secret-5464 no longer exists
STEP: Deleting pod pod-subpath-test-secret-5464
Apr 22 21:26:02.787: INFO: Deleting pod "pod-subpath-test-secret-5464" in namespace "subpath-7725"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:26:02.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7725" for this suite.

• [SLOW TEST:22.132 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":158,"skipped":2615,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:26:02.797: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-39bf978a-43ee-450c-bbae-77ee4150a576
STEP: Creating a pod to test consume secrets
Apr 22 21:26:02.838: INFO: Waiting up to 5m0s for pod "pod-secrets-086200da-27e4-4f76-9d79-cbb2ffb10f98" in namespace "secrets-9536" to be "Succeeded or Failed"
Apr 22 21:26:02.840: INFO: Pod "pod-secrets-086200da-27e4-4f76-9d79-cbb2ffb10f98": Phase="Pending", Reason="", readiness=false. Elapsed: 1.96179ms
Apr 22 21:26:04.846: INFO: Pod "pod-secrets-086200da-27e4-4f76-9d79-cbb2ffb10f98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008360448s
STEP: Saw pod success
Apr 22 21:26:04.846: INFO: Pod "pod-secrets-086200da-27e4-4f76-9d79-cbb2ffb10f98" satisfied condition "Succeeded or Failed"
Apr 22 21:26:04.848: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-secrets-086200da-27e4-4f76-9d79-cbb2ffb10f98 container secret-volume-test: <nil>
STEP: delete the pod
Apr 22 21:26:04.866: INFO: Waiting for pod pod-secrets-086200da-27e4-4f76-9d79-cbb2ffb10f98 to disappear
Apr 22 21:26:04.868: INFO: Pod pod-secrets-086200da-27e4-4f76-9d79-cbb2ffb10f98 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:26:04.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9536" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":159,"skipped":2619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:26:04.875: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 22 21:26:07.924: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:26:07.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8206" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":160,"skipped":2662,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:26:07.946: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Apr 22 21:26:07.971: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:26:23.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2969" for this suite.

• [SLOW TEST:15.576 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":161,"skipped":2736,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:26:23.522: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Apr 22 21:26:28.076: INFO: Successfully updated pod "adopt-release-lkdzn"
STEP: Checking that the Job readopts the Pod
Apr 22 21:26:28.076: INFO: Waiting up to 15m0s for pod "adopt-release-lkdzn" in namespace "job-4416" to be "adopted"
Apr 22 21:26:28.078: INFO: Pod "adopt-release-lkdzn": Phase="Running", Reason="", readiness=true. Elapsed: 2.026348ms
Apr 22 21:26:30.084: INFO: Pod "adopt-release-lkdzn": Phase="Running", Reason="", readiness=true. Elapsed: 2.008609746s
Apr 22 21:26:30.084: INFO: Pod "adopt-release-lkdzn" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Apr 22 21:26:30.597: INFO: Successfully updated pod "adopt-release-lkdzn"
STEP: Checking that the Job releases the Pod
Apr 22 21:26:30.597: INFO: Waiting up to 15m0s for pod "adopt-release-lkdzn" in namespace "job-4416" to be "released"
Apr 22 21:26:30.599: INFO: Pod "adopt-release-lkdzn": Phase="Running", Reason="", readiness=true. Elapsed: 2.024678ms
Apr 22 21:26:32.605: INFO: Pod "adopt-release-lkdzn": Phase="Running", Reason="", readiness=true. Elapsed: 2.008135287s
Apr 22 21:26:32.605: INFO: Pod "adopt-release-lkdzn" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:26:32.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4416" for this suite.

• [SLOW TEST:9.091 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":162,"skipped":2740,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:26:32.613: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 22 21:26:32.912: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 21:26:35.934: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:26:35.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6235" for this suite.
STEP: Destroying namespace "webhook-6235-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":163,"skipped":2756,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:26:36.018: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-3585
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 22 21:26:36.041: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 22 21:26:36.083: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 22 21:26:38.089: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 21:26:40.088: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 21:26:42.088: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 21:26:44.088: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 21:26:46.089: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 21:26:48.089: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 22 21:26:48.093: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 22 21:26:50.099: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 22 21:26:52.099: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 22 21:26:54.099: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 22 21:26:56.099: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 22 21:26:58.100: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr 22 21:26:58.104: INFO: The status of Pod netserver-2 is Running (Ready = true)
Apr 22 21:26:58.108: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Apr 22 21:27:00.139: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Apr 22 21:27:00.139: INFO: Going to poll 192.168.224.105 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Apr 22 21:27:00.140: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.224.105 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3585 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:27:00.140: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 21:27:01.195: INFO: Found all 1 expected endpoints: [netserver-0]
Apr 22 21:27:01.195: INFO: Going to poll 192.168.156.230 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Apr 22 21:27:01.199: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.156.230 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3585 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:27:01.199: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 21:27:02.251: INFO: Found all 1 expected endpoints: [netserver-1]
Apr 22 21:27:02.251: INFO: Going to poll 192.168.255.224 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Apr 22 21:27:02.255: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.255.224 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3585 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:27:02.255: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 21:27:03.302: INFO: Found all 1 expected endpoints: [netserver-2]
Apr 22 21:27:03.302: INFO: Going to poll 192.168.126.27 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Apr 22 21:27:03.306: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.126.27 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3585 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 21:27:03.306: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 21:27:04.361: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:27:04.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3585" for this suite.

• [SLOW TEST:28.356 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":164,"skipped":2818,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:27:04.374: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 22 21:27:04.629: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 22 21:27:06.639: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754723624, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754723624, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754723624, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754723624, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 21:27:09.651: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:27:09.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7692" for this suite.
STEP: Destroying namespace "webhook-7692-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.376 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":165,"skipped":2832,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:27:09.751: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-9c44662e-4cea-4f81-acf1-3cc72369d896
STEP: Creating a pod to test consume secrets
Apr 22 21:27:09.794: INFO: Waiting up to 5m0s for pod "pod-secrets-cfe1e7d5-16ee-4f0f-8460-32c16ec4596d" in namespace "secrets-130" to be "Succeeded or Failed"
Apr 22 21:27:09.797: INFO: Pod "pod-secrets-cfe1e7d5-16ee-4f0f-8460-32c16ec4596d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.068621ms
Apr 22 21:27:11.803: INFO: Pod "pod-secrets-cfe1e7d5-16ee-4f0f-8460-32c16ec4596d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009477764s
STEP: Saw pod success
Apr 22 21:27:11.803: INFO: Pod "pod-secrets-cfe1e7d5-16ee-4f0f-8460-32c16ec4596d" satisfied condition "Succeeded or Failed"
Apr 22 21:27:11.806: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-secrets-cfe1e7d5-16ee-4f0f-8460-32c16ec4596d container secret-volume-test: <nil>
STEP: delete the pod
Apr 22 21:27:11.823: INFO: Waiting for pod pod-secrets-cfe1e7d5-16ee-4f0f-8460-32c16ec4596d to disappear
Apr 22 21:27:11.825: INFO: Pod pod-secrets-cfe1e7d5-16ee-4f0f-8460-32c16ec4596d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:27:11.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-130" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":166,"skipped":2838,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:27:11.832: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4175.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4175.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4175.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4175.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4175.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4175.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4175.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4175.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4175.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4175.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 22 21:27:19.887: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:19.889: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:19.891: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:19.893: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:19.900: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:19.902: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:19.904: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:19.906: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:19.910: INFO: Lookups using dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4175.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4175.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local jessie_udp@dns-test-service-2.dns-4175.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4175.svc.cluster.local]

Apr 22 21:27:24.913: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:24.916: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:24.918: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:24.920: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:24.927: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:24.929: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:24.931: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:24.933: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:24.937: INFO: Lookups using dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4175.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4175.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local jessie_udp@dns-test-service-2.dns-4175.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4175.svc.cluster.local]

Apr 22 21:27:29.913: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:29.916: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:29.918: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:29.920: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:29.927: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:29.929: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:29.931: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:29.933: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:29.937: INFO: Lookups using dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4175.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4175.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local jessie_udp@dns-test-service-2.dns-4175.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4175.svc.cluster.local]

Apr 22 21:27:34.913: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:34.916: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:34.918: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:34.920: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:34.927: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:34.929: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:34.931: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:34.933: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:34.937: INFO: Lookups using dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4175.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4175.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local jessie_udp@dns-test-service-2.dns-4175.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4175.svc.cluster.local]

Apr 22 21:27:39.913: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:39.916: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:39.918: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:39.920: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:39.927: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:39.929: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:39.931: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:39.933: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:39.937: INFO: Lookups using dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4175.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4175.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local jessie_udp@dns-test-service-2.dns-4175.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4175.svc.cluster.local]

Apr 22 21:27:44.928: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local from pod dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db: the server could not find the requested resource (get pods dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db)
Apr 22 21:27:44.938: INFO: Lookups using dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db failed for: [jessie_udp@dns-querier-2.dns-test-service-2.dns-4175.svc.cluster.local]

Apr 22 21:27:49.937: INFO: DNS probes using dns-4175/dns-test-ca7a8c9a-5ea1-419d-883d-517e626ef6db succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:27:49.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4175" for this suite.

• [SLOW TEST:38.167 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":167,"skipped":2849,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:27:50.000: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 22 21:27:50.483: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 22 21:27:52.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754723670, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754723670, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754723670, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754723670, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 21:27:55.511: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:27:55.517: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4803-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:27:56.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9499" for this suite.
STEP: Destroying namespace "webhook-9499-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.661 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":168,"skipped":2902,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:27:56.661: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:27:56.697: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:27:57.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1235" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":169,"skipped":2909,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:27:57.239: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Apr 22 21:27:57.268: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:28:03.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9156" for this suite.

• [SLOW TEST:6.104 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":170,"skipped":2921,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:28:03.344: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 22 21:28:04.017: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 21:28:07.036: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:28:07.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6429" for this suite.
STEP: Destroying namespace "webhook-6429-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":171,"skipped":3004,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:28:07.135: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:28:07.166: INFO: The status of Pod test-webserver-98667484-14b7-4361-8450-3e1e871aaf45 is Pending, waiting for it to be Running (with Ready = true)
Apr 22 21:28:09.173: INFO: The status of Pod test-webserver-98667484-14b7-4361-8450-3e1e871aaf45 is Running (Ready = false)
Apr 22 21:28:11.174: INFO: The status of Pod test-webserver-98667484-14b7-4361-8450-3e1e871aaf45 is Running (Ready = false)
Apr 22 21:28:13.170: INFO: The status of Pod test-webserver-98667484-14b7-4361-8450-3e1e871aaf45 is Running (Ready = false)
Apr 22 21:28:15.174: INFO: The status of Pod test-webserver-98667484-14b7-4361-8450-3e1e871aaf45 is Running (Ready = false)
Apr 22 21:28:17.172: INFO: The status of Pod test-webserver-98667484-14b7-4361-8450-3e1e871aaf45 is Running (Ready = false)
Apr 22 21:28:19.173: INFO: The status of Pod test-webserver-98667484-14b7-4361-8450-3e1e871aaf45 is Running (Ready = false)
Apr 22 21:28:21.173: INFO: The status of Pod test-webserver-98667484-14b7-4361-8450-3e1e871aaf45 is Running (Ready = false)
Apr 22 21:28:23.202: INFO: The status of Pod test-webserver-98667484-14b7-4361-8450-3e1e871aaf45 is Running (Ready = false)
Apr 22 21:28:25.173: INFO: The status of Pod test-webserver-98667484-14b7-4361-8450-3e1e871aaf45 is Running (Ready = false)
Apr 22 21:28:27.171: INFO: The status of Pod test-webserver-98667484-14b7-4361-8450-3e1e871aaf45 is Running (Ready = true)
Apr 22 21:28:27.174: INFO: Container started at 2021-04-22 21:28:08 +0000 UTC, pod became ready at 2021-04-22 21:28:26 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:28:27.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8642" for this suite.

• [SLOW TEST:20.051 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":172,"skipped":3009,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:28:27.187: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9071 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9071;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9071 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9071;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9071.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9071.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9071.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9071.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9071.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9071.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9071.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9071.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9071.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9071.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9071.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9071.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9071.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 221.43.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.43.221_udp@PTR;check="$$(dig +tcp +noall +answer +search 221.43.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.43.221_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9071 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9071;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9071 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9071;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9071.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9071.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9071.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9071.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9071.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9071.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9071.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9071.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9071.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9071.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9071.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9071.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9071.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 221.43.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.43.221_udp@PTR;check="$$(dig +tcp +noall +answer +search 221.43.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.43.221_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 22 21:28:31.250: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:31.252: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:31.254: INFO: Unable to read wheezy_udp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:31.256: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:31.258: INFO: Unable to read wheezy_udp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:31.260: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:31.262: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:31.265: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:31.279: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:31.281: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:31.283: INFO: Unable to read jessie_udp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:31.285: INFO: Unable to read jessie_tcp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:31.287: INFO: Unable to read jessie_udp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:31.289: INFO: Unable to read jessie_tcp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:31.291: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:31.294: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:31.306: INFO: Lookups using dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9071 wheezy_tcp@dns-test-service.dns-9071 wheezy_udp@dns-test-service.dns-9071.svc wheezy_tcp@dns-test-service.dns-9071.svc wheezy_udp@_http._tcp.dns-test-service.dns-9071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9071 jessie_tcp@dns-test-service.dns-9071 jessie_udp@dns-test-service.dns-9071.svc jessie_tcp@dns-test-service.dns-9071.svc jessie_udp@_http._tcp.dns-test-service.dns-9071.svc jessie_tcp@_http._tcp.dns-test-service.dns-9071.svc]

Apr 22 21:28:36.310: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:36.312: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:36.315: INFO: Unable to read wheezy_udp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:36.317: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:36.319: INFO: Unable to read wheezy_udp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:36.321: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:36.323: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:36.325: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:36.340: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:36.342: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:36.344: INFO: Unable to read jessie_udp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:36.346: INFO: Unable to read jessie_tcp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:36.348: INFO: Unable to read jessie_udp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:36.350: INFO: Unable to read jessie_tcp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:36.352: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:36.354: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:36.366: INFO: Lookups using dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9071 wheezy_tcp@dns-test-service.dns-9071 wheezy_udp@dns-test-service.dns-9071.svc wheezy_tcp@dns-test-service.dns-9071.svc wheezy_udp@_http._tcp.dns-test-service.dns-9071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9071 jessie_tcp@dns-test-service.dns-9071 jessie_udp@dns-test-service.dns-9071.svc jessie_tcp@dns-test-service.dns-9071.svc jessie_udp@_http._tcp.dns-test-service.dns-9071.svc jessie_tcp@_http._tcp.dns-test-service.dns-9071.svc]

Apr 22 21:28:41.310: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:41.312: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:41.314: INFO: Unable to read wheezy_udp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:41.316: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:41.318: INFO: Unable to read wheezy_udp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:41.321: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:41.323: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:41.325: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:41.340: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:41.342: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:41.344: INFO: Unable to read jessie_udp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:41.346: INFO: Unable to read jessie_tcp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:41.348: INFO: Unable to read jessie_udp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:41.350: INFO: Unable to read jessie_tcp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:41.353: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:41.355: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:41.368: INFO: Lookups using dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9071 wheezy_tcp@dns-test-service.dns-9071 wheezy_udp@dns-test-service.dns-9071.svc wheezy_tcp@dns-test-service.dns-9071.svc wheezy_udp@_http._tcp.dns-test-service.dns-9071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9071 jessie_tcp@dns-test-service.dns-9071 jessie_udp@dns-test-service.dns-9071.svc jessie_tcp@dns-test-service.dns-9071.svc jessie_udp@_http._tcp.dns-test-service.dns-9071.svc jessie_tcp@_http._tcp.dns-test-service.dns-9071.svc]

Apr 22 21:28:46.310: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:46.312: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:46.314: INFO: Unable to read wheezy_udp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:46.316: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:46.319: INFO: Unable to read wheezy_udp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:46.321: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:46.323: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:46.325: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:46.339: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:46.341: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:46.343: INFO: Unable to read jessie_udp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:46.345: INFO: Unable to read jessie_tcp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:46.347: INFO: Unable to read jessie_udp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:46.349: INFO: Unable to read jessie_tcp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:46.351: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:46.353: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:46.365: INFO: Lookups using dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9071 wheezy_tcp@dns-test-service.dns-9071 wheezy_udp@dns-test-service.dns-9071.svc wheezy_tcp@dns-test-service.dns-9071.svc wheezy_udp@_http._tcp.dns-test-service.dns-9071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9071 jessie_tcp@dns-test-service.dns-9071 jessie_udp@dns-test-service.dns-9071.svc jessie_tcp@dns-test-service.dns-9071.svc jessie_udp@_http._tcp.dns-test-service.dns-9071.svc jessie_tcp@_http._tcp.dns-test-service.dns-9071.svc]

Apr 22 21:28:51.310: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:51.312: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:51.314: INFO: Unable to read wheezy_udp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:51.316: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:51.319: INFO: Unable to read wheezy_udp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:51.321: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:51.323: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:51.325: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:51.339: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:51.341: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:51.343: INFO: Unable to read jessie_udp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:51.345: INFO: Unable to read jessie_tcp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:51.347: INFO: Unable to read jessie_udp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:51.349: INFO: Unable to read jessie_tcp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:51.351: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:51.353: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:51.366: INFO: Lookups using dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9071 wheezy_tcp@dns-test-service.dns-9071 wheezy_udp@dns-test-service.dns-9071.svc wheezy_tcp@dns-test-service.dns-9071.svc wheezy_udp@_http._tcp.dns-test-service.dns-9071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9071 jessie_tcp@dns-test-service.dns-9071 jessie_udp@dns-test-service.dns-9071.svc jessie_tcp@dns-test-service.dns-9071.svc jessie_udp@_http._tcp.dns-test-service.dns-9071.svc jessie_tcp@_http._tcp.dns-test-service.dns-9071.svc]

Apr 22 21:28:56.310: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:56.312: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:56.315: INFO: Unable to read wheezy_udp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:56.317: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:56.319: INFO: Unable to read wheezy_udp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:56.321: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:56.323: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:56.325: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:56.340: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:56.342: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:56.344: INFO: Unable to read jessie_udp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:56.346: INFO: Unable to read jessie_tcp@dns-test-service.dns-9071 from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:56.348: INFO: Unable to read jessie_udp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:56.350: INFO: Unable to read jessie_tcp@dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:56.352: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:56.354: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9071.svc from pod dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6: the server could not find the requested resource (get pods dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6)
Apr 22 21:28:56.366: INFO: Lookups using dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9071 wheezy_tcp@dns-test-service.dns-9071 wheezy_udp@dns-test-service.dns-9071.svc wheezy_tcp@dns-test-service.dns-9071.svc wheezy_udp@_http._tcp.dns-test-service.dns-9071.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9071.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9071 jessie_tcp@dns-test-service.dns-9071 jessie_udp@dns-test-service.dns-9071.svc jessie_tcp@dns-test-service.dns-9071.svc jessie_udp@_http._tcp.dns-test-service.dns-9071.svc jessie_tcp@_http._tcp.dns-test-service.dns-9071.svc]

Apr 22 21:29:01.368: INFO: DNS probes using dns-9071/dns-test-1f2b8e21-85ca-4c80-a4d5-1bb8baf98ba6 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:29:01.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9071" for this suite.

• [SLOW TEST:34.262 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":173,"skipped":3014,"failed":0}
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:29:01.449: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-d49e6e92-44a0-48d2-9fe8-dd130da9c603
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-d49e6e92-44a0-48d2-9fe8-dd130da9c603
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:29:05.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9765" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":174,"skipped":3014,"failed":0}
SSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:29:05.535: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Apr 22 21:29:05.599: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Apr 22 21:29:05.605: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 22 21:29:05.605: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Apr 22 21:29:05.610: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 22 21:29:05.610: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Apr 22 21:29:05.619: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Apr 22 21:29:05.619: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Apr 22 21:29:12.660: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:29:12.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-1399" for this suite.

• [SLOW TEST:7.145 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":175,"skipped":3017,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:29:12.681: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-0b4c056a-acab-4af5-874b-294d278ec1ff
STEP: Creating a pod to test consume configMaps
Apr 22 21:29:12.712: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf9c8507-3400-4332-9444-7f4abefd5ba3" in namespace "projected-7297" to be "Succeeded or Failed"
Apr 22 21:29:12.718: INFO: Pod "pod-projected-configmaps-bf9c8507-3400-4332-9444-7f4abefd5ba3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.806237ms
Apr 22 21:29:14.723: INFO: Pod "pod-projected-configmaps-bf9c8507-3400-4332-9444-7f4abefd5ba3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010540123s
STEP: Saw pod success
Apr 22 21:29:14.723: INFO: Pod "pod-projected-configmaps-bf9c8507-3400-4332-9444-7f4abefd5ba3" satisfied condition "Succeeded or Failed"
Apr 22 21:29:14.725: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-projected-configmaps-bf9c8507-3400-4332-9444-7f4abefd5ba3 container agnhost-container: <nil>
STEP: delete the pod
Apr 22 21:29:14.743: INFO: Waiting for pod pod-projected-configmaps-bf9c8507-3400-4332-9444-7f4abefd5ba3 to disappear
Apr 22 21:29:14.745: INFO: Pod pod-projected-configmaps-bf9c8507-3400-4332-9444-7f4abefd5ba3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:29:14.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7297" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":176,"skipped":3019,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:29:14.752: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 22 21:29:14.778: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 22 21:29:14.784: INFO: Waiting for terminating namespaces to be deleted...
Apr 22 21:29:14.786: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-130-171.us-west-2.compute.internal before test
Apr 22 21:29:14.791: INFO: calico-node-zstxl from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 21:29:14.791: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 21:29:14.791: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 21:29:14.791: INFO: kube-proxy-246ms from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 21:29:14.791: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 21:29:14.791: INFO: pfpod from limitrange-1399 started at 2021-04-22 21:29:07 +0000 UTC (1 container statuses recorded)
Apr 22 21:29:14.791: INFO: 	Container pause ready: true, restart count 0
Apr 22 21:29:14.791: INFO: sonobuoy-e2e-job-f05335387339495e from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 21:29:14.791: INFO: 	Container e2e ready: true, restart count 0
Apr 22 21:29:14.791: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 22 21:29:14.791: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-x9h8q from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 21:29:14.791: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 22 21:29:14.791: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 22 21:29:14.791: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-130-230.us-west-2.compute.internal before test
Apr 22 21:29:14.796: INFO: calico-node-92wx7 from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 21:29:14.796: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 21:29:14.796: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 21:29:14.796: INFO: coredns-74ff55c5b-b2ghx from kube-system started at 2021-04-22 20:42:54 +0000 UTC (1 container statuses recorded)
Apr 22 21:29:14.796: INFO: 	Container coredns ready: true, restart count 0
Apr 22 21:29:14.796: INFO: kube-proxy-z7cgr from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 21:29:14.796: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 21:29:14.796: INFO: sonobuoy from sonobuoy started at 2021-04-22 20:39:15 +0000 UTC (1 container statuses recorded)
Apr 22 21:29:14.796: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 22 21:29:14.796: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-f46mv from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 21:29:14.796: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 22 21:29:14.796: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 22 21:29:14.796: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-131-122.us-west-2.compute.internal before test
Apr 22 21:29:14.801: INFO: calico-node-w7mgb from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 21:29:14.801: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 21:29:14.801: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 21:29:14.801: INFO: coredns-74ff55c5b-l4x4h from kube-system started at 2021-04-22 20:38:22 +0000 UTC (1 container statuses recorded)
Apr 22 21:29:14.801: INFO: 	Container coredns ready: false, restart count 0
Apr 22 21:29:14.801: INFO: kube-proxy-d5r2q from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 21:29:14.801: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 21:29:14.801: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-c9ssg from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 21:29:14.801: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 22 21:29:14.801: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 22 21:29:14.801: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-131-2.us-west-2.compute.internal before test
Apr 22 21:29:14.806: INFO: calico-node-bbjxk from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 21:29:14.806: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 21:29:14.806: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 21:29:14.806: INFO: coredns-74ff55c5b-6hrqm from kube-system started at 2021-04-22 21:15:14 +0000 UTC (1 container statuses recorded)
Apr 22 21:29:14.806: INFO: 	Container coredns ready: true, restart count 0
Apr 22 21:29:14.806: INFO: coredns-74ff55c5b-kj2n4 from kube-system started at 2021-04-22 20:38:22 +0000 UTC (1 container statuses recorded)
Apr 22 21:29:14.806: INFO: 	Container coredns ready: false, restart count 0
Apr 22 21:29:14.806: INFO: kube-proxy-b7qsd from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 21:29:14.806: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 21:29:14.806: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-4m6tb from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 21:29:14.806: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 22 21:29:14.806: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-f4ce1ea2-bbc4-4420-a62a-98e5f6758bc8 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.131.122 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-f4ce1ea2-bbc4-4420-a62a-98e5f6758bc8 off the node ip-10-0-131-122.us-west-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f4ce1ea2-bbc4-4420-a62a-98e5f6758bc8
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:34:18.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5987" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:304.161 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":177,"skipped":3049,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:34:18.913: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 21:34:18.946: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9b685306-916f-4ec6-9e8d-86d4b77d41bd" in namespace "downward-api-7800" to be "Succeeded or Failed"
Apr 22 21:34:18.948: INFO: Pod "downwardapi-volume-9b685306-916f-4ec6-9e8d-86d4b77d41bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.996544ms
Apr 22 21:34:20.953: INFO: Pod "downwardapi-volume-9b685306-916f-4ec6-9e8d-86d4b77d41bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007687855s
STEP: Saw pod success
Apr 22 21:34:20.953: INFO: Pod "downwardapi-volume-9b685306-916f-4ec6-9e8d-86d4b77d41bd" satisfied condition "Succeeded or Failed"
Apr 22 21:34:20.955: INFO: Trying to get logs from node ip-10-0-130-230.us-west-2.compute.internal pod downwardapi-volume-9b685306-916f-4ec6-9e8d-86d4b77d41bd container client-container: <nil>
STEP: delete the pod
Apr 22 21:34:20.979: INFO: Waiting for pod downwardapi-volume-9b685306-916f-4ec6-9e8d-86d4b77d41bd to disappear
Apr 22 21:34:20.981: INFO: Pod downwardapi-volume-9b685306-916f-4ec6-9e8d-86d4b77d41bd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:34:20.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7800" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":178,"skipped":3059,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:34:20.988: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:34:21.015: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 22 21:34:23.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-3311 --namespace=crd-publish-openapi-3311 create -f -'
Apr 22 21:34:24.285: INFO: stderr: ""
Apr 22 21:34:24.285: INFO: stdout: "e2e-test-crd-publish-openapi-4054-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 22 21:34:24.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-3311 --namespace=crd-publish-openapi-3311 delete e2e-test-crd-publish-openapi-4054-crds test-cr'
Apr 22 21:34:24.346: INFO: stderr: ""
Apr 22 21:34:24.346: INFO: stdout: "e2e-test-crd-publish-openapi-4054-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Apr 22 21:34:24.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-3311 --namespace=crd-publish-openapi-3311 apply -f -'
Apr 22 21:34:24.586: INFO: stderr: ""
Apr 22 21:34:24.586: INFO: stdout: "e2e-test-crd-publish-openapi-4054-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 22 21:34:24.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-3311 --namespace=crd-publish-openapi-3311 delete e2e-test-crd-publish-openapi-4054-crds test-cr'
Apr 22 21:34:24.648: INFO: stderr: ""
Apr 22 21:34:24.648: INFO: stdout: "e2e-test-crd-publish-openapi-4054-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Apr 22 21:34:24.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-3311 explain e2e-test-crd-publish-openapi-4054-crds'
Apr 22 21:34:24.802: INFO: stderr: ""
Apr 22 21:34:24.803: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4054-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:34:27.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3311" for this suite.

• [SLOW TEST:6.705 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":179,"skipped":3064,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:34:27.694: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Apr 22 21:34:27.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 create -f -'
Apr 22 21:34:27.948: INFO: stderr: ""
Apr 22 21:34:27.948: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 22 21:34:27.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 22 21:34:28.007: INFO: stderr: ""
Apr 22 21:34:28.007: INFO: stdout: "update-demo-nautilus-5w5lq update-demo-nautilus-xm8vv "
Apr 22 21:34:28.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods update-demo-nautilus-5w5lq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 22 21:34:28.062: INFO: stderr: ""
Apr 22 21:34:28.062: INFO: stdout: ""
Apr 22 21:34:28.062: INFO: update-demo-nautilus-5w5lq is created but not running
Apr 22 21:34:33.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 22 21:34:33.126: INFO: stderr: ""
Apr 22 21:34:33.126: INFO: stdout: "update-demo-nautilus-5w5lq update-demo-nautilus-xm8vv "
Apr 22 21:34:33.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods update-demo-nautilus-5w5lq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 22 21:34:33.180: INFO: stderr: ""
Apr 22 21:34:33.180: INFO: stdout: "true"
Apr 22 21:34:33.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods update-demo-nautilus-5w5lq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 22 21:34:33.240: INFO: stderr: ""
Apr 22 21:34:33.240: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 22 21:34:33.240: INFO: validating pod update-demo-nautilus-5w5lq
Apr 22 21:34:33.244: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 22 21:34:33.244: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 22 21:34:33.244: INFO: update-demo-nautilus-5w5lq is verified up and running
Apr 22 21:34:33.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods update-demo-nautilus-xm8vv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 22 21:34:33.298: INFO: stderr: ""
Apr 22 21:34:33.298: INFO: stdout: "true"
Apr 22 21:34:33.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods update-demo-nautilus-xm8vv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 22 21:34:33.359: INFO: stderr: ""
Apr 22 21:34:33.359: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 22 21:34:33.359: INFO: validating pod update-demo-nautilus-xm8vv
Apr 22 21:34:33.363: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 22 21:34:33.363: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 22 21:34:33.363: INFO: update-demo-nautilus-xm8vv is verified up and running
STEP: scaling down the replication controller
Apr 22 21:34:33.364: INFO: scanned /root for discovery docs: <nil>
Apr 22 21:34:33.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Apr 22 21:34:34.434: INFO: stderr: ""
Apr 22 21:34:34.434: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 22 21:34:34.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 22 21:34:34.495: INFO: stderr: ""
Apr 22 21:34:34.495: INFO: stdout: "update-demo-nautilus-5w5lq update-demo-nautilus-xm8vv "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 22 21:34:39.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 22 21:34:39.559: INFO: stderr: ""
Apr 22 21:34:39.559: INFO: stdout: "update-demo-nautilus-5w5lq "
Apr 22 21:34:39.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods update-demo-nautilus-5w5lq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 22 21:34:39.620: INFO: stderr: ""
Apr 22 21:34:39.620: INFO: stdout: "true"
Apr 22 21:34:39.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods update-demo-nautilus-5w5lq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 22 21:34:39.675: INFO: stderr: ""
Apr 22 21:34:39.675: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 22 21:34:39.675: INFO: validating pod update-demo-nautilus-5w5lq
Apr 22 21:34:39.678: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 22 21:34:39.678: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 22 21:34:39.678: INFO: update-demo-nautilus-5w5lq is verified up and running
STEP: scaling up the replication controller
Apr 22 21:34:39.679: INFO: scanned /root for discovery docs: <nil>
Apr 22 21:34:39.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Apr 22 21:34:40.756: INFO: stderr: ""
Apr 22 21:34:40.756: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 22 21:34:40.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 22 21:34:40.816: INFO: stderr: ""
Apr 22 21:34:40.816: INFO: stdout: "update-demo-nautilus-5w5lq update-demo-nautilus-jdvn8 "
Apr 22 21:34:40.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods update-demo-nautilus-5w5lq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 22 21:34:40.872: INFO: stderr: ""
Apr 22 21:34:40.873: INFO: stdout: "true"
Apr 22 21:34:40.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods update-demo-nautilus-5w5lq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 22 21:34:40.926: INFO: stderr: ""
Apr 22 21:34:40.926: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 22 21:34:40.926: INFO: validating pod update-demo-nautilus-5w5lq
Apr 22 21:34:40.929: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 22 21:34:40.929: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 22 21:34:40.929: INFO: update-demo-nautilus-5w5lq is verified up and running
Apr 22 21:34:40.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods update-demo-nautilus-jdvn8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 22 21:34:40.982: INFO: stderr: ""
Apr 22 21:34:40.982: INFO: stdout: ""
Apr 22 21:34:40.982: INFO: update-demo-nautilus-jdvn8 is created but not running
Apr 22 21:34:45.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 22 21:34:46.048: INFO: stderr: ""
Apr 22 21:34:46.048: INFO: stdout: "update-demo-nautilus-5w5lq update-demo-nautilus-jdvn8 "
Apr 22 21:34:46.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods update-demo-nautilus-5w5lq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 22 21:34:46.104: INFO: stderr: ""
Apr 22 21:34:46.104: INFO: stdout: "true"
Apr 22 21:34:46.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods update-demo-nautilus-5w5lq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 22 21:34:46.159: INFO: stderr: ""
Apr 22 21:34:46.159: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 22 21:34:46.159: INFO: validating pod update-demo-nautilus-5w5lq
Apr 22 21:34:46.162: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 22 21:34:46.162: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 22 21:34:46.162: INFO: update-demo-nautilus-5w5lq is verified up and running
Apr 22 21:34:46.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods update-demo-nautilus-jdvn8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 22 21:34:46.217: INFO: stderr: ""
Apr 22 21:34:46.218: INFO: stdout: "true"
Apr 22 21:34:46.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods update-demo-nautilus-jdvn8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 22 21:34:46.272: INFO: stderr: ""
Apr 22 21:34:46.272: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 22 21:34:46.272: INFO: validating pod update-demo-nautilus-jdvn8
Apr 22 21:34:46.275: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 22 21:34:46.275: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 22 21:34:46.275: INFO: update-demo-nautilus-jdvn8 is verified up and running
STEP: using delete to clean up resources
Apr 22 21:34:46.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 delete --grace-period=0 --force -f -'
Apr 22 21:34:46.335: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 22 21:34:46.335: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 22 21:34:46.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get rc,svc -l name=update-demo --no-headers'
Apr 22 21:34:46.395: INFO: stderr: "No resources found in kubectl-7340 namespace.\n"
Apr 22 21:34:46.395: INFO: stdout: ""
Apr 22 21:34:46.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 22 21:34:46.452: INFO: stderr: ""
Apr 22 21:34:46.452: INFO: stdout: "update-demo-nautilus-5w5lq\nupdate-demo-nautilus-jdvn8\n"
Apr 22 21:34:46.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get rc,svc -l name=update-demo --no-headers'
Apr 22 21:34:47.013: INFO: stderr: "No resources found in kubectl-7340 namespace.\n"
Apr 22 21:34:47.013: INFO: stdout: ""
Apr 22 21:34:47.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7340 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 22 21:34:47.070: INFO: stderr: ""
Apr 22 21:34:47.070: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:34:47.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7340" for this suite.

• [SLOW TEST:19.391 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":180,"skipped":3087,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:34:47.085: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Apr 22 21:34:47.115: INFO: created test-pod-1
Apr 22 21:34:47.120: INFO: created test-pod-2
Apr 22 21:34:47.124: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:34:47.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9480" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":181,"skipped":3142,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:34:47.166: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0422 21:34:53.237327      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 22 21:39:53.242: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:39:53.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3318" for this suite.

• [SLOW TEST:306.085 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":182,"skipped":3198,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:39:53.251: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:39:53.276: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:39:59.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7889" for this suite.

• [SLOW TEST:6.225 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":183,"skipped":3204,"failed":0}
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:39:59.476: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Apr 22 21:39:59.539: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6814  0f239c29-cd4c-47e7-bc0a-bb43fdfe5fcf 23567 0 2021-04-22 21:39:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-22 21:39:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 22 21:39:59.539: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6814  0f239c29-cd4c-47e7-bc0a-bb43fdfe5fcf 23568 0 2021-04-22 21:39:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-22 21:39:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Apr 22 21:39:59.552: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6814  0f239c29-cd4c-47e7-bc0a-bb43fdfe5fcf 23569 0 2021-04-22 21:39:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-22 21:39:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 22 21:39:59.552: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6814  0f239c29-cd4c-47e7-bc0a-bb43fdfe5fcf 23570 0 2021-04-22 21:39:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-22 21:39:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:39:59.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6814" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":184,"skipped":3204,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:39:59.570: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-h2rl
STEP: Creating a pod to test atomic-volume-subpath
Apr 22 21:39:59.626: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-h2rl" in namespace "subpath-2012" to be "Succeeded or Failed"
Apr 22 21:39:59.631: INFO: Pod "pod-subpath-test-projected-h2rl": Phase="Pending", Reason="", readiness=false. Elapsed: 4.438334ms
Apr 22 21:40:01.637: INFO: Pod "pod-subpath-test-projected-h2rl": Phase="Running", Reason="", readiness=true. Elapsed: 2.010881197s
Apr 22 21:40:03.644: INFO: Pod "pod-subpath-test-projected-h2rl": Phase="Running", Reason="", readiness=true. Elapsed: 4.017324394s
Apr 22 21:40:05.650: INFO: Pod "pod-subpath-test-projected-h2rl": Phase="Running", Reason="", readiness=true. Elapsed: 6.023285844s
Apr 22 21:40:07.655: INFO: Pod "pod-subpath-test-projected-h2rl": Phase="Running", Reason="", readiness=true. Elapsed: 8.028543515s
Apr 22 21:40:09.661: INFO: Pod "pod-subpath-test-projected-h2rl": Phase="Running", Reason="", readiness=true. Elapsed: 10.034512307s
Apr 22 21:40:11.667: INFO: Pod "pod-subpath-test-projected-h2rl": Phase="Running", Reason="", readiness=true. Elapsed: 12.040314296s
Apr 22 21:40:13.673: INFO: Pod "pod-subpath-test-projected-h2rl": Phase="Running", Reason="", readiness=true. Elapsed: 14.046356315s
Apr 22 21:40:15.678: INFO: Pod "pod-subpath-test-projected-h2rl": Phase="Running", Reason="", readiness=true. Elapsed: 16.052203823s
Apr 22 21:40:17.684: INFO: Pod "pod-subpath-test-projected-h2rl": Phase="Running", Reason="", readiness=true. Elapsed: 18.058049204s
Apr 22 21:40:19.690: INFO: Pod "pod-subpath-test-projected-h2rl": Phase="Running", Reason="", readiness=true. Elapsed: 20.064152405s
Apr 22 21:40:21.697: INFO: Pod "pod-subpath-test-projected-h2rl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.070405653s
STEP: Saw pod success
Apr 22 21:40:21.697: INFO: Pod "pod-subpath-test-projected-h2rl" satisfied condition "Succeeded or Failed"
Apr 22 21:40:21.699: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-subpath-test-projected-h2rl container test-container-subpath-projected-h2rl: <nil>
STEP: delete the pod
Apr 22 21:40:21.732: INFO: Waiting for pod pod-subpath-test-projected-h2rl to disappear
Apr 22 21:40:21.735: INFO: Pod pod-subpath-test-projected-h2rl no longer exists
STEP: Deleting pod pod-subpath-test-projected-h2rl
Apr 22 21:40:21.735: INFO: Deleting pod "pod-subpath-test-projected-h2rl" in namespace "subpath-2012"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:40:21.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2012" for this suite.

• [SLOW TEST:22.174 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":185,"skipped":3211,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:40:21.744: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 22 21:40:24.307: INFO: Successfully updated pod "pod-update-a3ed5c24-5743-42ff-aef4-0d5b764d8a6c"
STEP: verifying the updated pod is in kubernetes
Apr 22 21:40:24.312: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:40:24.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7840" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":186,"skipped":3219,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:40:24.319: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4398
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-4398
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4398
Apr 22 21:40:24.356: INFO: Found 0 stateful pods, waiting for 1
Apr 22 21:40:34.370: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Apr 22 21:40:34.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-4398 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 22 21:40:34.491: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 22 21:40:34.491: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 22 21:40:34.491: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 22 21:40:34.494: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 22 21:40:44.508: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 22 21:40:44.508: INFO: Waiting for statefulset status.replicas updated to 0
Apr 22 21:40:44.522: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Apr 22 21:40:44.522: INFO: ss-0  ip-10-0-130-171.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:40:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:40:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:40:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:40:24 +0000 UTC  }]
Apr 22 21:40:44.522: INFO: 
Apr 22 21:40:44.522: INFO: StatefulSet ss has not reached scale 3, at 1
Apr 22 21:40:45.526: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997752236s
Apr 22 21:40:46.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993139473s
Apr 22 21:40:47.535: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988984991s
Apr 22 21:40:48.541: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984140971s
Apr 22 21:40:49.546: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978942067s
Apr 22 21:40:50.550: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973937342s
Apr 22 21:40:51.555: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969381271s
Apr 22 21:40:52.560: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.964380585s
Apr 22 21:40:53.565: INFO: Verifying statefulset ss doesn't scale past 3 for another 959.587361ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4398
Apr 22 21:40:54.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-4398 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 22 21:40:54.691: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 22 21:40:54.691: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 22 21:40:54.691: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 22 21:40:54.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-4398 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 22 21:40:54.807: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 22 21:40:54.807: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 22 21:40:54.807: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 22 21:40:54.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-4398 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 22 21:40:54.934: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 22 21:40:54.934: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 22 21:40:54.934: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 22 21:40:54.938: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Apr 22 21:41:04.956: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 22 21:41:04.956: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 22 21:41:04.956: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Apr 22 21:41:04.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-4398 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 22 21:41:05.085: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 22 21:41:05.085: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 22 21:41:05.085: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 22 21:41:05.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-4398 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 22 21:41:05.211: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 22 21:41:05.211: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 22 21:41:05.211: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 22 21:41:05.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-4398 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 22 21:41:05.323: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 22 21:41:05.323: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 22 21:41:05.323: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 22 21:41:05.323: INFO: Waiting for statefulset status.replicas updated to 0
Apr 22 21:41:05.326: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Apr 22 21:41:15.344: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 22 21:41:15.344: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 22 21:41:15.344: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 22 21:41:15.356: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Apr 22 21:41:15.356: INFO: ss-0  ip-10-0-130-171.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:40:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:41:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:41:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:40:24 +0000 UTC  }]
Apr 22 21:41:15.356: INFO: ss-1  ip-10-0-131-122.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:40:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:41:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:41:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:40:44 +0000 UTC  }]
Apr 22 21:41:15.356: INFO: ss-2  ip-10-0-131-2.us-west-2.compute.internal    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:40:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:41:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:41:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:40:44 +0000 UTC  }]
Apr 22 21:41:15.356: INFO: 
Apr 22 21:41:15.356: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 22 21:41:16.360: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Apr 22 21:41:16.360: INFO: ss-0  ip-10-0-130-171.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:40:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:41:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:41:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:40:24 +0000 UTC  }]
Apr 22 21:41:16.360: INFO: ss-1  ip-10-0-131-122.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:40:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:41:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:41:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:40:44 +0000 UTC  }]
Apr 22 21:41:16.360: INFO: ss-2  ip-10-0-131-2.us-west-2.compute.internal    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:40:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:41:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:41:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-22 21:40:44 +0000 UTC  }]
Apr 22 21:41:16.360: INFO: 
Apr 22 21:41:16.360: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 22 21:41:17.365: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993742993s
Apr 22 21:41:18.369: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.989262011s
Apr 22 21:41:19.373: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.984914848s
Apr 22 21:41:20.378: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.98052471s
Apr 22 21:41:21.382: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.976053004s
Apr 22 21:41:22.386: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.971822225s
Apr 22 21:41:23.391: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.967560131s
Apr 22 21:41:24.395: INFO: Verifying statefulset ss doesn't scale past 0 for another 963.075061ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4398
Apr 22 21:41:25.400: INFO: Scaling statefulset ss to 0
Apr 22 21:41:25.415: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 22 21:41:25.417: INFO: Deleting all statefulset in ns statefulset-4398
Apr 22 21:41:25.419: INFO: Scaling statefulset ss to 0
Apr 22 21:41:25.425: INFO: Waiting for statefulset status.replicas updated to 0
Apr 22 21:41:25.427: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:41:25.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4398" for this suite.

• [SLOW TEST:61.127 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":187,"skipped":3222,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:41:25.446: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5894.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5894.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5894.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5894.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 22 21:41:29.502: INFO: DNS probes using dns-test-7b24d868-ce63-4813-b69d-1dbb63eb9db9 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5894.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5894.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5894.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5894.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 22 21:41:33.544: INFO: File wheezy_udp@dns-test-service-3.dns-5894.svc.cluster.local from pod  dns-5894/dns-test-4847da03-adb5-4406-858a-880c5642a3f2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 22 21:41:33.546: INFO: File jessie_udp@dns-test-service-3.dns-5894.svc.cluster.local from pod  dns-5894/dns-test-4847da03-adb5-4406-858a-880c5642a3f2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 22 21:41:33.546: INFO: Lookups using dns-5894/dns-test-4847da03-adb5-4406-858a-880c5642a3f2 failed for: [wheezy_udp@dns-test-service-3.dns-5894.svc.cluster.local jessie_udp@dns-test-service-3.dns-5894.svc.cluster.local]

Apr 22 21:41:38.550: INFO: File wheezy_udp@dns-test-service-3.dns-5894.svc.cluster.local from pod  dns-5894/dns-test-4847da03-adb5-4406-858a-880c5642a3f2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 22 21:41:38.553: INFO: File jessie_udp@dns-test-service-3.dns-5894.svc.cluster.local from pod  dns-5894/dns-test-4847da03-adb5-4406-858a-880c5642a3f2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 22 21:41:38.553: INFO: Lookups using dns-5894/dns-test-4847da03-adb5-4406-858a-880c5642a3f2 failed for: [wheezy_udp@dns-test-service-3.dns-5894.svc.cluster.local jessie_udp@dns-test-service-3.dns-5894.svc.cluster.local]

Apr 22 21:41:43.550: INFO: File wheezy_udp@dns-test-service-3.dns-5894.svc.cluster.local from pod  dns-5894/dns-test-4847da03-adb5-4406-858a-880c5642a3f2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 22 21:41:43.552: INFO: File jessie_udp@dns-test-service-3.dns-5894.svc.cluster.local from pod  dns-5894/dns-test-4847da03-adb5-4406-858a-880c5642a3f2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 22 21:41:43.552: INFO: Lookups using dns-5894/dns-test-4847da03-adb5-4406-858a-880c5642a3f2 failed for: [wheezy_udp@dns-test-service-3.dns-5894.svc.cluster.local jessie_udp@dns-test-service-3.dns-5894.svc.cluster.local]

Apr 22 21:41:48.551: INFO: File wheezy_udp@dns-test-service-3.dns-5894.svc.cluster.local from pod  dns-5894/dns-test-4847da03-adb5-4406-858a-880c5642a3f2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 22 21:41:48.554: INFO: File jessie_udp@dns-test-service-3.dns-5894.svc.cluster.local from pod  dns-5894/dns-test-4847da03-adb5-4406-858a-880c5642a3f2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 22 21:41:48.554: INFO: Lookups using dns-5894/dns-test-4847da03-adb5-4406-858a-880c5642a3f2 failed for: [wheezy_udp@dns-test-service-3.dns-5894.svc.cluster.local jessie_udp@dns-test-service-3.dns-5894.svc.cluster.local]

Apr 22 21:41:53.550: INFO: File wheezy_udp@dns-test-service-3.dns-5894.svc.cluster.local from pod  dns-5894/dns-test-4847da03-adb5-4406-858a-880c5642a3f2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 22 21:41:53.552: INFO: File jessie_udp@dns-test-service-3.dns-5894.svc.cluster.local from pod  dns-5894/dns-test-4847da03-adb5-4406-858a-880c5642a3f2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 22 21:41:53.552: INFO: Lookups using dns-5894/dns-test-4847da03-adb5-4406-858a-880c5642a3f2 failed for: [wheezy_udp@dns-test-service-3.dns-5894.svc.cluster.local jessie_udp@dns-test-service-3.dns-5894.svc.cluster.local]

Apr 22 21:41:58.552: INFO: DNS probes using dns-test-4847da03-adb5-4406-858a-880c5642a3f2 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5894.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5894.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5894.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5894.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 22 21:42:02.609: INFO: DNS probes using dns-test-0e40da51-e823-4e18-a2ee-ab69b6c2eca3 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:42:02.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5894" for this suite.

• [SLOW TEST:37.196 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":188,"skipped":3227,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:42:02.642: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0422 21:42:12.766267      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 22 21:47:12.771: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Apr 22 21:47:12.771: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vlxq" in namespace "gc-9044"
Apr 22 21:47:12.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ddfv" in namespace "gc-9044"
Apr 22 21:47:12.793: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7clf" in namespace "gc-9044"
Apr 22 21:47:12.806: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqvx6" in namespace "gc-9044"
Apr 22 21:47:12.816: INFO: Deleting pod "simpletest-rc-to-be-deleted-nxlpw" in namespace "gc-9044"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:47:12.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9044" for this suite.

• [SLOW TEST:310.193 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":189,"skipped":3243,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:47:12.835: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 22 21:47:13.171: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 22 21:47:15.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754724833, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754724833, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754724833, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754724833, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 21:47:18.200: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:47:18.203: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7989-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:47:19.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6117" for this suite.
STEP: Destroying namespace "webhook-6117-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.487 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":190,"skipped":3246,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:47:19.323: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:47:19.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8088" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":191,"skipped":3266,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:47:19.388: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Apr 22 21:47:19.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-7145 cluster-info'
Apr 22 21:47:19.609: INFO: stderr: ""
Apr 22 21:47:19.609: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:47:19.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7145" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":192,"skipped":3276,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:47:19.617: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-229
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-229
STEP: Deleting pre-stop pod
Apr 22 21:47:28.681: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:47:28.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-229" for this suite.

• [SLOW TEST:9.091 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":193,"skipped":3285,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:47:28.709: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0422 21:47:29.775529      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 22 21:52:29.779: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:52:29.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-594" for this suite.

• [SLOW TEST:301.087 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":194,"skipped":3293,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:52:29.796: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 21:52:29.844: INFO: Waiting up to 5m0s for pod "downwardapi-volume-517c4a72-2748-491f-86f7-b45b581ac6bb" in namespace "downward-api-4878" to be "Succeeded or Failed"
Apr 22 21:52:29.846: INFO: Pod "downwardapi-volume-517c4a72-2748-491f-86f7-b45b581ac6bb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.9631ms
Apr 22 21:52:31.852: INFO: Pod "downwardapi-volume-517c4a72-2748-491f-86f7-b45b581ac6bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00805944s
STEP: Saw pod success
Apr 22 21:52:31.852: INFO: Pod "downwardapi-volume-517c4a72-2748-491f-86f7-b45b581ac6bb" satisfied condition "Succeeded or Failed"
Apr 22 21:52:31.855: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod downwardapi-volume-517c4a72-2748-491f-86f7-b45b581ac6bb container client-container: <nil>
STEP: delete the pod
Apr 22 21:52:31.876: INFO: Waiting for pod downwardapi-volume-517c4a72-2748-491f-86f7-b45b581ac6bb to disappear
Apr 22 21:52:31.878: INFO: Pod downwardapi-volume-517c4a72-2748-491f-86f7-b45b581ac6bb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:52:31.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4878" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":195,"skipped":3308,"failed":0}
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:52:31.885: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Apr 22 21:52:31.910: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:52:36.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1170" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":196,"skipped":3312,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:52:36.240: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-3308
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3308 to expose endpoints map[]
Apr 22 21:52:36.285: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Apr 22 21:52:37.292: INFO: successfully validated that service endpoint-test2 in namespace services-3308 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3308
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3308 to expose endpoints map[pod1:[80]]
Apr 22 21:52:39.312: INFO: successfully validated that service endpoint-test2 in namespace services-3308 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-3308
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3308 to expose endpoints map[pod1:[80] pod2:[80]]
Apr 22 21:52:41.333: INFO: successfully validated that service endpoint-test2 in namespace services-3308 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-3308
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3308 to expose endpoints map[pod2:[80]]
Apr 22 21:52:41.354: INFO: successfully validated that service endpoint-test2 in namespace services-3308 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-3308
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3308 to expose endpoints map[]
Apr 22 21:52:41.373: INFO: successfully validated that service endpoint-test2 in namespace services-3308 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:52:41.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3308" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:5.170 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":197,"skipped":3317,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:52:41.410: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:52:43.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6190" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":198,"skipped":3345,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:52:43.468: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 22 21:52:43.500: INFO: Waiting up to 5m0s for pod "pod-2316df46-cc3f-4ecd-a62d-40854684be32" in namespace "emptydir-701" to be "Succeeded or Failed"
Apr 22 21:52:43.502: INFO: Pod "pod-2316df46-cc3f-4ecd-a62d-40854684be32": Phase="Pending", Reason="", readiness=false. Elapsed: 1.879344ms
Apr 22 21:52:45.508: INFO: Pod "pod-2316df46-cc3f-4ecd-a62d-40854684be32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007800337s
STEP: Saw pod success
Apr 22 21:52:45.508: INFO: Pod "pod-2316df46-cc3f-4ecd-a62d-40854684be32" satisfied condition "Succeeded or Failed"
Apr 22 21:52:45.510: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod pod-2316df46-cc3f-4ecd-a62d-40854684be32 container test-container: <nil>
STEP: delete the pod
Apr 22 21:52:45.533: INFO: Waiting for pod pod-2316df46-cc3f-4ecd-a62d-40854684be32 to disappear
Apr 22 21:52:45.535: INFO: Pod pod-2316df46-cc3f-4ecd-a62d-40854684be32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:52:45.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-701" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":199,"skipped":3364,"failed":0}
SS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:52:45.542: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7890
Apr 22 21:52:47.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-7890 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Apr 22 21:52:47.697: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Apr 22 21:52:47.697: INFO: stdout: "iptables"
Apr 22 21:52:47.697: INFO: proxyMode: iptables
Apr 22 21:52:47.708: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 22 21:52:47.710: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-7890
STEP: creating replication controller affinity-nodeport-timeout in namespace services-7890
I0422 21:52:47.725379      24 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-7890, replica count: 3
I0422 21:52:50.775636      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 22 21:52:50.786: INFO: Creating new exec pod
Apr 22 21:52:53.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-7890 exec execpod-affinityq2z27 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Apr 22 21:52:53.927: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Apr 22 21:52:53.927: INFO: stdout: ""
Apr 22 21:52:53.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-7890 exec execpod-affinityq2z27 -- /bin/sh -x -c nc -zv -t -w 2 10.0.41.197 80'
Apr 22 21:52:54.036: INFO: stderr: "+ nc -zv -t -w 2 10.0.41.197 80\nConnection to 10.0.41.197 80 port [tcp/http] succeeded!\n"
Apr 22 21:52:54.036: INFO: stdout: ""
Apr 22 21:52:54.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-7890 exec execpod-affinityq2z27 -- /bin/sh -x -c nc -zv -t -w 2 10.0.130.171 31637'
Apr 22 21:52:54.148: INFO: stderr: "+ nc -zv -t -w 2 10.0.130.171 31637\nConnection to 10.0.130.171 31637 port [tcp/31637] succeeded!\n"
Apr 22 21:52:54.148: INFO: stdout: ""
Apr 22 21:52:54.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-7890 exec execpod-affinityq2z27 -- /bin/sh -x -c nc -zv -t -w 2 10.0.131.2 31637'
Apr 22 21:52:54.256: INFO: stderr: "+ nc -zv -t -w 2 10.0.131.2 31637\nConnection to 10.0.131.2 31637 port [tcp/31637] succeeded!\n"
Apr 22 21:52:54.256: INFO: stdout: ""
Apr 22 21:52:54.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-7890 exec execpod-affinityq2z27 -- /bin/sh -x -c nc -zv -t -w 2 34.208.188.48 31637'
Apr 22 21:52:54.367: INFO: stderr: "+ nc -zv -t -w 2 34.208.188.48 31637\nConnection to 34.208.188.48 31637 port [tcp/31637] succeeded!\n"
Apr 22 21:52:54.368: INFO: stdout: ""
Apr 22 21:52:54.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-7890 exec execpod-affinityq2z27 -- /bin/sh -x -c nc -zv -t -w 2 54.189.106.97 31637'
Apr 22 21:52:54.480: INFO: stderr: "+ nc -zv -t -w 2 54.189.106.97 31637\nConnection to 54.189.106.97 31637 port [tcp/31637] succeeded!\n"
Apr 22 21:52:54.480: INFO: stdout: ""
Apr 22 21:52:54.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-7890 exec execpod-affinityq2z27 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.130.171:31637/ ; done'
Apr 22 21:52:54.649: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n"
Apr 22 21:52:54.649: INFO: stdout: "\naffinity-nodeport-timeout-cbq7m\naffinity-nodeport-timeout-cbq7m\naffinity-nodeport-timeout-cbq7m\naffinity-nodeport-timeout-cbq7m\naffinity-nodeport-timeout-cbq7m\naffinity-nodeport-timeout-cbq7m\naffinity-nodeport-timeout-cbq7m\naffinity-nodeport-timeout-cbq7m\naffinity-nodeport-timeout-cbq7m\naffinity-nodeport-timeout-cbq7m\naffinity-nodeport-timeout-cbq7m\naffinity-nodeport-timeout-cbq7m\naffinity-nodeport-timeout-cbq7m\naffinity-nodeport-timeout-cbq7m\naffinity-nodeport-timeout-cbq7m\naffinity-nodeport-timeout-cbq7m"
Apr 22 21:52:54.649: INFO: Received response from host: affinity-nodeport-timeout-cbq7m
Apr 22 21:52:54.649: INFO: Received response from host: affinity-nodeport-timeout-cbq7m
Apr 22 21:52:54.649: INFO: Received response from host: affinity-nodeport-timeout-cbq7m
Apr 22 21:52:54.649: INFO: Received response from host: affinity-nodeport-timeout-cbq7m
Apr 22 21:52:54.649: INFO: Received response from host: affinity-nodeport-timeout-cbq7m
Apr 22 21:52:54.649: INFO: Received response from host: affinity-nodeport-timeout-cbq7m
Apr 22 21:52:54.649: INFO: Received response from host: affinity-nodeport-timeout-cbq7m
Apr 22 21:52:54.649: INFO: Received response from host: affinity-nodeport-timeout-cbq7m
Apr 22 21:52:54.649: INFO: Received response from host: affinity-nodeport-timeout-cbq7m
Apr 22 21:52:54.649: INFO: Received response from host: affinity-nodeport-timeout-cbq7m
Apr 22 21:52:54.649: INFO: Received response from host: affinity-nodeport-timeout-cbq7m
Apr 22 21:52:54.649: INFO: Received response from host: affinity-nodeport-timeout-cbq7m
Apr 22 21:52:54.649: INFO: Received response from host: affinity-nodeport-timeout-cbq7m
Apr 22 21:52:54.649: INFO: Received response from host: affinity-nodeport-timeout-cbq7m
Apr 22 21:52:54.649: INFO: Received response from host: affinity-nodeport-timeout-cbq7m
Apr 22 21:52:54.649: INFO: Received response from host: affinity-nodeport-timeout-cbq7m
Apr 22 21:52:54.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-7890 exec execpod-affinityq2z27 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.130.171:31637/'
Apr 22 21:52:54.762: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n"
Apr 22 21:52:54.762: INFO: stdout: "affinity-nodeport-timeout-cbq7m"
Apr 22 21:53:14.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-7890 exec execpod-affinityq2z27 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.130.171:31637/'
Apr 22 21:53:14.883: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.130.171:31637/\n"
Apr 22 21:53:14.883: INFO: stdout: "affinity-nodeport-timeout-ps6d4"
Apr 22 21:53:14.883: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7890, will wait for the garbage collector to delete the pods
Apr 22 21:53:14.962: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 6.750024ms
Apr 22 21:53:15.662: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 700.133119ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:53:28.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7890" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:42.965 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":200,"skipped":3366,"failed":0}
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:53:28.508: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Apr 22 21:53:28.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-421 create -f -'
Apr 22 21:53:28.781: INFO: stderr: ""
Apr 22 21:53:28.781: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 22 21:53:28.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-421 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 22 21:53:28.840: INFO: stderr: ""
Apr 22 21:53:28.840: INFO: stdout: "update-demo-nautilus-4khsl update-demo-nautilus-6fxbx "
Apr 22 21:53:28.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-421 get pods update-demo-nautilus-4khsl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 22 21:53:28.893: INFO: stderr: ""
Apr 22 21:53:28.893: INFO: stdout: ""
Apr 22 21:53:28.893: INFO: update-demo-nautilus-4khsl is created but not running
Apr 22 21:53:33.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-421 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 22 21:53:33.959: INFO: stderr: ""
Apr 22 21:53:33.959: INFO: stdout: "update-demo-nautilus-4khsl update-demo-nautilus-6fxbx "
Apr 22 21:53:33.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-421 get pods update-demo-nautilus-4khsl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 22 21:53:34.016: INFO: stderr: ""
Apr 22 21:53:34.017: INFO: stdout: "true"
Apr 22 21:53:34.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-421 get pods update-demo-nautilus-4khsl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 22 21:53:34.070: INFO: stderr: ""
Apr 22 21:53:34.070: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 22 21:53:34.070: INFO: validating pod update-demo-nautilus-4khsl
Apr 22 21:53:34.074: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 22 21:53:34.074: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 22 21:53:34.074: INFO: update-demo-nautilus-4khsl is verified up and running
Apr 22 21:53:34.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-421 get pods update-demo-nautilus-6fxbx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 22 21:53:34.129: INFO: stderr: ""
Apr 22 21:53:34.129: INFO: stdout: "true"
Apr 22 21:53:34.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-421 get pods update-demo-nautilus-6fxbx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 22 21:53:34.183: INFO: stderr: ""
Apr 22 21:53:34.183: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 22 21:53:34.183: INFO: validating pod update-demo-nautilus-6fxbx
Apr 22 21:53:34.186: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 22 21:53:34.186: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 22 21:53:34.186: INFO: update-demo-nautilus-6fxbx is verified up and running
STEP: using delete to clean up resources
Apr 22 21:53:34.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-421 delete --grace-period=0 --force -f -'
Apr 22 21:53:34.243: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 22 21:53:34.243: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 22 21:53:34.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-421 get rc,svc -l name=update-demo --no-headers'
Apr 22 21:53:34.300: INFO: stderr: "No resources found in kubectl-421 namespace.\n"
Apr 22 21:53:34.300: INFO: stdout: ""
Apr 22 21:53:34.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-421 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 22 21:53:34.357: INFO: stderr: ""
Apr 22 21:53:34.357: INFO: stdout: "update-demo-nautilus-4khsl\nupdate-demo-nautilus-6fxbx\n"
Apr 22 21:53:34.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-421 get rc,svc -l name=update-demo --no-headers'
Apr 22 21:53:34.919: INFO: stderr: "No resources found in kubectl-421 namespace.\n"
Apr 22 21:53:34.919: INFO: stdout: ""
Apr 22 21:53:34.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-421 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 22 21:53:34.986: INFO: stderr: ""
Apr 22 21:53:34.986: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:53:34.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-421" for this suite.

• [SLOW TEST:6.490 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":201,"skipped":3366,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:53:34.998: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:53:35.031: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Apr 22 21:53:37.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-186 --namespace=crd-publish-openapi-186 create -f -'
Apr 22 21:53:38.309: INFO: stderr: ""
Apr 22 21:53:38.309: INFO: stdout: "e2e-test-crd-publish-openapi-7553-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 22 21:53:38.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-186 --namespace=crd-publish-openapi-186 delete e2e-test-crd-publish-openapi-7553-crds test-foo'
Apr 22 21:53:38.401: INFO: stderr: ""
Apr 22 21:53:38.401: INFO: stdout: "e2e-test-crd-publish-openapi-7553-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Apr 22 21:53:38.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-186 --namespace=crd-publish-openapi-186 apply -f -'
Apr 22 21:53:38.649: INFO: stderr: ""
Apr 22 21:53:38.649: INFO: stdout: "e2e-test-crd-publish-openapi-7553-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 22 21:53:38.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-186 --namespace=crd-publish-openapi-186 delete e2e-test-crd-publish-openapi-7553-crds test-foo'
Apr 22 21:53:38.709: INFO: stderr: ""
Apr 22 21:53:38.709: INFO: stdout: "e2e-test-crd-publish-openapi-7553-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Apr 22 21:53:38.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-186 --namespace=crd-publish-openapi-186 create -f -'
Apr 22 21:53:38.908: INFO: rc: 1
Apr 22 21:53:38.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-186 --namespace=crd-publish-openapi-186 apply -f -'
Apr 22 21:53:39.062: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Apr 22 21:53:39.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-186 --namespace=crd-publish-openapi-186 create -f -'
Apr 22 21:53:39.211: INFO: rc: 1
Apr 22 21:53:39.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-186 --namespace=crd-publish-openapi-186 apply -f -'
Apr 22 21:53:39.409: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Apr 22 21:53:39.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-186 explain e2e-test-crd-publish-openapi-7553-crds'
Apr 22 21:53:39.613: INFO: stderr: ""
Apr 22 21:53:39.613: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7553-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Apr 22 21:53:39.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-186 explain e2e-test-crd-publish-openapi-7553-crds.metadata'
Apr 22 21:53:39.823: INFO: stderr: ""
Apr 22 21:53:39.823: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7553-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Apr 22 21:53:39.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-186 explain e2e-test-crd-publish-openapi-7553-crds.spec'
Apr 22 21:53:40.050: INFO: stderr: ""
Apr 22 21:53:40.050: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7553-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Apr 22 21:53:40.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-186 explain e2e-test-crd-publish-openapi-7553-crds.spec.bars'
Apr 22 21:53:40.266: INFO: stderr: ""
Apr 22 21:53:40.266: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7553-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Apr 22 21:53:40.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=crd-publish-openapi-186 explain e2e-test-crd-publish-openapi-7553-crds.spec.bars2'
Apr 22 21:53:40.472: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:53:43.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-186" for this suite.

• [SLOW TEST:8.366 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":202,"skipped":3436,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:53:43.364: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-38446d89-357b-4828-b34b-0c925372476b
STEP: Creating a pod to test consume secrets
Apr 22 21:53:43.400: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-755641d9-63d8-4f70-ad1d-64fd1a84761b" in namespace "projected-4774" to be "Succeeded or Failed"
Apr 22 21:53:43.402: INFO: Pod "pod-projected-secrets-755641d9-63d8-4f70-ad1d-64fd1a84761b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.923374ms
Apr 22 21:53:45.408: INFO: Pod "pod-projected-secrets-755641d9-63d8-4f70-ad1d-64fd1a84761b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007437495s
STEP: Saw pod success
Apr 22 21:53:45.408: INFO: Pod "pod-projected-secrets-755641d9-63d8-4f70-ad1d-64fd1a84761b" satisfied condition "Succeeded or Failed"
Apr 22 21:53:45.410: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-projected-secrets-755641d9-63d8-4f70-ad1d-64fd1a84761b container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 22 21:53:45.426: INFO: Waiting for pod pod-projected-secrets-755641d9-63d8-4f70-ad1d-64fd1a84761b to disappear
Apr 22 21:53:45.428: INFO: Pod pod-projected-secrets-755641d9-63d8-4f70-ad1d-64fd1a84761b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:53:45.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4774" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":203,"skipped":3437,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:53:45.436: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-4b4ee174-dfd3-4461-8625-67ee6f3bea7a
STEP: Creating a pod to test consume secrets
Apr 22 21:53:45.470: INFO: Waiting up to 5m0s for pod "pod-secrets-f2433df4-4616-44e4-8062-8d3ad421dcd6" in namespace "secrets-1090" to be "Succeeded or Failed"
Apr 22 21:53:45.472: INFO: Pod "pod-secrets-f2433df4-4616-44e4-8062-8d3ad421dcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.825181ms
Apr 22 21:53:47.478: INFO: Pod "pod-secrets-f2433df4-4616-44e4-8062-8d3ad421dcd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007560271s
STEP: Saw pod success
Apr 22 21:53:47.478: INFO: Pod "pod-secrets-f2433df4-4616-44e4-8062-8d3ad421dcd6" satisfied condition "Succeeded or Failed"
Apr 22 21:53:47.480: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-secrets-f2433df4-4616-44e4-8062-8d3ad421dcd6 container secret-volume-test: <nil>
STEP: delete the pod
Apr 22 21:53:47.495: INFO: Waiting for pod pod-secrets-f2433df4-4616-44e4-8062-8d3ad421dcd6 to disappear
Apr 22 21:53:47.497: INFO: Pod pod-secrets-f2433df4-4616-44e4-8062-8d3ad421dcd6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:53:47.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1090" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":204,"skipped":3452,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:53:47.505: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 22 21:53:48.198: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 21:53:51.215: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Apr 22 21:53:53.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=webhook-2662 attach --namespace=webhook-2662 to-be-attached-pod -i -c=container1'
Apr 22 21:53:53.313: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:53:53.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2662" for this suite.
STEP: Destroying namespace "webhook-2662-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.856 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":205,"skipped":3456,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:53:53.361: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-18045e15-deb4-4957-9025-583b5326b716
STEP: Creating a pod to test consume configMaps
Apr 22 21:53:53.400: INFO: Waiting up to 5m0s for pod "pod-configmaps-7e1cd32e-7430-48ad-af76-9793c749953e" in namespace "configmap-8378" to be "Succeeded or Failed"
Apr 22 21:53:53.406: INFO: Pod "pod-configmaps-7e1cd32e-7430-48ad-af76-9793c749953e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.730364ms
Apr 22 21:53:55.412: INFO: Pod "pod-configmaps-7e1cd32e-7430-48ad-af76-9793c749953e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011903694s
STEP: Saw pod success
Apr 22 21:53:55.412: INFO: Pod "pod-configmaps-7e1cd32e-7430-48ad-af76-9793c749953e" satisfied condition "Succeeded or Failed"
Apr 22 21:53:55.414: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-configmaps-7e1cd32e-7430-48ad-af76-9793c749953e container agnhost-container: <nil>
STEP: delete the pod
Apr 22 21:53:55.432: INFO: Waiting for pod pod-configmaps-7e1cd32e-7430-48ad-af76-9793c749953e to disappear
Apr 22 21:53:55.434: INFO: Pod pod-configmaps-7e1cd32e-7430-48ad-af76-9793c749953e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:53:55.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8378" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":206,"skipped":3459,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:53:55.441: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6612
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Apr 22 21:53:55.475: INFO: Found 0 stateful pods, waiting for 3
Apr 22 21:54:05.487: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 22 21:54:05.487: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 22 21:54:05.487: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Apr 22 21:54:05.515: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Apr 22 21:54:15.553: INFO: Updating stateful set ss2
Apr 22 21:54:15.557: INFO: Waiting for Pod statefulset-6612/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 22 21:54:25.572: INFO: Waiting for Pod statefulset-6612/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Apr 22 21:54:35.607: INFO: Found 2 stateful pods, waiting for 3
Apr 22 21:54:45.619: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 22 21:54:45.619: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 22 21:54:45.619: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Apr 22 21:54:45.644: INFO: Updating stateful set ss2
Apr 22 21:54:45.648: INFO: Waiting for Pod statefulset-6612/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 22 21:54:55.683: INFO: Updating stateful set ss2
Apr 22 21:54:55.688: INFO: Waiting for StatefulSet statefulset-6612/ss2 to complete update
Apr 22 21:54:55.688: INFO: Waiting for Pod statefulset-6612/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 22 21:55:05.702: INFO: Waiting for StatefulSet statefulset-6612/ss2 to complete update
Apr 22 21:55:05.702: INFO: Waiting for Pod statefulset-6612/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 22 21:55:15.702: INFO: Deleting all statefulset in ns statefulset-6612
Apr 22 21:55:15.704: INFO: Scaling statefulset ss2 to 0
Apr 22 21:55:55.727: INFO: Waiting for statefulset status.replicas updated to 0
Apr 22 21:55:55.730: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:55:55.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6612" for this suite.

• [SLOW TEST:120.312 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":207,"skipped":3470,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:55:55.754: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-1213
STEP: creating service affinity-nodeport-transition in namespace services-1213
STEP: creating replication controller affinity-nodeport-transition in namespace services-1213
I0422 21:55:55.790800      24 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-1213, replica count: 3
I0422 21:55:58.841029      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 22 21:55:58.853: INFO: Creating new exec pod
Apr 22 21:56:01.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-1213 exec execpod-affinityhshwk -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Apr 22 21:56:01.987: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Apr 22 21:56:01.987: INFO: stdout: ""
Apr 22 21:56:01.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-1213 exec execpod-affinityhshwk -- /bin/sh -x -c nc -zv -t -w 2 10.0.63.218 80'
Apr 22 21:56:02.097: INFO: stderr: "+ nc -zv -t -w 2 10.0.63.218 80\nConnection to 10.0.63.218 80 port [tcp/http] succeeded!\n"
Apr 22 21:56:02.097: INFO: stdout: ""
Apr 22 21:56:02.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-1213 exec execpod-affinityhshwk -- /bin/sh -x -c nc -zv -t -w 2 10.0.130.230 30056'
Apr 22 21:56:02.215: INFO: stderr: "+ nc -zv -t -w 2 10.0.130.230 30056\nConnection to 10.0.130.230 30056 port [tcp/30056] succeeded!\n"
Apr 22 21:56:02.215: INFO: stdout: ""
Apr 22 21:56:02.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-1213 exec execpod-affinityhshwk -- /bin/sh -x -c nc -zv -t -w 2 10.0.131.2 30056'
Apr 22 21:56:02.325: INFO: stderr: "+ nc -zv -t -w 2 10.0.131.2 30056\nConnection to 10.0.131.2 30056 port [tcp/30056] succeeded!\n"
Apr 22 21:56:02.325: INFO: stdout: ""
Apr 22 21:56:02.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-1213 exec execpod-affinityhshwk -- /bin/sh -x -c nc -zv -t -w 2 18.237.25.155 30056'
Apr 22 21:56:02.431: INFO: stderr: "+ nc -zv -t -w 2 18.237.25.155 30056\nConnection to 18.237.25.155 30056 port [tcp/30056] succeeded!\n"
Apr 22 21:56:02.431: INFO: stdout: ""
Apr 22 21:56:02.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-1213 exec execpod-affinityhshwk -- /bin/sh -x -c nc -zv -t -w 2 54.189.106.97 30056'
Apr 22 21:56:02.540: INFO: stderr: "+ nc -zv -t -w 2 54.189.106.97 30056\nConnection to 54.189.106.97 30056 port [tcp/30056] succeeded!\n"
Apr 22 21:56:02.540: INFO: stdout: ""
Apr 22 21:56:02.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-1213 exec execpod-affinityhshwk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.130.171:30056/ ; done'
Apr 22 21:56:02.730: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n"
Apr 22 21:56:02.730: INFO: stdout: "\naffinity-nodeport-transition-rtkc6\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-txdpk\naffinity-nodeport-transition-txdpk\naffinity-nodeport-transition-txdpk\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-txdpk\naffinity-nodeport-transition-rtkc6\naffinity-nodeport-transition-txdpk\naffinity-nodeport-transition-rtkc6\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-rtkc6"
Apr 22 21:56:02.730: INFO: Received response from host: affinity-nodeport-transition-rtkc6
Apr 22 21:56:02.730: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.730: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.730: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.730: INFO: Received response from host: affinity-nodeport-transition-txdpk
Apr 22 21:56:02.730: INFO: Received response from host: affinity-nodeport-transition-txdpk
Apr 22 21:56:02.730: INFO: Received response from host: affinity-nodeport-transition-txdpk
Apr 22 21:56:02.730: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.730: INFO: Received response from host: affinity-nodeport-transition-txdpk
Apr 22 21:56:02.730: INFO: Received response from host: affinity-nodeport-transition-rtkc6
Apr 22 21:56:02.730: INFO: Received response from host: affinity-nodeport-transition-txdpk
Apr 22 21:56:02.730: INFO: Received response from host: affinity-nodeport-transition-rtkc6
Apr 22 21:56:02.730: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.730: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.730: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.730: INFO: Received response from host: affinity-nodeport-transition-rtkc6
Apr 22 21:56:02.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-1213 exec execpod-affinityhshwk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.130.171:30056/ ; done'
Apr 22 21:56:02.910: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.130.171:30056/\n"
Apr 22 21:56:02.910: INFO: stdout: "\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl\naffinity-nodeport-transition-trgfl"
Apr 22 21:56:02.910: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.910: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.910: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.910: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.910: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.910: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.910: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.910: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.910: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.910: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.910: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.910: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.910: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.910: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.910: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.910: INFO: Received response from host: affinity-nodeport-transition-trgfl
Apr 22 21:56:02.910: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1213, will wait for the garbage collector to delete the pods
Apr 22 21:56:02.978: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.846294ms
Apr 22 21:56:03.678: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 700.138174ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:56:18.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1213" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:22.970 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":208,"skipped":3564,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:56:18.724: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 22 21:56:19.143: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 21:56:22.161: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 21:56:22.166: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:56:23.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2135" for this suite.
STEP: Destroying namespace "webhook-2135-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":209,"skipped":3574,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:56:23.308: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 21:56:23.361: INFO: Waiting up to 5m0s for pod "downwardapi-volume-43b11021-8fc6-4b7a-ba5a-c738085bfb59" in namespace "projected-1333" to be "Succeeded or Failed"
Apr 22 21:56:23.363: INFO: Pod "downwardapi-volume-43b11021-8fc6-4b7a-ba5a-c738085bfb59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076505ms
Apr 22 21:56:25.369: INFO: Pod "downwardapi-volume-43b11021-8fc6-4b7a-ba5a-c738085bfb59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007980655s
STEP: Saw pod success
Apr 22 21:56:25.369: INFO: Pod "downwardapi-volume-43b11021-8fc6-4b7a-ba5a-c738085bfb59" satisfied condition "Succeeded or Failed"
Apr 22 21:56:25.371: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod downwardapi-volume-43b11021-8fc6-4b7a-ba5a-c738085bfb59 container client-container: <nil>
STEP: delete the pod
Apr 22 21:56:25.392: INFO: Waiting for pod downwardapi-volume-43b11021-8fc6-4b7a-ba5a-c738085bfb59 to disappear
Apr 22 21:56:25.394: INFO: Pod downwardapi-volume-43b11021-8fc6-4b7a-ba5a-c738085bfb59 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:56:25.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1333" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":210,"skipped":3575,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:56:25.404: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 22 21:56:25.894: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 22 21:56:27.904: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725385, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725385, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725385, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725385, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 21:56:30.920: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:56:43.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5097" for this suite.
STEP: Destroying namespace "webhook-5097-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:17.694 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":211,"skipped":3659,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:56:43.098: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:56:43.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6811" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":212,"skipped":3665,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:56:43.158: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 21:56:43.185: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1eedea39-1c83-4a53-8940-7c7e8e1129a1" in namespace "projected-4209" to be "Succeeded or Failed"
Apr 22 21:56:43.189: INFO: Pod "downwardapi-volume-1eedea39-1c83-4a53-8940-7c7e8e1129a1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.860396ms
Apr 22 21:56:45.194: INFO: Pod "downwardapi-volume-1eedea39-1c83-4a53-8940-7c7e8e1129a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009290433s
STEP: Saw pod success
Apr 22 21:56:45.194: INFO: Pod "downwardapi-volume-1eedea39-1c83-4a53-8940-7c7e8e1129a1" satisfied condition "Succeeded or Failed"
Apr 22 21:56:45.197: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod downwardapi-volume-1eedea39-1c83-4a53-8940-7c7e8e1129a1 container client-container: <nil>
STEP: delete the pod
Apr 22 21:56:45.217: INFO: Waiting for pod downwardapi-volume-1eedea39-1c83-4a53-8940-7c7e8e1129a1 to disappear
Apr 22 21:56:45.219: INFO: Pod downwardapi-volume-1eedea39-1c83-4a53-8940-7c7e8e1129a1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:56:45.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4209" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":213,"skipped":3674,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:56:45.226: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6408.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6408.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 22 21:56:49.288: INFO: DNS probes using dns-6408/dns-test-c9f61bcd-b1b8-4eb0-9dd6-d8413a90c171 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:56:49.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6408" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":214,"skipped":3678,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:56:49.314: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:56:49.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2761" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":215,"skipped":3716,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:56:49.350: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 22 21:56:49.378: INFO: Waiting up to 5m0s for pod "downward-api-601d7376-6e4b-40c8-9705-a153c94a1d98" in namespace "downward-api-4103" to be "Succeeded or Failed"
Apr 22 21:56:49.380: INFO: Pod "downward-api-601d7376-6e4b-40c8-9705-a153c94a1d98": Phase="Pending", Reason="", readiness=false. Elapsed: 1.981984ms
Apr 22 21:56:51.386: INFO: Pod "downward-api-601d7376-6e4b-40c8-9705-a153c94a1d98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007389548s
Apr 22 21:56:53.392: INFO: Pod "downward-api-601d7376-6e4b-40c8-9705-a153c94a1d98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013587692s
STEP: Saw pod success
Apr 22 21:56:53.392: INFO: Pod "downward-api-601d7376-6e4b-40c8-9705-a153c94a1d98" satisfied condition "Succeeded or Failed"
Apr 22 21:56:53.394: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod downward-api-601d7376-6e4b-40c8-9705-a153c94a1d98 container dapi-container: <nil>
STEP: delete the pod
Apr 22 21:56:53.408: INFO: Waiting for pod downward-api-601d7376-6e4b-40c8-9705-a153c94a1d98 to disappear
Apr 22 21:56:53.410: INFO: Pod downward-api-601d7376-6e4b-40c8-9705-a153c94a1d98 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:56:53.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4103" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":216,"skipped":3728,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:56:53.417: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9515
Apr 22 21:56:55.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-9515 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Apr 22 21:56:55.584: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Apr 22 21:56:55.584: INFO: stdout: "iptables"
Apr 22 21:56:55.584: INFO: proxyMode: iptables
Apr 22 21:56:55.596: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 22 21:56:55.598: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-9515
STEP: creating replication controller affinity-clusterip-timeout in namespace services-9515
I0422 21:56:55.612544      24 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9515, replica count: 3
I0422 21:56:58.662781      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 22 21:56:58.671: INFO: Creating new exec pod
Apr 22 21:57:01.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-9515 exec execpod-affinitysz5xt -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Apr 22 21:57:01.800: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Apr 22 21:57:01.800: INFO: stdout: ""
Apr 22 21:57:01.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-9515 exec execpod-affinitysz5xt -- /bin/sh -x -c nc -zv -t -w 2 10.0.17.86 80'
Apr 22 21:57:01.951: INFO: stderr: "+ nc -zv -t -w 2 10.0.17.86 80\nConnection to 10.0.17.86 80 port [tcp/http] succeeded!\n"
Apr 22 21:57:01.951: INFO: stdout: ""
Apr 22 21:57:01.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-9515 exec execpod-affinitysz5xt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.17.86:80/ ; done'
Apr 22 21:57:02.119: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n"
Apr 22 21:57:02.119: INFO: stdout: "\naffinity-clusterip-timeout-wvgkr\naffinity-clusterip-timeout-wvgkr\naffinity-clusterip-timeout-wvgkr\naffinity-clusterip-timeout-wvgkr\naffinity-clusterip-timeout-wvgkr\naffinity-clusterip-timeout-wvgkr\naffinity-clusterip-timeout-wvgkr\naffinity-clusterip-timeout-wvgkr\naffinity-clusterip-timeout-wvgkr\naffinity-clusterip-timeout-wvgkr\naffinity-clusterip-timeout-wvgkr\naffinity-clusterip-timeout-wvgkr\naffinity-clusterip-timeout-wvgkr\naffinity-clusterip-timeout-wvgkr\naffinity-clusterip-timeout-wvgkr\naffinity-clusterip-timeout-wvgkr"
Apr 22 21:57:02.119: INFO: Received response from host: affinity-clusterip-timeout-wvgkr
Apr 22 21:57:02.119: INFO: Received response from host: affinity-clusterip-timeout-wvgkr
Apr 22 21:57:02.119: INFO: Received response from host: affinity-clusterip-timeout-wvgkr
Apr 22 21:57:02.119: INFO: Received response from host: affinity-clusterip-timeout-wvgkr
Apr 22 21:57:02.119: INFO: Received response from host: affinity-clusterip-timeout-wvgkr
Apr 22 21:57:02.119: INFO: Received response from host: affinity-clusterip-timeout-wvgkr
Apr 22 21:57:02.119: INFO: Received response from host: affinity-clusterip-timeout-wvgkr
Apr 22 21:57:02.119: INFO: Received response from host: affinity-clusterip-timeout-wvgkr
Apr 22 21:57:02.119: INFO: Received response from host: affinity-clusterip-timeout-wvgkr
Apr 22 21:57:02.119: INFO: Received response from host: affinity-clusterip-timeout-wvgkr
Apr 22 21:57:02.119: INFO: Received response from host: affinity-clusterip-timeout-wvgkr
Apr 22 21:57:02.119: INFO: Received response from host: affinity-clusterip-timeout-wvgkr
Apr 22 21:57:02.119: INFO: Received response from host: affinity-clusterip-timeout-wvgkr
Apr 22 21:57:02.119: INFO: Received response from host: affinity-clusterip-timeout-wvgkr
Apr 22 21:57:02.119: INFO: Received response from host: affinity-clusterip-timeout-wvgkr
Apr 22 21:57:02.119: INFO: Received response from host: affinity-clusterip-timeout-wvgkr
Apr 22 21:57:02.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-9515 exec execpod-affinitysz5xt -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.17.86:80/'
Apr 22 21:57:02.234: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n"
Apr 22 21:57:02.234: INFO: stdout: "affinity-clusterip-timeout-wvgkr"
Apr 22 21:57:22.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-9515 exec execpod-affinitysz5xt -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.17.86:80/'
Apr 22 21:57:22.347: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.17.86:80/\n"
Apr 22 21:57:22.347: INFO: stdout: "affinity-clusterip-timeout-ztvj2"
Apr 22 21:57:22.347: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9515, will wait for the garbage collector to delete the pods
Apr 22 21:57:22.431: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.595006ms
Apr 22 21:57:23.131: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 700.125801ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:57:26.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9515" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:33.546 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":217,"skipped":3729,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:57:26.963: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 22 21:57:27.018: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:27.018: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:27.018: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:27.020: INFO: Number of nodes with available pods: 0
Apr 22 21:57:27.020: INFO: Node ip-10-0-130-171.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:57:28.026: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:28.026: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:28.026: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:28.029: INFO: Number of nodes with available pods: 0
Apr 22 21:57:28.029: INFO: Node ip-10-0-130-171.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:57:29.027: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:29.027: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:29.027: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:29.029: INFO: Number of nodes with available pods: 4
Apr 22 21:57:29.029: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
Apr 22 21:57:29.044: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:29.044: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:29.044: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:29.047: INFO: Number of nodes with available pods: 3
Apr 22 21:57:29.047: INFO: Node ip-10-0-131-2.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:57:30.051: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:30.051: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:30.051: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:30.054: INFO: Number of nodes with available pods: 3
Apr 22 21:57:30.054: INFO: Node ip-10-0-131-2.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:57:31.052: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:31.052: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:31.052: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:31.055: INFO: Number of nodes with available pods: 3
Apr 22 21:57:31.055: INFO: Node ip-10-0-131-2.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:57:32.056: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:32.056: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:32.056: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:32.058: INFO: Number of nodes with available pods: 3
Apr 22 21:57:32.058: INFO: Node ip-10-0-131-2.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:57:33.052: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:33.052: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:33.052: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:33.055: INFO: Number of nodes with available pods: 3
Apr 22 21:57:33.055: INFO: Node ip-10-0-131-2.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:57:34.053: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:34.053: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:34.053: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:34.056: INFO: Number of nodes with available pods: 3
Apr 22 21:57:34.056: INFO: Node ip-10-0-131-2.us-west-2.compute.internal is running more than one daemon pod
Apr 22 21:57:35.053: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:35.053: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:35.053: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 21:57:35.056: INFO: Number of nodes with available pods: 4
Apr 22 21:57:35.056: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7617, will wait for the garbage collector to delete the pods
Apr 22 21:57:35.118: INFO: Deleting DaemonSet.extensions daemon-set took: 8.263691ms
Apr 22 21:57:35.818: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.137267ms
Apr 22 21:57:48.733: INFO: Number of nodes with available pods: 0
Apr 22 21:57:48.733: INFO: Number of running nodes: 0, number of available pods: 0
Apr 22 21:57:48.735: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29114"},"items":null}

Apr 22 21:57:48.737: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29114"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:57:48.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7617" for this suite.

• [SLOW TEST:21.798 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":218,"skipped":3732,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:57:48.761: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:58:16.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6824" for this suite.

• [SLOW TEST:28.097 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":219,"skipped":3763,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:58:16.859: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 22 21:58:16.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-6257 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Apr 22 21:58:16.953: INFO: stderr: ""
Apr 22 21:58:16.953: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Apr 22 21:58:22.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-6257 get pod e2e-test-httpd-pod -o json'
Apr 22 21:58:22.059: INFO: stderr: ""
Apr 22 21:58:22.059: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.255.210/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.255.210/32\"\n        },\n        \"creationTimestamp\": \"2021-04-22T21:58:16Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-04-22T21:58:16Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-04-22T21:58:17Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"192.168.255.210\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-04-22T21:58:18Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6257\",\n        \"resourceVersion\": \"29248\",\n        \"uid\": \"dc41a1d5-07a7-405e-9e60-6978a23432da\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"Always\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-mwgjb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-131-122.us-west-2.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-mwgjb\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-mwgjb\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-22T21:58:16Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-22T21:58:18Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-22T21:58:18Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-22T21:58:16Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://141f355063c642692a9f167b2578224dc41f1696e2b36a7284ca9dce841c72c0\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-04-22T21:58:18Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.131.122\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.255.210\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.255.210\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-04-22T21:58:16Z\"\n    }\n}\n"
STEP: replace the image in the pod
Apr 22 21:58:22.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-6257 replace -f -'
Apr 22 21:58:22.282: INFO: stderr: ""
Apr 22 21:58:22.282: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Apr 22 21:58:22.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-6257 delete pods e2e-test-httpd-pod'
Apr 22 21:58:24.987: INFO: stderr: ""
Apr 22 21:58:24.987: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:58:24.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6257" for this suite.

• [SLOW TEST:8.144 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":220,"skipped":3780,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:58:25.003: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-faae7182-a14f-46e7-9df7-f9b185b86c3a
STEP: Creating configMap with name cm-test-opt-upd-bfda033f-ef06-4ddd-8bee-cd8d7b82f2ec
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-faae7182-a14f-46e7-9df7-f9b185b86c3a
STEP: Updating configmap cm-test-opt-upd-bfda033f-ef06-4ddd-8bee-cd8d7b82f2ec
STEP: Creating configMap with name cm-test-opt-create-f132bf68-434a-4fb9-bf30-1d2099ec205d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:58:29.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9533" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":221,"skipped":3808,"failed":0}
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:58:29.121: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Apr 22 21:58:29.159: INFO: Waiting up to 5m0s for pod "client-containers-c730cf68-f17e-4813-b4f3-b8e518be7872" in namespace "containers-294" to be "Succeeded or Failed"
Apr 22 21:58:29.161: INFO: Pod "client-containers-c730cf68-f17e-4813-b4f3-b8e518be7872": Phase="Pending", Reason="", readiness=false. Elapsed: 1.958114ms
Apr 22 21:58:31.165: INFO: Pod "client-containers-c730cf68-f17e-4813-b4f3-b8e518be7872": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006684274s
STEP: Saw pod success
Apr 22 21:58:31.165: INFO: Pod "client-containers-c730cf68-f17e-4813-b4f3-b8e518be7872" satisfied condition "Succeeded or Failed"
Apr 22 21:58:31.168: INFO: Trying to get logs from node ip-10-0-130-171.us-west-2.compute.internal pod client-containers-c730cf68-f17e-4813-b4f3-b8e518be7872 container agnhost-container: <nil>
STEP: delete the pod
Apr 22 21:58:31.189: INFO: Waiting for pod client-containers-c730cf68-f17e-4813-b4f3-b8e518be7872 to disappear
Apr 22 21:58:31.191: INFO: Pod client-containers-c730cf68-f17e-4813-b4f3-b8e518be7872 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:58:31.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-294" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":222,"skipped":3811,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:58:31.198: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-7387
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7387 to expose endpoints map[]
Apr 22 21:58:31.235: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Apr 22 21:58:32.241: INFO: successfully validated that service multi-endpoint-test in namespace services-7387 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7387
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7387 to expose endpoints map[pod1:[100]]
Apr 22 21:58:34.261: INFO: successfully validated that service multi-endpoint-test in namespace services-7387 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7387
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7387 to expose endpoints map[pod1:[100] pod2:[101]]
Apr 22 21:58:36.281: INFO: successfully validated that service multi-endpoint-test in namespace services-7387 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-7387
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7387 to expose endpoints map[pod2:[101]]
Apr 22 21:58:36.303: INFO: successfully validated that service multi-endpoint-test in namespace services-7387 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7387
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7387 to expose endpoints map[]
Apr 22 21:58:36.322: INFO: successfully validated that service multi-endpoint-test in namespace services-7387 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:58:36.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7387" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:5.150 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":223,"skipped":3823,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:58:36.348: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 22 21:58:36.384: INFO: Waiting up to 5m0s for pod "downward-api-77336216-12d4-49f2-a659-e57c2c0a9bbd" in namespace "downward-api-8903" to be "Succeeded or Failed"
Apr 22 21:58:36.388: INFO: Pod "downward-api-77336216-12d4-49f2-a659-e57c2c0a9bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.955034ms
Apr 22 21:58:38.394: INFO: Pod "downward-api-77336216-12d4-49f2-a659-e57c2c0a9bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009619553s
STEP: Saw pod success
Apr 22 21:58:38.394: INFO: Pod "downward-api-77336216-12d4-49f2-a659-e57c2c0a9bbd" satisfied condition "Succeeded or Failed"
Apr 22 21:58:38.396: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod downward-api-77336216-12d4-49f2-a659-e57c2c0a9bbd container dapi-container: <nil>
STEP: delete the pod
Apr 22 21:58:38.412: INFO: Waiting for pod downward-api-77336216-12d4-49f2-a659-e57c2c0a9bbd to disappear
Apr 22 21:58:38.414: INFO: Pod downward-api-77336216-12d4-49f2-a659-e57c2c0a9bbd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 21:58:38.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8903" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":224,"skipped":3851,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 21:58:38.421: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0422 21:58:39.491160      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 22 22:03:39.496: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:03:39.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1486" for this suite.

• [SLOW TEST:301.091 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":225,"skipped":3855,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:03:39.512: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 22:03:39.553: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr 22 22:03:44.560: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 22 22:03:44.560: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr 22 22:03:46.566: INFO: Creating deployment "test-rollover-deployment"
Apr 22 22:03:46.577: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr 22 22:03:48.586: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr 22 22:03:48.590: INFO: Ensure that both replica sets have 1 created replica
Apr 22 22:03:48.594: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr 22 22:03:48.603: INFO: Updating deployment test-rollover-deployment
Apr 22 22:03:48.603: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr 22 22:03:50.611: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr 22 22:03:50.615: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr 22 22:03:50.619: INFO: all replica sets need to contain the pod-template-hash label
Apr 22 22:03:50.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725826, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725826, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725829, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725826, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 22 22:03:52.627: INFO: all replica sets need to contain the pod-template-hash label
Apr 22 22:03:52.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725826, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725826, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725829, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725826, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 22 22:03:54.628: INFO: all replica sets need to contain the pod-template-hash label
Apr 22 22:03:54.628: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725826, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725826, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725829, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725826, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 22 22:03:56.627: INFO: all replica sets need to contain the pod-template-hash label
Apr 22 22:03:56.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725826, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725826, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725829, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725826, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 22 22:03:58.627: INFO: all replica sets need to contain the pod-template-hash label
Apr 22 22:03:58.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725826, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725826, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725829, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754725826, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 22 22:04:00.627: INFO: 
Apr 22 22:04:00.627: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 22 22:04:00.633: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8348  70f9d7f1-1b0b-49c2-9642-44705591159f 30397 2 2021-04-22 22:03:46 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-04-22 22:03:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-22 22:03:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c9e2c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-04-22 22:03:46 +0000 UTC,LastTransitionTime:2021-04-22 22:03:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-04-22 22:03:59 +0000 UTC,LastTransitionTime:2021-04-22 22:03:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 22 22:04:00.635: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-8348  e960bfcf-b445-4908-bf32-7bae246a5c9d 30388 2 2021-04-22 22:03:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 70f9d7f1-1b0b-49c2-9642-44705591159f 0xc003c9e757 0xc003c9e758}] []  [{kube-controller-manager Update apps/v1 2021-04-22 22:03:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f9d7f1-1b0b-49c2-9642-44705591159f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c9e7e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 22 22:04:00.635: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr 22 22:04:00.635: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8348  17b9aab5-6820-4fd5-bd32-33630a732034 30396 2 2021-04-22 22:03:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 70f9d7f1-1b0b-49c2-9642-44705591159f 0xc003c9e637 0xc003c9e638}] []  [{e2e.test Update apps/v1 2021-04-22 22:03:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-22 22:03:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f9d7f1-1b0b-49c2-9642-44705591159f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c9e6d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 22 22:04:00.635: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-8348  d758608f-775c-4ebc-9d54-9ed37204724e 30352 2 2021-04-22 22:03:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 70f9d7f1-1b0b-49c2-9642-44705591159f 0xc003c9e857 0xc003c9e858}] []  [{kube-controller-manager Update apps/v1 2021-04-22 22:03:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70f9d7f1-1b0b-49c2-9642-44705591159f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c9e8e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 22 22:04:00.637: INFO: Pod "test-rollover-deployment-668db69979-hxrlz" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-hxrlz test-rollover-deployment-668db69979- deployment-8348  b44c1e42-6881-4520-a7b1-9a88adacb24c 30365 0 2021-04-22 22:03:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[cni.projectcalico.org/podIP:192.168.255.213/32 cni.projectcalico.org/podIPs:192.168.255.213/32] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 e960bfcf-b445-4908-bf32-7bae246a5c9d 0xc001f45db7 0xc001f45db8}] []  [{kube-controller-manager Update v1 2021-04-22 22:03:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e960bfcf-b445-4908-bf32-7bae246a5c9d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-04-22 22:03:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-04-22 22:03:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.255.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gdbzf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gdbzf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gdbzf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-122.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:03:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:03:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:03:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:03:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.122,PodIP:192.168.255.213,StartTime:2021-04-22 22:03:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-22 22:03:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://f08520c0968258c7b428128d4e28dcf446d28b5383151caa1afc93fe9f4573a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.255.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:04:00.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8348" for this suite.

• [SLOW TEST:21.141 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":226,"skipped":3860,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:04:00.654: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Apr 22 22:04:00.680: INFO: Major version: 1
STEP: Confirm minor version
Apr 22 22:04:00.680: INFO: cleanMinorVersion: 20
Apr 22 22:04:00.680: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:04:00.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-2029" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":227,"skipped":3870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:04:00.687: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:04:11.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8163" for this suite.

• [SLOW TEST:11.081 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":228,"skipped":3895,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:04:11.768: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8186.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8186.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8186.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8186.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8186.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8186.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8186.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8186.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8186.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8186.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8186.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 226.63.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.63.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.63.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.63.226_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8186.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8186.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8186.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8186.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8186.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8186.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8186.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8186.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8186.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8186.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8186.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 226.63.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.63.226_udp@PTR;check="$$(dig +tcp +noall +answer +search 226.63.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.63.226_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 22 22:04:19.841: INFO: Unable to read wheezy_udp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:19.844: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:19.846: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:19.848: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:19.863: INFO: Unable to read jessie_udp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:19.866: INFO: Unable to read jessie_tcp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:19.868: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:19.870: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:19.882: INFO: Lookups using dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4 failed for: [wheezy_udp@dns-test-service.dns-8186.svc.cluster.local wheezy_tcp@dns-test-service.dns-8186.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local jessie_udp@dns-test-service.dns-8186.svc.cluster.local jessie_tcp@dns-test-service.dns-8186.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local]

Apr 22 22:04:24.886: INFO: Unable to read wheezy_udp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:24.888: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:24.890: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:24.893: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:24.907: INFO: Unable to read jessie_udp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:24.910: INFO: Unable to read jessie_tcp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:24.912: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:24.914: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:24.926: INFO: Lookups using dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4 failed for: [wheezy_udp@dns-test-service.dns-8186.svc.cluster.local wheezy_tcp@dns-test-service.dns-8186.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local jessie_udp@dns-test-service.dns-8186.svc.cluster.local jessie_tcp@dns-test-service.dns-8186.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local]

Apr 22 22:04:29.886: INFO: Unable to read wheezy_udp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:29.888: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:29.890: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:29.893: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:29.907: INFO: Unable to read jessie_udp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:29.910: INFO: Unable to read jessie_tcp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:29.912: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:29.914: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:29.926: INFO: Lookups using dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4 failed for: [wheezy_udp@dns-test-service.dns-8186.svc.cluster.local wheezy_tcp@dns-test-service.dns-8186.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local jessie_udp@dns-test-service.dns-8186.svc.cluster.local jessie_tcp@dns-test-service.dns-8186.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local]

Apr 22 22:04:34.886: INFO: Unable to read wheezy_udp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:34.888: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:34.890: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:34.893: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:34.907: INFO: Unable to read jessie_udp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:34.909: INFO: Unable to read jessie_tcp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:34.911: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:34.913: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:34.926: INFO: Lookups using dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4 failed for: [wheezy_udp@dns-test-service.dns-8186.svc.cluster.local wheezy_tcp@dns-test-service.dns-8186.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local jessie_udp@dns-test-service.dns-8186.svc.cluster.local jessie_tcp@dns-test-service.dns-8186.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local]

Apr 22 22:04:39.886: INFO: Unable to read wheezy_udp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:39.889: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:39.891: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:39.893: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:39.908: INFO: Unable to read jessie_udp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:39.910: INFO: Unable to read jessie_tcp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:39.912: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:39.914: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:39.927: INFO: Lookups using dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4 failed for: [wheezy_udp@dns-test-service.dns-8186.svc.cluster.local wheezy_tcp@dns-test-service.dns-8186.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local jessie_udp@dns-test-service.dns-8186.svc.cluster.local jessie_tcp@dns-test-service.dns-8186.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local]

Apr 22 22:04:44.925: INFO: Unable to read wheezy_udp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:44.928: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:44.930: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:44.932: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:44.947: INFO: Unable to read jessie_udp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:44.949: INFO: Unable to read jessie_tcp@dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:44.951: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:44.953: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local from pod dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4: the server could not find the requested resource (get pods dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4)
Apr 22 22:04:44.965: INFO: Lookups using dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4 failed for: [wheezy_udp@dns-test-service.dns-8186.svc.cluster.local wheezy_tcp@dns-test-service.dns-8186.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local jessie_udp@dns-test-service.dns-8186.svc.cluster.local jessie_tcp@dns-test-service.dns-8186.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8186.svc.cluster.local]

Apr 22 22:04:49.926: INFO: DNS probes using dns-8186/dns-test-2aed0989-0c37-4911-b5a2-ac91f0b8f1c4 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:04:49.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8186" for this suite.

• [SLOW TEST:38.236 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":229,"skipped":3919,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:04:50.005: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 22:04:50.050: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-9e7e0818-3fa0-4b25-8c78-8ac13bff94c5" in namespace "security-context-test-8072" to be "Succeeded or Failed"
Apr 22 22:04:50.052: INFO: Pod "alpine-nnp-false-9e7e0818-3fa0-4b25-8c78-8ac13bff94c5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.940252ms
Apr 22 22:04:52.055: INFO: Pod "alpine-nnp-false-9e7e0818-3fa0-4b25-8c78-8ac13bff94c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005065634s
Apr 22 22:04:54.061: INFO: Pod "alpine-nnp-false-9e7e0818-3fa0-4b25-8c78-8ac13bff94c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010759064s
Apr 22 22:04:54.061: INFO: Pod "alpine-nnp-false-9e7e0818-3fa0-4b25-8c78-8ac13bff94c5" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:04:54.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8072" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":230,"skipped":3938,"failed":0}

------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:04:54.085: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 22 22:04:54.124: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 22 22:05:54.158: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Apr 22 22:05:54.177: INFO: Created pod: pod0-sched-preemption-low-priority
Apr 22 22:05:54.193: INFO: Created pod: pod1-sched-preemption-medium-priority
Apr 22 22:05:54.206: INFO: Created pod: pod2-sched-preemption-medium-priority
Apr 22 22:05:54.220: INFO: Created pod: pod3-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:06:20.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3490" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:86.242 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":231,"skipped":3938,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:06:20.327: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Apr 22 22:06:20.373: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9999  5eb8c4c2-d8c4-43a1-9395-546d690cf662 31053 0 2021-04-22 22:06:20 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-04-22 22:06:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 22 22:06:20.373: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9999  5eb8c4c2-d8c4-43a1-9395-546d690cf662 31054 0 2021-04-22 22:06:20 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-04-22 22:06:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:06:20.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9999" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":232,"skipped":3968,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:06:20.381: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Apr 22 22:06:20.404: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Apr 22 22:06:31.680: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 22:06:34.561: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:06:45.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6896" for this suite.

• [SLOW TEST:25.583 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":233,"skipped":3971,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:06:45.964: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-2319/configmap-test-a589a387-078f-4cfa-967f-4a00482fdacb
STEP: Creating a pod to test consume configMaps
Apr 22 22:06:46.008: INFO: Waiting up to 5m0s for pod "pod-configmaps-f3c3dbe2-043b-458d-ae64-31a0c741d79e" in namespace "configmap-2319" to be "Succeeded or Failed"
Apr 22 22:06:46.010: INFO: Pod "pod-configmaps-f3c3dbe2-043b-458d-ae64-31a0c741d79e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.973423ms
Apr 22 22:06:48.016: INFO: Pod "pod-configmaps-f3c3dbe2-043b-458d-ae64-31a0c741d79e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007960359s
STEP: Saw pod success
Apr 22 22:06:48.016: INFO: Pod "pod-configmaps-f3c3dbe2-043b-458d-ae64-31a0c741d79e" satisfied condition "Succeeded or Failed"
Apr 22 22:06:48.018: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-configmaps-f3c3dbe2-043b-458d-ae64-31a0c741d79e container env-test: <nil>
STEP: delete the pod
Apr 22 22:06:48.038: INFO: Waiting for pod pod-configmaps-f3c3dbe2-043b-458d-ae64-31a0c741d79e to disappear
Apr 22 22:06:48.040: INFO: Pod pod-configmaps-f3c3dbe2-043b-458d-ae64-31a0c741d79e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:06:48.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2319" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":234,"skipped":3989,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:06:48.051: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 22:06:48.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2961 create -f -'
Apr 22 22:06:48.438: INFO: stderr: ""
Apr 22 22:06:48.438: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Apr 22 22:06:48.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2961 create -f -'
Apr 22 22:06:48.659: INFO: stderr: ""
Apr 22 22:06:48.659: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 22 22:06:49.664: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 22 22:06:49.664: INFO: Found 0 / 1
Apr 22 22:06:50.664: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 22 22:06:50.664: INFO: Found 1 / 1
Apr 22 22:06:50.664: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 22 22:06:50.666: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 22 22:06:50.666: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 22 22:06:50.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2961 describe pod agnhost-primary-d6hsh'
Apr 22 22:06:50.736: INFO: stderr: ""
Apr 22 22:06:50.737: INFO: stdout: "Name:         agnhost-primary-d6hsh\nNamespace:    kubectl-2961\nPriority:     0\nNode:         ip-10-0-131-122.us-west-2.compute.internal/10.0.131.122\nStart Time:   Thu, 22 Apr 2021 22:06:48 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 192.168.255.223/32\n              cni.projectcalico.org/podIPs: 192.168.255.223/32\nStatus:       Running\nIP:           192.168.255.223\nIPs:\n  IP:           192.168.255.223\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://00940828d85792e70ec6e5498221a8ed64e3a6ada81e5fb4b08ecee553736543\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 22 Apr 2021 22:06:49 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-5xhsx (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-5xhsx:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-5xhsx\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-2961/agnhost-primary-d6hsh to ip-10-0-131-122.us-west-2.compute.internal\n  Normal  Pulling    1s    kubelet            Pulling image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\"\n  Normal  Pulled     1s    kubelet            Successfully pulled image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" in 214.570576ms\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Apr 22 22:06:50.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2961 describe rc agnhost-primary'
Apr 22 22:06:50.806: INFO: stderr: ""
Apr 22 22:06:50.806: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2961\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-d6hsh\n"
Apr 22 22:06:50.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2961 describe service agnhost-primary'
Apr 22 22:06:50.868: INFO: stderr: ""
Apr 22 22:06:50.868: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2961\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.0.9.141\nIPs:               10.0.9.141\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.255.223:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr 22 22:06:50.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2961 describe node ip-10-0-130-171.us-west-2.compute.internal'
Apr 22 22:06:50.947: INFO: stderr: ""
Apr 22 22:06:50.947: INFO: stdout: "Name:               ip-10-0-130-171.us-west-2.compute.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.2xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-west-2\n                    failure-domain.beta.kubernetes.io/zone=us-west-2c\n                    konvoy.mesosphere.com/inventory_hostname=10.0.130.171\n                    konvoy.mesosphere.com/node_pool=worker\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-130-171.us-west-2.compute.internal\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=m5.2xlarge\n                    topology.kubernetes.io/region=us-west-2\n                    topology.kubernetes.io/zone=us-west-2c\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.130.171/32\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.224.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 22 Apr 2021 20:37:38 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-130-171.us-west-2.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 22 Apr 2021 22:06:41 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 22 Apr 2021 20:38:20 +0000   Thu, 22 Apr 2021 20:38:20 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 22 Apr 2021 22:06:01 +0000   Thu, 22 Apr 2021 20:37:38 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 22 Apr 2021 22:06:01 +0000   Thu, 22 Apr 2021 20:37:38 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 22 Apr 2021 22:06:01 +0000   Thu, 22 Apr 2021 20:37:38 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 22 Apr 2021 22:06:01 +0000   Thu, 22 Apr 2021 20:38:18 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.130.171\n  ExternalIP:   34.208.188.48\n  Hostname:     ip-10-0-130-171.us-west-2.compute.internal\n  InternalDNS:  ip-10-0-130-171.us-west-2.compute.internal\n  ExternalDNS:  ec2-34-208-188-48.us-west-2.compute.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         8\n  ephemeral-storage:           83874796Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      32304592Ki\n  pods:                        110\n  scheduling.k8s.io/foo:       3\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         8\n  ephemeral-storage:           77299011866\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      32202192Ki\n  pods:                        110\n  scheduling.k8s.io/foo:       3\nSystem Info:\n  Machine ID:                 cc2c86fe566741e6a2ff6d399c5d5daa\n  System UUID:                EC2F6D54-46B7-E564-93DF-25FD3151E65F\n  Boot ID:                    28fac304-bec4-480c-900f-9e4a419515dc\n  Kernel Version:             3.10.0-1160.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.3.9\n  Kubelet Version:            v1.20.6\n  Kube-Proxy Version:         v1.20.6\nProviderID:                   aws:///us-west-2c/i-0c2fc9dc88300fefa\nNon-terminated Pods:          (4 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-zstxl                                          300m (3%)     0 (0%)      32M (0%)         0 (0%)         88m\n  kube-system                 kube-proxy-246ms                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         89m\n  sonobuoy                    sonobuoy-e2e-job-f05335387339495e                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         87m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-x9h8q    0 (0%)        0 (0%)      0 (0%)           0 (0%)         87m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests   Limits\n  --------                    --------   ------\n  cpu                         300m (3%)  0 (0%)\n  memory                      32M (0%)   0 (0%)\n  ephemeral-storage           0 (0%)     0 (0%)\n  hugepages-1Gi               0 (0%)     0 (0%)\n  hugepages-2Mi               0 (0%)     0 (0%)\n  attachable-volumes-aws-ebs  0          0\n  scheduling.k8s.io/foo       0          0\nEvents:                       <none>\n"
Apr 22 22:06:50.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-2961 describe namespace kubectl-2961'
Apr 22 22:06:51.009: INFO: stderr: ""
Apr 22 22:06:51.009: INFO: stdout: "Name:         kubectl-2961\nLabels:       e2e-framework=kubectl\n              e2e-run=3c32a897-c486-4ee7-bcc8-1f01dd37d2e8\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:06:51.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2961" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":235,"skipped":4040,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:06:51.021: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 22 22:06:55.103: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 22 22:06:55.105: INFO: Pod pod-with-poststart-http-hook still exists
Apr 22 22:06:57.105: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 22 22:06:57.110: INFO: Pod pod-with-poststart-http-hook still exists
Apr 22 22:06:59.105: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 22 22:06:59.110: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:06:59.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4804" for this suite.

• [SLOW TEST:8.098 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":236,"skipped":4053,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:06:59.119: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:06:59.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-717" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":237,"skipped":4059,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:06:59.181: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 22:06:59.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-3427 version'
Apr 22 22:06:59.301: INFO: stderr: ""
Apr 22 22:06:59.301: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"8a62859e515889f07e3e3be6a1080413f17cf2c3\", GitTreeState:\"clean\", BuildDate:\"2021-04-15T03:28:42Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"8a62859e515889f07e3e3be6a1080413f17cf2c3\", GitTreeState:\"clean\", BuildDate:\"2021-04-15T03:19:55Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:06:59.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3427" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":238,"skipped":4096,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:06:59.309: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 22:06:59.337: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d4e0da8e-3a8e-424c-8cb7-79e98b3ad03c" in namespace "downward-api-8718" to be "Succeeded or Failed"
Apr 22 22:06:59.338: INFO: Pod "downwardapi-volume-d4e0da8e-3a8e-424c-8cb7-79e98b3ad03c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.759531ms
Apr 22 22:07:01.344: INFO: Pod "downwardapi-volume-d4e0da8e-3a8e-424c-8cb7-79e98b3ad03c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007477271s
STEP: Saw pod success
Apr 22 22:07:01.344: INFO: Pod "downwardapi-volume-d4e0da8e-3a8e-424c-8cb7-79e98b3ad03c" satisfied condition "Succeeded or Failed"
Apr 22 22:07:01.346: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod downwardapi-volume-d4e0da8e-3a8e-424c-8cb7-79e98b3ad03c container client-container: <nil>
STEP: delete the pod
Apr 22 22:07:01.367: INFO: Waiting for pod downwardapi-volume-d4e0da8e-3a8e-424c-8cb7-79e98b3ad03c to disappear
Apr 22 22:07:01.369: INFO: Pod downwardapi-volume-d4e0da8e-3a8e-424c-8cb7-79e98b3ad03c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:07:01.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8718" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":239,"skipped":4110,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:07:01.377: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:07:06.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5641" for this suite.

• [SLOW TEST:5.006 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":240,"skipped":4170,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:07:06.383: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 22:07:06.412: INFO: Creating deployment "test-recreate-deployment"
Apr 22 22:07:06.416: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr 22 22:07:06.420: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Apr 22 22:07:08.428: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr 22 22:07:08.430: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr 22 22:07:08.439: INFO: Updating deployment test-recreate-deployment
Apr 22 22:07:08.439: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 22 22:07:08.505: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9861  abd2c9da-20f9-4295-bae6-c2308a01f60f 31636 2 2021-04-22 22:07:06 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-04-22 22:07:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-22 22:07:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059ae968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-04-22 22:07:08 +0000 UTC,LastTransitionTime:2021-04-22 22:07:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-04-22 22:07:08 +0000 UTC,LastTransitionTime:2021-04-22 22:07:06 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Apr 22 22:07:08.507: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-9861  8367fd5b-6acc-4c0b-b117-df4cd372d74f 31634 1 2021-04-22 22:07:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment abd2c9da-20f9-4295-bae6-c2308a01f60f 0xc00598fb00 0xc00598fb01}] []  [{kube-controller-manager Update apps/v1 2021-04-22 22:07:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"abd2c9da-20f9-4295-bae6-c2308a01f60f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00598fb88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 22 22:07:08.507: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr 22 22:07:08.507: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-9861  634e0cf0-64e5-40f1-9e4e-f35d12a10241 31624 2 2021-04-22 22:07:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment abd2c9da-20f9-4295-bae6-c2308a01f60f 0xc00598f937 0xc00598f938}] []  [{kube-controller-manager Update apps/v1 2021-04-22 22:07:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"abd2c9da-20f9-4295-bae6-c2308a01f60f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00598fa78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 22 22:07:08.509: INFO: Pod "test-recreate-deployment-f79dd4667-fb4ws" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-fb4ws test-recreate-deployment-f79dd4667- deployment-9861  339871e5-5418-492f-b169-8eeb8079d94f 31635 0 2021-04-22 22:07:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 8367fd5b-6acc-4c0b-b117-df4cd372d74f 0xc0059d8130 0xc0059d8131}] []  [{kube-controller-manager Update v1 2021-04-22 22:07:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8367fd5b-6acc-4c0b-b117-df4cd372d74f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 22:07:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvzl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvzl4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvzl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-171.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:07:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:07:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:07:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:07:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.171,PodIP:,StartTime:2021-04-22 22:07:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:07:08.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9861" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":241,"skipped":4174,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:07:08.521: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-t5kx
STEP: Creating a pod to test atomic-volume-subpath
Apr 22 22:07:08.560: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-t5kx" in namespace "subpath-9462" to be "Succeeded or Failed"
Apr 22 22:07:08.562: INFO: Pod "pod-subpath-test-downwardapi-t5kx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.948427ms
Apr 22 22:07:10.568: INFO: Pod "pod-subpath-test-downwardapi-t5kx": Phase="Running", Reason="", readiness=true. Elapsed: 2.008376679s
Apr 22 22:07:12.572: INFO: Pod "pod-subpath-test-downwardapi-t5kx": Phase="Running", Reason="", readiness=true. Elapsed: 4.01224432s
Apr 22 22:07:14.578: INFO: Pod "pod-subpath-test-downwardapi-t5kx": Phase="Running", Reason="", readiness=true. Elapsed: 6.018386024s
Apr 22 22:07:16.584: INFO: Pod "pod-subpath-test-downwardapi-t5kx": Phase="Running", Reason="", readiness=true. Elapsed: 8.024627245s
Apr 22 22:07:18.590: INFO: Pod "pod-subpath-test-downwardapi-t5kx": Phase="Running", Reason="", readiness=true. Elapsed: 10.030603238s
Apr 22 22:07:20.597: INFO: Pod "pod-subpath-test-downwardapi-t5kx": Phase="Running", Reason="", readiness=true. Elapsed: 12.036778264s
Apr 22 22:07:22.601: INFO: Pod "pod-subpath-test-downwardapi-t5kx": Phase="Running", Reason="", readiness=true. Elapsed: 14.040773789s
Apr 22 22:07:24.606: INFO: Pod "pod-subpath-test-downwardapi-t5kx": Phase="Running", Reason="", readiness=true. Elapsed: 16.046674193s
Apr 22 22:07:26.613: INFO: Pod "pod-subpath-test-downwardapi-t5kx": Phase="Running", Reason="", readiness=true. Elapsed: 18.052956064s
Apr 22 22:07:28.619: INFO: Pod "pod-subpath-test-downwardapi-t5kx": Phase="Running", Reason="", readiness=true. Elapsed: 20.058819213s
Apr 22 22:07:30.625: INFO: Pod "pod-subpath-test-downwardapi-t5kx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.064924909s
STEP: Saw pod success
Apr 22 22:07:30.625: INFO: Pod "pod-subpath-test-downwardapi-t5kx" satisfied condition "Succeeded or Failed"
Apr 22 22:07:30.627: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod pod-subpath-test-downwardapi-t5kx container test-container-subpath-downwardapi-t5kx: <nil>
STEP: delete the pod
Apr 22 22:07:30.640: INFO: Waiting for pod pod-subpath-test-downwardapi-t5kx to disappear
Apr 22 22:07:30.642: INFO: Pod pod-subpath-test-downwardapi-t5kx no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-t5kx
Apr 22 22:07:30.642: INFO: Deleting pod "pod-subpath-test-downwardapi-t5kx" in namespace "subpath-9462"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:07:30.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9462" for this suite.

• [SLOW TEST:22.130 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":242,"skipped":4180,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:07:30.651: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 22 22:07:30.687: INFO: Waiting up to 5m0s for pod "pod-cbf9ed3a-2133-49c0-b43c-47226c29562e" in namespace "emptydir-2722" to be "Succeeded or Failed"
Apr 22 22:07:30.708: INFO: Pod "pod-cbf9ed3a-2133-49c0-b43c-47226c29562e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.672121ms
Apr 22 22:07:32.713: INFO: Pod "pod-cbf9ed3a-2133-49c0-b43c-47226c29562e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025014671s
STEP: Saw pod success
Apr 22 22:07:32.713: INFO: Pod "pod-cbf9ed3a-2133-49c0-b43c-47226c29562e" satisfied condition "Succeeded or Failed"
Apr 22 22:07:32.715: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-cbf9ed3a-2133-49c0-b43c-47226c29562e container test-container: <nil>
STEP: delete the pod
Apr 22 22:07:32.727: INFO: Waiting for pod pod-cbf9ed3a-2133-49c0-b43c-47226c29562e to disappear
Apr 22 22:07:32.729: INFO: Pod pod-cbf9ed3a-2133-49c0-b43c-47226c29562e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:07:32.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2722" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":243,"skipped":4186,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:07:32.737: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0422 22:07:42.810089      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 22 22:12:42.814: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:12:42.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3723" for this suite.

• [SLOW TEST:310.097 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":244,"skipped":4196,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:12:42.833: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-d502034c-6b98-4ae2-81b3-c06fbfb822e6
STEP: Creating a pod to test consume configMaps
Apr 22 22:12:42.883: INFO: Waiting up to 5m0s for pod "pod-configmaps-297fd18d-25f5-4558-b6c7-d25c042dc923" in namespace "configmap-4062" to be "Succeeded or Failed"
Apr 22 22:12:42.885: INFO: Pod "pod-configmaps-297fd18d-25f5-4558-b6c7-d25c042dc923": Phase="Pending", Reason="", readiness=false. Elapsed: 1.924354ms
Apr 22 22:12:44.889: INFO: Pod "pod-configmaps-297fd18d-25f5-4558-b6c7-d25c042dc923": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006830866s
STEP: Saw pod success
Apr 22 22:12:44.890: INFO: Pod "pod-configmaps-297fd18d-25f5-4558-b6c7-d25c042dc923" satisfied condition "Succeeded or Failed"
Apr 22 22:12:44.892: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-configmaps-297fd18d-25f5-4558-b6c7-d25c042dc923 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 22 22:12:44.912: INFO: Waiting for pod pod-configmaps-297fd18d-25f5-4558-b6c7-d25c042dc923 to disappear
Apr 22 22:12:44.914: INFO: Pod pod-configmaps-297fd18d-25f5-4558-b6c7-d25c042dc923 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:12:44.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4062" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":245,"skipped":4198,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:12:44.921: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-14734aa8-9e8a-487d-a244-121bae7c6f73
STEP: Creating a pod to test consume configMaps
Apr 22 22:12:44.953: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d8a9e21d-3431-472b-a1ea-a445ac710228" in namespace "projected-7481" to be "Succeeded or Failed"
Apr 22 22:12:44.955: INFO: Pod "pod-projected-configmaps-d8a9e21d-3431-472b-a1ea-a445ac710228": Phase="Pending", Reason="", readiness=false. Elapsed: 1.872514ms
Apr 22 22:12:46.961: INFO: Pod "pod-projected-configmaps-d8a9e21d-3431-472b-a1ea-a445ac710228": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007887403s
STEP: Saw pod success
Apr 22 22:12:46.961: INFO: Pod "pod-projected-configmaps-d8a9e21d-3431-472b-a1ea-a445ac710228" satisfied condition "Succeeded or Failed"
Apr 22 22:12:46.963: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-projected-configmaps-d8a9e21d-3431-472b-a1ea-a445ac710228 container agnhost-container: <nil>
STEP: delete the pod
Apr 22 22:12:46.978: INFO: Waiting for pod pod-projected-configmaps-d8a9e21d-3431-472b-a1ea-a445ac710228 to disappear
Apr 22 22:12:46.980: INFO: Pod pod-projected-configmaps-d8a9e21d-3431-472b-a1ea-a445ac710228 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:12:46.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7481" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":246,"skipped":4198,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:12:46.988: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-6954
STEP: creating replication controller nodeport-test in namespace services-6954
I0422 22:12:47.027880      24 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-6954, replica count: 2
Apr 22 22:12:50.078: INFO: Creating new exec pod
I0422 22:12:50.078152      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 22 22:12:53.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-6954 exec execpod7wk4j -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Apr 22 22:12:53.216: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Apr 22 22:12:53.216: INFO: stdout: ""
Apr 22 22:12:53.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-6954 exec execpod7wk4j -- /bin/sh -x -c nc -zv -t -w 2 10.0.44.215 80'
Apr 22 22:12:53.323: INFO: stderr: "+ nc -zv -t -w 2 10.0.44.215 80\nConnection to 10.0.44.215 80 port [tcp/http] succeeded!\n"
Apr 22 22:12:53.323: INFO: stdout: ""
Apr 22 22:12:53.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-6954 exec execpod7wk4j -- /bin/sh -x -c nc -zv -t -w 2 10.0.130.171 32175'
Apr 22 22:12:53.433: INFO: stderr: "+ nc -zv -t -w 2 10.0.130.171 32175\nConnection to 10.0.130.171 32175 port [tcp/32175] succeeded!\n"
Apr 22 22:12:53.433: INFO: stdout: ""
Apr 22 22:12:53.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-6954 exec execpod7wk4j -- /bin/sh -x -c nc -zv -t -w 2 10.0.130.230 32175'
Apr 22 22:12:53.542: INFO: stderr: "+ nc -zv -t -w 2 10.0.130.230 32175\nConnection to 10.0.130.230 32175 port [tcp/32175] succeeded!\n"
Apr 22 22:12:53.542: INFO: stdout: ""
Apr 22 22:12:53.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-6954 exec execpod7wk4j -- /bin/sh -x -c nc -zv -t -w 2 34.208.188.48 32175'
Apr 22 22:12:53.664: INFO: stderr: "+ nc -zv -t -w 2 34.208.188.48 32175\nConnection to 34.208.188.48 32175 port [tcp/32175] succeeded!\n"
Apr 22 22:12:53.664: INFO: stdout: ""
Apr 22 22:12:53.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-6954 exec execpod7wk4j -- /bin/sh -x -c nc -zv -t -w 2 18.237.25.155 32175'
Apr 22 22:12:53.774: INFO: stderr: "+ nc -zv -t -w 2 18.237.25.155 32175\nConnection to 18.237.25.155 32175 port [tcp/32175] succeeded!\n"
Apr 22 22:12:53.774: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:12:53.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6954" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.797 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":247,"skipped":4199,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:12:53.785: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9313
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9313
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9313
Apr 22 22:12:53.833: INFO: Found 0 stateful pods, waiting for 1
Apr 22 22:13:03.837: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Apr 22 22:13:03.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-9313 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 22 22:13:03.956: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 22 22:13:03.956: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 22 22:13:03.956: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 22 22:13:03.959: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 22 22:13:13.963: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 22 22:13:13.963: INFO: Waiting for statefulset status.replicas updated to 0
Apr 22 22:13:13.974: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999798s
Apr 22 22:13:14.979: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997841931s
Apr 22 22:13:15.984: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992610882s
Apr 22 22:13:16.989: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988003132s
Apr 22 22:13:17.994: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982591046s
Apr 22 22:13:19.001: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.977250464s
Apr 22 22:13:20.006: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.971139221s
Apr 22 22:13:21.011: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.965910686s
Apr 22 22:13:22.015: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.960903138s
Apr 22 22:13:23.021: INFO: Verifying statefulset ss doesn't scale past 1 for another 956.446306ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9313
Apr 22 22:13:24.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-9313 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 22 22:13:24.139: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 22 22:13:24.139: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 22 22:13:24.139: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 22 22:13:24.143: INFO: Found 1 stateful pods, waiting for 3
Apr 22 22:13:34.148: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 22 22:13:34.148: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 22 22:13:34.148: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Apr 22 22:13:34.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-9313 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 22 22:13:34.259: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 22 22:13:34.259: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 22 22:13:34.259: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 22 22:13:34.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-9313 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 22 22:13:34.387: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 22 22:13:34.387: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 22 22:13:34.387: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 22 22:13:34.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-9313 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 22 22:13:34.501: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 22 22:13:34.501: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 22 22:13:34.501: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 22 22:13:34.501: INFO: Waiting for statefulset status.replicas updated to 0
Apr 22 22:13:34.504: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Apr 22 22:13:44.511: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 22 22:13:44.511: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 22 22:13:44.511: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 22 22:13:44.522: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999806s
Apr 22 22:13:45.526: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997640706s
Apr 22 22:13:46.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992756249s
Apr 22 22:13:47.536: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98791204s
Apr 22 22:13:48.541: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983441219s
Apr 22 22:13:49.546: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978542857s
Apr 22 22:13:50.551: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.9735075s
Apr 22 22:13:51.556: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.968500451s
Apr 22 22:13:52.560: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.963620004s
Apr 22 22:13:53.565: INFO: Verifying statefulset ss doesn't scale past 3 for another 958.833065ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9313
Apr 22 22:13:54.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-9313 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 22 22:13:54.692: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 22 22:13:54.692: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 22 22:13:54.692: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 22 22:13:54.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-9313 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 22 22:13:54.800: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 22 22:13:54.800: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 22 22:13:54.800: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 22 22:13:54.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=statefulset-9313 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 22 22:13:54.904: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 22 22:13:54.904: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 22 22:13:54.904: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 22 22:13:54.904: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 22 22:14:14.919: INFO: Deleting all statefulset in ns statefulset-9313
Apr 22 22:14:14.921: INFO: Scaling statefulset ss to 0
Apr 22 22:14:14.930: INFO: Waiting for statefulset status.replicas updated to 0
Apr 22 22:14:14.932: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:14:14.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9313" for this suite.

• [SLOW TEST:81.168 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":248,"skipped":4240,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:14:14.953: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Apr 22 22:14:18.015: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:14:19.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5697" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":249,"skipped":4246,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:14:19.040: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Apr 22 22:14:21.607: INFO: Successfully updated pod "labelsupdateeb44e833-3425-4e94-9f0a-e7e44b146d6a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:14:23.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1978" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":250,"skipped":4281,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:14:23.626: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-dd927ae2-d7a4-43bc-a32d-8f53fdebce74 in namespace container-probe-2516
Apr 22 22:14:25.670: INFO: Started pod liveness-dd927ae2-d7a4-43bc-a32d-8f53fdebce74 in namespace container-probe-2516
STEP: checking the pod's current state and verifying that restartCount is present
Apr 22 22:14:25.673: INFO: Initial restart count of pod liveness-dd927ae2-d7a4-43bc-a32d-8f53fdebce74 is 0
Apr 22 22:14:41.722: INFO: Restart count of pod container-probe-2516/liveness-dd927ae2-d7a4-43bc-a32d-8f53fdebce74 is now 1 (16.049745138s elapsed)
Apr 22 22:15:01.782: INFO: Restart count of pod container-probe-2516/liveness-dd927ae2-d7a4-43bc-a32d-8f53fdebce74 is now 2 (36.109653778s elapsed)
Apr 22 22:15:21.844: INFO: Restart count of pod container-probe-2516/liveness-dd927ae2-d7a4-43bc-a32d-8f53fdebce74 is now 3 (56.171846573s elapsed)
Apr 22 22:15:41.906: INFO: Restart count of pod container-probe-2516/liveness-dd927ae2-d7a4-43bc-a32d-8f53fdebce74 is now 4 (1m16.233563174s elapsed)
Apr 22 22:16:50.115: INFO: Restart count of pod container-probe-2516/liveness-dd927ae2-d7a4-43bc-a32d-8f53fdebce74 is now 5 (2m24.442842197s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:16:50.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2516" for this suite.

• [SLOW TEST:146.514 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":251,"skipped":4286,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:16:50.140: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 22 22:16:52.195: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:16:52.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2631" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":252,"skipped":4290,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:16:52.215: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 22:16:52.243: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:16:53.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4973" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":253,"skipped":4312,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:16:53.431: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6322
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-6322
I0422 22:16:53.485177      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6322, replica count: 2
Apr 22 22:16:56.535: INFO: Creating new exec pod
I0422 22:16:56.535411      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 22 22:16:59.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-6322 exec execpodw22xc -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Apr 22 22:16:59.838: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 22 22:16:59.838: INFO: stdout: ""
Apr 22 22:16:59.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-6322 exec execpodw22xc -- /bin/sh -x -c nc -zv -t -w 2 10.0.37.249 80'
Apr 22 22:16:59.946: INFO: stderr: "+ nc -zv -t -w 2 10.0.37.249 80\nConnection to 10.0.37.249 80 port [tcp/http] succeeded!\n"
Apr 22 22:16:59.946: INFO: stdout: ""
Apr 22 22:16:59.946: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:16:59.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6322" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.548 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":254,"skipped":4317,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:16:59.980: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7332
STEP: creating service affinity-clusterip-transition in namespace services-7332
STEP: creating replication controller affinity-clusterip-transition in namespace services-7332
I0422 22:17:00.017358      24 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-7332, replica count: 3
I0422 22:17:03.067610      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 22 22:17:03.077: INFO: Creating new exec pod
Apr 22 22:17:06.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-7332 exec execpod-affinitytk47q -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Apr 22 22:17:06.208: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Apr 22 22:17:06.208: INFO: stdout: ""
Apr 22 22:17:06.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-7332 exec execpod-affinitytk47q -- /bin/sh -x -c nc -zv -t -w 2 10.0.1.224 80'
Apr 22 22:17:06.315: INFO: stderr: "+ nc -zv -t -w 2 10.0.1.224 80\nConnection to 10.0.1.224 80 port [tcp/http] succeeded!\n"
Apr 22 22:17:06.315: INFO: stdout: ""
Apr 22 22:17:06.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-7332 exec execpod-affinitytk47q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.224:80/ ; done'
Apr 22 22:17:06.499: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n"
Apr 22 22:17:06.499: INFO: stdout: "\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-cxg8s\naffinity-clusterip-transition-kqvxr\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-kqvxr\naffinity-clusterip-transition-cxg8s\naffinity-clusterip-transition-cxg8s\naffinity-clusterip-transition-cxg8s\naffinity-clusterip-transition-cxg8s\naffinity-clusterip-transition-kqvxr\naffinity-clusterip-transition-kqvxr\naffinity-clusterip-transition-kqvxr\naffinity-clusterip-transition-kqvxr\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-kqvxr"
Apr 22 22:17:06.499: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.499: INFO: Received response from host: affinity-clusterip-transition-cxg8s
Apr 22 22:17:06.499: INFO: Received response from host: affinity-clusterip-transition-kqvxr
Apr 22 22:17:06.499: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.499: INFO: Received response from host: affinity-clusterip-transition-kqvxr
Apr 22 22:17:06.499: INFO: Received response from host: affinity-clusterip-transition-cxg8s
Apr 22 22:17:06.499: INFO: Received response from host: affinity-clusterip-transition-cxg8s
Apr 22 22:17:06.499: INFO: Received response from host: affinity-clusterip-transition-cxg8s
Apr 22 22:17:06.499: INFO: Received response from host: affinity-clusterip-transition-cxg8s
Apr 22 22:17:06.499: INFO: Received response from host: affinity-clusterip-transition-kqvxr
Apr 22 22:17:06.499: INFO: Received response from host: affinity-clusterip-transition-kqvxr
Apr 22 22:17:06.499: INFO: Received response from host: affinity-clusterip-transition-kqvxr
Apr 22 22:17:06.499: INFO: Received response from host: affinity-clusterip-transition-kqvxr
Apr 22 22:17:06.499: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.499: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.499: INFO: Received response from host: affinity-clusterip-transition-kqvxr
Apr 22 22:17:06.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-7332 exec execpod-affinitytk47q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.224:80/ ; done'
Apr 22 22:17:06.668: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.224:80/\n"
Apr 22 22:17:06.668: INFO: stdout: "\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-wcqjj\naffinity-clusterip-transition-wcqjj"
Apr 22 22:17:06.668: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.668: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.668: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.668: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.668: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.668: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.668: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.668: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.668: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.668: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.668: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.668: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.668: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.668: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.668: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.668: INFO: Received response from host: affinity-clusterip-transition-wcqjj
Apr 22 22:17:06.668: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7332, will wait for the garbage collector to delete the pods
Apr 22 22:17:06.736: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.929775ms
Apr 22 22:17:07.436: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 700.128598ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:17:18.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7332" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:18.693 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":255,"skipped":4318,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:17:18.673: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-c61c9f40-5817-4e17-bbfd-2d9fe57a30ca in namespace container-probe-4302
Apr 22 22:17:22.719: INFO: Started pod busybox-c61c9f40-5817-4e17-bbfd-2d9fe57a30ca in namespace container-probe-4302
STEP: checking the pod's current state and verifying that restartCount is present
Apr 22 22:17:22.721: INFO: Initial restart count of pod busybox-c61c9f40-5817-4e17-bbfd-2d9fe57a30ca is 0
Apr 22 22:18:12.868: INFO: Restart count of pod container-probe-4302/busybox-c61c9f40-5817-4e17-bbfd-2d9fe57a30ca is now 1 (50.146967405s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:18:12.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4302" for this suite.

• [SLOW TEST:54.223 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":256,"skipped":4327,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:18:12.895: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8155
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8155
STEP: creating replication controller externalsvc in namespace services-8155
I0422 22:18:12.945288      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8155, replica count: 2
I0422 22:18:15.995534      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Apr 22 22:18:16.011: INFO: Creating new exec pod
Apr 22 22:18:18.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=services-8155 exec execpod99vq2 -- /bin/sh -x -c nslookup clusterip-service.services-8155.svc.cluster.local'
Apr 22 22:18:18.209: INFO: stderr: "+ nslookup clusterip-service.services-8155.svc.cluster.local\n"
Apr 22 22:18:18.209: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nclusterip-service.services-8155.svc.cluster.local\tcanonical name = externalsvc.services-8155.svc.cluster.local.\nName:\texternalsvc.services-8155.svc.cluster.local\nAddress: 10.0.11.14\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8155, will wait for the garbage collector to delete the pods
Apr 22 22:18:18.272: INFO: Deleting ReplicationController externalsvc took: 9.969987ms
Apr 22 22:18:18.972: INFO: Terminating ReplicationController externalsvc pods took: 700.11251ms
Apr 22 22:18:28.699: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:18:28.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8155" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:15.826 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":257,"skipped":4329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:18:28.722: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-34d57570-e800-4999-9c6f-794b154ed027
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:18:28.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5201" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":258,"skipped":4365,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:18:28.756: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Apr 22 22:18:28.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-6109 create -f -'
Apr 22 22:18:28.943: INFO: stderr: ""
Apr 22 22:18:28.943: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Apr 22 22:18:28.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-6109 diff -f -'
Apr 22 22:18:29.267: INFO: rc: 1
Apr 22 22:18:29.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-6109 delete -f -'
Apr 22 22:18:29.337: INFO: stderr: ""
Apr 22 22:18:29.337: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:18:29.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6109" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":259,"skipped":4385,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:18:29.346: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 22 22:18:30.143: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 22:18:33.163: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:18:33.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3063" for this suite.
STEP: Destroying namespace "webhook-3063-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":260,"skipped":4387,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:18:33.255: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:18:33.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1204" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":261,"skipped":4442,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:18:33.298: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:18:33.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-3272" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":262,"skipped":4454,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:18:33.364: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 22 22:18:33.399: INFO: Waiting up to 5m0s for pod "pod-403bc3d4-194d-4283-8f8d-4cd7bd09bdb4" in namespace "emptydir-3952" to be "Succeeded or Failed"
Apr 22 22:18:33.401: INFO: Pod "pod-403bc3d4-194d-4283-8f8d-4cd7bd09bdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.894753ms
Apr 22 22:18:35.405: INFO: Pod "pod-403bc3d4-194d-4283-8f8d-4cd7bd09bdb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005846084s
Apr 22 22:18:37.410: INFO: Pod "pod-403bc3d4-194d-4283-8f8d-4cd7bd09bdb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011272836s
STEP: Saw pod success
Apr 22 22:18:37.410: INFO: Pod "pod-403bc3d4-194d-4283-8f8d-4cd7bd09bdb4" satisfied condition "Succeeded or Failed"
Apr 22 22:18:37.412: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-403bc3d4-194d-4283-8f8d-4cd7bd09bdb4 container test-container: <nil>
STEP: delete the pod
Apr 22 22:18:37.433: INFO: Waiting for pod pod-403bc3d4-194d-4283-8f8d-4cd7bd09bdb4 to disappear
Apr 22 22:18:37.435: INFO: Pod pod-403bc3d4-194d-4283-8f8d-4cd7bd09bdb4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:18:37.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3952" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":263,"skipped":4559,"failed":0}
SSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:18:37.442: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 22:18:37.475: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-84de3b17-0918-40c2-b412-fda77b0356ca" in namespace "security-context-test-4312" to be "Succeeded or Failed"
Apr 22 22:18:37.477: INFO: Pod "busybox-privileged-false-84de3b17-0918-40c2-b412-fda77b0356ca": Phase="Pending", Reason="", readiness=false. Elapsed: 1.913102ms
Apr 22 22:18:39.483: INFO: Pod "busybox-privileged-false-84de3b17-0918-40c2-b412-fda77b0356ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007549909s
Apr 22 22:18:39.483: INFO: Pod "busybox-privileged-false-84de3b17-0918-40c2-b412-fda77b0356ca" satisfied condition "Succeeded or Failed"
Apr 22 22:18:39.492: INFO: Got logs for pod "busybox-privileged-false-84de3b17-0918-40c2-b412-fda77b0356ca": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:18:39.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4312" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":264,"skipped":4562,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:18:39.501: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-6abc2497-2767-4708-bb53-201475f2643f in namespace container-probe-4725
Apr 22 22:18:41.548: INFO: Started pod busybox-6abc2497-2767-4708-bb53-201475f2643f in namespace container-probe-4725
STEP: checking the pod's current state and verifying that restartCount is present
Apr 22 22:18:41.550: INFO: Initial restart count of pod busybox-6abc2497-2767-4708-bb53-201475f2643f is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:22:42.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4725" for this suite.

• [SLOW TEST:242.774 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":265,"skipped":4563,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:22:42.275: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 22 22:22:46.351: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 22 22:22:46.353: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 22 22:22:48.353: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 22 22:22:48.359: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 22 22:22:50.353: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 22 22:22:50.359: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 22 22:22:52.353: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 22 22:22:52.359: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 22 22:22:54.353: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 22 22:22:54.359: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 22 22:22:56.353: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 22 22:22:56.357: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 22 22:22:58.353: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 22 22:22:58.359: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 22 22:23:00.353: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 22 22:23:00.359: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:23:00.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3509" for this suite.

• [SLOW TEST:18.101 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":266,"skipped":4582,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:23:00.376: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 22:23:04.422: INFO: Deleting pod "var-expansion-edcaea78-7b42-43ee-9f15-ef337433ffb8" in namespace "var-expansion-7790"
Apr 22 22:23:04.428: INFO: Wait up to 5m0s for pod "var-expansion-edcaea78-7b42-43ee-9f15-ef337433ffb8" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:23:06.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7790" for this suite.

• [SLOW TEST:6.068 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":267,"skipped":4587,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:23:06.444: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Apr 22 22:23:06.486: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Apr 22 22:23:06.500: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:23:06.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9667" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":268,"skipped":4608,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:23:06.538: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Apr 22 22:23:08.575: INFO: Pod pod-hostip-722473b5-bd15-4108-a48a-55a20bb526f1 has hostIP: 10.0.131.122
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:23:08.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6132" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":269,"skipped":4629,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:23:08.586: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 22 22:23:08.967: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 22 22:23:10.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754726988, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754726988, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754726988, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754726988, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 22:23:13.996: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:23:14.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7507" for this suite.
STEP: Destroying namespace "webhook-7507-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.588 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":270,"skipped":4647,"failed":0}
SSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:23:14.174: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:23:14.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3563" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":271,"skipped":4650,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:23:14.209: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Apr 22 22:23:18.759: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7916 pod-service-account-fc997cc2-e41a-4fe3-9415-7809216672bc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Apr 22 22:23:18.876: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7916 pod-service-account-fc997cc2-e41a-4fe3-9415-7809216672bc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Apr 22 22:23:18.986: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7916 pod-service-account-fc997cc2-e41a-4fe3-9415-7809216672bc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:23:19.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7916" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":272,"skipped":4661,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:23:19.111: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 22:23:21.173: INFO: Waiting up to 5m0s for pod "client-envvars-25018ffe-6404-4b76-9aee-9d2af4d30130" in namespace "pods-2824" to be "Succeeded or Failed"
Apr 22 22:23:21.178: INFO: Pod "client-envvars-25018ffe-6404-4b76-9aee-9d2af4d30130": Phase="Pending", Reason="", readiness=false. Elapsed: 5.195114ms
Apr 22 22:23:23.184: INFO: Pod "client-envvars-25018ffe-6404-4b76-9aee-9d2af4d30130": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010329538s
Apr 22 22:23:25.189: INFO: Pod "client-envvars-25018ffe-6404-4b76-9aee-9d2af4d30130": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015571967s
STEP: Saw pod success
Apr 22 22:23:25.189: INFO: Pod "client-envvars-25018ffe-6404-4b76-9aee-9d2af4d30130" satisfied condition "Succeeded or Failed"
Apr 22 22:23:25.191: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod client-envvars-25018ffe-6404-4b76-9aee-9d2af4d30130 container env3cont: <nil>
STEP: delete the pod
Apr 22 22:23:25.217: INFO: Waiting for pod client-envvars-25018ffe-6404-4b76-9aee-9d2af4d30130 to disappear
Apr 22 22:23:25.219: INFO: Pod client-envvars-25018ffe-6404-4b76-9aee-9d2af4d30130 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:23:25.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2824" for this suite.

• [SLOW TEST:6.125 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":273,"skipped":4690,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:23:25.236: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 22:23:25.272: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d94f1b71-afe7-4693-8511-fb1b90c31120" in namespace "projected-1638" to be "Succeeded or Failed"
Apr 22 22:23:25.276: INFO: Pod "downwardapi-volume-d94f1b71-afe7-4693-8511-fb1b90c31120": Phase="Pending", Reason="", readiness=false. Elapsed: 3.535076ms
Apr 22 22:23:27.281: INFO: Pod "downwardapi-volume-d94f1b71-afe7-4693-8511-fb1b90c31120": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00848541s
STEP: Saw pod success
Apr 22 22:23:27.281: INFO: Pod "downwardapi-volume-d94f1b71-afe7-4693-8511-fb1b90c31120" satisfied condition "Succeeded or Failed"
Apr 22 22:23:27.283: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod downwardapi-volume-d94f1b71-afe7-4693-8511-fb1b90c31120 container client-container: <nil>
STEP: delete the pod
Apr 22 22:23:27.297: INFO: Waiting for pod downwardapi-volume-d94f1b71-afe7-4693-8511-fb1b90c31120 to disappear
Apr 22 22:23:27.299: INFO: Pod downwardapi-volume-d94f1b71-afe7-4693-8511-fb1b90c31120 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:23:27.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1638" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":274,"skipped":4713,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:23:27.307: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Apr 22 22:23:27.335: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:23:44.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6081" for this suite.

• [SLOW TEST:16.930 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":275,"skipped":4734,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:23:44.237: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-8224
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 22 22:23:44.266: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 22 22:23:44.296: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 22 22:23:46.299: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 22:23:48.302: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 22:23:50.301: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 22:23:52.302: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 22:23:54.301: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 22:23:56.299: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 22:23:58.302: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 22:24:00.302: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 22:24:02.302: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 22 22:24:04.302: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 22 22:24:04.306: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr 22 22:24:04.310: INFO: The status of Pod netserver-2 is Running (Ready = true)
Apr 22 22:24:04.313: INFO: The status of Pod netserver-3 is Running (Ready = false)
Apr 22 22:24:06.317: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Apr 22 22:24:08.346: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Apr 22 22:24:08.346: INFO: Going to poll 192.168.224.75 on port 8080 at least 0 times, with a maximum of 46 tries before failing
Apr 22 22:24:08.348: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.224.75:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8224 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 22:24:08.348: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 22:24:08.405: INFO: Found all 1 expected endpoints: [netserver-0]
Apr 22 22:24:08.405: INFO: Going to poll 192.168.156.245 on port 8080 at least 0 times, with a maximum of 46 tries before failing
Apr 22 22:24:08.407: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.156.245:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8224 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 22:24:08.407: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 22:24:08.461: INFO: Found all 1 expected endpoints: [netserver-1]
Apr 22 22:24:08.461: INFO: Going to poll 192.168.255.241 on port 8080 at least 0 times, with a maximum of 46 tries before failing
Apr 22 22:24:08.463: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.255.241:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8224 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 22:24:08.463: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 22:24:08.513: INFO: Found all 1 expected endpoints: [netserver-2]
Apr 22 22:24:08.513: INFO: Going to poll 192.168.126.3 on port 8080 at least 0 times, with a maximum of 46 tries before failing
Apr 22 22:24:08.516: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.126.3:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8224 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 22:24:08.516: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 22:24:08.571: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:24:08.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8224" for this suite.

• [SLOW TEST:24.344 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":276,"skipped":4740,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:24:08.581: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 22 22:24:08.609: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 22 22:24:08.617: INFO: Waiting for terminating namespaces to be deleted...
Apr 22 22:24:08.620: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-130-171.us-west-2.compute.internal before test
Apr 22 22:24:08.625: INFO: calico-node-zstxl from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 22:24:08.625: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 22:24:08.625: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 22:24:08.625: INFO: kube-proxy-246ms from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 22:24:08.625: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 22:24:08.625: INFO: netserver-0 from pod-network-test-8224 started at 2021-04-22 22:23:44 +0000 UTC (1 container statuses recorded)
Apr 22 22:24:08.625: INFO: 	Container webserver ready: true, restart count 0
Apr 22 22:24:08.625: INFO: sonobuoy-e2e-job-f05335387339495e from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 22:24:08.625: INFO: 	Container e2e ready: true, restart count 0
Apr 22 22:24:08.625: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 22 22:24:08.625: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-x9h8q from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 22:24:08.625: INFO: 	Container sonobuoy-worker ready: false, restart count 13
Apr 22 22:24:08.625: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 22 22:24:08.625: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-130-230.us-west-2.compute.internal before test
Apr 22 22:24:08.631: INFO: calico-node-92wx7 from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 22:24:08.631: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 22:24:08.631: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 22:24:08.631: INFO: coredns-74ff55c5b-b2ghx from kube-system started at 2021-04-22 20:42:54 +0000 UTC (1 container statuses recorded)
Apr 22 22:24:08.631: INFO: 	Container coredns ready: true, restart count 0
Apr 22 22:24:08.631: INFO: kube-proxy-z7cgr from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 22:24:08.631: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 22:24:08.631: INFO: netserver-1 from pod-network-test-8224 started at 2021-04-22 22:23:44 +0000 UTC (1 container statuses recorded)
Apr 22 22:24:08.631: INFO: 	Container webserver ready: true, restart count 0
Apr 22 22:24:08.631: INFO: test-container-pod from pod-network-test-8224 started at 2021-04-22 22:24:06 +0000 UTC (1 container statuses recorded)
Apr 22 22:24:08.631: INFO: 	Container webserver ready: true, restart count 0
Apr 22 22:24:08.631: INFO: sonobuoy from sonobuoy started at 2021-04-22 20:39:15 +0000 UTC (1 container statuses recorded)
Apr 22 22:24:08.631: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 22 22:24:08.631: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-f46mv from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 22:24:08.631: INFO: 	Container sonobuoy-worker ready: false, restart count 13
Apr 22 22:24:08.631: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 22 22:24:08.631: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-131-122.us-west-2.compute.internal before test
Apr 22 22:24:08.636: INFO: calico-node-w7mgb from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 22:24:08.636: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 22:24:08.636: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 22:24:08.636: INFO: coredns-74ff55c5b-l4x4h from kube-system started at 2021-04-22 20:38:22 +0000 UTC (1 container statuses recorded)
Apr 22 22:24:08.636: INFO: 	Container coredns ready: false, restart count 0
Apr 22 22:24:08.636: INFO: kube-proxy-d5r2q from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 22:24:08.636: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 22:24:08.636: INFO: host-test-container-pod from pod-network-test-8224 started at 2021-04-22 22:24:06 +0000 UTC (1 container statuses recorded)
Apr 22 22:24:08.636: INFO: 	Container agnhost-container ready: true, restart count 0
Apr 22 22:24:08.636: INFO: netserver-2 from pod-network-test-8224 started at 2021-04-22 22:23:44 +0000 UTC (1 container statuses recorded)
Apr 22 22:24:08.636: INFO: 	Container webserver ready: true, restart count 0
Apr 22 22:24:08.636: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-c9ssg from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 22:24:08.636: INFO: 	Container sonobuoy-worker ready: false, restart count 13
Apr 22 22:24:08.636: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 22 22:24:08.636: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-131-2.us-west-2.compute.internal before test
Apr 22 22:24:08.644: INFO: calico-node-bbjxk from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 22:24:08.644: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 22:24:08.644: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 22:24:08.644: INFO: coredns-74ff55c5b-6hrqm from kube-system started at 2021-04-22 21:15:14 +0000 UTC (1 container statuses recorded)
Apr 22 22:24:08.644: INFO: 	Container coredns ready: true, restart count 0
Apr 22 22:24:08.644: INFO: coredns-74ff55c5b-kj2n4 from kube-system started at 2021-04-22 20:38:22 +0000 UTC (1 container statuses recorded)
Apr 22 22:24:08.644: INFO: 	Container coredns ready: false, restart count 0
Apr 22 22:24:08.644: INFO: kube-proxy-b7qsd from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 22:24:08.644: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 22:24:08.644: INFO: netserver-3 from pod-network-test-8224 started at 2021-04-22 22:23:44 +0000 UTC (1 container statuses recorded)
Apr 22 22:24:08.644: INFO: 	Container webserver ready: true, restart count 0
Apr 22 22:24:08.644: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-4m6tb from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 22:24:08.644: INFO: 	Container sonobuoy-worker ready: false, restart count 13
Apr 22 22:24:08.644: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16784e61f4a86700], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 4 node(s) didn't match Pod's node affinity.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16784e61f50e596b], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 4 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:24:09.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9590" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":277,"skipped":4754,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:24:09.678: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 22:24:09.706: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:24:13.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8088" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":278,"skipped":4764,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:24:13.751: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Apr 22 22:24:13.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-8406 create -f -'
Apr 22 22:24:14.011: INFO: stderr: ""
Apr 22 22:24:14.011: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 22 22:24:15.017: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 22 22:24:15.017: INFO: Found 0 / 1
Apr 22 22:24:16.043: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 22 22:24:16.043: INFO: Found 1 / 1
Apr 22 22:24:16.043: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Apr 22 22:24:16.045: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 22 22:24:16.046: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 22 22:24:16.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-8406 patch pod agnhost-primary-b88rj -p {"metadata":{"annotations":{"x":"y"}}}'
Apr 22 22:24:16.109: INFO: stderr: ""
Apr 22 22:24:16.109: INFO: stdout: "pod/agnhost-primary-b88rj patched\n"
STEP: checking annotations
Apr 22 22:24:16.112: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 22 22:24:16.112: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:24:16.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8406" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":279,"skipped":4773,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:24:16.122: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Apr 22 22:24:16.156: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 22 22:24:16.156: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 22 22:24:16.162: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 22 22:24:16.162: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 22 22:24:16.172: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 22 22:24:16.172: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 22 22:24:16.192: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 22 22:24:16.192: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 22 22:24:17.963: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr 22 22:24:17.963: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr 22 22:24:18.095: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Apr 22 22:24:18.106: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Apr 22 22:24:18.107: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 0
Apr 22 22:24:18.107: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 0
Apr 22 22:24:18.107: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 0
Apr 22 22:24:18.107: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 0
Apr 22 22:24:18.107: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 0
Apr 22 22:24:18.107: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 0
Apr 22 22:24:18.107: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 0
Apr 22 22:24:18.107: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 0
Apr 22 22:24:18.107: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1
Apr 22 22:24:18.107: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1
Apr 22 22:24:18.107: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 2
Apr 22 22:24:18.107: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 2
Apr 22 22:24:18.107: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 2
Apr 22 22:24:18.107: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 2
Apr 22 22:24:18.114: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 2
Apr 22 22:24:18.114: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 2
Apr 22 22:24:18.125: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 2
Apr 22 22:24:18.125: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 2
Apr 22 22:24:18.138: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1
STEP: listing Deployments
Apr 22 22:24:18.143: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Apr 22 22:24:18.152: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Apr 22 22:24:18.156: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 22 22:24:18.167: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 22 22:24:18.177: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 22 22:24:18.192: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 22 22:24:18.203: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 22 22:24:18.221: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 22 22:24:18.229: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 22 22:24:18.238: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Apr 22 22:24:20.085: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1
Apr 22 22:24:20.085: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1
Apr 22 22:24:20.085: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1
Apr 22 22:24:20.085: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1
Apr 22 22:24:20.085: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1
Apr 22 22:24:20.085: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1
Apr 22 22:24:20.085: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1
Apr 22 22:24:20.085: INFO: observed Deployment test-deployment in namespace deployment-4683 with ReadyReplicas 1
STEP: deleting the Deployment
Apr 22 22:24:20.101: INFO: observed event type MODIFIED
Apr 22 22:24:20.101: INFO: observed event type MODIFIED
Apr 22 22:24:20.101: INFO: observed event type MODIFIED
Apr 22 22:24:20.101: INFO: observed event type MODIFIED
Apr 22 22:24:20.101: INFO: observed event type MODIFIED
Apr 22 22:24:20.101: INFO: observed event type MODIFIED
Apr 22 22:24:20.101: INFO: observed event type MODIFIED
Apr 22 22:24:20.101: INFO: observed event type MODIFIED
Apr 22 22:24:20.101: INFO: observed event type MODIFIED
Apr 22 22:24:20.101: INFO: observed event type MODIFIED
Apr 22 22:24:20.102: INFO: observed event type MODIFIED
Apr 22 22:24:20.102: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 22 22:24:20.107: INFO: Log out all the ReplicaSets if there is no deployment created
Apr 22 22:24:20.110: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-4683  87e9362b-9b7e-4316-8e92-c006d4cff55e 36291 3 2021-04-22 22:24:18 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 4c305c94-9625-417e-91e1-a5a353b4d551 0xc0046dea87 0xc0046dea88}] []  [{kube-controller-manager Update apps/v1 2021-04-22 22:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c305c94-9625-417e-91e1-a5a353b4d551\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0046deb00 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Apr 22 22:24:20.112: INFO: pod: "test-deployment-768947d6f5-b8ldx":
&Pod{ObjectMeta:{test-deployment-768947d6f5-b8ldx test-deployment-768947d6f5- deployment-4683  a4217cb8-74ed-4277-9e78-2a001879579a 36270 0 2021-04-22 22:24:18 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[cni.projectcalico.org/podIP:192.168.255.244/32 cni.projectcalico.org/podIPs:192.168.255.244/32] [{apps/v1 ReplicaSet test-deployment-768947d6f5 87e9362b-9b7e-4316-8e92-c006d4cff55e 0xc0047ec4a7 0xc0047ec4a8}] []  [{calico Update v1 2021-04-22 22:24:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2021-04-22 22:24:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"87e9362b-9b7e-4316-8e92-c006d4cff55e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 22:24:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.255.244\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-669zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-669zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-669zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-122.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:24:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:24:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:24:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:24:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.122,PodIP:192.168.255.244,StartTime:2021-04-22 22:24:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-22 22:24:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://011b10a3ab6f65e2f739227b1aee99b88473876857a479aa5f35f9503005c68c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.255.244,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 22 22:24:20.112: INFO: pod: "test-deployment-768947d6f5-wb868":
&Pod{ObjectMeta:{test-deployment-768947d6f5-wb868 test-deployment-768947d6f5- deployment-4683  801f5e13-fb95-4b08-896f-1a31a8ffade5 36290 0 2021-04-22 22:24:20 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-768947d6f5 87e9362b-9b7e-4316-8e92-c006d4cff55e 0xc0047ec6c7 0xc0047ec6c8}] []  [{kube-controller-manager Update v1 2021-04-22 22:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"87e9362b-9b7e-4316-8e92-c006d4cff55e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 22:24:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-669zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-669zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-669zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-230.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:24:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:24:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:24:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:24:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.230,PodIP:,StartTime:2021-04-22 22:24:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 22 22:24:20.112: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-4683  9027b2b8-d683-4731-a843-ba4ab3e1ed4b 36287 4 2021-04-22 22:24:18 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 4c305c94-9625-417e-91e1-a5a353b4d551 0xc0046deb67 0xc0046deb68}] []  [{kube-controller-manager Update apps/v1 2021-04-22 22:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c305c94-9625-417e-91e1-a5a353b4d551\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0046dec18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Apr 22 22:24:20.114: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-4683  94c399b2-6314-4fd0-81a3-eb0dc0abb3a6 36206 2 2021-04-22 22:24:16 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 4c305c94-9625-417e-91e1-a5a353b4d551 0xc0046dec87 0xc0046dec88}] []  [{kube-controller-manager Update apps/v1 2021-04-22 22:24:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c305c94-9625-417e-91e1-a5a353b4d551\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0046ded00 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Apr 22 22:24:20.117: INFO: pod: "test-deployment-8b6954bfb-pbr77":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-pbr77 test-deployment-8b6954bfb- deployment-4683  21fa9206-4cf5-41b2-92be-0c4036814b27 36174 0 2021-04-22 22:24:16 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[cni.projectcalico.org/podIP:192.168.224.76/32 cni.projectcalico.org/podIPs:192.168.224.76/32] [{apps/v1 ReplicaSet test-deployment-8b6954bfb 94c399b2-6314-4fd0-81a3-eb0dc0abb3a6 0xc0047edc07 0xc0047edc08}] []  [{calico Update v1 2021-04-22 22:24:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2021-04-22 22:24:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94c399b2-6314-4fd0-81a3-eb0dc0abb3a6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 22:24:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.224.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-669zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-669zx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-669zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-130-171.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:24:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:24:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:24:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.130.171,PodIP:192.168.224.76,StartTime:2021-04-22 22:24:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-22 22:24:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://633dc6669399f278489eed3c6ca13cfecc0050bc4026248c09985fde516b4c76,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.224.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:24:20.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4683" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":280,"skipped":4821,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:24:20.125: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 22:24:20.156: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8f76ddc-79f2-47a6-b376-42da266f7c6d" in namespace "projected-8556" to be "Succeeded or Failed"
Apr 22 22:24:20.158: INFO: Pod "downwardapi-volume-f8f76ddc-79f2-47a6-b376-42da266f7c6d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.950376ms
Apr 22 22:24:22.164: INFO: Pod "downwardapi-volume-f8f76ddc-79f2-47a6-b376-42da266f7c6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007745889s
STEP: Saw pod success
Apr 22 22:24:22.164: INFO: Pod "downwardapi-volume-f8f76ddc-79f2-47a6-b376-42da266f7c6d" satisfied condition "Succeeded or Failed"
Apr 22 22:24:22.166: INFO: Trying to get logs from node ip-10-0-130-171.us-west-2.compute.internal pod downwardapi-volume-f8f76ddc-79f2-47a6-b376-42da266f7c6d container client-container: <nil>
STEP: delete the pod
Apr 22 22:24:22.187: INFO: Waiting for pod downwardapi-volume-f8f76ddc-79f2-47a6-b376-42da266f7c6d to disappear
Apr 22 22:24:22.191: INFO: Pod downwardapi-volume-f8f76ddc-79f2-47a6-b376-42da266f7c6d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:24:22.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8556" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":281,"skipped":4826,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:24:22.199: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:24:24.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7954" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":282,"skipped":4871,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:24:24.257: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 22 22:24:24.291: INFO: Waiting up to 5m0s for pod "pod-219709fc-a2c9-46aa-902b-c844f6a802a9" in namespace "emptydir-7891" to be "Succeeded or Failed"
Apr 22 22:24:24.293: INFO: Pod "pod-219709fc-a2c9-46aa-902b-c844f6a802a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.972359ms
Apr 22 22:24:26.296: INFO: Pod "pod-219709fc-a2c9-46aa-902b-c844f6a802a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005445938s
STEP: Saw pod success
Apr 22 22:24:26.296: INFO: Pod "pod-219709fc-a2c9-46aa-902b-c844f6a802a9" satisfied condition "Succeeded or Failed"
Apr 22 22:24:26.298: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod pod-219709fc-a2c9-46aa-902b-c844f6a802a9 container test-container: <nil>
STEP: delete the pod
Apr 22 22:24:26.313: INFO: Waiting for pod pod-219709fc-a2c9-46aa-902b-c844f6a802a9 to disappear
Apr 22 22:24:26.315: INFO: Pod pod-219709fc-a2c9-46aa-902b-c844f6a802a9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:24:26.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7891" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":283,"skipped":4925,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:24:26.323: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4380.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4380.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4380.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4380.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4380.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4380.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 22 22:24:30.415: INFO: DNS probes using dns-4380/dns-test-e4bca6d1-118a-4bce-b4ab-90f19df662f7 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:24:30.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4380" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":284,"skipped":4951,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:24:30.458: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Apr 22 22:24:30.490: INFO: Waiting up to 5m0s for pod "var-expansion-f5511871-272f-4c75-934d-097cc6e98ff4" in namespace "var-expansion-2776" to be "Succeeded or Failed"
Apr 22 22:24:30.495: INFO: Pod "var-expansion-f5511871-272f-4c75-934d-097cc6e98ff4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.494065ms
Apr 22 22:24:32.501: INFO: Pod "var-expansion-f5511871-272f-4c75-934d-097cc6e98ff4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010472028s
Apr 22 22:24:34.507: INFO: Pod "var-expansion-f5511871-272f-4c75-934d-097cc6e98ff4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016283965s
STEP: Saw pod success
Apr 22 22:24:34.507: INFO: Pod "var-expansion-f5511871-272f-4c75-934d-097cc6e98ff4" satisfied condition "Succeeded or Failed"
Apr 22 22:24:34.512: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod var-expansion-f5511871-272f-4c75-934d-097cc6e98ff4 container dapi-container: <nil>
STEP: delete the pod
Apr 22 22:24:34.531: INFO: Waiting for pod var-expansion-f5511871-272f-4c75-934d-097cc6e98ff4 to disappear
Apr 22 22:24:34.533: INFO: Pod var-expansion-f5511871-272f-4c75-934d-097cc6e98ff4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:24:34.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2776" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":285,"skipped":4964,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:24:34.541: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Apr 22 22:24:34.571: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1846  2f4df49e-1e14-4915-81db-4a335b10a8fb 36594 0 2021-04-22 22:24:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-22 22:24:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 22 22:24:34.572: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1846  2f4df49e-1e14-4915-81db-4a335b10a8fb 36594 0 2021-04-22 22:24:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-22 22:24:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Apr 22 22:24:44.590: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1846  2f4df49e-1e14-4915-81db-4a335b10a8fb 36656 0 2021-04-22 22:24:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-22 22:24:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 22 22:24:44.590: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1846  2f4df49e-1e14-4915-81db-4a335b10a8fb 36656 0 2021-04-22 22:24:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-22 22:24:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Apr 22 22:24:54.609: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1846  2f4df49e-1e14-4915-81db-4a335b10a8fb 36685 0 2021-04-22 22:24:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-22 22:24:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 22 22:24:54.609: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1846  2f4df49e-1e14-4915-81db-4a335b10a8fb 36685 0 2021-04-22 22:24:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-22 22:24:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Apr 22 22:25:04.627: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1846  2f4df49e-1e14-4915-81db-4a335b10a8fb 36719 0 2021-04-22 22:24:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-22 22:24:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 22 22:25:04.627: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1846  2f4df49e-1e14-4915-81db-4a335b10a8fb 36719 0 2021-04-22 22:24:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-22 22:24:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Apr 22 22:25:14.645: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1846  a3f92269-7979-42d7-a420-33077d95f656 36743 0 2021-04-22 22:25:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-22 22:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 22 22:25:14.645: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1846  a3f92269-7979-42d7-a420-33077d95f656 36743 0 2021-04-22 22:25:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-22 22:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Apr 22 22:25:24.665: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1846  a3f92269-7979-42d7-a420-33077d95f656 36765 0 2021-04-22 22:25:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-22 22:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 22 22:25:24.665: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1846  a3f92269-7979-42d7-a420-33077d95f656 36765 0 2021-04-22 22:25:14 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-22 22:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:25:34.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1846" for this suite.

• [SLOW TEST:60.146 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":286,"skipped":5065,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:25:34.687: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-0fe808af-1ab1-4437-aca6-5e9ac5946038 in namespace container-probe-1381
Apr 22 22:25:36.734: INFO: Started pod test-webserver-0fe808af-1ab1-4437-aca6-5e9ac5946038 in namespace container-probe-1381
STEP: checking the pod's current state and verifying that restartCount is present
Apr 22 22:25:36.736: INFO: Initial restart count of pod test-webserver-0fe808af-1ab1-4437-aca6-5e9ac5946038 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:29:37.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1381" for this suite.

• [SLOW TEST:242.757 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":287,"skipped":5069,"failed":0}
SSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:29:37.444: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 22:29:37.482: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Apr 22 22:29:39.512: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:29:40.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-64" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":288,"skipped":5072,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:29:40.528: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 22 22:29:40.579: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:29:40.579: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:29:40.579: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:29:40.581: INFO: Number of nodes with available pods: 0
Apr 22 22:29:40.581: INFO: Node ip-10-0-130-171.us-west-2.compute.internal is running more than one daemon pod
Apr 22 22:29:41.587: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:29:41.587: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:29:41.587: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:29:41.590: INFO: Number of nodes with available pods: 0
Apr 22 22:29:41.590: INFO: Node ip-10-0-130-171.us-west-2.compute.internal is running more than one daemon pod
Apr 22 22:29:42.588: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:29:42.588: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:29:42.588: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:29:42.591: INFO: Number of nodes with available pods: 3
Apr 22 22:29:42.591: INFO: Node ip-10-0-131-122.us-west-2.compute.internal is running more than one daemon pod
Apr 22 22:29:43.587: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:29:43.587: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:29:43.587: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:29:43.590: INFO: Number of nodes with available pods: 4
Apr 22 22:29:43.590: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Apr 22 22:29:43.607: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:29:43.607: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:29:43.607: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:29:43.612: INFO: Number of nodes with available pods: 4
Apr 22 22:29:43.612: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8123, will wait for the garbage collector to delete the pods
Apr 22 22:29:44.681: INFO: Deleting DaemonSet.extensions daemon-set took: 6.864638ms
Apr 22 22:29:45.381: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.102038ms
Apr 22 22:31:08.686: INFO: Number of nodes with available pods: 0
Apr 22 22:31:08.686: INFO: Number of running nodes: 0, number of available pods: 0
Apr 22 22:31:08.688: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37778"},"items":null}

Apr 22 22:31:08.690: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37778"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:31:08.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8123" for this suite.

• [SLOW TEST:88.187 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":289,"skipped":5073,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:31:08.715: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 22 22:31:08.750: INFO: Waiting up to 5m0s for pod "pod-dc966a52-1f43-4207-b904-4016b6de257f" in namespace "emptydir-4041" to be "Succeeded or Failed"
Apr 22 22:31:08.753: INFO: Pod "pod-dc966a52-1f43-4207-b904-4016b6de257f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.128362ms
Apr 22 22:31:10.759: INFO: Pod "pod-dc966a52-1f43-4207-b904-4016b6de257f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008430798s
STEP: Saw pod success
Apr 22 22:31:10.759: INFO: Pod "pod-dc966a52-1f43-4207-b904-4016b6de257f" satisfied condition "Succeeded or Failed"
Apr 22 22:31:10.761: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-dc966a52-1f43-4207-b904-4016b6de257f container test-container: <nil>
STEP: delete the pod
Apr 22 22:31:10.785: INFO: Waiting for pod pod-dc966a52-1f43-4207-b904-4016b6de257f to disappear
Apr 22 22:31:10.787: INFO: Pod pod-dc966a52-1f43-4207-b904-4016b6de257f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:31:10.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4041" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":290,"skipped":5082,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:31:10.795: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-ab46572f-d537-49e2-b35e-20dcdaead5d8
STEP: Creating a pod to test consume secrets
Apr 22 22:31:10.832: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-af1bb97e-c554-42c6-9486-bd1c42499ddd" in namespace "projected-9780" to be "Succeeded or Failed"
Apr 22 22:31:10.834: INFO: Pod "pod-projected-secrets-af1bb97e-c554-42c6-9486-bd1c42499ddd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.96917ms
Apr 22 22:31:12.840: INFO: Pod "pod-projected-secrets-af1bb97e-c554-42c6-9486-bd1c42499ddd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007616601s
STEP: Saw pod success
Apr 22 22:31:12.840: INFO: Pod "pod-projected-secrets-af1bb97e-c554-42c6-9486-bd1c42499ddd" satisfied condition "Succeeded or Failed"
Apr 22 22:31:12.842: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-projected-secrets-af1bb97e-c554-42c6-9486-bd1c42499ddd container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 22 22:31:12.855: INFO: Waiting for pod pod-projected-secrets-af1bb97e-c554-42c6-9486-bd1c42499ddd to disappear
Apr 22 22:31:12.857: INFO: Pod pod-projected-secrets-af1bb97e-c554-42c6-9486-bd1c42499ddd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:31:12.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9780" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":291,"skipped":5098,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:31:12.865: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Apr 22 22:31:12.890: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:31:16.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9022" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":292,"skipped":5116,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:31:16.886: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 22:31:16.920: INFO: Waiting up to 5m0s for pod "downwardapi-volume-413127bb-cfe7-4e46-9ffc-cc968af6a905" in namespace "projected-2801" to be "Succeeded or Failed"
Apr 22 22:31:16.922: INFO: Pod "downwardapi-volume-413127bb-cfe7-4e46-9ffc-cc968af6a905": Phase="Pending", Reason="", readiness=false. Elapsed: 1.963678ms
Apr 22 22:31:18.928: INFO: Pod "downwardapi-volume-413127bb-cfe7-4e46-9ffc-cc968af6a905": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007594945s
STEP: Saw pod success
Apr 22 22:31:18.928: INFO: Pod "downwardapi-volume-413127bb-cfe7-4e46-9ffc-cc968af6a905" satisfied condition "Succeeded or Failed"
Apr 22 22:31:18.930: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod downwardapi-volume-413127bb-cfe7-4e46-9ffc-cc968af6a905 container client-container: <nil>
STEP: delete the pod
Apr 22 22:31:18.944: INFO: Waiting for pod downwardapi-volume-413127bb-cfe7-4e46-9ffc-cc968af6a905 to disappear
Apr 22 22:31:18.947: INFO: Pod downwardapi-volume-413127bb-cfe7-4e46-9ffc-cc968af6a905 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:31:18.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2801" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":293,"skipped":5119,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:31:18.954: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 22 22:31:19.247: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 22 22:31:21.258: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754727479, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754727479, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63754727479, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63754727479, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 22 22:31:24.276: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:31:34.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-726" for this suite.
STEP: Destroying namespace "webhook-726-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:15.460 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":294,"skipped":5134,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:31:34.414: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:31:34.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8363" for this suite.
STEP: Destroying namespace "nspatchtest-a6717a4d-b4a9-444e-8880-cb1982ab9450-6980" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":295,"skipped":5146,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:31:34.484: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 22:31:34.528: INFO: Create a RollingUpdate DaemonSet
Apr 22 22:31:34.532: INFO: Check that daemon pods launch on every node of the cluster
Apr 22 22:31:34.535: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:34.535: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:34.535: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:34.537: INFO: Number of nodes with available pods: 0
Apr 22 22:31:34.537: INFO: Node ip-10-0-130-171.us-west-2.compute.internal is running more than one daemon pod
Apr 22 22:31:35.543: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:35.543: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:35.543: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:35.546: INFO: Number of nodes with available pods: 0
Apr 22 22:31:35.546: INFO: Node ip-10-0-130-171.us-west-2.compute.internal is running more than one daemon pod
Apr 22 22:31:36.543: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:36.543: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:36.543: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:36.551: INFO: Number of nodes with available pods: 2
Apr 22 22:31:36.551: INFO: Node ip-10-0-131-122.us-west-2.compute.internal is running more than one daemon pod
Apr 22 22:31:37.543: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:37.543: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:37.543: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:37.546: INFO: Number of nodes with available pods: 4
Apr 22 22:31:37.546: INFO: Number of running nodes: 4, number of available pods: 4
Apr 22 22:31:37.546: INFO: Update the DaemonSet to trigger a rollout
Apr 22 22:31:37.555: INFO: Updating DaemonSet daemon-set
Apr 22 22:31:41.568: INFO: Roll back the DaemonSet before rollout is complete
Apr 22 22:31:41.575: INFO: Updating DaemonSet daemon-set
Apr 22 22:31:41.575: INFO: Make sure DaemonSet rollback is complete
Apr 22 22:31:41.577: INFO: Wrong image for pod: daemon-set-wtxgj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 22 22:31:41.577: INFO: Pod daemon-set-wtxgj is not available
Apr 22 22:31:41.580: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:41.580: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:41.580: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:42.585: INFO: Wrong image for pod: daemon-set-wtxgj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 22 22:31:42.585: INFO: Pod daemon-set-wtxgj is not available
Apr 22 22:31:42.589: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:42.589: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:42.589: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:43.585: INFO: Pod daemon-set-h4x8v is not available
Apr 22 22:31:43.589: INFO: DaemonSet pods can't tolerate node ip-10-0-192-148.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:43.589: INFO: DaemonSet pods can't tolerate node ip-10-0-193-128.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 22 22:31:43.589: INFO: DaemonSet pods can't tolerate node ip-10-0-195-172.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9795, will wait for the garbage collector to delete the pods
Apr 22 22:31:43.652: INFO: Deleting DaemonSet.extensions daemon-set took: 6.240565ms
Apr 22 22:31:43.752: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.129532ms
Apr 22 22:31:48.656: INFO: Number of nodes with available pods: 0
Apr 22 22:31:48.656: INFO: Number of running nodes: 0, number of available pods: 0
Apr 22 22:31:48.658: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38338"},"items":null}

Apr 22 22:31:48.660: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38338"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:31:48.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9795" for this suite.

• [SLOW TEST:14.199 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":296,"skipped":5153,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:31:48.682: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 22 22:31:48.708: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 22 22:31:48.714: INFO: Waiting for terminating namespaces to be deleted...
Apr 22 22:31:48.716: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-130-171.us-west-2.compute.internal before test
Apr 22 22:31:48.722: INFO: calico-node-zstxl from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 22:31:48.722: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 22:31:48.722: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 22:31:48.722: INFO: kube-proxy-246ms from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 22:31:48.722: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 22:31:48.722: INFO: sonobuoy-e2e-job-f05335387339495e from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 22:31:48.722: INFO: 	Container e2e ready: true, restart count 0
Apr 22 22:31:48.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 22 22:31:48.722: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-x9h8q from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 22:31:48.722: INFO: 	Container sonobuoy-worker ready: false, restart count 15
Apr 22 22:31:48.722: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 22 22:31:48.722: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-130-230.us-west-2.compute.internal before test
Apr 22 22:31:48.736: INFO: calico-node-92wx7 from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 22:31:48.736: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 22:31:48.736: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 22:31:48.736: INFO: coredns-74ff55c5b-b2ghx from kube-system started at 2021-04-22 20:42:54 +0000 UTC (1 container statuses recorded)
Apr 22 22:31:48.736: INFO: 	Container coredns ready: true, restart count 0
Apr 22 22:31:48.736: INFO: kube-proxy-z7cgr from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 22:31:48.736: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 22:31:48.736: INFO: sonobuoy from sonobuoy started at 2021-04-22 20:39:15 +0000 UTC (1 container statuses recorded)
Apr 22 22:31:48.736: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 22 22:31:48.736: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-f46mv from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 22:31:48.736: INFO: 	Container sonobuoy-worker ready: false, restart count 15
Apr 22 22:31:48.736: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 22 22:31:48.736: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-131-122.us-west-2.compute.internal before test
Apr 22 22:31:48.741: INFO: calico-node-w7mgb from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 22:31:48.741: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 22:31:48.741: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 22:31:48.741: INFO: coredns-74ff55c5b-l4x4h from kube-system started at 2021-04-22 20:38:22 +0000 UTC (1 container statuses recorded)
Apr 22 22:31:48.741: INFO: 	Container coredns ready: false, restart count 0
Apr 22 22:31:48.741: INFO: kube-proxy-d5r2q from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 22:31:48.741: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 22:31:48.741: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-c9ssg from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 22:31:48.741: INFO: 	Container sonobuoy-worker ready: false, restart count 14
Apr 22 22:31:48.741: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 22 22:31:48.741: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-131-2.us-west-2.compute.internal before test
Apr 22 22:31:48.747: INFO: calico-node-bbjxk from kube-system started at 2021-04-22 20:38:01 +0000 UTC (2 container statuses recorded)
Apr 22 22:31:48.747: INFO: 	Container bird-metrics ready: true, restart count 0
Apr 22 22:31:48.747: INFO: 	Container calico-node ready: true, restart count 0
Apr 22 22:31:48.747: INFO: coredns-74ff55c5b-6hrqm from kube-system started at 2021-04-22 21:15:14 +0000 UTC (1 container statuses recorded)
Apr 22 22:31:48.747: INFO: 	Container coredns ready: true, restart count 0
Apr 22 22:31:48.747: INFO: coredns-74ff55c5b-kj2n4 from kube-system started at 2021-04-22 20:38:22 +0000 UTC (1 container statuses recorded)
Apr 22 22:31:48.747: INFO: 	Container coredns ready: false, restart count 0
Apr 22 22:31:48.747: INFO: kube-proxy-b7qsd from kube-system started at 2021-04-22 20:37:38 +0000 UTC (1 container statuses recorded)
Apr 22 22:31:48.747: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 22 22:31:48.747: INFO: sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-4m6tb from sonobuoy started at 2021-04-22 20:39:18 +0000 UTC (2 container statuses recorded)
Apr 22 22:31:48.747: INFO: 	Container sonobuoy-worker ready: false, restart count 15
Apr 22 22:31:48.747: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node ip-10-0-130-171.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-130-230.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-131-122.us-west-2.compute.internal
STEP: verifying the node has the label node ip-10-0-131-2.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod calico-node-92wx7 requesting resource cpu=300m on Node ip-10-0-130-230.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod calico-node-bbjxk requesting resource cpu=300m on Node ip-10-0-131-2.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod calico-node-w7mgb requesting resource cpu=300m on Node ip-10-0-131-122.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod calico-node-zstxl requesting resource cpu=300m on Node ip-10-0-130-171.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod coredns-74ff55c5b-6hrqm requesting resource cpu=100m on Node ip-10-0-131-2.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod coredns-74ff55c5b-b2ghx requesting resource cpu=100m on Node ip-10-0-130-230.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod coredns-74ff55c5b-kj2n4 requesting resource cpu=100m on Node ip-10-0-131-2.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod coredns-74ff55c5b-l4x4h requesting resource cpu=100m on Node ip-10-0-131-122.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod kube-proxy-246ms requesting resource cpu=0m on Node ip-10-0-130-171.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod kube-proxy-b7qsd requesting resource cpu=0m on Node ip-10-0-131-2.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod kube-proxy-d5r2q requesting resource cpu=0m on Node ip-10-0-131-122.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod kube-proxy-z7cgr requesting resource cpu=0m on Node ip-10-0-130-230.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-130-230.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod sonobuoy-e2e-job-f05335387339495e requesting resource cpu=0m on Node ip-10-0-130-171.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-4m6tb requesting resource cpu=0m on Node ip-10-0-131-2.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-c9ssg requesting resource cpu=0m on Node ip-10-0-131-122.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-f46mv requesting resource cpu=0m on Node ip-10-0-130-230.us-west-2.compute.internal
Apr 22 22:31:48.805: INFO: Pod sonobuoy-systemd-logs-daemon-set-3d55458c1c7b4a49-x9h8q requesting resource cpu=0m on Node ip-10-0-130-171.us-west-2.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
Apr 22 22:31:48.805: INFO: Creating a pod which consumes cpu=5320m on Node ip-10-0-130-230.us-west-2.compute.internal
Apr 22 22:31:48.811: INFO: Creating a pod which consumes cpu=5320m on Node ip-10-0-131-122.us-west-2.compute.internal
Apr 22 22:31:48.817: INFO: Creating a pod which consumes cpu=5250m on Node ip-10-0-131-2.us-west-2.compute.internal
Apr 22 22:31:48.822: INFO: Creating a pod which consumes cpu=5390m on Node ip-10-0-130-171.us-west-2.compute.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1c6442cc-d9b0-4da3-8cca-c166ff186c86.16784ecd184c8a8a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8501/filler-pod-1c6442cc-d9b0-4da3-8cca-c166ff186c86 to ip-10-0-131-122.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1c6442cc-d9b0-4da3-8cca-c166ff186c86.16784ecd3bb051e1], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1c6442cc-d9b0-4da3-8cca-c166ff186c86.16784ecd491353f6], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2" in 224.572077ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1c6442cc-d9b0-4da3-8cca-c166ff186c86.16784ecd4ac0abc0], Reason = [Created], Message = [Created container filler-pod-1c6442cc-d9b0-4da3-8cca-c166ff186c86]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1c6442cc-d9b0-4da3-8cca-c166ff186c86.16784ecd50209170], Reason = [Started], Message = [Started container filler-pod-1c6442cc-d9b0-4da3-8cca-c166ff186c86]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7208aeee-9abc-46cf-96a4-65f008192254.16784ecd18bd94d9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8501/filler-pod-7208aeee-9abc-46cf-96a4-65f008192254 to ip-10-0-131-2.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7208aeee-9abc-46cf-96a4-65f008192254.16784ecd3be7de49], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7208aeee-9abc-46cf-96a4-65f008192254.16784ecd46ead2af], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2" in 184.722518ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7208aeee-9abc-46cf-96a4-65f008192254.16784ecd48ec4afb], Reason = [Created], Message = [Created container filler-pod-7208aeee-9abc-46cf-96a4-65f008192254]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7208aeee-9abc-46cf-96a4-65f008192254.16784ecd4df74b20], Reason = [Started], Message = [Started container filler-pod-7208aeee-9abc-46cf-96a4-65f008192254]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9fad3794-5421-4281-b7a9-af38685dc32c.16784ecd190d2eb0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8501/filler-pod-9fad3794-5421-4281-b7a9-af38685dc32c to ip-10-0-130-171.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9fad3794-5421-4281-b7a9-af38685dc32c.16784ecd3b40e8b9], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9fad3794-5421-4281-b7a9-af38685dc32c.16784ecd46908185], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2" in 189.748157ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9fad3794-5421-4281-b7a9-af38685dc32c.16784ecd481ddcb4], Reason = [Created], Message = [Created container filler-pod-9fad3794-5421-4281-b7a9-af38685dc32c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9fad3794-5421-4281-b7a9-af38685dc32c.16784ecd4d0363eb], Reason = [Started], Message = [Started container filler-pod-9fad3794-5421-4281-b7a9-af38685dc32c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dad11ee0-588a-488c-b945-8f1ce4ce6541.16784ecd17de2d1c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8501/filler-pod-dad11ee0-588a-488c-b945-8f1ce4ce6541 to ip-10-0-130-230.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dad11ee0-588a-488c-b945-8f1ce4ce6541.16784ecd3abc7799], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dad11ee0-588a-488c-b945-8f1ce4ce6541.16784ecd48b0556b], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2" in 234.070585ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dad11ee0-588a-488c-b945-8f1ce4ce6541.16784ecd4a564185], Reason = [Created], Message = [Created container filler-pod-dad11ee0-588a-488c-b945-8f1ce4ce6541]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dad11ee0-588a-488c-b945-8f1ce4ce6541.16784ecd4f22004a], Reason = [Started], Message = [Started container filler-pod-dad11ee0-588a-488c-b945-8f1ce4ce6541]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16784ecd91954095], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 4 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16784ecd920863a7], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 4 Insufficient cpu.]
STEP: removing the label node off the node ip-10-0-130-171.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-130-230.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-131-122.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-131-2.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:31:51.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8501" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":297,"skipped":5165,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:31:51.921: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:32:05.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9175" for this suite.

• [SLOW TEST:13.115 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":298,"skipped":5170,"failed":0}
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:32:05.036: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Apr 22 22:32:05.064: INFO: namespace kubectl-8737
Apr 22 22:32:05.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-8737 create -f -'
Apr 22 22:32:05.422: INFO: stderr: ""
Apr 22 22:32:05.422: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 22 22:32:06.426: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 22 22:32:06.427: INFO: Found 0 / 1
Apr 22 22:32:07.426: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 22 22:32:07.426: INFO: Found 1 / 1
Apr 22 22:32:07.426: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 22 22:32:07.429: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 22 22:32:07.429: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 22 22:32:07.429: INFO: wait on agnhost-primary startup in kubectl-8737 
Apr 22 22:32:07.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-8737 logs agnhost-primary-f8546 agnhost-primary'
Apr 22 22:32:07.492: INFO: stderr: ""
Apr 22 22:32:07.492: INFO: stdout: "Paused\n"
STEP: exposing RC
Apr 22 22:32:07.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-8737 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Apr 22 22:32:07.564: INFO: stderr: ""
Apr 22 22:32:07.564: INFO: stdout: "service/rm2 exposed\n"
Apr 22 22:32:07.569: INFO: Service rm2 in namespace kubectl-8737 found.
STEP: exposing service
Apr 22 22:32:09.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-002341601 --namespace=kubectl-8737 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Apr 22 22:32:09.654: INFO: stderr: ""
Apr 22 22:32:09.654: INFO: stdout: "service/rm3 exposed\n"
Apr 22 22:32:09.659: INFO: Service rm3 in namespace kubectl-8737 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:32:11.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8737" for this suite.

• [SLOW TEST:6.647 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":299,"skipped":5170,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:32:11.684: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Apr 22 22:32:17.745: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-848 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 22:32:17.745: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 22:32:17.800: INFO: Exec stderr: ""
Apr 22 22:32:17.800: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-848 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 22:32:17.800: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 22:32:17.847: INFO: Exec stderr: ""
Apr 22 22:32:17.847: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-848 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 22:32:17.847: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 22:32:17.894: INFO: Exec stderr: ""
Apr 22 22:32:17.894: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-848 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 22:32:17.894: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 22:32:17.943: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Apr 22 22:32:17.943: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-848 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 22:32:17.943: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 22:32:17.991: INFO: Exec stderr: ""
Apr 22 22:32:17.991: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-848 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 22:32:17.991: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 22:32:18.040: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Apr 22 22:32:18.040: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-848 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 22:32:18.040: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 22:32:18.099: INFO: Exec stderr: ""
Apr 22 22:32:18.099: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-848 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 22:32:18.099: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 22:32:18.150: INFO: Exec stderr: ""
Apr 22 22:32:18.150: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-848 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 22:32:18.150: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 22:32:18.198: INFO: Exec stderr: ""
Apr 22 22:32:18.198: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-848 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 22 22:32:18.198: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
Apr 22 22:32:18.246: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:32:18.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-848" for this suite.

• [SLOW TEST:6.574 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":300,"skipped":5187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:32:18.258: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 22:32:20.303: INFO: Deleting pod "var-expansion-6ce42640-8869-4c30-a556-b0f586661862" in namespace "var-expansion-2580"
Apr 22 22:32:20.310: INFO: Wait up to 5m0s for pod "var-expansion-6ce42640-8869-4c30-a556-b0f586661862" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:32:22.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2580" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":301,"skipped":5213,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:32:22.329: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 22 22:32:25.382: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:32:25.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-646" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":302,"skipped":5236,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:32:25.405: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-89e602fe-aea6-4226-96a2-3f41b287a387
STEP: Creating a pod to test consume secrets
Apr 22 22:32:25.441: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b8821e9f-6091-48bc-8117-68413d0d6efd" in namespace "projected-3734" to be "Succeeded or Failed"
Apr 22 22:32:25.444: INFO: Pod "pod-projected-secrets-b8821e9f-6091-48bc-8117-68413d0d6efd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.27597ms
Apr 22 22:32:27.450: INFO: Pod "pod-projected-secrets-b8821e9f-6091-48bc-8117-68413d0d6efd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008206155s
STEP: Saw pod success
Apr 22 22:32:27.450: INFO: Pod "pod-projected-secrets-b8821e9f-6091-48bc-8117-68413d0d6efd" satisfied condition "Succeeded or Failed"
Apr 22 22:32:27.452: INFO: Trying to get logs from node ip-10-0-130-171.us-west-2.compute.internal pod pod-projected-secrets-b8821e9f-6091-48bc-8117-68413d0d6efd container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 22 22:32:27.477: INFO: Waiting for pod pod-projected-secrets-b8821e9f-6091-48bc-8117-68413d0d6efd to disappear
Apr 22 22:32:27.480: INFO: Pod pod-projected-secrets-b8821e9f-6091-48bc-8117-68413d0d6efd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:32:27.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3734" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":303,"skipped":5267,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:32:27.489: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Apr 22 22:32:28.030: INFO: created pod pod-service-account-defaultsa
Apr 22 22:32:28.030: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr 22 22:32:28.034: INFO: created pod pod-service-account-mountsa
Apr 22 22:32:28.034: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr 22 22:32:28.040: INFO: created pod pod-service-account-nomountsa
Apr 22 22:32:28.040: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr 22 22:32:28.044: INFO: created pod pod-service-account-defaultsa-mountspec
Apr 22 22:32:28.044: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr 22 22:32:28.050: INFO: created pod pod-service-account-mountsa-mountspec
Apr 22 22:32:28.050: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr 22 22:32:28.056: INFO: created pod pod-service-account-nomountsa-mountspec
Apr 22 22:32:28.056: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr 22 22:32:28.065: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr 22 22:32:28.065: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr 22 22:32:28.080: INFO: created pod pod-service-account-mountsa-nomountspec
Apr 22 22:32:28.080: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr 22 22:32:28.093: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr 22 22:32:28.093: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:32:28.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1167" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":304,"skipped":5269,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:32:28.107: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Apr 22 22:32:28.145: INFO: Waiting up to 5m0s for pod "pod-5e25e35d-509e-4b96-8d3a-e281d4e783c6" in namespace "emptydir-1746" to be "Succeeded or Failed"
Apr 22 22:32:28.149: INFO: Pod "pod-5e25e35d-509e-4b96-8d3a-e281d4e783c6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.389876ms
Apr 22 22:32:30.155: INFO: Pod "pod-5e25e35d-509e-4b96-8d3a-e281d4e783c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0097057s
STEP: Saw pod success
Apr 22 22:32:30.155: INFO: Pod "pod-5e25e35d-509e-4b96-8d3a-e281d4e783c6" satisfied condition "Succeeded or Failed"
Apr 22 22:32:30.157: INFO: Trying to get logs from node ip-10-0-131-2.us-west-2.compute.internal pod pod-5e25e35d-509e-4b96-8d3a-e281d4e783c6 container test-container: <nil>
STEP: delete the pod
Apr 22 22:32:30.179: INFO: Waiting for pod pod-5e25e35d-509e-4b96-8d3a-e281d4e783c6 to disappear
Apr 22 22:32:30.181: INFO: Pod pod-5e25e35d-509e-4b96-8d3a-e281d4e783c6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:32:30.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1746" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":305,"skipped":5280,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:32:30.188: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 22 22:32:30.220: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr 22 22:32:35.230: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 22 22:32:35.230: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 22 22:32:37.255: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5982  40645123-2335-454b-a932-d1389b026e07 39152 1 2021-04-22 22:32:35 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-04-22 22:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-22 22:32:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040256c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-04-22 22:32:35 +0000 UTC,LastTransitionTime:2021-04-22 22:32:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-685c4f8568" has successfully progressed.,LastUpdateTime:2021-04-22 22:32:36 +0000 UTC,LastTransitionTime:2021-04-22 22:32:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 22 22:32:37.257: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-5982  a0bc1b2f-1ef8-455f-abd9-6bfc3d2a9270 39141 1 2021-04-22 22:32:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 40645123-2335-454b-a932-d1389b026e07 0xc004025a77 0xc004025a78}] []  [{kube-controller-manager Update apps/v1 2021-04-22 22:32:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40645123-2335-454b-a932-d1389b026e07\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004025b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 22 22:32:37.259: INFO: Pod "test-cleanup-deployment-685c4f8568-z6ddg" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-685c4f8568-z6ddg test-cleanup-deployment-685c4f8568- deployment-5982  00c8c4c9-a2d8-4809-8ddb-2f8160575e2e 39140 0 2021-04-22 22:32:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[cni.projectcalico.org/podIP:192.168.126.17/32 cni.projectcalico.org/podIPs:192.168.126.17/32] [{apps/v1 ReplicaSet test-cleanup-deployment-685c4f8568 a0bc1b2f-1ef8-455f-abd9-6bfc3d2a9270 0xc004025e67 0xc004025e68}] []  [{calico Update v1 2021-04-22 22:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2021-04-22 22:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0bc1b2f-1ef8-455f-abd9-6bfc3d2a9270\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-22 22:32:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.126.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ldfhf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ldfhf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ldfhf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-131-2.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:32:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:32:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:32:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-22 22:32:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.131.2,PodIP:192.168.126.17,StartTime:2021-04-22 22:32:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-22 22:32:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://881dd5e6b182c46a4dc70b4088fd5d229e8960fdee183852aab6baa42680ba12,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.126.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:32:37.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5982" for this suite.

• [SLOW TEST:7.082 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":306,"skipped":5290,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:32:37.270: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 22 22:32:37.305: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1708d4b-64ca-4197-a4f0-0afc51c211b2" in namespace "projected-366" to be "Succeeded or Failed"
Apr 22 22:32:37.307: INFO: Pod "downwardapi-volume-e1708d4b-64ca-4197-a4f0-0afc51c211b2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.977669ms
Apr 22 22:32:39.312: INFO: Pod "downwardapi-volume-e1708d4b-64ca-4197-a4f0-0afc51c211b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006734621s
STEP: Saw pod success
Apr 22 22:32:39.312: INFO: Pod "downwardapi-volume-e1708d4b-64ca-4197-a4f0-0afc51c211b2" satisfied condition "Succeeded or Failed"
Apr 22 22:32:39.314: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod downwardapi-volume-e1708d4b-64ca-4197-a4f0-0afc51c211b2 container client-container: <nil>
STEP: delete the pod
Apr 22 22:32:39.327: INFO: Waiting for pod downwardapi-volume-e1708d4b-64ca-4197-a4f0-0afc51c211b2 to disappear
Apr 22 22:32:39.329: INFO: Pod downwardapi-volume-e1708d4b-64ca-4197-a4f0-0afc51c211b2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:32:39.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-366" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":307,"skipped":5297,"failed":0}
SSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:32:39.336: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 22 22:32:39.386: INFO: starting watch
STEP: patching
STEP: updating
Apr 22 22:32:39.394: INFO: waiting for watch events with expected annotations
Apr 22 22:32:39.394: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:32:39.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-4373" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":308,"skipped":5305,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:32:39.423: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-606ea44b-be99-45e9-bbdf-b2df7bfbaf5a
STEP: Creating a pod to test consume configMaps
Apr 22 22:32:39.457: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e77d9a9b-b1d0-4dbf-81af-29bb800784ce" in namespace "projected-7038" to be "Succeeded or Failed"
Apr 22 22:32:39.459: INFO: Pod "pod-projected-configmaps-e77d9a9b-b1d0-4dbf-81af-29bb800784ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073018ms
Apr 22 22:32:41.465: INFO: Pod "pod-projected-configmaps-e77d9a9b-b1d0-4dbf-81af-29bb800784ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007955535s
STEP: Saw pod success
Apr 22 22:32:41.465: INFO: Pod "pod-projected-configmaps-e77d9a9b-b1d0-4dbf-81af-29bb800784ce" satisfied condition "Succeeded or Failed"
Apr 22 22:32:41.467: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-projected-configmaps-e77d9a9b-b1d0-4dbf-81af-29bb800784ce container agnhost-container: <nil>
STEP: delete the pod
Apr 22 22:32:41.482: INFO: Waiting for pod pod-projected-configmaps-e77d9a9b-b1d0-4dbf-81af-29bb800784ce to disappear
Apr 22 22:32:41.484: INFO: Pod pod-projected-configmaps-e77d9a9b-b1d0-4dbf-81af-29bb800784ce no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:32:41.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7038" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":309,"skipped":5330,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:32:41.493: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-fsk5g in namespace proxy-8028
I0422 22:32:41.535810      24 runners.go:190] Created replication controller with name: proxy-service-fsk5g, namespace: proxy-8028, replica count: 1
I0422 22:32:42.586028      24 runners.go:190] proxy-service-fsk5g Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0422 22:32:43.586161      24 runners.go:190] proxy-service-fsk5g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0422 22:32:44.586277      24 runners.go:190] proxy-service-fsk5g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0422 22:32:45.586414      24 runners.go:190] proxy-service-fsk5g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0422 22:32:46.586541      24 runners.go:190] proxy-service-fsk5g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0422 22:32:47.586673      24 runners.go:190] proxy-service-fsk5g Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 22 22:32:47.597: INFO: setup took 6.075985158s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Apr 22 22:32:47.605: INFO: (0) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 8.237772ms)
Apr 22 22:32:47.605: INFO: (0) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 8.461248ms)
Apr 22 22:32:47.605: INFO: (0) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 8.420676ms)
Apr 22 22:32:47.605: INFO: (0) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 8.449952ms)
Apr 22 22:32:47.605: INFO: (0) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 8.49704ms)
Apr 22 22:32:47.605: INFO: (0) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 8.508065ms)
Apr 22 22:32:47.605: INFO: (0) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 8.630174ms)
Apr 22 22:32:47.606: INFO: (0) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 9.038461ms)
Apr 22 22:32:47.606: INFO: (0) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 9.531123ms)
Apr 22 22:32:47.606: INFO: (0) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 9.545622ms)
Apr 22 22:32:47.606: INFO: (0) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 9.646882ms)
Apr 22 22:32:47.611: INFO: (0) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 14.051011ms)
Apr 22 22:32:47.611: INFO: (0) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 14.130639ms)
Apr 22 22:32:47.611: INFO: (0) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 14.126344ms)
Apr 22 22:32:47.611: INFO: (0) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 14.063128ms)
Apr 22 22:32:47.611: INFO: (0) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 14.172251ms)
Apr 22 22:32:47.615: INFO: (1) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 3.929946ms)
Apr 22 22:32:47.616: INFO: (1) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 4.325939ms)
Apr 22 22:32:47.616: INFO: (1) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.857755ms)
Apr 22 22:32:47.616: INFO: (1) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 4.169975ms)
Apr 22 22:32:47.617: INFO: (1) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 5.429744ms)
Apr 22 22:32:47.617: INFO: (1) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.992013ms)
Apr 22 22:32:47.617: INFO: (1) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 4.746581ms)
Apr 22 22:32:47.617: INFO: (1) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 4.805042ms)
Apr 22 22:32:47.617: INFO: (1) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 5.152275ms)
Apr 22 22:32:47.617: INFO: (1) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 5.093945ms)
Apr 22 22:32:47.618: INFO: (1) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 6.1951ms)
Apr 22 22:32:47.618: INFO: (1) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 6.363982ms)
Apr 22 22:32:47.618: INFO: (1) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 6.793318ms)
Apr 22 22:32:47.619: INFO: (1) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 7.180469ms)
Apr 22 22:32:47.619: INFO: (1) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 7.546856ms)
Apr 22 22:32:47.619: INFO: (1) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 7.323291ms)
Apr 22 22:32:47.622: INFO: (2) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 3.187652ms)
Apr 22 22:32:47.623: INFO: (2) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 4.511838ms)
Apr 22 22:32:47.624: INFO: (2) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 4.829228ms)
Apr 22 22:32:47.624: INFO: (2) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 5.175319ms)
Apr 22 22:32:47.624: INFO: (2) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 5.26017ms)
Apr 22 22:32:47.624: INFO: (2) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 5.198572ms)
Apr 22 22:32:47.624: INFO: (2) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 5.237673ms)
Apr 22 22:32:47.624: INFO: (2) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 5.360072ms)
Apr 22 22:32:47.624: INFO: (2) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 5.34058ms)
Apr 22 22:32:47.624: INFO: (2) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 5.447339ms)
Apr 22 22:32:47.624: INFO: (2) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 5.337398ms)
Apr 22 22:32:47.626: INFO: (2) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 6.70537ms)
Apr 22 22:32:47.626: INFO: (2) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 6.725503ms)
Apr 22 22:32:47.626: INFO: (2) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 6.776819ms)
Apr 22 22:32:47.626: INFO: (2) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 6.886957ms)
Apr 22 22:32:47.626: INFO: (2) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 7.527257ms)
Apr 22 22:32:47.635: INFO: (3) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 8.584489ms)
Apr 22 22:32:47.635: INFO: (3) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 8.727098ms)
Apr 22 22:32:47.635: INFO: (3) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 8.785315ms)
Apr 22 22:32:47.635: INFO: (3) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 8.770183ms)
Apr 22 22:32:47.635: INFO: (3) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 8.991586ms)
Apr 22 22:32:47.635: INFO: (3) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 8.915969ms)
Apr 22 22:32:47.635: INFO: (3) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 9.012622ms)
Apr 22 22:32:47.635: INFO: (3) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 9.032142ms)
Apr 22 22:32:47.635: INFO: (3) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 8.938291ms)
Apr 22 22:32:47.635: INFO: (3) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 8.958802ms)
Apr 22 22:32:47.640: INFO: (3) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 13.641948ms)
Apr 22 22:32:47.640: INFO: (3) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 13.667157ms)
Apr 22 22:32:47.640: INFO: (3) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 13.851848ms)
Apr 22 22:32:47.640: INFO: (3) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 13.918347ms)
Apr 22 22:32:47.640: INFO: (3) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 13.915918ms)
Apr 22 22:32:47.640: INFO: (3) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 13.99246ms)
Apr 22 22:32:47.644: INFO: (4) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 3.891313ms)
Apr 22 22:32:47.647: INFO: (4) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 6.463023ms)
Apr 22 22:32:47.647: INFO: (4) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 6.487777ms)
Apr 22 22:32:47.647: INFO: (4) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 6.526619ms)
Apr 22 22:32:47.647: INFO: (4) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 6.554396ms)
Apr 22 22:32:47.647: INFO: (4) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 6.524422ms)
Apr 22 22:32:47.647: INFO: (4) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 6.555248ms)
Apr 22 22:32:47.647: INFO: (4) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 6.521966ms)
Apr 22 22:32:47.647: INFO: (4) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 6.559533ms)
Apr 22 22:32:47.647: INFO: (4) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 6.657732ms)
Apr 22 22:32:47.647: INFO: (4) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 6.657375ms)
Apr 22 22:32:47.648: INFO: (4) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 7.58837ms)
Apr 22 22:32:47.649: INFO: (4) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 8.410489ms)
Apr 22 22:32:47.649: INFO: (4) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 8.526026ms)
Apr 22 22:32:47.649: INFO: (4) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 8.508241ms)
Apr 22 22:32:47.649: INFO: (4) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 8.56164ms)
Apr 22 22:32:47.652: INFO: (5) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 2.846353ms)
Apr 22 22:32:47.653: INFO: (5) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 3.699844ms)
Apr 22 22:32:47.654: INFO: (5) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 5.223629ms)
Apr 22 22:32:47.655: INFO: (5) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 5.534516ms)
Apr 22 22:32:47.655: INFO: (5) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 5.651441ms)
Apr 22 22:32:47.655: INFO: (5) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 5.718139ms)
Apr 22 22:32:47.655: INFO: (5) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 5.693449ms)
Apr 22 22:32:47.655: INFO: (5) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 5.701927ms)
Apr 22 22:32:47.655: INFO: (5) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 5.701922ms)
Apr 22 22:32:47.655: INFO: (5) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 6.206145ms)
Apr 22 22:32:47.656: INFO: (5) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 6.543301ms)
Apr 22 22:32:47.656: INFO: (5) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 7.33527ms)
Apr 22 22:32:47.657: INFO: (5) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 7.51675ms)
Apr 22 22:32:47.657: INFO: (5) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 7.479232ms)
Apr 22 22:32:47.657: INFO: (5) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 7.562541ms)
Apr 22 22:32:47.657: INFO: (5) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 7.587271ms)
Apr 22 22:32:47.662: INFO: (6) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 5.360162ms)
Apr 22 22:32:47.662: INFO: (6) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 5.325597ms)
Apr 22 22:32:47.662: INFO: (6) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 5.516203ms)
Apr 22 22:32:47.663: INFO: (6) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 5.792089ms)
Apr 22 22:32:47.663: INFO: (6) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 5.732782ms)
Apr 22 22:32:47.663: INFO: (6) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 5.686542ms)
Apr 22 22:32:47.663: INFO: (6) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 5.749413ms)
Apr 22 22:32:47.663: INFO: (6) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 5.77285ms)
Apr 22 22:32:47.663: INFO: (6) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 5.729614ms)
Apr 22 22:32:47.663: INFO: (6) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 6.085071ms)
Apr 22 22:32:47.663: INFO: (6) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 6.075677ms)
Apr 22 22:32:47.663: INFO: (6) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 6.15321ms)
Apr 22 22:32:47.664: INFO: (6) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 6.921724ms)
Apr 22 22:32:47.664: INFO: (6) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 7.051053ms)
Apr 22 22:32:47.664: INFO: (6) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 7.059902ms)
Apr 22 22:32:47.664: INFO: (6) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 7.125076ms)
Apr 22 22:32:47.668: INFO: (7) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.0422ms)
Apr 22 22:32:47.668: INFO: (7) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 4.38206ms)
Apr 22 22:32:47.669: INFO: (7) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 4.745495ms)
Apr 22 22:32:47.669: INFO: (7) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 4.687923ms)
Apr 22 22:32:47.669: INFO: (7) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 4.728878ms)
Apr 22 22:32:47.669: INFO: (7) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 4.775558ms)
Apr 22 22:32:47.669: INFO: (7) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 4.738488ms)
Apr 22 22:32:47.669: INFO: (7) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.811128ms)
Apr 22 22:32:47.669: INFO: (7) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 4.818527ms)
Apr 22 22:32:47.669: INFO: (7) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 4.963577ms)
Apr 22 22:32:47.669: INFO: (7) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 5.234743ms)
Apr 22 22:32:47.670: INFO: (7) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 5.562745ms)
Apr 22 22:32:47.670: INFO: (7) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 6.326965ms)
Apr 22 22:32:47.670: INFO: (7) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 6.399375ms)
Apr 22 22:32:47.670: INFO: (7) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 6.461906ms)
Apr 22 22:32:47.671: INFO: (7) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 6.438107ms)
Apr 22 22:32:47.675: INFO: (8) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.060471ms)
Apr 22 22:32:47.675: INFO: (8) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.3861ms)
Apr 22 22:32:47.676: INFO: (8) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 5.243744ms)
Apr 22 22:32:47.676: INFO: (8) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 5.224697ms)
Apr 22 22:32:47.676: INFO: (8) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 5.35229ms)
Apr 22 22:32:47.676: INFO: (8) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 5.348485ms)
Apr 22 22:32:47.676: INFO: (8) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 5.387857ms)
Apr 22 22:32:47.676: INFO: (8) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 5.429048ms)
Apr 22 22:32:47.676: INFO: (8) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 5.480408ms)
Apr 22 22:32:47.676: INFO: (8) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 5.399098ms)
Apr 22 22:32:47.678: INFO: (8) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 7.259907ms)
Apr 22 22:32:47.678: INFO: (8) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 7.771458ms)
Apr 22 22:32:47.678: INFO: (8) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 7.769831ms)
Apr 22 22:32:47.678: INFO: (8) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 7.825097ms)
Apr 22 22:32:47.679: INFO: (8) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 7.91893ms)
Apr 22 22:32:47.679: INFO: (8) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 7.96964ms)
Apr 22 22:32:47.682: INFO: (9) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 3.757529ms)
Apr 22 22:32:47.683: INFO: (9) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 4.552957ms)
Apr 22 22:32:47.683: INFO: (9) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 4.583252ms)
Apr 22 22:32:47.683: INFO: (9) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.674095ms)
Apr 22 22:32:47.683: INFO: (9) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 4.602773ms)
Apr 22 22:32:47.683: INFO: (9) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 4.659113ms)
Apr 22 22:32:47.683: INFO: (9) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 4.701194ms)
Apr 22 22:32:47.683: INFO: (9) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.784781ms)
Apr 22 22:32:47.684: INFO: (9) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 4.818484ms)
Apr 22 22:32:47.684: INFO: (9) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 4.791687ms)
Apr 22 22:32:47.685: INFO: (9) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 6.035321ms)
Apr 22 22:32:47.685: INFO: (9) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 6.713271ms)
Apr 22 22:32:47.686: INFO: (9) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 6.881619ms)
Apr 22 22:32:47.686: INFO: (9) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 6.842863ms)
Apr 22 22:32:47.686: INFO: (9) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 6.910689ms)
Apr 22 22:32:47.686: INFO: (9) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 6.931981ms)
Apr 22 22:32:47.690: INFO: (10) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 4.507908ms)
Apr 22 22:32:47.690: INFO: (10) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 4.623211ms)
Apr 22 22:32:47.690: INFO: (10) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 4.597289ms)
Apr 22 22:32:47.690: INFO: (10) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 4.841473ms)
Apr 22 22:32:47.690: INFO: (10) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.771898ms)
Apr 22 22:32:47.691: INFO: (10) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.825532ms)
Apr 22 22:32:47.691: INFO: (10) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 5.03953ms)
Apr 22 22:32:47.691: INFO: (10) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 4.991889ms)
Apr 22 22:32:47.691: INFO: (10) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 5.101419ms)
Apr 22 22:32:47.691: INFO: (10) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 5.057038ms)
Apr 22 22:32:47.691: INFO: (10) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 5.059063ms)
Apr 22 22:32:47.692: INFO: (10) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 6.472089ms)
Apr 22 22:32:47.693: INFO: (10) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 7.315718ms)
Apr 22 22:32:47.693: INFO: (10) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 7.496872ms)
Apr 22 22:32:47.693: INFO: (10) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 7.439913ms)
Apr 22 22:32:47.693: INFO: (10) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 7.488331ms)
Apr 22 22:32:47.699: INFO: (11) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 5.355842ms)
Apr 22 22:32:47.699: INFO: (11) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 5.362331ms)
Apr 22 22:32:47.699: INFO: (11) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 5.44813ms)
Apr 22 22:32:47.699: INFO: (11) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 5.393004ms)
Apr 22 22:32:47.699: INFO: (11) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 5.44084ms)
Apr 22 22:32:47.699: INFO: (11) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 5.476778ms)
Apr 22 22:32:47.699: INFO: (11) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 5.561502ms)
Apr 22 22:32:47.699: INFO: (11) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 5.564382ms)
Apr 22 22:32:47.699: INFO: (11) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 5.545738ms)
Apr 22 22:32:47.699: INFO: (11) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 5.721327ms)
Apr 22 22:32:47.699: INFO: (11) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 5.941292ms)
Apr 22 22:32:47.700: INFO: (11) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 6.597597ms)
Apr 22 22:32:47.700: INFO: (11) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 6.995599ms)
Apr 22 22:32:47.700: INFO: (11) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 7.116109ms)
Apr 22 22:32:47.700: INFO: (11) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 7.152854ms)
Apr 22 22:32:47.700: INFO: (11) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 7.110502ms)
Apr 22 22:32:47.703: INFO: (12) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 2.330946ms)
Apr 22 22:32:47.705: INFO: (12) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 4.452935ms)
Apr 22 22:32:47.705: INFO: (12) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 4.650343ms)
Apr 22 22:32:47.705: INFO: (12) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 4.674004ms)
Apr 22 22:32:47.705: INFO: (12) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.696314ms)
Apr 22 22:32:47.705: INFO: (12) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 4.763089ms)
Apr 22 22:32:47.705: INFO: (12) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 4.903102ms)
Apr 22 22:32:47.706: INFO: (12) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 5.129531ms)
Apr 22 22:32:47.706: INFO: (12) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 5.235812ms)
Apr 22 22:32:47.706: INFO: (12) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 5.366335ms)
Apr 22 22:32:47.707: INFO: (12) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 6.263221ms)
Apr 22 22:32:47.708: INFO: (12) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 7.032717ms)
Apr 22 22:32:47.708: INFO: (12) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 7.051897ms)
Apr 22 22:32:47.708: INFO: (12) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 7.131592ms)
Apr 22 22:32:47.708: INFO: (12) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 7.131554ms)
Apr 22 22:32:47.708: INFO: (12) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 7.063589ms)
Apr 22 22:32:47.711: INFO: (13) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 3.617089ms)
Apr 22 22:32:47.712: INFO: (13) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 3.906377ms)
Apr 22 22:32:47.712: INFO: (13) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 4.038202ms)
Apr 22 22:32:47.712: INFO: (13) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 4.0891ms)
Apr 22 22:32:47.712: INFO: (13) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 4.08587ms)
Apr 22 22:32:47.712: INFO: (13) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 4.167852ms)
Apr 22 22:32:47.712: INFO: (13) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 4.257856ms)
Apr 22 22:32:47.713: INFO: (13) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 5.233228ms)
Apr 22 22:32:47.713: INFO: (13) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 5.321487ms)
Apr 22 22:32:47.713: INFO: (13) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 5.404685ms)
Apr 22 22:32:47.713: INFO: (13) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 5.448986ms)
Apr 22 22:32:47.714: INFO: (13) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 6.2407ms)
Apr 22 22:32:47.715: INFO: (13) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 7.032034ms)
Apr 22 22:32:47.715: INFO: (13) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 7.013615ms)
Apr 22 22:32:47.715: INFO: (13) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 7.159312ms)
Apr 22 22:32:47.715: INFO: (13) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 7.239894ms)
Apr 22 22:32:47.717: INFO: (14) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 2.384369ms)
Apr 22 22:32:47.719: INFO: (14) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 3.467726ms)
Apr 22 22:32:47.719: INFO: (14) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 3.606261ms)
Apr 22 22:32:47.719: INFO: (14) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 3.56082ms)
Apr 22 22:32:47.720: INFO: (14) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 4.603042ms)
Apr 22 22:32:47.720: INFO: (14) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 4.745267ms)
Apr 22 22:32:47.720: INFO: (14) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 4.833108ms)
Apr 22 22:32:47.720: INFO: (14) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.905872ms)
Apr 22 22:32:47.720: INFO: (14) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 4.951093ms)
Apr 22 22:32:47.720: INFO: (14) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 5.056688ms)
Apr 22 22:32:47.721: INFO: (14) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 5.927786ms)
Apr 22 22:32:47.722: INFO: (14) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 6.936163ms)
Apr 22 22:32:47.723: INFO: (14) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 7.403573ms)
Apr 22 22:32:47.723: INFO: (14) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 7.440543ms)
Apr 22 22:32:47.723: INFO: (14) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 7.541925ms)
Apr 22 22:32:47.723: INFO: (14) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 7.479794ms)
Apr 22 22:32:47.726: INFO: (15) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 3.485543ms)
Apr 22 22:32:47.727: INFO: (15) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.373468ms)
Apr 22 22:32:47.727: INFO: (15) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 4.597146ms)
Apr 22 22:32:47.727: INFO: (15) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 4.758708ms)
Apr 22 22:32:47.727: INFO: (15) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 4.708257ms)
Apr 22 22:32:47.727: INFO: (15) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 4.736727ms)
Apr 22 22:32:47.728: INFO: (15) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 4.913988ms)
Apr 22 22:32:47.728: INFO: (15) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 4.955198ms)
Apr 22 22:32:47.728: INFO: (15) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 4.939671ms)
Apr 22 22:32:47.728: INFO: (15) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.948378ms)
Apr 22 22:32:47.729: INFO: (15) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 6.593512ms)
Apr 22 22:32:47.729: INFO: (15) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 6.678628ms)
Apr 22 22:32:47.730: INFO: (15) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 7.395203ms)
Apr 22 22:32:47.730: INFO: (15) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 7.691578ms)
Apr 22 22:32:47.730: INFO: (15) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 7.63111ms)
Apr 22 22:32:47.730: INFO: (15) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 7.719321ms)
Apr 22 22:32:47.733: INFO: (16) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 2.234511ms)
Apr 22 22:32:47.734: INFO: (16) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 4.001137ms)
Apr 22 22:32:47.735: INFO: (16) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.264667ms)
Apr 22 22:32:47.735: INFO: (16) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 4.959887ms)
Apr 22 22:32:47.736: INFO: (16) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 5.263583ms)
Apr 22 22:32:47.736: INFO: (16) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 5.690199ms)
Apr 22 22:32:47.736: INFO: (16) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 5.702436ms)
Apr 22 22:32:47.736: INFO: (16) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 5.859648ms)
Apr 22 22:32:47.736: INFO: (16) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 5.930203ms)
Apr 22 22:32:47.736: INFO: (16) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 5.99326ms)
Apr 22 22:32:47.736: INFO: (16) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 5.90749ms)
Apr 22 22:32:47.736: INFO: (16) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 6.095597ms)
Apr 22 22:32:47.737: INFO: (16) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 6.617734ms)
Apr 22 22:32:47.737: INFO: (16) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 6.672629ms)
Apr 22 22:32:47.737: INFO: (16) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 6.620574ms)
Apr 22 22:32:47.737: INFO: (16) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 6.616664ms)
Apr 22 22:32:47.740: INFO: (17) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 2.354576ms)
Apr 22 22:32:47.741: INFO: (17) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 3.411718ms)
Apr 22 22:32:47.741: INFO: (17) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 3.218363ms)
Apr 22 22:32:47.741: INFO: (17) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 3.950599ms)
Apr 22 22:32:47.741: INFO: (17) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 3.924505ms)
Apr 22 22:32:47.741: INFO: (17) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 4.121764ms)
Apr 22 22:32:47.742: INFO: (17) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 4.412741ms)
Apr 22 22:32:47.742: INFO: (17) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.478689ms)
Apr 22 22:32:47.742: INFO: (17) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 4.931131ms)
Apr 22 22:32:47.742: INFO: (17) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 5.005675ms)
Apr 22 22:32:47.743: INFO: (17) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 5.780828ms)
Apr 22 22:32:47.744: INFO: (17) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 6.378ms)
Apr 22 22:32:47.744: INFO: (17) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 6.719674ms)
Apr 22 22:32:47.744: INFO: (17) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 7.018316ms)
Apr 22 22:32:47.744: INFO: (17) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 6.948374ms)
Apr 22 22:32:47.744: INFO: (17) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 7.117769ms)
Apr 22 22:32:47.749: INFO: (18) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 4.950412ms)
Apr 22 22:32:47.749: INFO: (18) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.993614ms)
Apr 22 22:32:47.750: INFO: (18) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 5.076409ms)
Apr 22 22:32:47.750: INFO: (18) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 5.060529ms)
Apr 22 22:32:47.750: INFO: (18) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 5.033592ms)
Apr 22 22:32:47.750: INFO: (18) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 5.488046ms)
Apr 22 22:32:47.750: INFO: (18) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 5.424808ms)
Apr 22 22:32:47.750: INFO: (18) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 5.465879ms)
Apr 22 22:32:47.750: INFO: (18) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 5.501137ms)
Apr 22 22:32:47.750: INFO: (18) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 5.595224ms)
Apr 22 22:32:47.750: INFO: (18) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 5.542899ms)
Apr 22 22:32:47.751: INFO: (18) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 7.004253ms)
Apr 22 22:32:47.751: INFO: (18) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 6.913301ms)
Apr 22 22:32:47.751: INFO: (18) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 6.929893ms)
Apr 22 22:32:47.751: INFO: (18) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 6.929527ms)
Apr 22 22:32:47.751: INFO: (18) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 6.999193ms)
Apr 22 22:32:47.757: INFO: (19) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 5.07109ms)
Apr 22 22:32:47.757: INFO: (19) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:462/proxy/: tls qux (200; 4.932009ms)
Apr 22 22:32:47.757: INFO: (19) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 5.060674ms)
Apr 22 22:32:47.757: INFO: (19) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">... (200; 5.000242ms)
Apr 22 22:32:47.757: INFO: (19) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:160/proxy/: foo (200; 4.806602ms)
Apr 22 22:32:47.757: INFO: (19) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9:1080/proxy/rewriteme">test<... (200; 5.20682ms)
Apr 22 22:32:47.757: INFO: (19) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname1/proxy/: foo (200; 5.529216ms)
Apr 22 22:32:47.757: INFO: (19) /api/v1/namespaces/proxy-8028/pods/http:proxy-service-fsk5g-f6rc9:162/proxy/: bar (200; 5.335394ms)
Apr 22 22:32:47.757: INFO: (19) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:460/proxy/: tls baz (200; 5.090814ms)
Apr 22 22:32:47.757: INFO: (19) /api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/https:proxy-service-fsk5g-f6rc9:443/proxy/tlsrewritem... (200; 5.419837ms)
Apr 22 22:32:47.757: INFO: (19) /api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/: <a href="/api/v1/namespaces/proxy-8028/pods/proxy-service-fsk5g-f6rc9/proxy/rewriteme">test</a> (200; 5.292039ms)
Apr 22 22:32:47.758: INFO: (19) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname1/proxy/: tls baz (200; 6.135969ms)
Apr 22 22:32:47.759: INFO: (19) /api/v1/namespaces/proxy-8028/services/proxy-service-fsk5g:portname2/proxy/: bar (200; 6.870029ms)
Apr 22 22:32:47.759: INFO: (19) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname2/proxy/: bar (200; 7.124162ms)
Apr 22 22:32:47.759: INFO: (19) /api/v1/namespaces/proxy-8028/services/https:proxy-service-fsk5g:tlsportname2/proxy/: tls qux (200; 6.864727ms)
Apr 22 22:32:47.759: INFO: (19) /api/v1/namespaces/proxy-8028/services/http:proxy-service-fsk5g:portname1/proxy/: foo (200; 7.08648ms)
STEP: deleting ReplicationController proxy-service-fsk5g in namespace proxy-8028, will wait for the garbage collector to delete the pods
Apr 22 22:32:47.822: INFO: Deleting ReplicationController proxy-service-fsk5g took: 10.800687ms
Apr 22 22:32:47.922: INFO: Terminating ReplicationController proxy-service-fsk5g pods took: 100.110744ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:32:58.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8028" for this suite.

• [SLOW TEST:17.145 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":310,"skipped":5346,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 22 22:32:58.638: INFO: >>> kubeConfig: /tmp/kubeconfig-002341601
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-c343abd2-ce07-428c-8317-236d2463f03a
STEP: Creating a pod to test consume configMaps
Apr 22 22:32:58.671: INFO: Waiting up to 5m0s for pod "pod-configmaps-49a3c5e1-c705-41d5-b136-493c3b767149" in namespace "configmap-6069" to be "Succeeded or Failed"
Apr 22 22:32:58.674: INFO: Pod "pod-configmaps-49a3c5e1-c705-41d5-b136-493c3b767149": Phase="Pending", Reason="", readiness=false. Elapsed: 2.701073ms
Apr 22 22:33:00.680: INFO: Pod "pod-configmaps-49a3c5e1-c705-41d5-b136-493c3b767149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008993851s
STEP: Saw pod success
Apr 22 22:33:00.680: INFO: Pod "pod-configmaps-49a3c5e1-c705-41d5-b136-493c3b767149" satisfied condition "Succeeded or Failed"
Apr 22 22:33:00.682: INFO: Trying to get logs from node ip-10-0-131-122.us-west-2.compute.internal pod pod-configmaps-49a3c5e1-c705-41d5-b136-493c3b767149 container agnhost-container: <nil>
STEP: delete the pod
Apr 22 22:33:00.696: INFO: Waiting for pod pod-configmaps-49a3c5e1-c705-41d5-b136-493c3b767149 to disappear
Apr 22 22:33:00.698: INFO: Pod pod-configmaps-49a3c5e1-c705-41d5-b136-493c3b767149 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 22 22:33:00.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6069" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":311,"skipped":5353,"failed":0}
SSSApr 22 22:33:00.705: INFO: Running AfterSuite actions on all nodes
Apr 22 22:33:00.705: INFO: Running AfterSuite actions on node 1
Apr 22 22:33:00.705: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 6806.250 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
PASS

Ginkgo ran 1 suite in 1h53m27.361985208s
Test Suite Passed
